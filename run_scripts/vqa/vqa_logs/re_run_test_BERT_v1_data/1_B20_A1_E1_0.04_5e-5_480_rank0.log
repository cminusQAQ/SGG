2023-01-04 21:02:22 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-04 21:02:22 - utils.py[line:261] - INFO: Start init
2023-01-04 21:02:22 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-04 21:02:22 - utils.py[line:261] - INFO: Start init
2023-01-04 21:02:22 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-04 21:02:22 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-04 21:02:22 - utils.py[line:274] - INFO: initialized host node4 as rank 0
2023-01-04 21:02:22 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.single-machine distributed training is initialized.

2023-01-04 21:02:32 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/re_run_test_BERT_v1_data', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='none', distill_alpha=0.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/re_run_test_BERT_v1_data', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-01-04 21:02:32 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-01-04 21:02:32 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-01-04 21:02:36 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-01-04 21:02:36 - train.py[line:118] - INFO: task: VqaGenTask
2023-01-04 21:02:36 - train.py[line:119] - INFO: model: OFAModel
2023-01-04 21:02:36 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-01-04 21:02:36 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-01-04 21:02:36 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Traceback (most recent call last):
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
  File "../../train.py", line 632, in <module>
    cli_main()    cli_main()

  File "../../train.py", line 625, in cli_main
  File "../../train.py", line 625, in cli_main
    distributed_utils.call_main(cfg, main)    distributed_utils.call_main(cfg, main)

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)    
  File "../../train.py", line 142, in main
main(cfg, **kwargs)
    task.load_dataset(valid_sub_split, combine=False, epoch=1)  File "../../train.py", line 142, in main

  File "/data/private/yutianyu/OFA/tasks/mm_tasks/vqa_gen.py", line 147, in load_dataset
    task.load_dataset(valid_sub_split, combine=False, epoch=1)    label_proxy=self.cfg.label_proxy
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/vqa_gen.py", line 147, in load_dataset

  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 227, in __init__
        self.KB = json.load(open('../../../datasets/OFA_data/sgg/CCKB.json'))label_proxy=self.cfg.label_proxy

  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 227, in __init__
FileNotFoundError:     self.KB = json.load(open('../../../datasets/OFA_data/sgg/CCKB.json'))[Errno 2] No such file or directory: '../../../datasets/OFA_data/sgg/CCKB.json'

FileNotFoundError: [Errno 2] No such file or directory: '../../../datasets/OFA_data/sgg/CCKB.json'
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/re_run_test_BERT_v1_data', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'none', '--distill-alpha=0.0', '--batch-size=20', '--batch-size-valid=15', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=1', '--warmup-ratio=0.04', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=2000', '--validate-interval-updates=2000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1639956
Killing subprocess 1639957
2023-01-04 21:04:09 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-04 21:04:09 - utils.py[line:261] - INFO: Start init
2023-01-04 21:04:09 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-04 21:04:09 - utils.py[line:261] - INFO: Start init
2023-01-04 21:04:10 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-04 21:04:10 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-04 21:04:10 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.2023-01-04 21:04:10 - utils.py[line:274] - INFO: initialized host node4 as rank 1

single-machine distributed training is initialized.
2023-01-04 21:04:16 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/re_run_test_BERT_v1_data', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='none', distill_alpha=0.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/re_run_test_BERT_v1_data', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-01-04 21:04:16 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-01-04 21:04:16 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-04 21:04:21 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-01-04 21:04:21 - train.py[line:118] - INFO: task: VqaGenTask
2023-01-04 21:04:21 - train.py[line:119] - INFO: model: OFAModel
2023-01-04 21:04:21 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-01-04 21:04:21 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-01-04 21:04:21 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-04 21:04:21 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-01-04 21:04:21 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-01-04 21:04:22 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
2023-01-04 21:04:22 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
    cli_main()2023-01-04 21:04:22 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          

  File "../../train.py", line 625, in cli_main
2023-01-04 21:04:22 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
    distributed_utils.call_main(cfg, main)Traceback (most recent call last):
  File "../../train.py", line 632, in <module>

    cli_main()  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main

  File "../../train.py", line 625, in cli_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)    distributed_utils.call_main(cfg, main)

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
        main(cfg, **kwargs)distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)

  File "../../train.py", line 156, in main
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    trainer = Trainer(cfg, task, model, criterion, quantizer)    main(cfg, **kwargs)
  File "/data/private/yutianyu/OFA/trainer.py", line 172, in __init__

  File "../../train.py", line 156, in main
    self.distill = cfg.common.distill    trainer = Trainer(cfg, task, model, criterion, quantizer)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 305, in __getattr__

  File "/data/private/yutianyu/OFA/trainer.py", line 172, in __init__
    self.distill = cfg.common.distill
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 305, in __getattr__
    self._format_and_raise(key=key, value=None, cause=e)    self._format_and_raise(key=key, value=None, cause=e)

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/base.py", line 101, in _format_and_raise
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/base.py", line 101, in _format_and_raise
    type_override=type_override,    type_override=type_override,
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 629, in format_and_raise

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 629, in format_and_raise
    _raise(ex, cause)    _raise(ex, cause)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 610, in _raise

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 610, in _raise
    raise ex  # set end OC_CAUSE=1 for full backtrace    raise ex  # set end OC_CAUSE=1 for full backtrace

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 303, in __getattr__
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 303, in __getattr__
    return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)    return self._get_impl(key=key, default_value=DEFAULT_VALUE_MARKER)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 361, in _get_impl

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 361, in _get_impl
    node = self._get_node(key=key)    
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 383, in _get_node
node = self._get_node(key=key)    self._validate_get(key)

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 136, in _validate_get
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 383, in _get_node
    key=key, value=value, cause=ConfigAttributeError(msg)    
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/base.py", line 101, in _format_and_raise
self._validate_get(key)    type_override=type_override,

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 694, in format_and_raise
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/dictconfig.py", line 136, in _validate_get
    _raise(ex, cause)    key=key, value=value, cause=ConfigAttributeError(msg)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 610, in _raise

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/base.py", line 101, in _format_and_raise
    raise ex  # set end OC_CAUSE=1 for full backtrace    
omegaconf.errorstype_override=type_override,
.ConfigAttributeError  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 694, in format_and_raise
: Key 'distill' is not in struct
	full_key: common.distill
	reference_type=Any
	object_type=dict    _raise(ex, cause)

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/omegaconf/_utils.py", line 610, in _raise
    raise ex  # set end OC_CAUSE=1 for full backtrace
omegaconf.errors.ConfigAttributeError: Key 'distill' is not in struct
	full_key: common.distill
	reference_type=Any
	object_type=dict
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/re_run_test_BERT_v1_data', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'none', '--distill-alpha=0.0', '--batch-size=20', '--batch-size-valid=15', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=1', '--warmup-ratio=0.04', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=2000', '--validate-interval-updates=2000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1640931
Killing subprocess 1640932
2023-01-04 21:05:55 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-04 21:05:55 - utils.py[line:261] - INFO: Start init
2023-01-04 21:05:55 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-04 21:05:55 - utils.py[line:261] - INFO: Start init
2023-01-04 21:05:55 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-04 21:05:55 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-04 21:05:55 - utils.py[line:274] - INFO: initialized host node4 as rank 0
2023-01-04 21:05:55 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.single-machine distributed training is initialized.

2023-01-04 21:06:01 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/re_run_test_BERT_v1_data', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='none', distill_alpha=0.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/re_run_test_BERT_v1_data', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-01-04 21:06:01 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-01-04 21:06:01 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-01-04 21:06:05 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-01-04 21:06:05 - train.py[line:118] - INFO: task: VqaGenTask
2023-01-04 21:06:05 - train.py[line:119] - INFO: model: OFAModel
2023-01-04 21:06:05 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-01-04 21:06:05 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-01-04 21:06:05 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-04 21:06:06 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-01-04 21:06:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-01-04 21:06:06 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-04 21:06:06 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-04 21:06:06 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-04 21:06:06 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-04 21:06:06 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-01-04 21:06:06 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-01-04 21:06:06 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-01-04 21:06:16 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-04 21:06:16 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-04 21:06:16 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-01-04 21:06:17 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-01-04 21:06:17 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-01-04 21:06:17 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv slice_id 0 row count 2045757 total row count 4091514
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_BERT_v1train_NA1_E0.tsv slice_id 1 row count 2045757 total row count 4091514
2023-01-04 21:06:26 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 102288, warmup steps 4091, warmup_factor 0.0002444390124663896
Total steps 102288, warmup steps 4091, warmup_factor 0.0002444390124663896
2023-01-04 21:06:27 - trainer.py[line:758] - INFO: begin training epoch 1
2023-01-04 21:06:27 - train.py[line:312] - INFO: Start iterating over samples
2023-01-04 21:06:48 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 102288 loss=1.294, loss_v1=0, loss_v2=0, nll_loss=1.152, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=2.22, wps=81.9, ups=0.74, wpb=111, bsz=40, num_updates=10, lr=1.2222e-07, gnorm=12.546, clip=100, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=41
2023-01-04 21:07:00 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 102288 loss=1.332, loss_v1=0, loss_v2=0, nll_loss=1.203, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=2.3, wps=93.6, ups=0.85, wpb=110.6, bsz=40, num_updates=20, lr=2.44439e-07, gnorm=12.981, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54
2023-01-04 21:07:12 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 102288 loss=1.329, loss_v1=0, loss_v2=0, nll_loss=1.199, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=2.3, wps=97, ups=0.87, wpb=111.4, bsz=40, num_updates=30, lr=3.66659e-07, gnorm=13.417, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=65
2023-01-04 21:07:23 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 102288 loss=1.438, loss_v1=0, loss_v2=0, nll_loss=1.312, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=2.48, wps=98.7, ups=0.91, wpb=108.4, bsz=40, num_updates=40, lr=4.88878e-07, gnorm=15.201, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=77
2023-01-04 21:07:34 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 102288 loss=1.278, loss_v1=0, loss_v2=0, nll_loss=1.156, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=101, ups=0.92, wpb=109.9, bsz=40, num_updates=50, lr=6.11098e-07, gnorm=12.022, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=88
2023-01-04 21:07:46 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 102288 loss=1.224, loss_v1=0, loss_v2=0, nll_loss=1.105, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=2.15, wps=97.5, ups=0.87, wpb=111.8, bsz=40, num_updates=60, lr=7.33317e-07, gnorm=11.305, clip=100, loss_scale=128, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=99
2023-01-04 21:07:58 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 102288 loss=1.228, loss_v1=0, loss_v2=0, nll_loss=1.121, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=97.1, ups=0.89, wpb=109, bsz=40, num_updates=70, lr=8.55537e-07, gnorm=10.129, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=111
2023-01-04 21:08:09 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 102288 loss=1.116, loss_v1=0, loss_v2=0, nll_loss=1.014, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=102.9, ups=0.92, wpb=111.6, bsz=40, num_updates=80, lr=9.77756e-07, gnorm=9.069, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=122
2023-01-04 21:08:20 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 102288 loss=1.156, loss_v1=0, loss_v2=0, nll_loss=1.067, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=2.1, wps=95, ups=0.87, wpb=109.5, bsz=40, num_updates=90, lr=1.09998e-06, gnorm=8.144, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=134
2023-01-04 21:08:32 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 102288 loss=1.012, loss_v1=0, loss_v2=0, nll_loss=0.921, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=97.4, ups=0.89, wpb=109.5, bsz=40, num_updates=100, lr=1.2222e-06, gnorm=7.117, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=145
2023-01-04 21:08:43 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 102288 loss=1.003, loss_v1=0, loss_v2=0, nll_loss=0.915, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=96.2, ups=0.88, wpb=109.6, bsz=40, num_updates=110, lr=1.34441e-06, gnorm=6.387, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=157
2023-01-04 21:08:55 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 102288 loss=0.882, loss_v1=0, loss_v2=0, nll_loss=0.79, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=120, lr=1.46663e-06, gnorm=5.571, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=168
2023-01-04 21:09:07 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 102288 loss=0.926, loss_v1=0, loss_v2=0, nll_loss=0.839, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=95.4, ups=0.87, wpb=109.7, bsz=40, num_updates=130, lr=1.58885e-06, gnorm=5.161, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=180
2023-01-04 21:09:18 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 102288 loss=0.924, loss_v1=0, loss_v2=0, nll_loss=0.84, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=98.2, ups=0.9, wpb=108.8, bsz=40, num_updates=140, lr=1.71107e-06, gnorm=4.641, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=191
2023-01-04 21:09:29 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 102288 loss=0.909, loss_v1=0, loss_v2=0, nll_loss=0.833, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=99.6, ups=0.9, wpb=110.7, bsz=40, num_updates=150, lr=1.83329e-06, gnorm=4.437, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=203
2023-01-04 21:09:41 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 102288 loss=0.903, loss_v1=0, loss_v2=0, nll_loss=0.828, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=160, lr=1.95551e-06, gnorm=4.042, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=214
2023-01-04 21:09:52 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 102288 loss=0.847, loss_v1=0, loss_v2=0, nll_loss=0.771, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=95.9, ups=0.87, wpb=110.5, bsz=40, num_updates=170, lr=2.07773e-06, gnorm=3.798, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=226
2023-01-04 21:10:04 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 102288 loss=0.866, loss_v1=0, loss_v2=0, nll_loss=0.793, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=98.7, ups=0.9, wpb=109.5, bsz=40, num_updates=180, lr=2.19995e-06, gnorm=3.703, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=237
2023-01-04 21:10:15 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 102288 loss=0.767, loss_v1=0, loss_v2=0, nll_loss=0.684, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=98.6, ups=0.89, wpb=110.8, bsz=40, num_updates=190, lr=2.32217e-06, gnorm=3.192, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=248
2023-01-04 21:10:27 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 102288 loss=0.809, loss_v1=0, loss_v2=0, nll_loss=0.727, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=200, lr=2.44439e-06, gnorm=3.203, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=260
2023-01-04 21:10:38 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 102288 loss=0.812, loss_v1=0, loss_v2=0, nll_loss=0.738, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=210, lr=2.56661e-06, gnorm=3.144, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=271
2023-01-04 21:10:49 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 102288 loss=0.819, loss_v1=0, loss_v2=0, nll_loss=0.747, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=98.4, ups=0.89, wpb=110.8, bsz=40, num_updates=220, lr=2.68883e-06, gnorm=3.074, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=283
2023-01-04 21:11:01 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 102288 loss=0.786, loss_v1=0, loss_v2=0, nll_loss=0.705, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=96.3, ups=0.88, wpb=109.1, bsz=40, num_updates=230, lr=2.81105e-06, gnorm=2.869, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=294
2023-01-04 21:11:13 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 102288 loss=0.799, loss_v1=0, loss_v2=0, nll_loss=0.724, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=240, lr=2.93327e-06, gnorm=2.62, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=306
2023-01-04 21:11:24 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 102288 loss=0.764, loss_v1=0, loss_v2=0, nll_loss=0.685, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=100.2, ups=0.9, wpb=110.7, bsz=40, num_updates=250, lr=3.05549e-06, gnorm=2.775, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=317
2023-01-04 21:11:35 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 102288 loss=0.749, loss_v1=0, loss_v2=0, nll_loss=0.667, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=98, ups=0.88, wpb=111.1, bsz=40, num_updates=260, lr=3.17771e-06, gnorm=2.689, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=329
2023-01-04 21:11:47 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 102288 loss=0.758, loss_v1=0, loss_v2=0, nll_loss=0.672, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=96.9, ups=0.88, wpb=110.2, bsz=40, num_updates=270, lr=3.29993e-06, gnorm=2.622, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=340
2023-01-04 21:11:58 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 102288 loss=0.796, loss_v1=0, loss_v2=0, nll_loss=0.721, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=97.4, ups=0.89, wpb=109.3, bsz=40, num_updates=280, lr=3.42215e-06, gnorm=2.978, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=352
2023-01-04 21:12:10 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 102288 loss=0.775, loss_v1=0, loss_v2=0, nll_loss=0.694, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=290, lr=3.54437e-06, gnorm=2.601, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=363
2023-01-04 21:12:21 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 102288 loss=0.779, loss_v1=0, loss_v2=0, nll_loss=0.702, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=101.4, ups=0.91, wpb=110.8, bsz=40, num_updates=300, lr=3.66659e-06, gnorm=2.663, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=374
2023-01-04 21:12:32 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 102288 loss=0.737, loss_v1=0, loss_v2=0, nll_loss=0.652, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=310, lr=3.7888e-06, gnorm=2.432, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=386
2023-01-04 21:12:44 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 102288 loss=0.755, loss_v1=0, loss_v2=0, nll_loss=0.672, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=99.4, ups=0.9, wpb=110.3, bsz=40, num_updates=320, lr=3.91102e-06, gnorm=2.327, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=397
2023-01-04 21:12:55 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 102288 loss=0.759, loss_v1=0, loss_v2=0, nll_loss=0.677, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=100.3, ups=0.92, wpb=109.3, bsz=40, num_updates=330, lr=4.03324e-06, gnorm=2.277, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=408
2023-01-04 21:13:06 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 102288 loss=0.734, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.2, ups=0.89, wpb=110.5, bsz=40, num_updates=340, lr=4.15546e-06, gnorm=2.322, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=420
2023-01-04 21:13:18 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 102288 loss=0.767, loss_v1=0, loss_v2=0, nll_loss=0.689, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=99.3, ups=0.9, wpb=110.3, bsz=40, num_updates=350, lr=4.27768e-06, gnorm=2.231, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=431
2023-01-04 21:13:29 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 102288 loss=0.756, loss_v1=0, loss_v2=0, nll_loss=0.673, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=360, lr=4.3999e-06, gnorm=2.198, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=442
2023-01-04 21:13:40 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 102288 loss=0.721, loss_v1=0, loss_v2=0, nll_loss=0.633, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=370, lr=4.52212e-06, gnorm=2.191, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=454
2023-01-04 21:13:52 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 102288 loss=0.73, loss_v1=0, loss_v2=0, nll_loss=0.644, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=95.9, ups=0.87, wpb=110.3, bsz=40, num_updates=380, lr=4.64434e-06, gnorm=2.103, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=465
2023-01-04 21:14:04 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 102288 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=98.3, ups=0.88, wpb=111.4, bsz=40, num_updates=390, lr=4.76656e-06, gnorm=2.095, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=477
2023-01-04 21:14:15 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 102288 loss=0.756, loss_v1=0, loss_v2=0, nll_loss=0.673, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=99.3, ups=0.91, wpb=109.6, bsz=40, num_updates=400, lr=4.88878e-06, gnorm=2.235, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=488
2023-01-04 21:14:26 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 102288 loss=0.754, loss_v1=0, loss_v2=0, nll_loss=0.671, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=98.5, ups=0.9, wpb=110, bsz=40, num_updates=410, lr=5.011e-06, gnorm=2.288, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=500
2023-01-04 21:14:38 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 102288 loss=0.743, loss_v1=0, loss_v2=0, nll_loss=0.658, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=420, lr=5.13322e-06, gnorm=2.261, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=511
2023-01-04 21:14:49 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 102288 loss=0.717, loss_v1=0, loss_v2=0, nll_loss=0.626, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=101.7, ups=0.91, wpb=111.3, bsz=40, num_updates=430, lr=5.25544e-06, gnorm=2.261, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=522
2023-01-04 21:15:00 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 102288 loss=0.71, loss_v1=0, loss_v2=0, nll_loss=0.624, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=98.9, ups=0.89, wpb=110.5, bsz=40, num_updates=440, lr=5.37766e-06, gnorm=2.174, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=533
2023-01-04 21:15:12 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 102288 loss=0.709, loss_v1=0, loss_v2=0, nll_loss=0.613, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=450, lr=5.49988e-06, gnorm=2.19, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=545
2023-01-04 21:15:23 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 102288 loss=0.715, loss_v1=0, loss_v2=0, nll_loss=0.625, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=100.6, ups=0.92, wpb=109.9, bsz=40, num_updates=460, lr=5.6221e-06, gnorm=2.107, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=556
2023-01-04 21:15:34 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 102288 loss=0.747, loss_v1=0, loss_v2=0, nll_loss=0.66, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=97.3, ups=0.89, wpb=108.9, bsz=40, num_updates=470, lr=5.74432e-06, gnorm=2.274, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=567
2023-01-04 21:15:45 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 102288 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.6, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=480, lr=5.86654e-06, gnorm=2.074, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=579
2023-01-04 21:15:57 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 102288 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.581, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=98.8, ups=0.89, wpb=110.4, bsz=40, num_updates=490, lr=5.98876e-06, gnorm=1.862, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=590
2023-01-04 21:16:07 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 102288 loss=0.694, loss_v1=0, loss_v2=0, nll_loss=0.601, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=106.4, ups=0.97, wpb=110.2, bsz=40, num_updates=500, lr=6.11098e-06, gnorm=2.031, clip=100, loss_scale=128, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=601
2023-01-04 21:16:19 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 102288 loss=0.68, loss_v1=0, loss_v2=0, nll_loss=0.581, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=99.7, ups=0.89, wpb=111.4, bsz=40, num_updates=510, lr=6.23319e-06, gnorm=1.908, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=612
2023-01-04 21:16:30 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 102288 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.591, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=520, lr=6.35541e-06, gnorm=2.089, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=623
2023-01-04 21:16:41 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 102288 loss=0.697, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=99.1, ups=0.9, wpb=109.5, bsz=40, num_updates=530, lr=6.47763e-06, gnorm=2.326, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=635
2023-01-04 21:16:53 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 102288 loss=0.714, loss_v1=0, loss_v2=0, nll_loss=0.623, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=540, lr=6.59985e-06, gnorm=2.226, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=646
2023-01-04 21:17:04 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 102288 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.579, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=550, lr=6.72207e-06, gnorm=2.035, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=657
2023-01-04 21:17:15 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 102288 loss=0.689, loss_v1=0, loss_v2=0, nll_loss=0.59, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=98.2, ups=0.89, wpb=110.2, bsz=40, num_updates=560, lr=6.84429e-06, gnorm=1.976, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=669
2023-01-04 21:17:27 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 102288 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.567, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=99, ups=0.89, wpb=110.7, bsz=40, num_updates=570, lr=6.96651e-06, gnorm=2.064, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=680
2023-01-04 21:17:38 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 102288 loss=0.71, loss_v1=0, loss_v2=0, nll_loss=0.614, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=580, lr=7.08873e-06, gnorm=2.141, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=691
2023-01-04 21:17:49 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 102288 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=97.9, ups=0.88, wpb=111, bsz=40, num_updates=590, lr=7.21095e-06, gnorm=2.1, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=703
2023-01-04 21:18:01 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 102288 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.576, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=600, lr=7.33317e-06, gnorm=1.991, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=714
2023-01-04 21:18:12 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 102288 loss=0.659, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=100.7, ups=0.92, wpb=109.9, bsz=40, num_updates=610, lr=7.45539e-06, gnorm=1.987, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=725
2023-01-04 21:18:23 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 102288 loss=0.693, loss_v1=0, loss_v2=0, nll_loss=0.586, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=97.4, ups=0.89, wpb=109.3, bsz=40, num_updates=620, lr=7.57761e-06, gnorm=2.166, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=737
2023-01-04 21:18:34 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 102288 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=103.5, ups=0.93, wpb=111.4, bsz=40, num_updates=630, lr=7.69983e-06, gnorm=1.937, clip=100, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=748
2023-01-04 21:18:45 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 102288 loss=0.689, loss_v1=0, loss_v2=0, nll_loss=0.588, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=102.7, ups=0.93, wpb=110.6, bsz=40, num_updates=640, lr=7.82205e-06, gnorm=2.192, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=759
2023-01-04 21:18:57 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 102288 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.571, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=98.2, ups=0.89, wpb=110.8, bsz=40, num_updates=650, lr=7.94427e-06, gnorm=2.146, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=770
2023-01-04 21:19:08 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 102288 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=660, lr=8.06649e-06, gnorm=1.992, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=781
2023-01-04 21:19:19 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 102288 loss=0.681, loss_v1=0, loss_v2=0, nll_loss=0.581, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=99.8, ups=0.91, wpb=109.2, bsz=40, num_updates=670, lr=8.18871e-06, gnorm=2.032, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=792
2023-01-04 21:19:30 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 102288 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=101.8, ups=0.92, wpb=111.2, bsz=40, num_updates=680, lr=8.31093e-06, gnorm=2.006, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=804
2023-01-04 21:19:42 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 102288 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=690, lr=8.43315e-06, gnorm=2.264, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=815
2023-01-04 21:19:53 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 102288 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.538, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=700, lr=8.55537e-06, gnorm=2.524, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=826
2023-01-04 21:20:04 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 102288 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.571, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=710, lr=8.67758e-06, gnorm=2.331, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=838
2023-01-04 21:20:16 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 102288 loss=0.683, loss_v1=0, loss_v2=0, nll_loss=0.585, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=720, lr=8.7998e-06, gnorm=2.487, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=849
2023-01-04 21:20:28 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 102288 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=99.7, ups=0.91, wpb=110.1, bsz=40, num_updates=730, lr=8.92202e-06, gnorm=2.327, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=861
2023-01-04 21:20:39 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 102288 loss=0.693, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=95, ups=0.87, wpb=109.1, bsz=40, num_updates=740, lr=9.04424e-06, gnorm=2.197, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=873
2023-01-04 21:20:51 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 102288 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=97, ups=0.88, wpb=110.2, bsz=40, num_updates=750, lr=9.16646e-06, gnorm=2.131, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=884
2023-01-04 21:21:02 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 102288 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=104.9, ups=0.94, wpb=111.4, bsz=40, num_updates=760, lr=9.28868e-06, gnorm=2.125, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=895
2023-01-04 21:21:13 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 102288 loss=0.697, loss_v1=0, loss_v2=0, nll_loss=0.6, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=98.3, ups=0.91, wpb=108.5, bsz=40, num_updates=770, lr=9.4109e-06, gnorm=2.182, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=907
2023-01-04 21:21:25 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 102288 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=98.2, ups=0.9, wpb=108.6, bsz=40, num_updates=780, lr=9.53312e-06, gnorm=2.081, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=918
2023-01-04 21:21:37 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 102288 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.518, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=790, lr=9.65534e-06, gnorm=2.204, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=930
2023-01-04 21:21:48 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 102288 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.5, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=800, lr=9.77756e-06, gnorm=2.018, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=941
2023-01-04 21:22:00 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 102288 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=98.5, ups=0.88, wpb=111.9, bsz=40, num_updates=810, lr=9.89978e-06, gnorm=2.085, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=953
2023-01-04 21:22:11 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 102288 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=101.5, ups=0.92, wpb=110.6, bsz=40, num_updates=820, lr=1.0022e-05, gnorm=2.11, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=964
2023-01-04 21:22:23 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 102288 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.573, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=101.1, ups=0.92, wpb=110.4, bsz=40, num_updates=830, lr=1.01442e-05, gnorm=2.121, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=976
2023-01-04 21:22:35 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 102288 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.515, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=840, lr=1.02664e-05, gnorm=2.04, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=987
2023-01-04 21:22:46 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 102288 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=105.6, ups=0.95, wpb=111, bsz=40, num_updates=850, lr=1.03887e-05, gnorm=2.011, clip=100, loss_scale=256, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=999
2023-01-04 21:22:58 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 102288 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.503, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=860, lr=1.05109e-05, gnorm=1.899, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1010
2023-01-04 21:23:09 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 102288 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=98.5, ups=0.9, wpb=109.4, bsz=40, num_updates=870, lr=1.06331e-05, gnorm=2.042, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1022
2023-01-04 21:23:21 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 102288 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=101.1, ups=0.91, wpb=110.5, bsz=40, num_updates=880, lr=1.07553e-05, gnorm=1.763, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1034
2023-01-04 21:23:33 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 102288 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.511, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=890, lr=1.08775e-05, gnorm=1.829, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1046
2023-01-04 21:23:45 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 102288 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=900, lr=1.09998e-05, gnorm=1.875, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1058
2023-01-04 21:23:57 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 102288 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.572, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=910, lr=1.1122e-05, gnorm=1.95, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1070
2023-01-04 21:24:09 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 102288 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.4, ups=0.91, wpb=109.6, bsz=40, num_updates=920, lr=1.12442e-05, gnorm=1.872, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1082
2023-01-04 21:24:21 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 102288 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.493, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=930, lr=1.13664e-05, gnorm=1.849, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1094
2023-01-04 21:24:33 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 102288 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=940, lr=1.14886e-05, gnorm=1.773, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1106
2023-01-04 21:24:45 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 102288 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.504, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=950, lr=1.16109e-05, gnorm=1.779, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1118
2023-01-04 21:24:57 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 102288 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.486, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=960, lr=1.17331e-05, gnorm=1.834, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1130
2023-01-04 21:25:10 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 102288 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.505, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=970, lr=1.18553e-05, gnorm=1.844, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1142
2023-01-04 21:25:21 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 102288 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=100.3, ups=0.89, wpb=112.1, bsz=40, num_updates=980, lr=1.19775e-05, gnorm=1.889, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1154
2023-01-04 21:25:34 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 102288 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.547, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=990, lr=1.20997e-05, gnorm=1.818, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1166
2023-01-04 21:25:45 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 102288 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=1000, lr=1.2222e-05, gnorm=1.795, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1178
2023-01-04 21:25:57 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 102288 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=1010, lr=1.23442e-05, gnorm=1.818, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1190
2023-01-04 21:26:09 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 102288 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.493, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=102.9, ups=0.93, wpb=111.1, bsz=40, num_updates=1020, lr=1.24664e-05, gnorm=1.635, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1202
2023-01-04 21:26:21 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 102288 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=1030, lr=1.25886e-05, gnorm=1.712, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1214
2023-01-04 21:26:33 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 102288 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.489, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=101.1, ups=0.92, wpb=110.4, bsz=40, num_updates=1040, lr=1.27108e-05, gnorm=1.698, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1225
2023-01-04 21:26:45 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 102288 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.5, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=1050, lr=1.2833e-05, gnorm=1.923, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1237
2023-01-04 21:26:56 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 102288 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.489, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.7, ups=0.91, wpb=110.2, bsz=40, num_updates=1060, lr=1.29553e-05, gnorm=1.924, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1249
2023-01-04 21:27:07 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 102288 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.528, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.6, ups=0.91, wpb=108.1, bsz=40, num_updates=1070, lr=1.30775e-05, gnorm=1.941, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1261
2023-01-04 21:27:19 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 102288 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=1080, lr=1.31997e-05, gnorm=1.865, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1272
2023-01-04 21:27:30 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 102288 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.6, ups=0.9, wpb=108.8, bsz=40, num_updates=1090, lr=1.33219e-05, gnorm=1.704, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1283
2023-01-04 21:27:41 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 102288 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.493, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=101.6, ups=0.92, wpb=110.2, bsz=40, num_updates=1100, lr=1.34441e-05, gnorm=1.85, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1295
2023-01-04 21:27:53 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 102288 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=97.5, ups=0.89, wpb=110, bsz=40, num_updates=1110, lr=1.35664e-05, gnorm=1.871, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1306
2023-01-04 21:28:04 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 102288 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.4, ups=0.91, wpb=111.6, bsz=40, num_updates=1120, lr=1.36886e-05, gnorm=1.672, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1317
2023-01-04 21:28:15 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 102288 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=103.2, ups=0.94, wpb=109.8, bsz=40, num_updates=1130, lr=1.38108e-05, gnorm=1.74, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1328
2023-01-04 21:28:26 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 102288 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=102.5, ups=0.93, wpb=110.6, bsz=40, num_updates=1140, lr=1.3933e-05, gnorm=1.902, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1340
2023-01-04 21:28:38 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 102288 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=1150, lr=1.40552e-05, gnorm=1.717, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1351
2023-01-04 21:28:49 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 102288 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99, ups=0.89, wpb=111.2, bsz=40, num_updates=1160, lr=1.41775e-05, gnorm=1.678, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1362
2023-01-04 21:29:01 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 102288 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=97.1, ups=0.88, wpb=110.5, bsz=40, num_updates=1170, lr=1.42997e-05, gnorm=1.758, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1374
2023-01-04 21:29:13 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 102288 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=95.9, ups=0.88, wpb=108.7, bsz=40, num_updates=1180, lr=1.44219e-05, gnorm=1.714, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1386
2023-01-04 21:29:24 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 102288 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=99.1, ups=0.9, wpb=109.5, bsz=40, num_updates=1190, lr=1.45441e-05, gnorm=1.723, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1397
2023-01-04 21:29:35 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 102288 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=1200, lr=1.46663e-05, gnorm=1.557, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1409
2023-01-04 21:29:47 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 102288 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=96.2, ups=0.88, wpb=109.1, bsz=40, num_updates=1210, lr=1.47886e-05, gnorm=1.812, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1420
2023-01-04 21:29:58 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 102288 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=1220, lr=1.49108e-05, gnorm=1.692, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1432
2023-01-04 21:30:10 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 102288 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=102, ups=0.93, wpb=109.9, bsz=40, num_updates=1230, lr=1.5033e-05, gnorm=1.721, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1443
2023-01-04 21:30:21 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 102288 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=100, ups=0.92, wpb=109, bsz=40, num_updates=1240, lr=1.51552e-05, gnorm=1.8, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1454
2023-01-04 21:30:33 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 102288 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=98.1, ups=0.88, wpb=111.2, bsz=40, num_updates=1250, lr=1.52774e-05, gnorm=1.687, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1466
2023-01-04 21:30:44 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 102288 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=1260, lr=1.53997e-05, gnorm=1.554, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1477
2023-01-04 21:30:56 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 102288 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=97.6, ups=0.89, wpb=109.3, bsz=40, num_updates=1270, lr=1.55219e-05, gnorm=1.758, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1489
2023-01-04 21:31:07 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 102288 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=104.1, ups=0.95, wpb=109.1, bsz=40, num_updates=1280, lr=1.56441e-05, gnorm=1.749, clip=100, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=1500
2023-01-04 21:31:18 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 102288 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=94.9, ups=0.88, wpb=107.8, bsz=40, num_updates=1290, lr=1.57663e-05, gnorm=1.629, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1512
2023-01-04 21:31:30 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 102288 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=1300, lr=1.58885e-05, gnorm=1.704, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1523
2023-01-04 21:31:42 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 102288 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=1310, lr=1.60108e-05, gnorm=1.567, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1535
2023-01-04 21:31:54 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 102288 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=102, ups=0.92, wpb=110.9, bsz=40, num_updates=1320, lr=1.6133e-05, gnorm=1.786, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1547
2023-01-04 21:32:05 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 102288 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.2, ups=0.91, wpb=110.5, bsz=40, num_updates=1330, lr=1.62552e-05, gnorm=1.638, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1558
2023-01-04 21:32:17 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 102288 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=1340, lr=1.63774e-05, gnorm=1.547, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1570
2023-01-04 21:32:28 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 102288 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=1350, lr=1.64996e-05, gnorm=1.549, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1581
2023-01-04 21:32:39 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 102288 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.5, ups=0.93, wpb=110.2, bsz=40, num_updates=1360, lr=1.66219e-05, gnorm=1.656, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1593
2023-01-04 21:32:51 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 102288 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.6, ups=0.92, wpb=109.7, bsz=40, num_updates=1370, lr=1.67441e-05, gnorm=1.623, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1604
2023-01-04 21:33:02 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 102288 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.5, ups=0.89, wpb=111.8, bsz=40, num_updates=1380, lr=1.68663e-05, gnorm=1.637, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1615
2023-01-04 21:33:14 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 102288 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=1390, lr=1.69885e-05, gnorm=1.709, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1627
2023-01-04 21:33:25 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 102288 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=1400, lr=1.71107e-05, gnorm=1.652, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1638
2023-01-04 21:33:37 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 102288 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=1410, lr=1.7233e-05, gnorm=1.49, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1650
2023-01-04 21:33:49 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 102288 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.2, ups=0.9, wpb=109.8, bsz=40, num_updates=1420, lr=1.73552e-05, gnorm=1.455, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1662
2023-01-04 21:34:00 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 102288 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=1430, lr=1.74774e-05, gnorm=1.583, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1673
2023-01-04 21:34:12 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 102288 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=96.5, ups=0.87, wpb=110.4, bsz=40, num_updates=1440, lr=1.75996e-05, gnorm=1.54, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1685
2023-01-04 21:34:24 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 102288 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=94.5, ups=0.86, wpb=109.8, bsz=40, num_updates=1450, lr=1.77218e-05, gnorm=1.537, clip=100, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=1697
2023-01-04 21:34:36 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 102288 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.7, ups=0.91, wpb=111.1, bsz=40, num_updates=1460, lr=1.7844e-05, gnorm=1.497, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1709
2023-01-04 21:34:47 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 102288 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101, ups=0.92, wpb=109.9, bsz=40, num_updates=1470, lr=1.79663e-05, gnorm=1.6, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1720
2023-01-04 21:34:59 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 102288 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=1480, lr=1.80885e-05, gnorm=1.543, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=1732
2023-01-04 21:35:11 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 102288 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.2, ups=0.89, wpb=112.1, bsz=40, num_updates=1490, lr=1.82107e-05, gnorm=1.516, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1743
2023-01-04 21:35:22 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 102288 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=95.3, ups=0.87, wpb=109.3, bsz=40, num_updates=1500, lr=1.83329e-05, gnorm=1.605, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1756
2023-01-04 21:35:35 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 102288 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=1510, lr=1.84551e-05, gnorm=1.555, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1767
2023-01-04 21:35:46 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 102288 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.6, ups=0.91, wpb=111, bsz=40, num_updates=1520, lr=1.85774e-05, gnorm=1.549, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1779
2023-01-04 21:35:58 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 102288 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.9, ups=0.88, wpb=111, bsz=40, num_updates=1530, lr=1.86996e-05, gnorm=1.547, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1791
2023-01-04 21:36:06 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 21:36:11 - progress_bar.py[line:274] - INFO: epoch 001:   1541 / 102288 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=90.4, ups=0.82, wpb=110.4, bsz=40, num_updates=1540, lr=1.88218e-05, gnorm=1.572, clip=100, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1804
2023-01-04 21:36:22 - progress_bar.py[line:274] - INFO: epoch 001:   1551 / 102288 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=96.4, ups=0.88, wpb=109.6, bsz=40, num_updates=1550, lr=1.8944e-05, gnorm=1.581, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1816
2023-01-04 21:36:34 - progress_bar.py[line:274] - INFO: epoch 001:   1561 / 102288 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=96.4, ups=0.88, wpb=109.2, bsz=40, num_updates=1560, lr=1.90662e-05, gnorm=1.589, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1827
2023-01-04 21:36:46 - progress_bar.py[line:274] - INFO: epoch 001:   1571 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.9, ups=0.91, wpb=110.5, bsz=40, num_updates=1570, lr=1.91885e-05, gnorm=1.629, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1839
2023-01-04 21:36:57 - progress_bar.py[line:274] - INFO: epoch 001:   1581 / 102288 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.9, ups=0.92, wpb=110.7, bsz=40, num_updates=1580, lr=1.93107e-05, gnorm=1.694, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1850
2023-01-04 21:37:08 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 102288 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=1590, lr=1.94329e-05, gnorm=1.51, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1862
2023-01-04 21:37:20 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 102288 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.9, ups=0.89, wpb=110.3, bsz=40, num_updates=1600, lr=1.95551e-05, gnorm=1.496, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1873
2023-01-04 21:37:32 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 102288 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=1610, lr=1.96773e-05, gnorm=1.655, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1885
2023-01-04 21:37:43 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 102288 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=1620, lr=1.97996e-05, gnorm=1.596, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1896
2023-01-04 21:37:55 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 102288 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.7, ups=0.92, wpb=109.9, bsz=40, num_updates=1630, lr=1.99218e-05, gnorm=1.501, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1908
2023-01-04 21:38:06 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 102288 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.7, ups=0.9, wpb=110.6, bsz=40, num_updates=1640, lr=2.0044e-05, gnorm=1.436, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1919
2023-01-04 21:38:17 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 102288 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=1650, lr=2.01662e-05, gnorm=1.486, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1931
2023-01-04 21:38:29 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 102288 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=1660, lr=2.02884e-05, gnorm=1.555, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1942
2023-01-04 21:38:40 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=1670, lr=2.04107e-05, gnorm=1.462, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1953
2023-01-04 21:38:51 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 102288 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100, ups=0.92, wpb=108.7, bsz=40, num_updates=1680, lr=2.05329e-05, gnorm=1.67, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1965
2023-01-04 21:39:03 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 102288 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102, ups=0.92, wpb=110.9, bsz=40, num_updates=1690, lr=2.06551e-05, gnorm=1.566, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1976
2023-01-04 21:39:14 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 102288 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=104.3, ups=0.94, wpb=110.8, bsz=40, num_updates=1700, lr=2.07773e-05, gnorm=1.4, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1987
2023-01-04 21:39:25 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 102288 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=1710, lr=2.08995e-05, gnorm=1.503, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1998
2023-01-04 21:39:37 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 102288 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.8, ups=0.9, wpb=110.3, bsz=40, num_updates=1720, lr=2.10218e-05, gnorm=1.519, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2010
2023-01-04 21:39:48 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 102288 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.9, ups=0.88, wpb=111, bsz=40, num_updates=1730, lr=2.1144e-05, gnorm=1.538, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2021
2023-01-04 21:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 102288 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.4, ups=0.93, wpb=109.2, bsz=40, num_updates=1740, lr=2.12662e-05, gnorm=1.614, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=2032
2023-01-04 21:40:11 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 102288 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.1, ups=0.91, wpb=110.5, bsz=40, num_updates=1750, lr=2.13884e-05, gnorm=1.84, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2044
2023-01-04 21:40:22 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 102288 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.6, ups=0.89, wpb=109.3, bsz=40, num_updates=1760, lr=2.15106e-05, gnorm=1.452, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2055
2023-01-04 21:40:34 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 102288 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=96.7, ups=0.89, wpb=108.3, bsz=40, num_updates=1770, lr=2.16329e-05, gnorm=1.569, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2067
2023-01-04 21:40:45 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 102288 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.9, ups=0.91, wpb=111.2, bsz=40, num_updates=1780, lr=2.17551e-05, gnorm=1.417, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2078
2023-01-04 21:40:56 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 102288 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=103.2, ups=0.94, wpb=109.9, bsz=40, num_updates=1790, lr=2.18773e-05, gnorm=1.383, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=2089
2023-01-04 21:41:07 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.3, ups=0.93, wpb=110.6, bsz=40, num_updates=1800, lr=2.19995e-05, gnorm=1.429, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2100
2023-01-04 21:41:19 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 102288 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=1810, lr=2.21217e-05, gnorm=1.44, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2112
2023-01-04 21:41:30 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 102288 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=1820, lr=2.2244e-05, gnorm=1.452, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2123
2023-01-04 21:41:41 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.6, ups=0.93, wpb=110.4, bsz=40, num_updates=1830, lr=2.23662e-05, gnorm=1.313, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2134
2023-01-04 21:41:53 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 102288 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=1840, lr=2.24884e-05, gnorm=1.382, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2146
2023-01-04 21:42:04 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 102288 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=1850, lr=2.26106e-05, gnorm=1.36, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2158
2023-01-04 21:42:16 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 102288 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=1860, lr=2.27328e-05, gnorm=1.351, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2169
2023-01-04 21:42:26 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 102288 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=104.5, ups=0.95, wpb=109.6, bsz=40, num_updates=1870, lr=2.2855e-05, gnorm=1.319, clip=90, loss_scale=512, train_wall=10, gb_free=10.9, ema_decay=0.9999, wall=2180
2023-01-04 21:42:38 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 102288 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=1880, lr=2.29773e-05, gnorm=1.381, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2191
2023-01-04 21:42:49 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 102288 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=1890, lr=2.30995e-05, gnorm=1.681, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2203
2023-01-04 21:43:00 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 102288 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=104.2, ups=0.94, wpb=110.5, bsz=40, num_updates=1900, lr=2.32217e-05, gnorm=1.245, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2214
2023-01-04 21:43:12 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 102288 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=1910, lr=2.33439e-05, gnorm=1.355, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2225
2023-01-04 21:43:23 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 102288 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=97.4, ups=0.9, wpb=107.9, bsz=40, num_updates=1920, lr=2.34661e-05, gnorm=1.583, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2236
2023-01-04 21:43:35 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 102288 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=1930, lr=2.35884e-05, gnorm=1.498, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2248
2023-01-04 21:43:46 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 102288 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=1940, lr=2.37106e-05, gnorm=1.515, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2259
2023-01-04 21:43:57 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 102288 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=1950, lr=2.38328e-05, gnorm=1.264, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2271
2023-01-04 21:44:09 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.7, ups=0.87, wpb=111.3, bsz=40, num_updates=1960, lr=2.3955e-05, gnorm=1.267, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2282
2023-01-04 21:44:20 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 102288 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=103.1, ups=0.94, wpb=109.3, bsz=40, num_updates=1970, lr=2.40772e-05, gnorm=1.501, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2293
2023-01-04 21:44:31 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 102288 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.7, ups=0.93, wpb=109.9, bsz=40, num_updates=1980, lr=2.41995e-05, gnorm=1.375, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2305
2023-01-04 21:44:43 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 102288 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=1990, lr=2.43217e-05, gnorm=1.335, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2316
2023-01-04 21:44:54 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 102288 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=2000, lr=2.44439e-05, gnorm=1.272, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2328
2023-01-04 21:44:54 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-04 21:44:54 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-01-04 21:44:56 - train.py[line:549] - INFO: 0 / 4988
2023-01-04 21:44:56 - train.py[line:551] - INFO: load:0.82 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-04 21:47:31 - train.py[line:549] - INFO: 200 / 4988
2023-01-04 21:47:31 - train.py[line:551] - INFO: load:0.84 valid_run:155.29 task_valid:150.89 collect_output:3.37
2023-01-04 21:50:00 - train.py[line:549] - INFO: 400 / 4988
2023-01-04 21:50:00 - train.py[line:551] - INFO: load:0.86 valid_run:304.17 task_valid:293.55 collect_output:8.57
2023-01-04 21:52:33 - train.py[line:549] - INFO: 600 / 4988
2023-01-04 21:52:33 - train.py[line:551] - INFO: load:0.89 valid_run:457.18 task_valid:436.12 collect_output:17.98
2023-01-04 21:55:02 - train.py[line:549] - INFO: 800 / 4988
2023-01-04 21:55:02 - train.py[line:551] - INFO: load:0.91 valid_run:606.41 task_valid:580.90 collect_output:21.41
2023-01-04 21:57:35 - train.py[line:549] - INFO: 1000 / 4988
2023-01-04 21:57:35 - train.py[line:551] - INFO: load:0.93 valid_run:758.72 task_valid:727.90 collect_output:25.71
2023-01-04 22:00:06 - train.py[line:549] - INFO: 1200 / 4988
2023-01-04 22:00:06 - train.py[line:551] - INFO: load:0.96 valid_run:910.46 task_valid:872.91 collect_output:31.44
2023-01-04 22:02:40 - train.py[line:549] - INFO: 1400 / 4988
2023-01-04 22:02:40 - train.py[line:551] - INFO: load:0.98 valid_run:1064.07 task_valid:1018.62 collect_output:38.32
2023-01-04 22:05:12 - train.py[line:549] - INFO: 1600 / 4988
2023-01-04 22:05:12 - train.py[line:551] - INFO: load:1.01 valid_run:1215.81 task_valid:1159.42 collect_output:48.28
2023-01-04 22:07:42 - train.py[line:549] - INFO: 1800 / 4988
2023-01-04 22:07:42 - train.py[line:551] - INFO: load:1.03 valid_run:1365.40 task_valid:1303.84 collect_output:52.38
2023-01-04 22:10:10 - train.py[line:549] - INFO: 2000 / 4988
2023-01-04 22:10:10 - train.py[line:551] - INFO: load:1.06 valid_run:1514.02 task_valid:1446.61 collect_output:57.19
2023-01-04 22:12:40 - train.py[line:549] - INFO: 2200 / 4988
2023-01-04 22:12:40 - train.py[line:551] - INFO: load:1.08 valid_run:1663.70 task_valid:1591.19 collect_output:61.26
2023-01-04 22:15:10 - train.py[line:549] - INFO: 2400 / 4988
2023-01-04 22:15:10 - train.py[line:551] - INFO: load:1.11 valid_run:1813.93 task_valid:1736.02 collect_output:65.65
2023-01-04 22:17:40 - train.py[line:549] - INFO: 2600 / 4988
2023-01-04 22:17:40 - train.py[line:551] - INFO: load:1.13 valid_run:1963.89 task_valid:1877.56 collect_output:73.05
2023-01-04 22:20:11 - train.py[line:549] - INFO: 2800 / 4988
2023-01-04 22:20:11 - train.py[line:551] - INFO: load:1.16 valid_run:2114.46 task_valid:2022.75 collect_output:77.42
2023-01-04 22:22:41 - train.py[line:549] - INFO: 3000 / 4988
2023-01-04 22:22:41 - train.py[line:551] - INFO: load:1.18 valid_run:2264.34 task_valid:2168.81 collect_output:80.21
2023-01-04 22:25:11 - train.py[line:549] - INFO: 3200 / 4988
2023-01-04 22:25:11 - train.py[line:551] - INFO: load:1.21 valid_run:2414.61 task_valid:2312.76 collect_output:85.49
2023-01-04 22:27:43 - train.py[line:549] - INFO: 3400 / 4988
2023-01-04 22:27:43 - train.py[line:551] - INFO: load:1.23 valid_run:2566.38 task_valid:2457.84 collect_output:91.10
2023-01-04 22:30:14 - train.py[line:549] - INFO: 3600 / 4988
2023-01-04 22:30:14 - train.py[line:551] - INFO: load:1.26 valid_run:2716.75 task_valid:2604.39 collect_output:93.90
2023-01-04 22:32:42 - train.py[line:549] - INFO: 3800 / 4988
2023-01-04 22:32:42 - train.py[line:551] - INFO: load:1.28 valid_run:2865.36 task_valid:2745.54 collect_output:100.33
2023-01-04 22:35:13 - train.py[line:549] - INFO: 4000 / 4988
2023-01-04 22:35:13 - train.py[line:551] - INFO: load:1.31 valid_run:3016.14 task_valid:2890.39 collect_output:105.26
2023-01-04 22:37:45 - train.py[line:549] - INFO: 4200 / 4988
2023-01-04 22:37:45 - train.py[line:551] - INFO: load:1.33 valid_run:3168.29 task_valid:3034.50 collect_output:112.26
2023-01-04 22:40:15 - train.py[line:549] - INFO: 4400 / 4988
2023-01-04 22:40:15 - train.py[line:551] - INFO: load:1.36 valid_run:3317.80 task_valid:3178.61 collect_output:116.62
2023-01-04 22:42:47 - train.py[line:549] - INFO: 4600 / 4988
2023-01-04 22:42:47 - train.py[line:551] - INFO: load:1.38 valid_run:3469.27 task_valid:3324.46 collect_output:121.20
2023-01-04 22:45:18 - train.py[line:549] - INFO: 4800 / 4988
2023-01-04 22:45:18 - train.py[line:551] - INFO: load:1.41 valid_run:3620.86 task_valid:3470.69 collect_output:125.54

====================================================================================================
SGG eval:     R @ 50: 0.3107;     R @ 100: 0.3680;     R @ 500: 0.4327;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1442;    mR @ 100: 0.1924;    mR @ 500: 0.2307;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0390) (covered in:0.0000) (covering:0.1429) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2083) (playing:0.0000) (riding:0.4363) (says:0.0000) (sitting on:0.4603) (standing on:0.4925) (using:0.2000) (walking in:0.0000) (walking on:0.2027) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.3107;     R @ 100: 0.3680;     R @ 500: 0.4327;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1442;    mR @ 100: 0.1924;    mR @ 500: 0.2307;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0390) (covered in:0.0000) (covering:0.1429) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2083) (playing:0.0000) (riding:0.4363) (says:0.0000) (sitting on:0.4603) (standing on:0.4925) (using:0.2000) (walking in:0.0000) (walking on:0.2027) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-04 22:47:49 - train.py[line:487] - INFO: 0.3680333333333333
2023-01-04 22:47:49 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-04 22:47:50 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.392 | loss_v1 0 | loss_v2 0 | nll_loss 0.25 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.368033 | ppl 1.19 | vqa_score 0.223 | wps 118.9 | wpb 89.9 | bsz 30 | num_updates 2000
2023-01-04 22:47:50 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-01-04 22:47:50 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-04 22:48:27 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-04 22:51:16 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.3680333333333333) (writing took 206.17283701337874 seconds)
2023-01-04 22:51:27 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 102288 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=0.3, ups=0, wpb=109.2, bsz=40, num_updates=2010, lr=2.45661e-05, gnorm=1.385, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6320
2023-01-04 22:51:39 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 102288 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.2, ups=0.87, wpb=110.8, bsz=40, num_updates=2020, lr=2.46883e-05, gnorm=1.622, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6332
2023-01-04 22:51:50 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 102288 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=102, ups=0.93, wpb=109.8, bsz=40, num_updates=2030, lr=2.48106e-05, gnorm=1.365, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6343
2023-01-04 22:52:01 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 102288 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=2040, lr=2.49328e-05, gnorm=1.379, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6354
2023-01-04 22:52:12 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 102288 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.4, ups=0.93, wpb=108.5, bsz=40, num_updates=2050, lr=2.5055e-05, gnorm=1.342, clip=100, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6365
2023-01-04 22:52:23 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 102288 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=2060, lr=2.51772e-05, gnorm=1.261, clip=90, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6377
2023-01-04 22:52:35 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 102288 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.1, ups=0.88, wpb=109.1, bsz=40, num_updates=2070, lr=2.52994e-05, gnorm=1.445, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6388
2023-01-04 22:52:46 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 102288 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.3, ups=0.92, wpb=109, bsz=40, num_updates=2080, lr=2.54217e-05, gnorm=1.382, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6399
2023-01-04 22:52:57 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.7, ups=0.92, wpb=109.8, bsz=40, num_updates=2090, lr=2.55439e-05, gnorm=1.29, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6410
2023-01-04 22:53:08 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 102288 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.8, ups=0.92, wpb=109, bsz=40, num_updates=2100, lr=2.56661e-05, gnorm=1.288, clip=90, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6422
2023-01-04 22:53:20 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 102288 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=2110, lr=2.57883e-05, gnorm=1.33, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6433
2023-01-04 22:53:31 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 102288 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97, ups=0.87, wpb=111.5, bsz=40, num_updates=2120, lr=2.59105e-05, gnorm=1.257, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6445
2023-01-04 22:53:43 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 102288 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.8, ups=0.91, wpb=109.6, bsz=40, num_updates=2130, lr=2.60328e-05, gnorm=1.273, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6456
2023-01-04 22:53:54 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 102288 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=2140, lr=2.6155e-05, gnorm=1.371, clip=100, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6467
2023-01-04 22:54:05 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 102288 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.1, ups=0.92, wpb=109.3, bsz=40, num_updates=2150, lr=2.62772e-05, gnorm=1.402, clip=100, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6478
2023-01-04 22:54:16 - progress_bar.py[line:274] - INFO: epoch 001:   2161 / 102288 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=2160, lr=2.63994e-05, gnorm=1.422, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6490
2023-01-04 22:54:23 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 22:54:29 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 102288 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=93.7, ups=0.85, wpb=110.5, bsz=40, num_updates=2170, lr=2.65216e-05, gnorm=1.447, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6502
2023-01-04 22:54:40 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 102288 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.5, ups=0.91, wpb=111, bsz=40, num_updates=2180, lr=2.66439e-05, gnorm=1.258, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6513
2023-01-04 22:54:51 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 102288 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=96.3, ups=0.88, wpb=109.2, bsz=40, num_updates=2190, lr=2.67661e-05, gnorm=1.299, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6525
2023-01-04 22:55:02 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 102288 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.5, ups=0.92, wpb=110.9, bsz=40, num_updates=2200, lr=2.68883e-05, gnorm=1.308, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6536
2023-01-04 22:55:13 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 102288 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.8, ups=0.93, wpb=110.5, bsz=40, num_updates=2210, lr=2.70105e-05, gnorm=1.21, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6547
2023-01-04 22:55:24 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=103.5, ups=0.94, wpb=110.2, bsz=40, num_updates=2220, lr=2.71327e-05, gnorm=1.432, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6558
2023-01-04 22:55:36 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=2230, lr=2.72549e-05, gnorm=1.221, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6569
2023-01-04 22:55:47 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 102288 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.6, ups=0.9, wpb=111.5, bsz=40, num_updates=2240, lr=2.73772e-05, gnorm=1.15, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6580
2023-01-04 22:55:58 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 102288 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.9, ups=0.9, wpb=109.5, bsz=40, num_updates=2250, lr=2.74994e-05, gnorm=1.234, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6592
2023-01-04 22:56:09 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 102288 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.1, ups=0.91, wpb=109.6, bsz=40, num_updates=2260, lr=2.76216e-05, gnorm=1.332, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6603
2023-01-04 22:56:21 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 102288 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=2270, lr=2.77438e-05, gnorm=1.242, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6614
2023-01-04 22:56:32 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 102288 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101, ups=0.91, wpb=111.5, bsz=40, num_updates=2280, lr=2.7866e-05, gnorm=1.148, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6625
2023-01-04 22:56:43 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 102288 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=2290, lr=2.79883e-05, gnorm=1.208, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6637
2023-01-04 22:56:55 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 102288 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=2300, lr=2.81105e-05, gnorm=1.354, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6648
2023-01-04 22:57:06 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 102288 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.6, ups=0.92, wpb=108.6, bsz=40, num_updates=2310, lr=2.82327e-05, gnorm=1.208, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6659
2023-01-04 22:57:17 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 102288 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.1, ups=0.9, wpb=109.8, bsz=40, num_updates=2320, lr=2.83549e-05, gnorm=1.206, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6670
2023-01-04 22:57:29 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 102288 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.5, ups=0.89, wpb=109.6, bsz=40, num_updates=2330, lr=2.84771e-05, gnorm=1.237, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6682
2023-01-04 22:57:40 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 102288 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=2340, lr=2.85994e-05, gnorm=1.239, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6693
2023-01-04 22:57:52 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 102288 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=2350, lr=2.87216e-05, gnorm=1.296, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6705
2023-01-04 22:58:03 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.6, ups=0.91, wpb=110.2, bsz=40, num_updates=2360, lr=2.88438e-05, gnorm=1.119, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6716
2023-01-04 22:58:14 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 102288 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.9, ups=0.92, wpb=109, bsz=40, num_updates=2370, lr=2.8966e-05, gnorm=1.337, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6727
2023-01-04 22:58:26 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 102288 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=2380, lr=2.90882e-05, gnorm=1.171, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6739
2023-01-04 22:58:37 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 102288 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.3, ups=0.92, wpb=110.5, bsz=40, num_updates=2390, lr=2.92105e-05, gnorm=1.193, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6750
2023-01-04 22:58:48 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 102288 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=2400, lr=2.93327e-05, gnorm=1.255, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6762
2023-01-04 22:59:00 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 102288 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=2410, lr=2.94549e-05, gnorm=1.363, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6773
2023-01-04 22:59:11 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 102288 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=2420, lr=2.95771e-05, gnorm=1.195, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6785
2023-01-04 22:59:23 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 102288 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.6, ups=0.88, wpb=109.9, bsz=40, num_updates=2430, lr=2.96993e-05, gnorm=1.162, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6796
2023-01-04 22:59:35 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=2440, lr=2.98216e-05, gnorm=1.344, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6808
2023-01-04 22:59:46 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 102288 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=2450, lr=2.99438e-05, gnorm=1.106, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6819
2023-01-04 22:59:57 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 102288 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=2460, lr=3.0066e-05, gnorm=1.198, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6831
2023-01-04 23:00:08 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 102288 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=2470, lr=3.01882e-05, gnorm=1.207, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6842
2023-01-04 23:00:19 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 102288 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.7, ups=0.93, wpb=109.5, bsz=40, num_updates=2480, lr=3.03104e-05, gnorm=1.158, clip=90, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6853
2023-01-04 23:00:31 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 102288 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.2, ups=0.88, wpb=109.2, bsz=40, num_updates=2490, lr=3.04327e-05, gnorm=1.118, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6864
2023-01-04 23:00:42 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 102288 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.5, ups=0.92, wpb=110.5, bsz=40, num_updates=2500, lr=3.05549e-05, gnorm=1.092, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6875
2023-01-04 23:00:53 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 102288 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.3, ups=0.92, wpb=111.6, bsz=40, num_updates=2510, lr=3.06771e-05, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6887
2023-01-04 23:01:04 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 102288 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=104.5, ups=0.95, wpb=109.4, bsz=40, num_updates=2520, lr=3.07993e-05, gnorm=1.108, clip=60, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=6897
2023-01-04 23:01:15 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 102288 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.1, ups=0.92, wpb=110.5, bsz=40, num_updates=2530, lr=3.09215e-05, gnorm=1.043, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6908
2023-01-04 23:01:26 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99, ups=0.89, wpb=110.7, bsz=40, num_updates=2540, lr=3.10438e-05, gnorm=1.138, clip=90, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6920
2023-01-04 23:01:38 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 102288 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=2550, lr=3.1166e-05, gnorm=1.355, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6931
2023-01-04 23:01:49 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 102288 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.8, ups=0.91, wpb=111.2, bsz=40, num_updates=2560, lr=3.12882e-05, gnorm=1.171, clip=100, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6942
2023-01-04 23:02:01 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 102288 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=2570, lr=3.14104e-05, gnorm=1.248, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6954
2023-01-04 23:02:12 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 102288 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103, ups=0.92, wpb=112.3, bsz=40, num_updates=2580, lr=3.15326e-05, gnorm=1.121, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6965
2023-01-04 23:02:23 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=2590, lr=3.16549e-05, gnorm=1.155, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6977
2023-01-04 23:02:35 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 102288 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=2600, lr=3.17771e-05, gnorm=1.195, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6988
2023-01-04 23:02:46 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 102288 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=2610, lr=3.18993e-05, gnorm=1.137, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6999
2023-01-04 23:02:57 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 102288 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=2620, lr=3.20215e-05, gnorm=1.309, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7011
2023-01-04 23:03:09 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=2630, lr=3.21437e-05, gnorm=1.232, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7022
2023-01-04 23:03:20 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97, ups=0.88, wpb=109.9, bsz=40, num_updates=2640, lr=3.22659e-05, gnorm=1.216, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7034
2023-01-04 23:03:32 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 102288 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=2650, lr=3.23882e-05, gnorm=1.287, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7045
2023-01-04 23:03:43 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 102288 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=2660, lr=3.25104e-05, gnorm=1.2, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7057
2023-01-04 23:03:54 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 102288 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99, ups=0.9, wpb=109.4, bsz=40, num_updates=2670, lr=3.26326e-05, gnorm=1.17, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7068
2023-01-04 23:04:06 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 102288 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.2, ups=0.92, wpb=110.3, bsz=40, num_updates=2680, lr=3.27548e-05, gnorm=1.023, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7079
2023-01-04 23:04:17 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 102288 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=2690, lr=3.2877e-05, gnorm=1.328, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7090
2023-01-04 23:04:29 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 102288 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.3, ups=0.87, wpb=109.8, bsz=40, num_updates=2700, lr=3.29993e-05, gnorm=1.066, clip=70, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7102
2023-01-04 23:04:30 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 23:04:41 - progress_bar.py[line:274] - INFO: epoch 001:   2713 / 102288 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=88.6, ups=0.81, wpb=109.5, bsz=40, num_updates=2710, lr=3.31215e-05, gnorm=1.201, clip=90, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=7115
2023-01-04 23:04:53 - progress_bar.py[line:274] - INFO: epoch 001:   2723 / 102288 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=2720, lr=3.32437e-05, gnorm=1.145, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7126
2023-01-04 23:05:04 - progress_bar.py[line:274] - INFO: epoch 001:   2733 / 102288 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100, ups=0.91, wpb=110.3, bsz=40, num_updates=2730, lr=3.33659e-05, gnorm=1.125, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7137
2023-01-04 23:05:15 - progress_bar.py[line:274] - INFO: epoch 001:   2743 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=2740, lr=3.34881e-05, gnorm=1.013, clip=40, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=7148
2023-01-04 23:05:26 - progress_bar.py[line:274] - INFO: epoch 001:   2753 / 102288 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.4, ups=0.92, wpb=111.6, bsz=40, num_updates=2750, lr=3.36104e-05, gnorm=1.054, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7160
2023-01-04 23:05:38 - progress_bar.py[line:274] - INFO: epoch 001:   2763 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.5, ups=0.89, wpb=110.6, bsz=40, num_updates=2760, lr=3.37326e-05, gnorm=1.002, clip=40, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7171
2023-01-04 23:05:49 - progress_bar.py[line:274] - INFO: epoch 001:   2773 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.6, ups=0.91, wpb=111.1, bsz=40, num_updates=2770, lr=3.38548e-05, gnorm=1.118, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7182
2023-01-04 23:06:00 - progress_bar.py[line:274] - INFO: epoch 001:   2783 / 102288 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=2780, lr=3.3977e-05, gnorm=1.242, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7194
2023-01-04 23:06:12 - progress_bar.py[line:274] - INFO: epoch 001:   2793 / 102288 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=2790, lr=3.40992e-05, gnorm=1.019, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7205
2023-01-04 23:06:23 - progress_bar.py[line:274] - INFO: epoch 001:   2803 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.5, ups=0.92, wpb=110.9, bsz=40, num_updates=2800, lr=3.42215e-05, gnorm=1.063, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7216
2023-01-04 23:06:34 - progress_bar.py[line:274] - INFO: epoch 001:   2813 / 102288 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.1, ups=0.91, wpb=111.4, bsz=40, num_updates=2810, lr=3.43437e-05, gnorm=1.177, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7228
2023-01-04 23:06:45 - progress_bar.py[line:274] - INFO: epoch 001:   2823 / 102288 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=2820, lr=3.44659e-05, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7239
2023-01-04 23:06:57 - progress_bar.py[line:274] - INFO: epoch 001:   2833 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.7, ups=0.9, wpb=110.6, bsz=40, num_updates=2830, lr=3.45881e-05, gnorm=1.119, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7250
2023-01-04 23:07:08 - progress_bar.py[line:274] - INFO: epoch 001:   2843 / 102288 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=2840, lr=3.47103e-05, gnorm=0.997, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7262
2023-01-04 23:07:20 - progress_bar.py[line:274] - INFO: epoch 001:   2853 / 102288 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=2850, lr=3.48326e-05, gnorm=1.079, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7273
2023-01-04 23:07:31 - progress_bar.py[line:274] - INFO: epoch 001:   2863 / 102288 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=2860, lr=3.49548e-05, gnorm=1.024, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7285
2023-01-04 23:07:43 - progress_bar.py[line:274] - INFO: epoch 001:   2873 / 102288 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=2870, lr=3.5077e-05, gnorm=1.088, clip=80, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7296
2023-01-04 23:07:54 - progress_bar.py[line:274] - INFO: epoch 001:   2883 / 102288 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=2880, lr=3.51992e-05, gnorm=1.067, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7308
2023-01-04 23:08:05 - progress_bar.py[line:274] - INFO: epoch 001:   2893 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.1, ups=0.93, wpb=109.7, bsz=40, num_updates=2890, lr=3.53214e-05, gnorm=1.181, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7319
2023-01-04 23:08:16 - progress_bar.py[line:274] - INFO: epoch 001:   2903 / 102288 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.3, ups=0.92, wpb=110.3, bsz=40, num_updates=2900, lr=3.54437e-05, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7330
2023-01-04 23:08:28 - progress_bar.py[line:274] - INFO: epoch 001:   2913 / 102288 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.9, ups=0.87, wpb=111.2, bsz=40, num_updates=2910, lr=3.55659e-05, gnorm=1.068, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7341
2023-01-04 23:08:40 - progress_bar.py[line:274] - INFO: epoch 001:   2923 / 102288 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=2920, lr=3.56881e-05, gnorm=1.101, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7353
2023-01-04 23:08:51 - progress_bar.py[line:274] - INFO: epoch 001:   2933 / 102288 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=2930, lr=3.58103e-05, gnorm=1.091, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7364
2023-01-04 23:09:02 - progress_bar.py[line:274] - INFO: epoch 001:   2943 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.6, ups=0.93, wpb=111.5, bsz=40, num_updates=2940, lr=3.59325e-05, gnorm=0.966, clip=40, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7375
2023-01-04 23:09:14 - progress_bar.py[line:274] - INFO: epoch 001:   2953 / 102288 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.4, ups=0.87, wpb=110.8, bsz=40, num_updates=2950, lr=3.60548e-05, gnorm=0.975, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7387
2023-01-04 23:09:25 - progress_bar.py[line:274] - INFO: epoch 001:   2963 / 102288 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.8, ups=0.91, wpb=109.1, bsz=40, num_updates=2960, lr=3.6177e-05, gnorm=1.145, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7398
2023-01-04 23:09:36 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=2970, lr=3.62992e-05, gnorm=1.056, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7410
2023-01-04 23:09:48 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 102288 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.1, ups=0.92, wpb=112, bsz=40, num_updates=2980, lr=3.64214e-05, gnorm=1.077, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7421
2023-01-04 23:09:59 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.5, ups=0.92, wpb=110.7, bsz=40, num_updates=2990, lr=3.65436e-05, gnorm=1.066, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7432
2023-01-04 23:10:10 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 102288 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.9, ups=0.88, wpb=111, bsz=40, num_updates=3000, lr=3.66659e-05, gnorm=1.06, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7443
2023-01-04 23:10:21 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 102288 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=3010, lr=3.67881e-05, gnorm=1.004, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7455
2023-01-04 23:10:33 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 102288 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.7, ups=0.9, wpb=110.2, bsz=40, num_updates=3020, lr=3.69103e-05, gnorm=0.987, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7466
2023-01-04 23:10:44 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.5, ups=0.87, wpb=111.1, bsz=40, num_updates=3030, lr=3.70325e-05, gnorm=1.044, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7478
2023-01-04 23:10:56 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 102288 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.5, ups=0.9, wpb=109.9, bsz=40, num_updates=3040, lr=3.71547e-05, gnorm=1.013, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7489
2023-01-04 23:11:07 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.2, ups=0.91, wpb=109.1, bsz=40, num_updates=3050, lr=3.72769e-05, gnorm=1.231, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7500
2023-01-04 23:11:18 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.3, ups=0.92, wpb=110.5, bsz=40, num_updates=3060, lr=3.73992e-05, gnorm=1.244, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7511
2023-01-04 23:11:29 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=3070, lr=3.75214e-05, gnorm=1.086, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7523
2023-01-04 23:11:41 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 102288 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=3080, lr=3.76436e-05, gnorm=1.113, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7534
2023-01-04 23:11:52 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.7, ups=0.88, wpb=110.9, bsz=40, num_updates=3090, lr=3.77658e-05, gnorm=1.041, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7545
2023-01-04 23:12:04 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=3100, lr=3.7888e-05, gnorm=1.043, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7557
2023-01-04 23:12:15 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 102288 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.7, ups=0.9, wpb=109.4, bsz=40, num_updates=3110, lr=3.80103e-05, gnorm=1.165, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7568
2023-01-04 23:12:26 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 102288 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.7, ups=0.89, wpb=108.8, bsz=40, num_updates=3120, lr=3.81325e-05, gnorm=1.078, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7580
2023-01-04 23:12:38 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 102288 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.7, ups=0.9, wpb=109.6, bsz=40, num_updates=3130, lr=3.82547e-05, gnorm=1.1, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7591
2023-01-04 23:12:49 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 102288 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.5, ups=0.88, wpb=111.5, bsz=40, num_updates=3140, lr=3.83769e-05, gnorm=0.961, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7603
2023-01-04 23:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95, ups=0.86, wpb=110.4, bsz=40, num_updates=3150, lr=3.84991e-05, gnorm=1.026, clip=60, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7614
2023-01-04 23:13:13 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 102288 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=3160, lr=3.86214e-05, gnorm=1.011, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7626
2023-01-04 23:13:24 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 102288 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=3170, lr=3.87436e-05, gnorm=1.043, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7637
2023-01-04 23:13:35 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 102288 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.7, ups=0.92, wpb=110.5, bsz=40, num_updates=3180, lr=3.88658e-05, gnorm=0.95, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7648
2023-01-04 23:13:46 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 102288 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.4, ups=0.9, wpb=110.9, bsz=40, num_updates=3190, lr=3.8988e-05, gnorm=1.017, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7660
2023-01-04 23:13:58 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 102288 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.9, ups=0.92, wpb=110, bsz=40, num_updates=3200, lr=3.91102e-05, gnorm=1.046, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7671
2023-01-04 23:14:09 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=3210, lr=3.92325e-05, gnorm=1.01, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7683
2023-01-04 23:14:20 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 102288 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.5, ups=0.92, wpb=110.7, bsz=40, num_updates=3220, lr=3.93547e-05, gnorm=0.961, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7694
2023-01-04 23:14:31 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 102288 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.6, ups=0.91, wpb=110, bsz=40, num_updates=3230, lr=3.94769e-05, gnorm=1.064, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7705
2023-01-04 23:14:43 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 102288 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=94.7, ups=0.87, wpb=108.8, bsz=40, num_updates=3240, lr=3.95991e-05, gnorm=0.921, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7716
2023-01-04 23:14:54 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 102288 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=3250, lr=3.97213e-05, gnorm=0.979, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7728
2023-01-04 23:15:06 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 102288 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=3260, lr=3.98436e-05, gnorm=0.982, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7739
2023-01-04 23:15:17 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 102288 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=3270, lr=3.99658e-05, gnorm=0.904, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7750
2023-01-04 23:15:28 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.4, ups=0.91, wpb=110.5, bsz=40, num_updates=3280, lr=4.0088e-05, gnorm=0.886, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7762
2023-01-04 23:15:40 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=3290, lr=4.02102e-05, gnorm=0.972, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7773
2023-01-04 23:15:51 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 102288 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=3300, lr=4.03324e-05, gnorm=0.999, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7785
2023-01-04 23:16:03 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.5, ups=0.89, wpb=109.1, bsz=40, num_updates=3310, lr=4.04547e-05, gnorm=0.964, clip=40, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7796
2023-01-04 23:16:06 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 23:16:15 - progress_bar.py[line:274] - INFO: epoch 001:   3324 / 102288 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=92, ups=0.83, wpb=110.9, bsz=40, num_updates=3320, lr=4.05769e-05, gnorm=1.039, clip=60, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7808
2023-01-04 23:16:26 - progress_bar.py[line:274] - INFO: epoch 001:   3334 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=3330, lr=4.06991e-05, gnorm=1.02, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7820
2023-01-04 23:16:38 - progress_bar.py[line:274] - INFO: epoch 001:   3344 / 102288 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=3340, lr=4.08213e-05, gnorm=1.082, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7831
2023-01-04 23:16:49 - progress_bar.py[line:274] - INFO: epoch 001:   3354 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102, ups=0.92, wpb=111.3, bsz=40, num_updates=3350, lr=4.09435e-05, gnorm=1.105, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7842
2023-01-04 23:17:00 - progress_bar.py[line:274] - INFO: epoch 001:   3364 / 102288 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=3360, lr=4.10658e-05, gnorm=1.034, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7853
2023-01-04 23:17:11 - progress_bar.py[line:274] - INFO: epoch 001:   3374 / 102288 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=3370, lr=4.1188e-05, gnorm=1.014, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7865
2023-01-04 23:17:23 - progress_bar.py[line:274] - INFO: epoch 001:   3384 / 102288 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.6, ups=0.91, wpb=110, bsz=40, num_updates=3380, lr=4.13102e-05, gnorm=0.944, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7876
2023-01-04 23:17:34 - progress_bar.py[line:274] - INFO: epoch 001:   3394 / 102288 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=3390, lr=4.14324e-05, gnorm=0.959, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7887
2023-01-04 23:17:45 - progress_bar.py[line:274] - INFO: epoch 001:   3404 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.6, ups=0.93, wpb=110.5, bsz=40, num_updates=3400, lr=4.15546e-05, gnorm=1.021, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7898
2023-01-04 23:17:56 - progress_bar.py[line:274] - INFO: epoch 001:   3414 / 102288 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=3410, lr=4.16769e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7910
2023-01-04 23:18:08 - progress_bar.py[line:274] - INFO: epoch 001:   3424 / 102288 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.6, ups=0.89, wpb=109.6, bsz=40, num_updates=3420, lr=4.17991e-05, gnorm=0.912, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7921
2023-01-04 23:18:19 - progress_bar.py[line:274] - INFO: epoch 001:   3434 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.8, ups=0.89, wpb=109.8, bsz=40, num_updates=3430, lr=4.19213e-05, gnorm=1.036, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7933
2023-01-04 23:18:30 - progress_bar.py[line:274] - INFO: epoch 001:   3444 / 102288 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.3, ups=0.92, wpb=109.4, bsz=40, num_updates=3440, lr=4.20435e-05, gnorm=0.958, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7944
2023-01-04 23:18:42 - progress_bar.py[line:274] - INFO: epoch 001:   3454 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.8, ups=0.89, wpb=111.8, bsz=40, num_updates=3450, lr=4.21657e-05, gnorm=0.963, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7955
2023-01-04 23:18:53 - progress_bar.py[line:274] - INFO: epoch 001:   3464 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103, ups=0.93, wpb=110.3, bsz=40, num_updates=3460, lr=4.22879e-05, gnorm=0.937, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7966
2023-01-04 23:19:04 - progress_bar.py[line:274] - INFO: epoch 001:   3474 / 102288 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.7, ups=0.93, wpb=109.5, bsz=40, num_updates=3470, lr=4.24102e-05, gnorm=1.117, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7977
2023-01-04 23:19:15 - progress_bar.py[line:274] - INFO: epoch 001:   3484 / 102288 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.8, ups=0.93, wpb=110.4, bsz=40, num_updates=3480, lr=4.25324e-05, gnorm=0.962, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7988
2023-01-04 23:19:26 - progress_bar.py[line:274] - INFO: epoch 001:   3494 / 102288 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.3, ups=0.88, wpb=109.3, bsz=40, num_updates=3490, lr=4.26546e-05, gnorm=0.831, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=8000
2023-01-04 23:19:38 - progress_bar.py[line:274] - INFO: epoch 001:   3504 / 102288 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=3500, lr=4.27768e-05, gnorm=0.948, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8011
2023-01-04 23:19:49 - progress_bar.py[line:274] - INFO: epoch 001:   3514 / 102288 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.5, ups=0.91, wpb=108.8, bsz=40, num_updates=3510, lr=4.2899e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8022
2023-01-04 23:20:00 - progress_bar.py[line:274] - INFO: epoch 001:   3524 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.3, ups=0.91, wpb=110.9, bsz=40, num_updates=3520, lr=4.30213e-05, gnorm=0.974, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8033
2023-01-04 23:20:12 - progress_bar.py[line:274] - INFO: epoch 001:   3534 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=3530, lr=4.31435e-05, gnorm=0.965, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8045
2023-01-04 23:20:23 - progress_bar.py[line:274] - INFO: epoch 001:   3544 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=3540, lr=4.32657e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8056
2023-01-04 23:20:35 - progress_bar.py[line:274] - INFO: epoch 001:   3554 / 102288 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.9, ups=0.88, wpb=111, bsz=40, num_updates=3550, lr=4.33879e-05, gnorm=0.952, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8068
2023-01-04 23:20:46 - progress_bar.py[line:274] - INFO: epoch 001:   3564 / 102288 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=3560, lr=4.35101e-05, gnorm=0.962, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8079
2023-01-04 23:20:57 - progress_bar.py[line:274] - INFO: epoch 001:   3574 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.1, ups=0.92, wpb=110.2, bsz=40, num_updates=3570, lr=4.36324e-05, gnorm=1.013, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8090
2023-01-04 23:21:09 - progress_bar.py[line:274] - INFO: epoch 001:   3584 / 102288 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.2, ups=0.87, wpb=109.4, bsz=40, num_updates=3580, lr=4.37546e-05, gnorm=0.925, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8102
2023-01-04 23:21:20 - progress_bar.py[line:274] - INFO: epoch 001:   3594 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.8, ups=0.89, wpb=111.7, bsz=40, num_updates=3590, lr=4.38768e-05, gnorm=0.876, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8114
2023-01-04 23:21:32 - progress_bar.py[line:274] - INFO: epoch 001:   3604 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=3600, lr=4.3999e-05, gnorm=0.837, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8125
2023-01-04 23:21:43 - progress_bar.py[line:274] - INFO: epoch 001:   3614 / 102288 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=3610, lr=4.41212e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8136
2023-01-04 23:21:54 - progress_bar.py[line:274] - INFO: epoch 001:   3624 / 102288 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=3620, lr=4.42435e-05, gnorm=0.866, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8148
2023-01-04 23:22:06 - progress_bar.py[line:274] - INFO: epoch 001:   3634 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=3630, lr=4.43657e-05, gnorm=0.876, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8159
2023-01-04 23:22:17 - progress_bar.py[line:274] - INFO: epoch 001:   3644 / 102288 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=3640, lr=4.44879e-05, gnorm=0.914, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8170
2023-01-04 23:22:28 - progress_bar.py[line:274] - INFO: epoch 001:   3654 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.8, ups=0.9, wpb=110.6, bsz=40, num_updates=3650, lr=4.46101e-05, gnorm=0.809, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8181
2023-01-04 23:22:39 - progress_bar.py[line:274] - INFO: epoch 001:   3664 / 102288 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=3660, lr=4.47323e-05, gnorm=0.902, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8193
2023-01-04 23:22:51 - progress_bar.py[line:274] - INFO: epoch 001:   3674 / 102288 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=3670, lr=4.48546e-05, gnorm=1.063, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8204
2023-01-04 23:23:02 - progress_bar.py[line:274] - INFO: epoch 001:   3684 / 102288 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.5, ups=0.92, wpb=110.9, bsz=40, num_updates=3680, lr=4.49768e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8215
2023-01-04 23:23:13 - progress_bar.py[line:274] - INFO: epoch 001:   3694 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.2, ups=0.93, wpb=110, bsz=40, num_updates=3690, lr=4.5099e-05, gnorm=0.841, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8226
2023-01-04 23:23:24 - progress_bar.py[line:274] - INFO: epoch 001:   3704 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=3700, lr=4.52212e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8238
2023-01-04 23:23:36 - progress_bar.py[line:274] - INFO: epoch 001:   3714 / 102288 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.1, ups=0.91, wpb=110.5, bsz=40, num_updates=3710, lr=4.53434e-05, gnorm=1.005, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8249
2023-01-04 23:23:47 - progress_bar.py[line:274] - INFO: epoch 001:   3724 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.8, ups=0.88, wpb=109.2, bsz=40, num_updates=3720, lr=4.54657e-05, gnorm=0.938, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8260
2023-01-04 23:23:58 - progress_bar.py[line:274] - INFO: epoch 001:   3734 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=104.5, ups=0.95, wpb=109.6, bsz=40, num_updates=3730, lr=4.55879e-05, gnorm=0.799, clip=0, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=8271
2023-01-04 23:24:09 - progress_bar.py[line:274] - INFO: epoch 001:   3744 / 102288 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.4, ups=0.92, wpb=109.5, bsz=40, num_updates=3740, lr=4.57101e-05, gnorm=0.988, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8282
2023-01-04 23:24:20 - progress_bar.py[line:274] - INFO: epoch 001:   3754 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=3750, lr=4.58323e-05, gnorm=0.821, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8294
2023-01-04 23:24:32 - progress_bar.py[line:274] - INFO: epoch 001:   3764 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.7, ups=0.9, wpb=109.7, bsz=40, num_updates=3760, lr=4.59545e-05, gnorm=0.85, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=8305
2023-01-04 23:24:43 - progress_bar.py[line:274] - INFO: epoch 001:   3774 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=3770, lr=4.60768e-05, gnorm=0.78, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8317
2023-01-04 23:24:55 - progress_bar.py[line:274] - INFO: epoch 001:   3784 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=3780, lr=4.6199e-05, gnorm=0.958, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8328
2023-01-04 23:25:06 - progress_bar.py[line:274] - INFO: epoch 001:   3794 / 102288 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=3790, lr=4.63212e-05, gnorm=0.874, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8339
2023-01-04 23:25:17 - progress_bar.py[line:274] - INFO: epoch 001:   3804 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=3800, lr=4.64434e-05, gnorm=0.917, clip=40, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=8351
2023-01-04 23:25:29 - progress_bar.py[line:274] - INFO: epoch 001:   3814 / 102288 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=3810, lr=4.65656e-05, gnorm=0.965, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8362
2023-01-04 23:25:40 - progress_bar.py[line:274] - INFO: epoch 001:   3824 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.8, ups=0.94, wpb=110.2, bsz=40, num_updates=3820, lr=4.66879e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8373
2023-01-04 23:25:51 - progress_bar.py[line:274] - INFO: epoch 001:   3834 / 102288 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.8, ups=0.93, wpb=109.6, bsz=40, num_updates=3830, lr=4.68101e-05, gnorm=0.869, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8384
2023-01-04 23:26:02 - progress_bar.py[line:274] - INFO: epoch 001:   3844 / 102288 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=3840, lr=4.69323e-05, gnorm=0.892, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8395
2023-01-04 23:26:14 - progress_bar.py[line:274] - INFO: epoch 001:   3854 / 102288 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.4, ups=0.87, wpb=110.5, bsz=40, num_updates=3850, lr=4.70545e-05, gnorm=0.896, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8407
2023-01-04 23:26:25 - progress_bar.py[line:274] - INFO: epoch 001:   3864 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=3860, lr=4.71767e-05, gnorm=0.98, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8418
2023-01-04 23:26:26 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 23:26:37 - progress_bar.py[line:274] - INFO: epoch 001:   3875 / 102288 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=90.8, ups=0.83, wpb=109.8, bsz=40, num_updates=3870, lr=4.72989e-05, gnorm=1.008, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=8431
2023-01-04 23:26:49 - progress_bar.py[line:274] - INFO: epoch 001:   3885 / 102288 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.2, ups=0.9, wpb=109.6, bsz=40, num_updates=3880, lr=4.74212e-05, gnorm=0.925, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8442
2023-01-04 23:27:00 - progress_bar.py[line:274] - INFO: epoch 001:   3895 / 102288 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=3890, lr=4.75434e-05, gnorm=0.825, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8453
2023-01-04 23:27:11 - progress_bar.py[line:274] - INFO: epoch 001:   3905 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=3900, lr=4.76656e-05, gnorm=0.827, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8464
2023-01-04 23:27:22 - progress_bar.py[line:274] - INFO: epoch 001:   3915 / 102288 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.5, ups=0.91, wpb=109.9, bsz=40, num_updates=3910, lr=4.77878e-05, gnorm=0.956, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8476
2023-01-04 23:27:33 - progress_bar.py[line:274] - INFO: epoch 001:   3925 / 102288 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=3920, lr=4.791e-05, gnorm=0.94, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8487
2023-01-04 23:27:45 - progress_bar.py[line:274] - INFO: epoch 001:   3935 / 102288 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.3, ups=0.89, wpb=109.2, bsz=40, num_updates=3930, lr=4.80323e-05, gnorm=0.938, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8498
2023-01-04 23:27:56 - progress_bar.py[line:274] - INFO: epoch 001:   3945 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.5, ups=0.93, wpb=109.5, bsz=40, num_updates=3940, lr=4.81545e-05, gnorm=0.866, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8509
2023-01-04 23:28:07 - progress_bar.py[line:274] - INFO: epoch 001:   3955 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.3, ups=0.91, wpb=112.9, bsz=40, num_updates=3950, lr=4.82767e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8520
2023-01-04 23:28:18 - progress_bar.py[line:274] - INFO: epoch 001:   3965 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=3960, lr=4.83989e-05, gnorm=0.794, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8532
2023-01-04 23:28:30 - progress_bar.py[line:274] - INFO: epoch 001:   3975 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=3970, lr=4.85211e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8543
2023-01-04 23:28:41 - progress_bar.py[line:274] - INFO: epoch 001:   3985 / 102288 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.6, ups=0.9, wpb=111.1, bsz=40, num_updates=3980, lr=4.86434e-05, gnorm=0.841, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8554
2023-01-04 23:28:53 - progress_bar.py[line:274] - INFO: epoch 001:   3995 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=3990, lr=4.87656e-05, gnorm=0.959, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8566
2023-01-04 23:29:04 - progress_bar.py[line:274] - INFO: epoch 001:   4005 / 102288 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.3, ups=0.89, wpb=109.3, bsz=40, num_updates=4000, lr=4.88878e-05, gnorm=0.955, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8577
2023-01-04 23:29:04 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-04 23:29:05 - train.py[line:549] - INFO: 0 / 4988
2023-01-04 23:29:05 - train.py[line:551] - INFO: load:0.80 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-04 23:31:38 - train.py[line:549] - INFO: 200 / 4988
2023-01-04 23:31:38 - train.py[line:551] - INFO: load:0.83 valid_run:152.57 task_valid:148.75 collect_output:2.73
2023-01-04 23:34:06 - train.py[line:549] - INFO: 400 / 4988
2023-01-04 23:34:06 - train.py[line:551] - INFO: load:0.85 valid_run:301.09 task_valid:291.33 collect_output:7.63
2023-01-04 23:36:39 - train.py[line:549] - INFO: 600 / 4988
2023-01-04 23:36:39 - train.py[line:551] - INFO: load:0.88 valid_run:453.48 task_valid:433.91 collect_output:16.42
2023-01-04 23:39:08 - train.py[line:549] - INFO: 800 / 4988
2023-01-04 23:39:08 - train.py[line:551] - INFO: load:0.90 valid_run:602.79 task_valid:578.47 collect_output:20.17
2023-01-04 23:41:41 - train.py[line:549] - INFO: 1000 / 4988
2023-01-04 23:41:41 - train.py[line:551] - INFO: load:0.93 valid_run:755.09 task_valid:725.44 collect_output:24.48
2023-01-04 23:44:12 - train.py[line:549] - INFO: 1200 / 4988
2023-01-04 23:44:12 - train.py[line:551] - INFO: load:0.96 valid_run:906.89 task_valid:870.40 collect_output:30.31
2023-01-04 23:46:46 - train.py[line:549] - INFO: 1400 / 4988
2023-01-04 23:46:46 - train.py[line:551] - INFO: load:0.98 valid_run:1060.69 task_valid:1016.07 collect_output:37.42
2023-01-04 23:49:18 - train.py[line:549] - INFO: 1600 / 4988
2023-01-04 23:49:18 - train.py[line:551] - INFO: load:1.01 valid_run:1212.35 task_valid:1156.65 collect_output:47.45
2023-01-04 23:51:48 - train.py[line:549] - INFO: 1800 / 4988
2023-01-04 23:51:48 - train.py[line:551] - INFO: load:1.03 valid_run:1361.91 task_valid:1300.89 collect_output:51.76
2023-01-04 23:54:16 - train.py[line:549] - INFO: 2000 / 4988
2023-01-04 23:54:16 - train.py[line:551] - INFO: load:1.06 valid_run:1510.59 task_valid:1443.56 collect_output:56.74
2023-01-04 23:56:46 - train.py[line:549] - INFO: 2200 / 4988
2023-01-04 23:56:46 - train.py[line:551] - INFO: load:1.08 valid_run:1660.40 task_valid:1588.04 collect_output:61.07
2023-01-04 23:59:16 - train.py[line:549] - INFO: 2400 / 4988
2023-01-04 23:59:16 - train.py[line:551] - INFO: load:1.11 valid_run:1810.49 task_valid:1732.51 collect_output:65.66
2023-01-05 00:01:46 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 00:01:46 - train.py[line:551] - INFO: load:1.13 valid_run:1960.42 task_valid:1873.89 collect_output:73.19
2023-01-05 00:04:17 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 00:04:17 - train.py[line:551] - INFO: load:1.16 valid_run:2110.81 task_valid:2018.77 collect_output:77.66
2023-01-05 00:06:47 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 00:06:47 - train.py[line:551] - INFO: load:1.18 valid_run:2260.75 task_valid:2164.74 collect_output:80.65
2023-01-05 00:09:17 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 00:09:17 - train.py[line:551] - INFO: load:1.21 valid_run:2410.78 task_valid:2308.33 collect_output:86.10
2023-01-05 00:11:49 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 00:11:49 - train.py[line:551] - INFO: load:1.24 valid_run:2562.44 task_valid:2453.38 collect_output:91.68
2023-01-05 00:14:19 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 00:14:19 - train.py[line:551] - INFO: load:1.26 valid_run:2712.78 task_valid:2599.87 collect_output:94.51
2023-01-05 00:16:48 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 00:16:48 - train.py[line:551] - INFO: load:1.29 valid_run:2861.66 task_valid:2741.03 collect_output:101.21
2023-01-05 00:19:19 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 00:19:19 - train.py[line:551] - INFO: load:1.31 valid_run:3012.06 task_valid:2885.63 collect_output:106.00
2023-01-05 00:21:51 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 00:21:51 - train.py[line:551] - INFO: load:1.34 valid_run:3164.26 task_valid:3029.65 collect_output:113.17
2023-01-05 00:24:20 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 00:24:20 - train.py[line:551] - INFO: load:1.36 valid_run:3313.55 task_valid:3173.68 collect_output:117.41
2023-01-05 00:26:52 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 00:26:52 - train.py[line:551] - INFO: load:1.39 valid_run:3465.17 task_valid:3319.48 collect_output:122.20
2023-01-05 00:29:23 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 00:29:23 - train.py[line:551] - INFO: load:1.42 valid_run:3616.47 task_valid:3465.46 collect_output:126.50

====================================================================================================
SGG eval:     R @ 50: 0.5348;     R @ 100: 0.5828;     R @ 500: 0.6265;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3231;    mR @ 100: 0.3980;    mR @ 500: 0.4451;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5878) (covered in:0.2500) (covering:0.5143) (eating:0.7647) (flying in:0.8182) (growing on:0.5000) (hanging from:0.4323) (lying on:0.0500) (mounted on:0.0000) (painted on:0.0833) (parked on:0.7583) (playing:0.0000) (riding:0.8791) (says:0.0000) (sitting on:0.6922) (standing on:0.2450) (using:0.3500) (walking in:0.0000) (walking on:0.7297) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 00:31:54 - train.py[line:487] - INFO: 0.5827508785332315

====================================================================================================
SGG eval:     R @ 50: 0.5348;     R @ 100: 0.5828;     R @ 500: 0.6265;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3231;    mR @ 100: 0.3980;    mR @ 500: 0.4451;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5878) (covered in:0.2500) (covering:0.5143) (eating:0.7647) (flying in:0.8182) (growing on:0.5000) (hanging from:0.4323) (lying on:0.0500) (mounted on:0.0000) (painted on:0.0833) (parked on:0.7583) (playing:0.0000) (riding:0.8791) (says:0.0000) (sitting on:0.6922) (standing on:0.2450) (using:0.3500) (walking in:0.0000) (walking on:0.7297) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 00:31:54 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 00:31:54 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.396 | loss_v1 0 | loss_v2 0 | nll_loss 0.254 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.582751 | ppl 1.19 | vqa_score 0.4369 | wps 119 | wpb 89.9 | bsz 30 | num_updates 4000 | best_R@100 0.582751
2023-01-05 00:31:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-01-05 00:31:54 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-05 00:32:37 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-05 00:35:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.5827508785332315) (writing took 221.19409951008856 seconds)
2023-01-05 00:35:47 - progress_bar.py[line:274] - INFO: epoch 001:   4015 / 102288 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=0.3, ups=0, wpb=111.2, bsz=40, num_updates=4010, lr=4.901e-05, gnorm=0.935, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12580
2023-01-05 00:35:58 - progress_bar.py[line:274] - INFO: epoch 001:   4025 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=4020, lr=4.91322e-05, gnorm=0.776, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12591
2023-01-05 00:36:09 - progress_bar.py[line:274] - INFO: epoch 001:   4035 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=4030, lr=4.92545e-05, gnorm=0.769, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12603
2023-01-05 00:36:21 - progress_bar.py[line:274] - INFO: epoch 001:   4045 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=4040, lr=4.93767e-05, gnorm=0.782, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12614
2023-01-05 00:36:32 - progress_bar.py[line:274] - INFO: epoch 001:   4055 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=4050, lr=4.94989e-05, gnorm=0.772, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12625
2023-01-05 00:36:43 - progress_bar.py[line:274] - INFO: epoch 001:   4065 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.1, ups=0.9, wpb=111.6, bsz=40, num_updates=4060, lr=4.96211e-05, gnorm=0.845, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12637
2023-01-05 00:36:55 - progress_bar.py[line:274] - INFO: epoch 001:   4075 / 102288 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.7, ups=0.87, wpb=110.2, bsz=40, num_updates=4070, lr=4.97433e-05, gnorm=0.927, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12648
2023-01-05 00:37:06 - progress_bar.py[line:274] - INFO: epoch 001:   4085 / 102288 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.8, ups=0.9, wpb=108.5, bsz=40, num_updates=4080, lr=4.98656e-05, gnorm=0.871, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12660
2023-01-05 00:37:18 - progress_bar.py[line:274] - INFO: epoch 001:   4095 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=4090, lr=4.99878e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12671
2023-01-05 00:37:30 - progress_bar.py[line:274] - INFO: epoch 001:   4105 / 102288 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.1, ups=0.87, wpb=109.3, bsz=40, num_updates=4100, lr=4.99954e-05, gnorm=0.854, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12683
2023-01-05 00:37:41 - progress_bar.py[line:274] - INFO: epoch 001:   4115 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=4110, lr=4.99903e-05, gnorm=1.008, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12695
2023-01-05 00:37:53 - progress_bar.py[line:274] - INFO: epoch 001:   4125 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=4120, lr=4.99852e-05, gnorm=0.808, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12706
2023-01-05 00:38:04 - progress_bar.py[line:274] - INFO: epoch 001:   4135 / 102288 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.5, ups=0.9, wpb=109.1, bsz=40, num_updates=4130, lr=4.99801e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12717
2023-01-05 00:38:15 - progress_bar.py[line:274] - INFO: epoch 001:   4145 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.4, ups=0.9, wpb=108.8, bsz=40, num_updates=4140, lr=4.99751e-05, gnorm=0.818, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12729
2023-01-05 00:38:27 - progress_bar.py[line:274] - INFO: epoch 001:   4155 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=4150, lr=4.997e-05, gnorm=0.755, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12740
2023-01-05 00:38:38 - progress_bar.py[line:274] - INFO: epoch 001:   4165 / 102288 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.3, ups=0.88, wpb=109.2, bsz=40, num_updates=4160, lr=4.99649e-05, gnorm=0.818, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12751
2023-01-05 00:38:50 - progress_bar.py[line:274] - INFO: epoch 001:   4175 / 102288 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=4170, lr=4.99598e-05, gnorm=0.854, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12763
2023-01-05 00:39:01 - progress_bar.py[line:274] - INFO: epoch 001:   4185 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=4180, lr=4.99547e-05, gnorm=0.865, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12774
2023-01-05 00:39:12 - progress_bar.py[line:274] - INFO: epoch 001:   4195 / 102288 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99, ups=0.9, wpb=109.6, bsz=40, num_updates=4190, lr=4.99496e-05, gnorm=0.884, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12786
2023-01-05 00:39:24 - progress_bar.py[line:274] - INFO: epoch 001:   4205 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=4200, lr=4.99445e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12797
2023-01-05 00:39:35 - progress_bar.py[line:274] - INFO: epoch 001:   4215 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=4210, lr=4.99394e-05, gnorm=0.758, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12808
2023-01-05 00:39:46 - progress_bar.py[line:274] - INFO: epoch 001:   4225 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.2, ups=0.9, wpb=111, bsz=40, num_updates=4220, lr=4.99343e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12820
2023-01-05 00:39:58 - progress_bar.py[line:274] - INFO: epoch 001:   4235 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98, ups=0.89, wpb=109.5, bsz=40, num_updates=4230, lr=4.99292e-05, gnorm=0.846, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12831
2023-01-05 00:40:09 - progress_bar.py[line:274] - INFO: epoch 001:   4245 / 102288 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=4240, lr=4.99241e-05, gnorm=0.821, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12842
2023-01-05 00:40:21 - progress_bar.py[line:274] - INFO: epoch 001:   4255 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=4250, lr=4.9919e-05, gnorm=0.929, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12854
2023-01-05 00:40:32 - progress_bar.py[line:274] - INFO: epoch 001:   4265 / 102288 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=4260, lr=4.99139e-05, gnorm=0.879, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12865
2023-01-05 00:40:44 - progress_bar.py[line:274] - INFO: epoch 001:   4275 / 102288 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=4270, lr=4.99089e-05, gnorm=0.821, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12877
2023-01-05 00:40:55 - progress_bar.py[line:274] - INFO: epoch 001:   4285 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=4280, lr=4.99038e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12888
2023-01-05 00:41:06 - progress_bar.py[line:274] - INFO: epoch 001:   4295 / 102288 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.5, ups=0.89, wpb=109, bsz=40, num_updates=4290, lr=4.98987e-05, gnorm=0.788, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12900
2023-01-05 00:41:18 - progress_bar.py[line:274] - INFO: epoch 001:   4305 / 102288 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.7, ups=0.92, wpb=111, bsz=40, num_updates=4300, lr=4.98936e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12911
2023-01-05 00:41:29 - progress_bar.py[line:274] - INFO: epoch 001:   4315 / 102288 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.9, ups=0.89, wpb=110.6, bsz=40, num_updates=4310, lr=4.98885e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12922
2023-01-05 00:41:40 - progress_bar.py[line:274] - INFO: epoch 001:   4325 / 102288 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=4320, lr=4.98834e-05, gnorm=0.89, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12934
2023-01-05 00:41:53 - progress_bar.py[line:274] - INFO: epoch 001:   4335 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=4330, lr=4.98783e-05, gnorm=0.83, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12945
2023-01-05 00:42:04 - progress_bar.py[line:274] - INFO: epoch 001:   4345 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.9, ups=0.87, wpb=110.2, bsz=40, num_updates=4340, lr=4.98732e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=12958
2023-01-05 00:42:15 - progress_bar.py[line:274] - INFO: epoch 001:   4355 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=103.2, ups=0.93, wpb=111, bsz=40, num_updates=4350, lr=4.98681e-05, gnorm=0.808, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12969
2023-01-05 00:42:27 - progress_bar.py[line:274] - INFO: epoch 001:   4365 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.2, ups=0.88, wpb=108.9, bsz=40, num_updates=4360, lr=4.9863e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12980
2023-01-05 00:42:38 - progress_bar.py[line:274] - INFO: epoch 001:   4375 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.6, ups=0.9, wpb=111.2, bsz=40, num_updates=4370, lr=4.98579e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12991
2023-01-05 00:42:50 - progress_bar.py[line:274] - INFO: epoch 001:   4385 / 102288 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=94.7, ups=0.87, wpb=108.9, bsz=40, num_updates=4380, lr=4.98528e-05, gnorm=0.896, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13003
2023-01-05 00:42:59 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 00:43:02 - progress_bar.py[line:274] - INFO: epoch 001:   4396 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=92.8, ups=0.84, wpb=110.7, bsz=40, num_updates=4390, lr=4.98478e-05, gnorm=0.826, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13016
2023-01-05 00:43:13 - progress_bar.py[line:274] - INFO: epoch 001:   4406 / 102288 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=103.8, ups=0.94, wpb=110.7, bsz=40, num_updates=4400, lr=4.98427e-05, gnorm=0.831, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13027
2023-01-05 00:43:25 - progress_bar.py[line:274] - INFO: epoch 001:   4416 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=4410, lr=4.98376e-05, gnorm=0.749, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13038
2023-01-05 00:43:36 - progress_bar.py[line:274] - INFO: epoch 001:   4426 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=4420, lr=4.98325e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13050
2023-01-05 00:43:47 - progress_bar.py[line:274] - INFO: epoch 001:   4436 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.1, ups=0.95, wpb=109.1, bsz=40, num_updates=4430, lr=4.98274e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13060
2023-01-05 00:43:59 - progress_bar.py[line:274] - INFO: epoch 001:   4446 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=4440, lr=4.98223e-05, gnorm=0.889, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13072
2023-01-05 00:44:10 - progress_bar.py[line:274] - INFO: epoch 001:   4456 / 102288 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.1, ups=0.88, wpb=109.2, bsz=40, num_updates=4450, lr=4.98172e-05, gnorm=0.88, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13084
2023-01-05 00:44:21 - progress_bar.py[line:274] - INFO: epoch 001:   4466 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.5, ups=0.92, wpb=110.1, bsz=40, num_updates=4460, lr=4.98121e-05, gnorm=0.779, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13095
2023-01-05 00:44:33 - progress_bar.py[line:274] - INFO: epoch 001:   4476 / 102288 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=4470, lr=4.9807e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13106
2023-01-05 00:44:44 - progress_bar.py[line:274] - INFO: epoch 001:   4486 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.9, ups=0.9, wpb=109.5, bsz=40, num_updates=4480, lr=4.98019e-05, gnorm=0.784, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13117
2023-01-05 00:44:56 - progress_bar.py[line:274] - INFO: epoch 001:   4496 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.6, ups=0.9, wpb=110.2, bsz=40, num_updates=4490, lr=4.97968e-05, gnorm=0.822, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13129
2023-01-05 00:45:07 - progress_bar.py[line:274] - INFO: epoch 001:   4506 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=4500, lr=4.97917e-05, gnorm=0.78, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13140
2023-01-05 00:45:18 - progress_bar.py[line:274] - INFO: epoch 001:   4516 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=103.6, ups=0.93, wpb=111.3, bsz=40, num_updates=4510, lr=4.97867e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13151
2023-01-05 00:45:29 - progress_bar.py[line:274] - INFO: epoch 001:   4526 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=4520, lr=4.97816e-05, gnorm=0.817, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13163
2023-01-05 00:45:41 - progress_bar.py[line:274] - INFO: epoch 001:   4536 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.2, ups=0.89, wpb=110.5, bsz=40, num_updates=4530, lr=4.97765e-05, gnorm=0.756, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13174
2023-01-05 00:45:52 - progress_bar.py[line:274] - INFO: epoch 001:   4546 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=4540, lr=4.97714e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13186
2023-01-05 00:46:04 - progress_bar.py[line:274] - INFO: epoch 001:   4556 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.8, ups=0.89, wpb=110, bsz=40, num_updates=4550, lr=4.97663e-05, gnorm=0.761, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13197
2023-01-05 00:46:15 - progress_bar.py[line:274] - INFO: epoch 001:   4566 / 102288 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=4560, lr=4.97612e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13209
2023-01-05 00:46:26 - progress_bar.py[line:274] - INFO: epoch 001:   4576 / 102288 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.6, ups=0.92, wpb=109.8, bsz=40, num_updates=4570, lr=4.97561e-05, gnorm=0.761, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13220
2023-01-05 00:46:38 - progress_bar.py[line:274] - INFO: epoch 001:   4586 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.2, ups=0.9, wpb=109.6, bsz=40, num_updates=4580, lr=4.9751e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13231
2023-01-05 00:46:49 - progress_bar.py[line:274] - INFO: epoch 001:   4596 / 102288 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=95.6, ups=0.88, wpb=108.1, bsz=40, num_updates=4590, lr=4.97459e-05, gnorm=0.919, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13243
2023-01-05 00:47:01 - progress_bar.py[line:274] - INFO: epoch 001:   4606 / 102288 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=4600, lr=4.97408e-05, gnorm=0.798, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13254
2023-01-05 00:47:12 - progress_bar.py[line:274] - INFO: epoch 001:   4616 / 102288 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.3, ups=0.88, wpb=109.2, bsz=40, num_updates=4610, lr=4.97357e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13265
2023-01-05 00:47:23 - progress_bar.py[line:274] - INFO: epoch 001:   4626 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=4620, lr=4.97306e-05, gnorm=0.754, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13277
2023-01-05 00:47:35 - progress_bar.py[line:274] - INFO: epoch 001:   4636 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.9, ups=0.88, wpb=111.7, bsz=40, num_updates=4630, lr=4.97256e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13288
2023-01-05 00:47:47 - progress_bar.py[line:274] - INFO: epoch 001:   4646 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99, ups=0.89, wpb=111.8, bsz=40, num_updates=4640, lr=4.97205e-05, gnorm=0.751, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13300
2023-01-05 00:47:58 - progress_bar.py[line:274] - INFO: epoch 001:   4656 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=4650, lr=4.97154e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13311
2023-01-05 00:48:10 - progress_bar.py[line:274] - INFO: epoch 001:   4666 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=4660, lr=4.97103e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13323
2023-01-05 00:48:21 - progress_bar.py[line:274] - INFO: epoch 001:   4676 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=4670, lr=4.97052e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13334
2023-01-05 00:48:32 - progress_bar.py[line:274] - INFO: epoch 001:   4686 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.5, ups=0.91, wpb=108.7, bsz=40, num_updates=4680, lr=4.97001e-05, gnorm=0.809, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13346
2023-01-05 00:48:44 - progress_bar.py[line:274] - INFO: epoch 001:   4696 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=4690, lr=4.9695e-05, gnorm=0.786, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13357
2023-01-05 00:48:55 - progress_bar.py[line:274] - INFO: epoch 001:   4706 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.9, ups=0.89, wpb=110.5, bsz=40, num_updates=4700, lr=4.96899e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13368
2023-01-05 00:49:06 - progress_bar.py[line:274] - INFO: epoch 001:   4716 / 102288 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=4710, lr=4.96848e-05, gnorm=0.832, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13380
2023-01-05 00:49:18 - progress_bar.py[line:274] - INFO: epoch 001:   4726 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=4720, lr=4.96797e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13391
2023-01-05 00:49:29 - progress_bar.py[line:274] - INFO: epoch 001:   4736 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96, ups=0.87, wpb=110.4, bsz=40, num_updates=4730, lr=4.96746e-05, gnorm=0.826, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13403
2023-01-05 00:49:41 - progress_bar.py[line:274] - INFO: epoch 001:   4746 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.6, ups=0.9, wpb=110.5, bsz=40, num_updates=4740, lr=4.96695e-05, gnorm=0.759, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13414
2023-01-05 00:49:52 - progress_bar.py[line:274] - INFO: epoch 001:   4756 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=4750, lr=4.96645e-05, gnorm=0.91, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13425
2023-01-05 00:50:04 - progress_bar.py[line:274] - INFO: epoch 001:   4766 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=4760, lr=4.96594e-05, gnorm=0.935, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13437
2023-01-05 00:50:15 - progress_bar.py[line:274] - INFO: epoch 001:   4776 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99, ups=0.9, wpb=109.7, bsz=40, num_updates=4770, lr=4.96543e-05, gnorm=0.792, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13448
2023-01-05 00:50:26 - progress_bar.py[line:274] - INFO: epoch 001:   4786 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=4780, lr=4.96492e-05, gnorm=0.674, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13460
2023-01-05 00:50:38 - progress_bar.py[line:274] - INFO: epoch 001:   4796 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=4790, lr=4.96441e-05, gnorm=0.734, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=13471
2023-01-05 00:50:49 - progress_bar.py[line:274] - INFO: epoch 001:   4806 / 102288 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.3, ups=0.93, wpb=108.2, bsz=40, num_updates=4800, lr=4.9639e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13482
2023-01-05 00:51:00 - progress_bar.py[line:274] - INFO: epoch 001:   4816 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=4810, lr=4.96339e-05, gnorm=0.738, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13494
2023-01-05 00:51:12 - progress_bar.py[line:274] - INFO: epoch 001:   4826 / 102288 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.6, ups=0.9, wpb=110.2, bsz=40, num_updates=4820, lr=4.96288e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13505
2023-01-05 00:51:23 - progress_bar.py[line:274] - INFO: epoch 001:   4836 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=4830, lr=4.96237e-05, gnorm=0.793, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13516
2023-01-05 00:51:34 - progress_bar.py[line:274] - INFO: epoch 001:   4846 / 102288 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=4840, lr=4.96186e-05, gnorm=0.761, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13528
2023-01-05 00:51:46 - progress_bar.py[line:274] - INFO: epoch 001:   4856 / 102288 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=4850, lr=4.96135e-05, gnorm=0.739, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13539
2023-01-05 00:51:57 - progress_bar.py[line:274] - INFO: epoch 001:   4866 / 102288 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.7, ups=0.92, wpb=108.9, bsz=40, num_updates=4860, lr=4.96084e-05, gnorm=0.759, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13550
2023-01-05 00:52:09 - progress_bar.py[line:274] - INFO: epoch 001:   4876 / 102288 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=4870, lr=4.96033e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13562
2023-01-05 00:52:20 - progress_bar.py[line:274] - INFO: epoch 001:   4886 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=4880, lr=4.95983e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13573
2023-01-05 00:52:32 - progress_bar.py[line:274] - INFO: epoch 001:   4896 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96, ups=0.88, wpb=109, bsz=40, num_updates=4890, lr=4.95932e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13585
2023-01-05 00:52:43 - progress_bar.py[line:274] - INFO: epoch 001:   4906 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.4, ups=0.91, wpb=110.3, bsz=40, num_updates=4900, lr=4.95881e-05, gnorm=0.772, clip=10, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=13596
2023-01-05 00:52:54 - progress_bar.py[line:274] - INFO: epoch 001:   4916 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=4910, lr=4.9583e-05, gnorm=0.681, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13607
2023-01-05 00:53:05 - progress_bar.py[line:274] - INFO: epoch 001:   4926 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=4920, lr=4.95779e-05, gnorm=0.695, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13618
2023-01-05 00:53:16 - progress_bar.py[line:274] - INFO: epoch 001:   4936 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102, ups=0.91, wpb=111.7, bsz=40, num_updates=4930, lr=4.95728e-05, gnorm=0.7, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13630
2023-01-05 00:53:28 - progress_bar.py[line:274] - INFO: epoch 001:   4946 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=4940, lr=4.95677e-05, gnorm=0.674, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13641
2023-01-05 00:53:39 - progress_bar.py[line:274] - INFO: epoch 001:   4956 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=4950, lr=4.95626e-05, gnorm=0.724, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13652
2023-01-05 00:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   4966 / 102288 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100, ups=0.91, wpb=110.3, bsz=40, num_updates=4960, lr=4.95575e-05, gnorm=0.8, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13664
2023-01-05 00:54:02 - progress_bar.py[line:274] - INFO: epoch 001:   4976 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.9, ups=0.91, wpb=111.3, bsz=40, num_updates=4970, lr=4.95524e-05, gnorm=0.779, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13675
2023-01-05 00:54:13 - progress_bar.py[line:274] - INFO: epoch 001:   4986 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.1, ups=0.91, wpb=110.6, bsz=40, num_updates=4980, lr=4.95473e-05, gnorm=0.641, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13686
2023-01-05 00:54:24 - progress_bar.py[line:274] - INFO: epoch 001:   4996 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=4990, lr=4.95422e-05, gnorm=0.765, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13698
2023-01-05 00:54:36 - progress_bar.py[line:274] - INFO: epoch 001:   5006 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=5000, lr=4.95372e-05, gnorm=0.809, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13709
2023-01-05 00:54:47 - progress_bar.py[line:274] - INFO: epoch 001:   5016 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.9, ups=0.89, wpb=111.8, bsz=40, num_updates=5010, lr=4.95321e-05, gnorm=0.64, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13720
2023-01-05 00:54:58 - progress_bar.py[line:274] - INFO: epoch 001:   5026 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.2, ups=0.89, wpb=109.7, bsz=40, num_updates=5020, lr=4.9527e-05, gnorm=0.703, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13732
2023-01-05 00:55:10 - progress_bar.py[line:274] - INFO: epoch 001:   5036 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99, ups=0.9, wpb=109.6, bsz=40, num_updates=5030, lr=4.95219e-05, gnorm=0.768, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13743
2023-01-05 00:55:12 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 00:55:22 - progress_bar.py[line:274] - INFO: epoch 001:   5047 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=89.7, ups=0.82, wpb=108.9, bsz=40, num_updates=5040, lr=4.95168e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13755
2023-01-05 00:55:33 - progress_bar.py[line:274] - INFO: epoch 001:   5057 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97, ups=0.89, wpb=109.1, bsz=40, num_updates=5050, lr=4.95117e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13767
2023-01-05 00:55:45 - progress_bar.py[line:274] - INFO: epoch 001:   5067 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=5060, lr=4.95066e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13778
2023-01-05 00:55:57 - progress_bar.py[line:274] - INFO: epoch 001:   5077 / 102288 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.5, ups=0.87, wpb=110.8, bsz=40, num_updates=5070, lr=4.95015e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13790
2023-01-05 00:56:08 - progress_bar.py[line:274] - INFO: epoch 001:   5087 / 102288 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.9, ups=0.91, wpb=110.2, bsz=40, num_updates=5080, lr=4.94964e-05, gnorm=0.704, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13801
2023-01-05 00:56:19 - progress_bar.py[line:274] - INFO: epoch 001:   5097 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=5090, lr=4.94913e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13813
2023-01-05 00:56:31 - progress_bar.py[line:274] - INFO: epoch 001:   5107 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=5100, lr=4.94862e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13824
2023-01-05 00:56:42 - progress_bar.py[line:274] - INFO: epoch 001:   5117 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98, ups=0.89, wpb=110, bsz=40, num_updates=5110, lr=4.94811e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13836
2023-01-05 00:56:54 - progress_bar.py[line:274] - INFO: epoch 001:   5127 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=5120, lr=4.94761e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13847
2023-01-05 00:57:05 - progress_bar.py[line:274] - INFO: epoch 001:   5137 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.7, ups=0.87, wpb=110.4, bsz=40, num_updates=5130, lr=4.9471e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=13859
2023-01-05 00:57:16 - progress_bar.py[line:274] - INFO: epoch 001:   5147 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=105.1, ups=0.95, wpb=111.1, bsz=40, num_updates=5140, lr=4.94659e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13869
2023-01-05 00:57:28 - progress_bar.py[line:274] - INFO: epoch 001:   5157 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=5150, lr=4.94608e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13881
2023-01-05 00:57:39 - progress_bar.py[line:274] - INFO: epoch 001:   5167 / 102288 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=5160, lr=4.94557e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13892
2023-01-05 00:57:50 - progress_bar.py[line:274] - INFO: epoch 001:   5177 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=104.6, ups=0.93, wpb=112.1, bsz=40, num_updates=5170, lr=4.94506e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13903
2023-01-05 00:58:01 - progress_bar.py[line:274] - INFO: epoch 001:   5187 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=5180, lr=4.94455e-05, gnorm=0.601, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13914
2023-01-05 00:58:12 - progress_bar.py[line:274] - INFO: epoch 001:   5197 / 102288 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.4, ups=0.91, wpb=111.5, bsz=40, num_updates=5190, lr=4.94404e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13926
2023-01-05 00:58:24 - progress_bar.py[line:274] - INFO: epoch 001:   5207 / 102288 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=5200, lr=4.94353e-05, gnorm=0.75, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13937
2023-01-05 00:58:35 - progress_bar.py[line:274] - INFO: epoch 001:   5217 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.8, ups=0.92, wpb=110, bsz=40, num_updates=5210, lr=4.94302e-05, gnorm=0.612, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13948
2023-01-05 00:58:46 - progress_bar.py[line:274] - INFO: epoch 001:   5227 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.1, ups=0.88, wpb=109.2, bsz=40, num_updates=5220, lr=4.94251e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13960
2023-01-05 00:58:58 - progress_bar.py[line:274] - INFO: epoch 001:   5237 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.7, ups=0.9, wpb=109.1, bsz=40, num_updates=5230, lr=4.942e-05, gnorm=0.771, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13971
2023-01-05 00:59:09 - progress_bar.py[line:274] - INFO: epoch 001:   5247 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.6, ups=0.92, wpb=109.5, bsz=40, num_updates=5240, lr=4.9415e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13982
2023-01-05 00:59:20 - progress_bar.py[line:274] - INFO: epoch 001:   5257 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.4, ups=0.87, wpb=109.8, bsz=40, num_updates=5250, lr=4.94099e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13994
2023-01-05 00:59:32 - progress_bar.py[line:274] - INFO: epoch 001:   5267 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=5260, lr=4.94048e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14005
2023-01-05 00:59:43 - progress_bar.py[line:274] - INFO: epoch 001:   5277 / 102288 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.9, ups=0.88, wpb=108.9, bsz=40, num_updates=5270, lr=4.93997e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14017
2023-01-05 00:59:54 - progress_bar.py[line:274] - INFO: epoch 001:   5287 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.6, ups=0.91, wpb=108.9, bsz=40, num_updates=5280, lr=4.93946e-05, gnorm=0.824, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14028
2023-01-05 01:00:05 - progress_bar.py[line:274] - INFO: epoch 001:   5297 / 102288 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.3, ups=0.93, wpb=110.1, bsz=40, num_updates=5290, lr=4.93895e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14039
2023-01-05 01:00:17 - progress_bar.py[line:274] - INFO: epoch 001:   5307 / 102288 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=5300, lr=4.93844e-05, gnorm=0.822, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14050
2023-01-05 01:00:28 - progress_bar.py[line:274] - INFO: epoch 001:   5317 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=103.5, ups=0.93, wpb=110.8, bsz=40, num_updates=5310, lr=4.93793e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14061
2023-01-05 01:00:39 - progress_bar.py[line:274] - INFO: epoch 001:   5327 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.1, ups=0.93, wpb=110, bsz=40, num_updates=5320, lr=4.93742e-05, gnorm=0.786, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14072
2023-01-05 01:00:51 - progress_bar.py[line:274] - INFO: epoch 001:   5337 / 102288 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=5330, lr=4.93691e-05, gnorm=0.848, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14084
2023-01-05 01:01:02 - progress_bar.py[line:274] - INFO: epoch 001:   5347 / 102288 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.4, ups=0.88, wpb=110.8, bsz=40, num_updates=5340, lr=4.9364e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14095
2023-01-05 01:01:13 - progress_bar.py[line:274] - INFO: epoch 001:   5357 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.2, ups=0.92, wpb=110.1, bsz=40, num_updates=5350, lr=4.93589e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14106
2023-01-05 01:01:25 - progress_bar.py[line:274] - INFO: epoch 001:   5367 / 102288 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=5360, lr=4.93538e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=14118
2023-01-05 01:01:36 - progress_bar.py[line:274] - INFO: epoch 001:   5377 / 102288 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=5370, lr=4.93488e-05, gnorm=0.77, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14129
2023-01-05 01:01:47 - progress_bar.py[line:274] - INFO: epoch 001:   5387 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.6, ups=0.92, wpb=108.8, bsz=40, num_updates=5380, lr=4.93437e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14141
2023-01-05 01:01:59 - progress_bar.py[line:274] - INFO: epoch 001:   5397 / 102288 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=5390, lr=4.93386e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14152
2023-01-05 01:02:09 - progress_bar.py[line:274] - INFO: epoch 001:   5407 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.9, ups=0.93, wpb=110.6, bsz=40, num_updates=5400, lr=4.93335e-05, gnorm=0.798, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14163
2023-01-05 01:02:21 - progress_bar.py[line:274] - INFO: epoch 001:   5417 / 102288 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=5410, lr=4.93284e-05, gnorm=0.758, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14174
2023-01-05 01:02:32 - progress_bar.py[line:274] - INFO: epoch 001:   5427 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.6, ups=0.9, wpb=111.2, bsz=40, num_updates=5420, lr=4.93233e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14185
2023-01-05 01:02:43 - progress_bar.py[line:274] - INFO: epoch 001:   5437 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=5430, lr=4.93182e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14197
2023-01-05 01:02:55 - progress_bar.py[line:274] - INFO: epoch 001:   5447 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=5440, lr=4.93131e-05, gnorm=0.716, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14208
2023-01-05 01:03:06 - progress_bar.py[line:274] - INFO: epoch 001:   5457 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=5450, lr=4.9308e-05, gnorm=0.728, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14219
2023-01-05 01:03:17 - progress_bar.py[line:274] - INFO: epoch 001:   5467 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=5460, lr=4.93029e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14231
2023-01-05 01:03:29 - progress_bar.py[line:274] - INFO: epoch 001:   5477 / 102288 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=5470, lr=4.92978e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14242
2023-01-05 01:03:40 - progress_bar.py[line:274] - INFO: epoch 001:   5487 / 102288 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=102.8, ups=0.93, wpb=110.7, bsz=40, num_updates=5480, lr=4.92927e-05, gnorm=0.638, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14253
2023-01-05 01:03:51 - progress_bar.py[line:274] - INFO: epoch 001:   5497 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.3, ups=0.93, wpb=109.1, bsz=40, num_updates=5490, lr=4.92877e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14264
2023-01-05 01:04:02 - progress_bar.py[line:274] - INFO: epoch 001:   5507 / 102288 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=5500, lr=4.92826e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14275
2023-01-05 01:04:13 - progress_bar.py[line:274] - INFO: epoch 001:   5517 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=103.9, ups=0.93, wpb=112, bsz=40, num_updates=5510, lr=4.92775e-05, gnorm=0.65, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14286
2023-01-05 01:04:25 - progress_bar.py[line:274] - INFO: epoch 001:   5527 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.5, ups=0.9, wpb=109.1, bsz=40, num_updates=5520, lr=4.92724e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14298
2023-01-05 01:04:36 - progress_bar.py[line:274] - INFO: epoch 001:   5537 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.5, ups=0.87, wpb=111.5, bsz=40, num_updates=5530, lr=4.92673e-05, gnorm=0.571, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=14310
2023-01-05 01:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   5547 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=5540, lr=4.92622e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14321
2023-01-05 01:04:59 - progress_bar.py[line:274] - INFO: epoch 001:   5557 / 102288 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=5550, lr=4.92571e-05, gnorm=0.626, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14333
2023-01-05 01:05:11 - progress_bar.py[line:274] - INFO: epoch 001:   5567 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.2, ups=0.89, wpb=109.3, bsz=40, num_updates=5560, lr=4.9252e-05, gnorm=0.722, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14344
2023-01-05 01:05:22 - progress_bar.py[line:274] - INFO: epoch 001:   5577 / 102288 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=95.3, ups=0.89, wpb=107.4, bsz=40, num_updates=5570, lr=4.92469e-05, gnorm=0.799, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14355
2023-01-05 01:05:33 - progress_bar.py[line:274] - INFO: epoch 001:   5587 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102, ups=0.92, wpb=111.2, bsz=40, num_updates=5580, lr=4.92418e-05, gnorm=0.739, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14367
2023-01-05 01:05:45 - progress_bar.py[line:274] - INFO: epoch 001:   5597 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.8, ups=0.9, wpb=109.9, bsz=40, num_updates=5590, lr=4.92367e-05, gnorm=0.71, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14378
2023-01-05 01:05:56 - progress_bar.py[line:274] - INFO: epoch 001:   5607 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.7, ups=0.89, wpb=110.1, bsz=40, num_updates=5600, lr=4.92316e-05, gnorm=0.772, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14389
2023-01-05 01:06:07 - progress_bar.py[line:274] - INFO: epoch 001:   5617 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.5, ups=0.9, wpb=110.6, bsz=40, num_updates=5610, lr=4.92266e-05, gnorm=0.639, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14401
2023-01-05 01:06:19 - progress_bar.py[line:274] - INFO: epoch 001:   5627 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.7, ups=0.87, wpb=109.8, bsz=40, num_updates=5620, lr=4.92215e-05, gnorm=0.75, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14412
2023-01-05 01:06:31 - progress_bar.py[line:274] - INFO: epoch 001:   5637 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.8, ups=0.88, wpb=109.3, bsz=40, num_updates=5630, lr=4.92164e-05, gnorm=0.719, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14424
2023-01-05 01:06:36 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 01:06:43 - progress_bar.py[line:274] - INFO: epoch 001:   5648 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=91.4, ups=0.83, wpb=110.7, bsz=40, num_updates=5640, lr=4.92113e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=14436
2023-01-05 01:06:54 - progress_bar.py[line:274] - INFO: epoch 001:   5658 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=103.5, ups=0.94, wpb=109.9, bsz=40, num_updates=5650, lr=4.92062e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14447
2023-01-05 01:07:05 - progress_bar.py[line:274] - INFO: epoch 001:   5668 / 102288 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.9, ups=0.93, wpb=110.3, bsz=40, num_updates=5660, lr=4.92011e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14458
2023-01-05 01:07:16 - progress_bar.py[line:274] - INFO: epoch 001:   5678 / 102288 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.6, ups=0.92, wpb=109.8, bsz=40, num_updates=5670, lr=4.9196e-05, gnorm=0.732, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=14469
2023-01-05 01:07:27 - progress_bar.py[line:274] - INFO: epoch 001:   5688 / 102288 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.6, ups=0.9, wpb=109.3, bsz=40, num_updates=5680, lr=4.91909e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14480
2023-01-05 01:07:38 - progress_bar.py[line:274] - INFO: epoch 001:   5698 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.8, ups=0.9, wpb=109.2, bsz=40, num_updates=5690, lr=4.91858e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14492
2023-01-05 01:07:50 - progress_bar.py[line:274] - INFO: epoch 001:   5708 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.2, ups=0.87, wpb=110.4, bsz=40, num_updates=5700, lr=4.91807e-05, gnorm=0.708, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14503
2023-01-05 01:08:01 - progress_bar.py[line:274] - INFO: epoch 001:   5718 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.5, ups=0.91, wpb=109.9, bsz=40, num_updates=5710, lr=4.91756e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14515
2023-01-05 01:08:13 - progress_bar.py[line:274] - INFO: epoch 001:   5728 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.3, ups=0.9, wpb=112, bsz=40, num_updates=5720, lr=4.91705e-05, gnorm=0.68, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14526
2023-01-05 01:08:24 - progress_bar.py[line:274] - INFO: epoch 001:   5738 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.8, ups=0.89, wpb=109.1, bsz=40, num_updates=5730, lr=4.91655e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14537
2023-01-05 01:08:35 - progress_bar.py[line:274] - INFO: epoch 001:   5748 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.3, ups=0.93, wpb=109.4, bsz=40, num_updates=5740, lr=4.91604e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14548
2023-01-05 01:08:47 - progress_bar.py[line:274] - INFO: epoch 001:   5758 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.4, ups=0.88, wpb=110.9, bsz=40, num_updates=5750, lr=4.91553e-05, gnorm=0.629, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14560
2023-01-05 01:08:58 - progress_bar.py[line:274] - INFO: epoch 001:   5768 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=5760, lr=4.91502e-05, gnorm=0.676, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14571
2023-01-05 01:09:09 - progress_bar.py[line:274] - INFO: epoch 001:   5778 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=5770, lr=4.91451e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14583
2023-01-05 01:09:21 - progress_bar.py[line:274] - INFO: epoch 001:   5788 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=5780, lr=4.914e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14594
2023-01-05 01:09:32 - progress_bar.py[line:274] - INFO: epoch 001:   5798 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.5, ups=0.88, wpb=109.8, bsz=40, num_updates=5790, lr=4.91349e-05, gnorm=0.689, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14606
2023-01-05 01:09:43 - progress_bar.py[line:274] - INFO: epoch 001:   5808 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.1, ups=0.93, wpb=109, bsz=40, num_updates=5800, lr=4.91298e-05, gnorm=0.897, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14617
2023-01-05 01:09:54 - progress_bar.py[line:274] - INFO: epoch 001:   5818 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.8, ups=0.9, wpb=109.5, bsz=40, num_updates=5810, lr=4.91247e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14628
2023-01-05 01:10:06 - progress_bar.py[line:274] - INFO: epoch 001:   5828 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.5, ups=0.88, wpb=110.9, bsz=40, num_updates=5820, lr=4.91196e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14639
2023-01-05 01:10:18 - progress_bar.py[line:274] - INFO: epoch 001:   5838 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.9, ups=0.88, wpb=110.2, bsz=40, num_updates=5830, lr=4.91145e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14651
2023-01-05 01:10:29 - progress_bar.py[line:274] - INFO: epoch 001:   5848 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=5840, lr=4.91094e-05, gnorm=0.699, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14662
2023-01-05 01:10:40 - progress_bar.py[line:274] - INFO: epoch 001:   5858 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.6, ups=0.91, wpb=111.2, bsz=40, num_updates=5850, lr=4.91044e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14674
2023-01-05 01:10:51 - progress_bar.py[line:274] - INFO: epoch 001:   5868 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=103.4, ups=0.95, wpb=109.4, bsz=40, num_updates=5860, lr=4.90993e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14684
2023-01-05 01:11:03 - progress_bar.py[line:274] - INFO: epoch 001:   5878 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=95, ups=0.87, wpb=109.6, bsz=40, num_updates=5870, lr=4.90942e-05, gnorm=0.709, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14696
2023-01-05 01:11:14 - progress_bar.py[line:274] - INFO: epoch 001:   5888 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.9, wpb=111.1, bsz=40, num_updates=5880, lr=4.90891e-05, gnorm=0.746, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14708
2023-01-05 01:11:26 - progress_bar.py[line:274] - INFO: epoch 001:   5898 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.7, ups=0.9, wpb=109.6, bsz=40, num_updates=5890, lr=4.9084e-05, gnorm=0.705, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14719
2023-01-05 01:11:37 - progress_bar.py[line:274] - INFO: epoch 001:   5908 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=103.3, ups=0.94, wpb=110.2, bsz=40, num_updates=5900, lr=4.90789e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14730
2023-01-05 01:11:48 - progress_bar.py[line:274] - INFO: epoch 001:   5918 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=5910, lr=4.90738e-05, gnorm=0.774, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14741
2023-01-05 01:12:00 - progress_bar.py[line:274] - INFO: epoch 001:   5928 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.2, ups=0.87, wpb=110.9, bsz=40, num_updates=5920, lr=4.90687e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14753
2023-01-05 01:12:11 - progress_bar.py[line:274] - INFO: epoch 001:   5938 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.7, ups=0.89, wpb=110.2, bsz=40, num_updates=5930, lr=4.90636e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14764
2023-01-05 01:12:23 - progress_bar.py[line:274] - INFO: epoch 001:   5948 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=5940, lr=4.90585e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14776
2023-01-05 01:12:34 - progress_bar.py[line:274] - INFO: epoch 001:   5958 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.3, ups=0.93, wpb=110.4, bsz=40, num_updates=5950, lr=4.90534e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14787
2023-01-05 01:12:44 - progress_bar.py[line:274] - INFO: epoch 001:   5968 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.4, ups=0.93, wpb=110.2, bsz=40, num_updates=5960, lr=4.90483e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14798
2023-01-05 01:12:56 - progress_bar.py[line:274] - INFO: epoch 001:   5978 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=5970, lr=4.90432e-05, gnorm=0.703, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14809
2023-01-05 01:13:07 - progress_bar.py[line:274] - INFO: epoch 001:   5988 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=5980, lr=4.90382e-05, gnorm=0.735, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14821
2023-01-05 01:13:18 - progress_bar.py[line:274] - INFO: epoch 001:   5998 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.6, ups=0.92, wpb=108.7, bsz=40, num_updates=5990, lr=4.90331e-05, gnorm=0.68, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14832
2023-01-05 01:13:30 - progress_bar.py[line:274] - INFO: epoch 001:   6008 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=6000, lr=4.9028e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14843
2023-01-05 01:13:30 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 01:13:31 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 01:13:31 - train.py[line:551] - INFO: load:0.99 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 01:16:03 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 01:16:03 - train.py[line:551] - INFO: load:1.01 valid_run:152.19 task_valid:148.25 collect_output:2.90
2023-01-05 01:18:32 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 01:18:32 - train.py[line:551] - INFO: load:1.03 valid_run:301.15 task_valid:291.21 collect_output:7.86
2023-01-05 01:21:05 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 01:21:05 - train.py[line:551] - INFO: load:1.06 valid_run:453.86 task_valid:433.90 collect_output:16.88
2023-01-05 01:23:35 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 01:23:35 - train.py[line:551] - INFO: load:1.08 valid_run:603.28 task_valid:578.58 collect_output:20.61
2023-01-05 01:26:07 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 01:26:07 - train.py[line:551] - INFO: load:1.10 valid_run:755.93 task_valid:725.76 collect_output:25.04
2023-01-05 01:28:40 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 01:28:40 - train.py[line:551] - INFO: load:1.13 valid_run:908.17 task_valid:870.99 collect_output:31.04
2023-01-05 01:31:13 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 01:31:13 - train.py[line:551] - INFO: load:1.15 valid_run:1062.04 task_valid:1016.60 collect_output:38.27
2023-01-05 01:33:45 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 01:33:45 - train.py[line:551] - INFO: load:1.18 valid_run:1213.52 task_valid:1157.21 collect_output:48.13
2023-01-05 01:36:15 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 01:36:15 - train.py[line:551] - INFO: load:1.20 valid_run:1363.58 task_valid:1301.75 collect_output:52.57
2023-01-05 01:38:44 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 01:38:44 - train.py[line:551] - INFO: load:1.23 valid_run:1512.51 task_valid:1444.67 collect_output:57.57
2023-01-05 01:41:14 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 01:41:14 - train.py[line:551] - INFO: load:1.25 valid_run:1662.50 task_valid:1589.20 collect_output:61.98
2023-01-05 01:43:44 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 01:43:44 - train.py[line:551] - INFO: load:1.28 valid_run:1812.60 task_valid:1733.84 collect_output:66.43
2023-01-05 01:46:15 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 01:46:15 - train.py[line:551] - INFO: load:1.30 valid_run:1963.05 task_valid:1875.27 collect_output:74.38
2023-01-05 01:48:46 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 01:48:46 - train.py[line:551] - INFO: load:1.33 valid_run:2113.73 task_valid:2020.43 collect_output:78.86
2023-01-05 01:51:16 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 01:51:16 - train.py[line:551] - INFO: load:1.35 valid_run:2263.55 task_valid:2166.39 collect_output:81.72
2023-01-05 01:53:46 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 01:53:46 - train.py[line:551] - INFO: load:1.38 valid_run:2413.75 task_valid:2310.34 collect_output:86.87
2023-01-05 01:56:18 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 01:56:18 - train.py[line:551] - INFO: load:1.41 valid_run:2565.73 task_valid:2455.57 collect_output:92.59
2023-01-05 01:58:48 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 01:58:48 - train.py[line:551] - INFO: load:1.43 valid_run:2716.22 task_valid:2602.24 collect_output:95.37
2023-01-05 02:01:17 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 02:01:17 - train.py[line:551] - INFO: load:1.46 valid_run:2864.84 task_valid:2743.59 collect_output:101.60
2023-01-05 02:03:48 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 02:03:48 - train.py[line:551] - INFO: load:1.48 valid_run:3015.59 task_valid:2888.47 collect_output:106.42
2023-01-05 02:06:21 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 02:06:21 - train.py[line:551] - INFO: load:1.51 valid_run:3168.14 task_valid:3032.95 collect_output:113.44
2023-01-05 02:08:50 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 02:08:50 - train.py[line:551] - INFO: load:1.53 valid_run:3317.48 task_valid:3177.09 collect_output:117.60
2023-01-05 02:11:21 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 02:11:21 - train.py[line:551] - INFO: load:1.56 valid_run:3468.81 task_valid:3322.97 collect_output:122.02
2023-01-05 02:13:53 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 02:13:53 - train.py[line:551] - INFO: load:1.58 valid_run:3620.20 task_valid:3469.08 collect_output:126.26

====================================================================================================
SGG eval:     R @ 50: 0.5745;     R @ 100: 0.6112;     R @ 500: 0.6448;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4044;    mR @ 100: 0.4614;    mR @ 500: 0.5137;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6049) (covered in:0.6875) (covering:0.3714) (eating:0.7647) (flying in:0.8636) (growing on:0.3750) (hanging from:0.3871) (lying on:0.0500) (mounted on:0.0476) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9183) (says:0.0000) (sitting on:0.7387) (standing on:0.1200) (using:0.6000) (walking in:0.3333) (walking on:0.8649) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-01-05 02:16:24 - train.py[line:487] - INFO: 0.6111993633817162

====================================================================================================
SGG eval:     R @ 50: 0.5745;     R @ 100: 0.6112;     R @ 500: 0.6448;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4044;    mR @ 100: 0.4614;    mR @ 500: 0.5137;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6049) (covered in:0.6875) (covering:0.3714) (eating:0.7647) (flying in:0.8636) (growing on:0.3750) (hanging from:0.3871) (lying on:0.0500) (mounted on:0.0476) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9183) (says:0.0000) (sitting on:0.7387) (standing on:0.1200) (using:0.6000) (walking in:0.3333) (walking on:0.8649) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-01-05 02:16:24 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 02:16:25 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.383 | loss_v1 0 | loss_v2 0 | nll_loss 0.235 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.611199 | ppl 1.18 | vqa_score 0.5203 | wps 118.9 | wpb 89.9 | bsz 30 | num_updates 6000 | best_R@100 0.611199
2023-01-05 02:16:25 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2023-01-05 02:16:25 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-05 02:17:13 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-05 02:20:25 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6111993633817162) (writing took 239.92695727944374 seconds)
2023-01-05 02:20:36 - progress_bar.py[line:274] - INFO: epoch 001:   6018 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=0.3, ups=0, wpb=109, bsz=40, num_updates=6010, lr=4.90229e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18869
2023-01-05 02:20:47 - progress_bar.py[line:274] - INFO: epoch 001:   6028 / 102288 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.3, ups=0.9, wpb=111.3, bsz=40, num_updates=6020, lr=4.90178e-05, gnorm=0.839, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18881
2023-01-05 02:20:59 - progress_bar.py[line:274] - INFO: epoch 001:   6038 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=6030, lr=4.90127e-05, gnorm=0.565, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18892
2023-01-05 02:21:10 - progress_bar.py[line:274] - INFO: epoch 001:   6048 / 102288 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.1, ups=0.87, wpb=109.7, bsz=40, num_updates=6040, lr=4.90076e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=18904
2023-01-05 02:21:22 - progress_bar.py[line:274] - INFO: epoch 001:   6058 / 102288 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.4, ups=0.88, wpb=108.3, bsz=40, num_updates=6050, lr=4.90025e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=18915
2023-01-05 02:21:33 - progress_bar.py[line:274] - INFO: epoch 001:   6068 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.2, ups=0.91, wpb=109.8, bsz=40, num_updates=6060, lr=4.89974e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18926
2023-01-05 02:21:44 - progress_bar.py[line:274] - INFO: epoch 001:   6078 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=102.9, ups=0.92, wpb=112.4, bsz=40, num_updates=6070, lr=4.89923e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18938
2023-01-05 02:21:55 - progress_bar.py[line:274] - INFO: epoch 001:   6088 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=6080, lr=4.89872e-05, gnorm=0.59, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18949
2023-01-05 02:22:06 - progress_bar.py[line:274] - INFO: epoch 001:   6098 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=102.2, ups=0.93, wpb=110.1, bsz=40, num_updates=6090, lr=4.89821e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18960
2023-01-05 02:22:18 - progress_bar.py[line:274] - INFO: epoch 001:   6108 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.8, ups=0.89, wpb=110.1, bsz=40, num_updates=6100, lr=4.89771e-05, gnorm=0.689, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18971
2023-01-05 02:22:29 - progress_bar.py[line:274] - INFO: epoch 001:   6118 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.6, ups=0.9, wpb=110.6, bsz=40, num_updates=6110, lr=4.8972e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18983
2023-01-05 02:22:41 - progress_bar.py[line:274] - INFO: epoch 001:   6128 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=6120, lr=4.89669e-05, gnorm=0.744, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=18994
2023-01-05 02:22:52 - progress_bar.py[line:274] - INFO: epoch 001:   6138 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.2, ups=0.89, wpb=109.1, bsz=40, num_updates=6130, lr=4.89618e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19005
2023-01-05 02:23:04 - progress_bar.py[line:274] - INFO: epoch 001:   6148 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=6140, lr=4.89567e-05, gnorm=0.682, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19017
2023-01-05 02:23:15 - progress_bar.py[line:274] - INFO: epoch 001:   6158 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.5, ups=0.92, wpb=108.2, bsz=40, num_updates=6150, lr=4.89516e-05, gnorm=0.735, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19028
2023-01-05 02:23:26 - progress_bar.py[line:274] - INFO: epoch 001:   6168 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.8, ups=0.9, wpb=110.6, bsz=40, num_updates=6160, lr=4.89465e-05, gnorm=0.78, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19040
2023-01-05 02:23:38 - progress_bar.py[line:274] - INFO: epoch 001:   6178 / 102288 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.7, ups=0.89, wpb=110.3, bsz=40, num_updates=6170, lr=4.89414e-05, gnorm=0.687, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19051
2023-01-05 02:23:49 - progress_bar.py[line:274] - INFO: epoch 001:   6188 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.2, ups=0.93, wpb=110.3, bsz=40, num_updates=6180, lr=4.89363e-05, gnorm=0.624, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19062
2023-01-05 02:24:01 - progress_bar.py[line:274] - INFO: epoch 001:   6198 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.2, ups=0.88, wpb=109.8, bsz=40, num_updates=6190, lr=4.89312e-05, gnorm=0.631, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19074
2023-01-05 02:24:11 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 02:24:13 - progress_bar.py[line:274] - INFO: epoch 001:   6209 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=92.8, ups=0.84, wpb=110, bsz=40, num_updates=6200, lr=4.89261e-05, gnorm=0.607, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=19086
2023-01-05 02:24:24 - progress_bar.py[line:274] - INFO: epoch 001:   6219 / 102288 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.4, ups=0.94, wpb=107.9, bsz=40, num_updates=6210, lr=4.8921e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19097
2023-01-05 02:24:35 - progress_bar.py[line:274] - INFO: epoch 001:   6229 / 102288 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.3, ups=0.91, wpb=109.6, bsz=40, num_updates=6220, lr=4.8916e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19108
2023-01-05 02:24:46 - progress_bar.py[line:274] - INFO: epoch 001:   6239 / 102288 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=6230, lr=4.89109e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19120
2023-01-05 02:24:58 - progress_bar.py[line:274] - INFO: epoch 001:   6249 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.3, ups=0.88, wpb=110.7, bsz=40, num_updates=6240, lr=4.89058e-05, gnorm=0.583, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19131
2023-01-05 02:25:09 - progress_bar.py[line:274] - INFO: epoch 001:   6259 / 102288 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=6250, lr=4.89007e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19143
2023-01-05 02:25:20 - progress_bar.py[line:274] - INFO: epoch 001:   6269 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=103.3, ups=0.93, wpb=111.3, bsz=40, num_updates=6260, lr=4.88956e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19154
2023-01-05 02:25:32 - progress_bar.py[line:274] - INFO: epoch 001:   6279 / 102288 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.4, ups=0.87, wpb=109.7, bsz=40, num_updates=6270, lr=4.88905e-05, gnorm=0.671, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19165
2023-01-05 02:25:43 - progress_bar.py[line:274] - INFO: epoch 001:   6289 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=6280, lr=4.88854e-05, gnorm=0.604, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19177
2023-01-05 02:25:55 - progress_bar.py[line:274] - INFO: epoch 001:   6299 / 102288 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.9, ups=0.88, wpb=108.9, bsz=40, num_updates=6290, lr=4.88803e-05, gnorm=0.776, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19188
2023-01-05 02:26:06 - progress_bar.py[line:274] - INFO: epoch 001:   6309 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=6300, lr=4.88752e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19200
2023-01-05 02:26:18 - progress_bar.py[line:274] - INFO: epoch 001:   6319 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.7, ups=0.87, wpb=110, bsz=40, num_updates=6310, lr=4.88701e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19211
2023-01-05 02:26:29 - progress_bar.py[line:274] - INFO: epoch 001:   6329 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99, ups=0.91, wpb=109.2, bsz=40, num_updates=6320, lr=4.8865e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19223
2023-01-05 02:26:41 - progress_bar.py[line:274] - INFO: epoch 001:   6339 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=6330, lr=4.88599e-05, gnorm=0.759, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19234
2023-01-05 02:26:52 - progress_bar.py[line:274] - INFO: epoch 001:   6349 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=6340, lr=4.88549e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19246
2023-01-05 02:27:04 - progress_bar.py[line:274] - INFO: epoch 001:   6359 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99, ups=0.89, wpb=110.8, bsz=40, num_updates=6350, lr=4.88498e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19257
2023-01-05 02:27:15 - progress_bar.py[line:274] - INFO: epoch 001:   6369 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=6360, lr=4.88447e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19269
2023-01-05 02:27:27 - progress_bar.py[line:274] - INFO: epoch 001:   6379 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=6370, lr=4.88396e-05, gnorm=0.6, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19280
2023-01-05 02:27:38 - progress_bar.py[line:274] - INFO: epoch 001:   6389 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=6380, lr=4.88345e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19292
2023-01-05 02:27:49 - progress_bar.py[line:274] - INFO: epoch 001:   6399 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=104.4, ups=0.94, wpb=110.5, bsz=40, num_updates=6390, lr=4.88294e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19303
2023-01-05 02:28:01 - progress_bar.py[line:274] - INFO: epoch 001:   6409 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=6400, lr=4.88243e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19314
2023-01-05 02:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   6419 / 102288 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.5, ups=0.89, wpb=109.5, bsz=40, num_updates=6410, lr=4.88192e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19325
2023-01-05 02:28:23 - progress_bar.py[line:274] - INFO: epoch 001:   6429 / 102288 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=6420, lr=4.88141e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19337
2023-01-05 02:28:35 - progress_bar.py[line:274] - INFO: epoch 001:   6439 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=6430, lr=4.8809e-05, gnorm=0.729, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19348
2023-01-05 02:28:46 - progress_bar.py[line:274] - INFO: epoch 001:   6449 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.2, ups=0.94, wpb=109.5, bsz=40, num_updates=6440, lr=4.88039e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19359
2023-01-05 02:28:57 - progress_bar.py[line:274] - INFO: epoch 001:   6459 / 102288 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.1, ups=0.9, wpb=108.8, bsz=40, num_updates=6450, lr=4.87988e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19370
2023-01-05 02:29:08 - progress_bar.py[line:274] - INFO: epoch 001:   6469 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=103.4, ups=0.93, wpb=111.5, bsz=40, num_updates=6460, lr=4.87938e-05, gnorm=0.544, clip=0, loss_scale=512, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=19381
2023-01-05 02:29:19 - progress_bar.py[line:274] - INFO: epoch 001:   6479 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.5, ups=0.93, wpb=110.5, bsz=40, num_updates=6470, lr=4.87887e-05, gnorm=0.729, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19392
2023-01-05 02:29:31 - progress_bar.py[line:274] - INFO: epoch 001:   6489 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.7, ups=0.88, wpb=110, bsz=40, num_updates=6480, lr=4.87836e-05, gnorm=0.583, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19404
2023-01-05 02:29:42 - progress_bar.py[line:274] - INFO: epoch 001:   6499 / 102288 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=103.2, ups=0.93, wpb=111.3, bsz=40, num_updates=6490, lr=4.87785e-05, gnorm=0.629, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19415
2023-01-05 02:29:52 - progress_bar.py[line:274] - INFO: epoch 001:   6509 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.6, ups=0.93, wpb=108.7, bsz=40, num_updates=6500, lr=4.87734e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19426
2023-01-05 02:30:04 - progress_bar.py[line:274] - INFO: epoch 001:   6519 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=6510, lr=4.87683e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19437
2023-01-05 02:30:15 - progress_bar.py[line:274] - INFO: epoch 001:   6529 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.3, ups=0.9, wpb=109.9, bsz=40, num_updates=6520, lr=4.87632e-05, gnorm=0.615, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19448
2023-01-05 02:30:26 - progress_bar.py[line:274] - INFO: epoch 001:   6539 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=6530, lr=4.87581e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19460
2023-01-05 02:30:37 - progress_bar.py[line:274] - INFO: epoch 001:   6549 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=103.1, ups=0.93, wpb=111.2, bsz=40, num_updates=6540, lr=4.8753e-05, gnorm=0.634, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=19471
2023-01-05 02:30:49 - progress_bar.py[line:274] - INFO: epoch 001:   6559 / 102288 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=6550, lr=4.87479e-05, gnorm=0.573, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19482
2023-01-05 02:31:00 - progress_bar.py[line:274] - INFO: epoch 001:   6569 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=103.3, ups=0.93, wpb=111.6, bsz=40, num_updates=6560, lr=4.87428e-05, gnorm=0.617, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19493
2023-01-05 02:31:11 - progress_bar.py[line:274] - INFO: epoch 001:   6579 / 102288 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.2, ups=0.9, wpb=109.8, bsz=40, num_updates=6570, lr=4.87377e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19504
2023-01-05 02:31:22 - progress_bar.py[line:274] - INFO: epoch 001:   6589 / 102288 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.5, ups=0.91, wpb=111.1, bsz=40, num_updates=6580, lr=4.87326e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19516
2023-01-05 02:31:33 - progress_bar.py[line:274] - INFO: epoch 001:   6599 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.7, ups=0.93, wpb=110.8, bsz=40, num_updates=6590, lr=4.87276e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19527
2023-01-05 02:31:45 - progress_bar.py[line:274] - INFO: epoch 001:   6609 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.9, ups=0.87, wpb=110.7, bsz=40, num_updates=6600, lr=4.87225e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19538
2023-01-05 02:31:56 - progress_bar.py[line:274] - INFO: epoch 001:   6619 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.4, ups=0.92, wpb=109.4, bsz=40, num_updates=6610, lr=4.87174e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19549
2023-01-05 02:32:07 - progress_bar.py[line:274] - INFO: epoch 001:   6629 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=103, ups=0.93, wpb=110.9, bsz=40, num_updates=6620, lr=4.87123e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19560
2023-01-05 02:32:18 - progress_bar.py[line:274] - INFO: epoch 001:   6639 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=6630, lr=4.87072e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19572
2023-01-05 02:32:30 - progress_bar.py[line:274] - INFO: epoch 001:   6649 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.8, ups=0.88, wpb=108.7, bsz=40, num_updates=6640, lr=4.87021e-05, gnorm=0.569, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19583
2023-01-05 02:32:41 - progress_bar.py[line:274] - INFO: epoch 001:   6659 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=6650, lr=4.8697e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19595
2023-01-05 02:32:53 - progress_bar.py[line:274] - INFO: epoch 001:   6669 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.4, ups=0.9, wpb=109, bsz=40, num_updates=6660, lr=4.86919e-05, gnorm=0.675, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19606
2023-01-05 02:33:04 - progress_bar.py[line:274] - INFO: epoch 001:   6679 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.5, ups=0.88, wpb=108.4, bsz=40, num_updates=6670, lr=4.86868e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19618
2023-01-05 02:33:15 - progress_bar.py[line:274] - INFO: epoch 001:   6689 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.4, ups=0.91, wpb=110.9, bsz=40, num_updates=6680, lr=4.86817e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19629
2023-01-05 02:33:27 - progress_bar.py[line:274] - INFO: epoch 001:   6699 / 102288 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.9, ups=0.91, wpb=110.3, bsz=40, num_updates=6690, lr=4.86766e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19640
2023-01-05 02:33:38 - progress_bar.py[line:274] - INFO: epoch 001:   6709 / 102288 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=95.1, ups=0.87, wpb=109.2, bsz=40, num_updates=6700, lr=4.86715e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19652
2023-01-05 02:33:49 - progress_bar.py[line:274] - INFO: epoch 001:   6719 / 102288 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=102.9, ups=0.93, wpb=110.5, bsz=40, num_updates=6710, lr=4.86665e-05, gnorm=0.563, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19662
2023-01-05 02:33:58 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 02:34:02 - progress_bar.py[line:274] - INFO: epoch 001:   6730 / 102288 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=89.2, ups=0.81, wpb=110.3, bsz=40, num_updates=6720, lr=4.86614e-05, gnorm=0.609, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19675
2023-01-05 02:34:13 - progress_bar.py[line:274] - INFO: epoch 001:   6740 / 102288 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=6730, lr=4.86563e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19686
2023-01-05 02:34:25 - progress_bar.py[line:274] - INFO: epoch 001:   6750 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=94.5, ups=0.86, wpb=110.5, bsz=40, num_updates=6740, lr=4.86512e-05, gnorm=0.582, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19698
2023-01-05 02:34:36 - progress_bar.py[line:274] - INFO: epoch 001:   6760 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.3, ups=0.91, wpb=109.7, bsz=40, num_updates=6750, lr=4.86461e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19710
2023-01-05 02:34:47 - progress_bar.py[line:274] - INFO: epoch 001:   6770 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.9, ups=0.93, wpb=111, bsz=40, num_updates=6760, lr=4.8641e-05, gnorm=0.636, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19721
2023-01-05 02:34:58 - progress_bar.py[line:274] - INFO: epoch 001:   6780 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.4, ups=0.93, wpb=109.3, bsz=40, num_updates=6770, lr=4.86359e-05, gnorm=0.635, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19732
2023-01-05 02:35:09 - progress_bar.py[line:274] - INFO: epoch 001:   6790 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.5, ups=0.93, wpb=110.5, bsz=40, num_updates=6780, lr=4.86308e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19743
2023-01-05 02:35:21 - progress_bar.py[line:274] - INFO: epoch 001:   6800 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=6790, lr=4.86257e-05, gnorm=0.544, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19754
2023-01-05 02:35:33 - progress_bar.py[line:274] - INFO: epoch 001:   6810 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=95.7, ups=0.87, wpb=110.1, bsz=40, num_updates=6800, lr=4.86206e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19766
2023-01-05 02:35:44 - progress_bar.py[line:274] - INFO: epoch 001:   6820 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=6810, lr=4.86155e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19777
2023-01-05 02:35:55 - progress_bar.py[line:274] - INFO: epoch 001:   6830 / 102288 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.9, ups=0.92, wpb=109, bsz=40, num_updates=6820, lr=4.86104e-05, gnorm=0.641, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19788
2023-01-05 02:36:07 - progress_bar.py[line:274] - INFO: epoch 001:   6840 / 102288 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=93.9, ups=0.87, wpb=108.4, bsz=40, num_updates=6830, lr=4.86054e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19800
2023-01-05 02:36:18 - progress_bar.py[line:274] - INFO: epoch 001:   6850 / 102288 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=6840, lr=4.86003e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19811
2023-01-05 02:36:30 - progress_bar.py[line:274] - INFO: epoch 001:   6860 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=6850, lr=4.85952e-05, gnorm=0.607, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19823
2023-01-05 02:36:41 - progress_bar.py[line:274] - INFO: epoch 001:   6870 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=6860, lr=4.85901e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19834
2023-01-05 02:36:52 - progress_bar.py[line:274] - INFO: epoch 001:   6880 / 102288 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.8, ups=0.89, wpb=109.4, bsz=40, num_updates=6870, lr=4.8585e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19845
2023-01-05 02:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   6890 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.5, ups=0.89, wpb=111.9, bsz=40, num_updates=6880, lr=4.85799e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19857
2023-01-05 02:37:15 - progress_bar.py[line:274] - INFO: epoch 001:   6900 / 102288 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.4, ups=0.9, wpb=111.6, bsz=40, num_updates=6890, lr=4.85748e-05, gnorm=0.584, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19868
2023-01-05 02:37:27 - progress_bar.py[line:274] - INFO: epoch 001:   6910 / 102288 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.5, ups=0.88, wpb=109.8, bsz=40, num_updates=6900, lr=4.85697e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19880
2023-01-05 02:37:37 - progress_bar.py[line:274] - INFO: epoch 001:   6920 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=104.5, ups=0.94, wpb=111.2, bsz=40, num_updates=6910, lr=4.85646e-05, gnorm=0.532, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19891
2023-01-05 02:37:49 - progress_bar.py[line:274] - INFO: epoch 001:   6930 / 102288 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=6920, lr=4.85595e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19902
2023-01-05 02:38:00 - progress_bar.py[line:274] - INFO: epoch 001:   6940 / 102288 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=6930, lr=4.85544e-05, gnorm=0.573, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19914
2023-01-05 02:38:11 - progress_bar.py[line:274] - INFO: epoch 001:   6950 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.8, ups=0.93, wpb=110.4, bsz=40, num_updates=6940, lr=4.85493e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19925
2023-01-05 02:38:23 - progress_bar.py[line:274] - INFO: epoch 001:   6960 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=6950, lr=4.85443e-05, gnorm=0.622, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19936
2023-01-05 02:38:34 - progress_bar.py[line:274] - INFO: epoch 001:   6970 / 102288 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=6960, lr=4.85392e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19947
2023-01-05 02:38:45 - progress_bar.py[line:274] - INFO: epoch 001:   6980 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=6970, lr=4.85341e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19959
2023-01-05 02:38:57 - progress_bar.py[line:274] - INFO: epoch 001:   6990 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.5, ups=0.89, wpb=109.1, bsz=40, num_updates=6980, lr=4.8529e-05, gnorm=0.683, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19970
2023-01-05 02:39:08 - progress_bar.py[line:274] - INFO: epoch 001:   7000 / 102288 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.3, ups=0.88, wpb=110.5, bsz=40, num_updates=6990, lr=4.85239e-05, gnorm=0.622, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=19982
2023-01-05 02:39:19 - progress_bar.py[line:274] - INFO: epoch 001:   7010 / 102288 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=102.6, ups=0.93, wpb=110.6, bsz=40, num_updates=7000, lr=4.85188e-05, gnorm=0.553, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19993
2023-01-05 02:39:31 - progress_bar.py[line:274] - INFO: epoch 001:   7020 / 102288 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=7010, lr=4.85137e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20004
2023-01-05 02:39:42 - progress_bar.py[line:274] - INFO: epoch 001:   7030 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.6, ups=0.88, wpb=110, bsz=40, num_updates=7020, lr=4.85086e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20016
2023-01-05 02:39:54 - progress_bar.py[line:274] - INFO: epoch 001:   7040 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=7030, lr=4.85035e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20027
2023-01-05 02:40:05 - progress_bar.py[line:274] - INFO: epoch 001:   7050 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.1, ups=0.88, wpb=110.6, bsz=40, num_updates=7040, lr=4.84984e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20039
2023-01-05 02:40:17 - progress_bar.py[line:274] - INFO: epoch 001:   7060 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.4, ups=0.92, wpb=109.6, bsz=40, num_updates=7050, lr=4.84933e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20050
2023-01-05 02:40:28 - progress_bar.py[line:274] - INFO: epoch 001:   7070 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=7060, lr=4.84882e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20061
2023-01-05 02:40:40 - progress_bar.py[line:274] - INFO: epoch 001:   7080 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.3, ups=0.87, wpb=110.4, bsz=40, num_updates=7070, lr=4.84832e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20073
2023-01-05 02:40:51 - progress_bar.py[line:274] - INFO: epoch 001:   7090 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=7080, lr=4.84781e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20084
2023-01-05 02:41:02 - progress_bar.py[line:274] - INFO: epoch 001:   7100 / 102288 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.8, ups=0.91, wpb=110, bsz=40, num_updates=7090, lr=4.8473e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20096
2023-01-05 02:41:14 - progress_bar.py[line:274] - INFO: epoch 001:   7110 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=7100, lr=4.84679e-05, gnorm=0.582, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=20107
2023-01-05 02:41:25 - progress_bar.py[line:274] - INFO: epoch 001:   7120 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.5, ups=0.91, wpb=109.1, bsz=40, num_updates=7110, lr=4.84628e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20118
2023-01-05 02:41:36 - progress_bar.py[line:274] - INFO: epoch 001:   7130 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=7120, lr=4.84577e-05, gnorm=0.629, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20129
2023-01-05 02:41:47 - progress_bar.py[line:274] - INFO: epoch 001:   7140 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=7130, lr=4.84526e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20141
2023-01-05 02:41:59 - progress_bar.py[line:274] - INFO: epoch 001:   7150 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=7140, lr=4.84475e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20152
2023-01-05 02:42:10 - progress_bar.py[line:274] - INFO: epoch 001:   7160 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.2, ups=0.93, wpb=110.4, bsz=40, num_updates=7150, lr=4.84424e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20163
2023-01-05 02:42:22 - progress_bar.py[line:274] - INFO: epoch 001:   7170 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.4, ups=0.88, wpb=112.1, bsz=40, num_updates=7160, lr=4.84373e-05, gnorm=0.636, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20175
2023-01-05 02:42:33 - progress_bar.py[line:274] - INFO: epoch 001:   7180 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.9, ups=0.89, wpb=108.9, bsz=40, num_updates=7170, lr=4.84322e-05, gnorm=0.647, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20186
2023-01-05 02:42:44 - progress_bar.py[line:274] - INFO: epoch 001:   7190 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=7180, lr=4.84271e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20198
2023-01-05 02:42:56 - progress_bar.py[line:274] - INFO: epoch 001:   7200 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.9, ups=0.88, wpb=110.7, bsz=40, num_updates=7190, lr=4.8422e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20209
2023-01-05 02:43:07 - progress_bar.py[line:274] - INFO: epoch 001:   7210 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.1, ups=0.92, wpb=109.9, bsz=40, num_updates=7200, lr=4.8417e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20220
2023-01-05 02:43:18 - progress_bar.py[line:274] - INFO: epoch 001:   7220 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101, ups=0.91, wpb=110.4, bsz=40, num_updates=7210, lr=4.84119e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20232
2023-01-05 02:43:29 - progress_bar.py[line:274] - INFO: epoch 001:   7230 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=105, ups=0.95, wpb=110.1, bsz=40, num_updates=7220, lr=4.84068e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=20242
2023-01-05 02:43:40 - progress_bar.py[line:274] - INFO: epoch 001:   7240 / 102288 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.5, ups=0.91, wpb=109.8, bsz=40, num_updates=7230, lr=4.84017e-05, gnorm=0.619, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20254
2023-01-05 02:43:51 - progress_bar.py[line:274] - INFO: epoch 001:   7250 / 102288 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.3, ups=0.92, wpb=110.3, bsz=40, num_updates=7240, lr=4.83966e-05, gnorm=0.631, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20265
2023-01-05 02:44:03 - progress_bar.py[line:274] - INFO: epoch 001:   7260 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=7250, lr=4.83915e-05, gnorm=0.667, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20276
2023-01-05 02:44:14 - progress_bar.py[line:274] - INFO: epoch 001:   7270 / 102288 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=103.7, ups=0.93, wpb=111.8, bsz=40, num_updates=7260, lr=4.83864e-05, gnorm=0.581, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20287
2023-01-05 02:44:25 - progress_bar.py[line:274] - INFO: epoch 001:   7280 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.8, ups=0.91, wpb=110.2, bsz=40, num_updates=7270, lr=4.83813e-05, gnorm=0.553, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20298
2023-01-05 02:44:36 - progress_bar.py[line:274] - INFO: epoch 001:   7290 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=7280, lr=4.83762e-05, gnorm=0.522, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20310
2023-01-05 02:44:47 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 02:44:49 - progress_bar.py[line:274] - INFO: epoch 001:   7301 / 102288 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=90.2, ups=0.83, wpb=109.2, bsz=40, num_updates=7290, lr=4.83711e-05, gnorm=0.823, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=20322
2023-01-05 02:45:00 - progress_bar.py[line:274] - INFO: epoch 001:   7311 / 102288 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.4, ups=0.9, wpb=110.4, bsz=40, num_updates=7300, lr=4.8366e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20333
2023-01-05 02:45:11 - progress_bar.py[line:274] - INFO: epoch 001:   7321 / 102288 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.3, ups=0.88, wpb=111.6, bsz=40, num_updates=7310, lr=4.83609e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20345
2023-01-05 02:45:23 - progress_bar.py[line:274] - INFO: epoch 001:   7331 / 102288 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=94.6, ups=0.86, wpb=110.1, bsz=40, num_updates=7320, lr=4.83559e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=20357
2023-01-05 02:45:35 - progress_bar.py[line:274] - INFO: epoch 001:   7341 / 102288 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.7, ups=0.89, wpb=110.9, bsz=40, num_updates=7330, lr=4.83508e-05, gnorm=0.676, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20368
2023-01-05 02:45:45 - progress_bar.py[line:274] - INFO: epoch 001:   7351 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=105.6, ups=0.95, wpb=110.8, bsz=40, num_updates=7340, lr=4.83457e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=10, gb_free=10.5, ema_decay=0.9999, wall=20379
2023-01-05 02:45:57 - progress_bar.py[line:274] - INFO: epoch 001:   7361 / 102288 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.1, ups=0.89, wpb=108.8, bsz=40, num_updates=7350, lr=4.83406e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20390
2023-01-05 02:46:08 - progress_bar.py[line:274] - INFO: epoch 001:   7371 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=7360, lr=4.83355e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20402
2023-01-05 02:46:20 - progress_bar.py[line:274] - INFO: epoch 001:   7381 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.2, ups=0.9, wpb=109.9, bsz=40, num_updates=7370, lr=4.83304e-05, gnorm=0.702, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20413
2023-01-05 02:46:31 - progress_bar.py[line:274] - INFO: epoch 001:   7391 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.1, ups=0.92, wpb=108.9, bsz=40, num_updates=7380, lr=4.83253e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20424
2023-01-05 02:46:42 - progress_bar.py[line:274] - INFO: epoch 001:   7401 / 102288 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.8, ups=0.92, wpb=110, bsz=40, num_updates=7390, lr=4.83202e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20435
2023-01-05 02:46:53 - progress_bar.py[line:274] - INFO: epoch 001:   7411 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=7400, lr=4.83151e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20446
2023-01-05 02:47:05 - progress_bar.py[line:274] - INFO: epoch 001:   7421 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.4, ups=0.88, wpb=109.5, bsz=40, num_updates=7410, lr=4.831e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20458
2023-01-05 02:47:16 - progress_bar.py[line:274] - INFO: epoch 001:   7431 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=102.4, ups=0.92, wpb=111.9, bsz=40, num_updates=7420, lr=4.83049e-05, gnorm=0.554, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20469
2023-01-05 02:47:27 - progress_bar.py[line:274] - INFO: epoch 001:   7441 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=7430, lr=4.82998e-05, gnorm=0.594, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20481
2023-01-05 02:47:39 - progress_bar.py[line:274] - INFO: epoch 001:   7451 / 102288 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99, ups=0.9, wpb=109.9, bsz=40, num_updates=7440, lr=4.82948e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20492
2023-01-05 02:47:50 - progress_bar.py[line:274] - INFO: epoch 001:   7461 / 102288 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=7450, lr=4.82897e-05, gnorm=0.485, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20503
2023-01-05 02:48:01 - progress_bar.py[line:274] - INFO: epoch 001:   7471 / 102288 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.7, ups=0.89, wpb=109.3, bsz=40, num_updates=7460, lr=4.82846e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20515
2023-01-05 02:48:13 - progress_bar.py[line:274] - INFO: epoch 001:   7481 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=7470, lr=4.82795e-05, gnorm=0.574, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20526
2023-01-05 02:48:24 - progress_bar.py[line:274] - INFO: epoch 001:   7491 / 102288 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.9, ups=0.91, wpb=111, bsz=40, num_updates=7480, lr=4.82744e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=20537
2023-01-05 02:48:35 - progress_bar.py[line:274] - INFO: epoch 001:   7501 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.6, ups=0.9, wpb=110.1, bsz=40, num_updates=7490, lr=4.82693e-05, gnorm=0.581, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20549
2023-01-05 02:48:47 - progress_bar.py[line:274] - INFO: epoch 001:   7511 / 102288 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=7500, lr=4.82642e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20560
2023-01-05 02:48:58 - progress_bar.py[line:274] - INFO: epoch 001:   7521 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=7510, lr=4.82591e-05, gnorm=0.595, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20572
2023-01-05 02:49:10 - progress_bar.py[line:274] - INFO: epoch 001:   7531 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=7520, lr=4.8254e-05, gnorm=0.572, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20583
2023-01-05 02:49:21 - progress_bar.py[line:274] - INFO: epoch 001:   7541 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.5, ups=0.87, wpb=110.9, bsz=40, num_updates=7530, lr=4.82489e-05, gnorm=0.551, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20595
2023-01-05 02:49:33 - progress_bar.py[line:274] - INFO: epoch 001:   7551 / 102288 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.6, ups=0.9, wpb=109.1, bsz=40, num_updates=7540, lr=4.82438e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20606
2023-01-05 02:49:44 - progress_bar.py[line:274] - INFO: epoch 001:   7561 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=7550, lr=4.82387e-05, gnorm=0.604, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20617
2023-01-05 02:49:55 - progress_bar.py[line:274] - INFO: epoch 001:   7571 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=7560, lr=4.82337e-05, gnorm=0.523, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20629
2023-01-05 02:50:07 - progress_bar.py[line:274] - INFO: epoch 001:   7581 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.1, ups=0.9, wpb=109.5, bsz=40, num_updates=7570, lr=4.82286e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20640
2023-01-05 02:50:18 - progress_bar.py[line:274] - INFO: epoch 001:   7591 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=7580, lr=4.82235e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20651
2023-01-05 02:50:29 - progress_bar.py[line:274] - INFO: epoch 001:   7601 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=7590, lr=4.82184e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20663
2023-01-05 02:50:41 - progress_bar.py[line:274] - INFO: epoch 001:   7611 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=7600, lr=4.82133e-05, gnorm=0.676, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20674
2023-01-05 02:50:52 - progress_bar.py[line:274] - INFO: epoch 001:   7621 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=7610, lr=4.82082e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20685
2023-01-05 02:51:03 - progress_bar.py[line:274] - INFO: epoch 001:   7631 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=103.3, ups=0.93, wpb=111.2, bsz=40, num_updates=7620, lr=4.82031e-05, gnorm=0.818, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20696
2023-01-05 02:51:14 - progress_bar.py[line:274] - INFO: epoch 001:   7641 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.8, ups=0.91, wpb=109.2, bsz=40, num_updates=7630, lr=4.8198e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20708
2023-01-05 02:51:25 - progress_bar.py[line:274] - INFO: epoch 001:   7651 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=7640, lr=4.81929e-05, gnorm=0.59, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20719
2023-01-05 02:51:36 - progress_bar.py[line:274] - INFO: epoch 001:   7661 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.7, ups=0.91, wpb=111.5, bsz=40, num_updates=7650, lr=4.81878e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20730
2023-01-05 02:51:48 - progress_bar.py[line:274] - INFO: epoch 001:   7671 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.8, ups=0.88, wpb=110.2, bsz=40, num_updates=7660, lr=4.81827e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20741
2023-01-05 02:51:59 - progress_bar.py[line:274] - INFO: epoch 001:   7681 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.4, ups=0.93, wpb=109.1, bsz=40, num_updates=7670, lr=4.81776e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20752
2023-01-05 02:52:11 - progress_bar.py[line:274] - INFO: epoch 001:   7691 / 102288 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=7680, lr=4.81726e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20764
2023-01-05 02:52:22 - progress_bar.py[line:274] - INFO: epoch 001:   7701 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.2, ups=0.9, wpb=111.1, bsz=40, num_updates=7690, lr=4.81675e-05, gnorm=0.549, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20775
2023-01-05 02:52:34 - progress_bar.py[line:274] - INFO: epoch 001:   7711 / 102288 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=93.7, ups=0.87, wpb=107.9, bsz=40, num_updates=7700, lr=4.81624e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20787
2023-01-05 02:52:45 - progress_bar.py[line:274] - INFO: epoch 001:   7721 / 102288 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.5, ups=0.89, wpb=109.5, bsz=40, num_updates=7710, lr=4.81573e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20799
2023-01-05 02:52:57 - progress_bar.py[line:274] - INFO: epoch 001:   7731 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.5, ups=0.87, wpb=109.7, bsz=40, num_updates=7720, lr=4.81522e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20810
2023-01-05 02:53:09 - progress_bar.py[line:274] - INFO: epoch 001:   7741 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=7730, lr=4.81471e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20822
2023-01-05 02:53:21 - progress_bar.py[line:274] - INFO: epoch 001:   7751 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.9, ups=0.91, wpb=109.2, bsz=40, num_updates=7740, lr=4.8142e-05, gnorm=0.617, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20834
2023-01-05 02:53:32 - progress_bar.py[line:274] - INFO: epoch 001:   7761 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.5, ups=0.9, wpb=111, bsz=40, num_updates=7750, lr=4.81369e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20845
2023-01-05 02:53:43 - progress_bar.py[line:274] - INFO: epoch 001:   7771 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=7760, lr=4.81318e-05, gnorm=0.652, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20856
2023-01-05 02:53:55 - progress_bar.py[line:274] - INFO: epoch 001:   7781 / 102288 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=7770, lr=4.81267e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20868
2023-01-05 02:54:06 - progress_bar.py[line:274] - INFO: epoch 001:   7791 / 102288 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=7780, lr=4.81216e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20879
2023-01-05 02:54:17 - progress_bar.py[line:274] - INFO: epoch 001:   7801 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.9, ups=0.92, wpb=110.2, bsz=40, num_updates=7790, lr=4.81165e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20890
2023-01-05 02:54:28 - progress_bar.py[line:274] - INFO: epoch 001:   7811 / 102288 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.6, ups=0.92, wpb=109.7, bsz=40, num_updates=7800, lr=4.81114e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20902
2023-01-05 02:54:39 - progress_bar.py[line:274] - INFO: epoch 001:   7821 / 102288 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.3, ups=0.93, wpb=110.4, bsz=40, num_updates=7810, lr=4.81064e-05, gnorm=0.667, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20913
2023-01-05 02:54:51 - progress_bar.py[line:274] - INFO: epoch 001:   7831 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.9, ups=0.89, wpb=110.6, bsz=40, num_updates=7820, lr=4.81013e-05, gnorm=0.621, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20924
2023-01-05 02:55:02 - progress_bar.py[line:274] - INFO: epoch 001:   7841 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=7830, lr=4.80962e-05, gnorm=0.55, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20935
2023-01-05 02:55:14 - progress_bar.py[line:274] - INFO: epoch 001:   7851 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.7, ups=0.89, wpb=110.8, bsz=40, num_updates=7840, lr=4.80911e-05, gnorm=0.599, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20947
2023-01-05 02:55:25 - progress_bar.py[line:274] - INFO: epoch 001:   7861 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=7850, lr=4.8086e-05, gnorm=0.599, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20958
2023-01-05 02:55:36 - progress_bar.py[line:274] - INFO: epoch 001:   7871 / 102288 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=7860, lr=4.80809e-05, gnorm=0.584, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20969
2023-01-05 02:55:48 - progress_bar.py[line:274] - INFO: epoch 001:   7881 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97, ups=0.88, wpb=110.4, bsz=40, num_updates=7870, lr=4.80758e-05, gnorm=0.615, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20981
2023-01-05 02:55:59 - progress_bar.py[line:274] - INFO: epoch 001:   7891 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=7880, lr=4.80707e-05, gnorm=0.541, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20992
2023-01-05 02:56:11 - progress_bar.py[line:274] - INFO: epoch 001:   7901 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=7890, lr=4.80656e-05, gnorm=0.538, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=21004
2023-01-05 02:56:22 - progress_bar.py[line:274] - INFO: epoch 001:   7911 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.4, ups=0.9, wpb=112.4, bsz=40, num_updates=7900, lr=4.80605e-05, gnorm=0.666, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=21015
2023-01-05 02:56:33 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 02:56:34 - progress_bar.py[line:274] - INFO: epoch 001:   7922 / 102288 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=92.6, ups=0.84, wpb=110.4, bsz=40, num_updates=7910, lr=4.80554e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=21027
2023-01-05 02:56:45 - progress_bar.py[line:274] - INFO: epoch 001:   7932 / 102288 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.1, ups=0.91, wpb=109.5, bsz=40, num_updates=7920, lr=4.80503e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=21038
2023-01-05 02:56:57 - progress_bar.py[line:274] - INFO: epoch 001:   7942 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=102.3, ups=0.91, wpb=112.2, bsz=40, num_updates=7930, lr=4.80453e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=21050
2023-01-05 02:57:08 - progress_bar.py[line:274] - INFO: epoch 001:   7952 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102, ups=0.93, wpb=109.9, bsz=40, num_updates=7940, lr=4.80402e-05, gnorm=0.582, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=21061
2023-01-05 02:57:19 - progress_bar.py[line:274] - INFO: epoch 001:   7962 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=7950, lr=4.80351e-05, gnorm=0.552, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=21072
2023-01-05 02:57:30 - progress_bar.py[line:274] - INFO: epoch 001:   7972 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=7960, lr=4.803e-05, gnorm=0.584, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=21083
2023-01-05 02:57:41 - progress_bar.py[line:274] - INFO: epoch 001:   7982 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.3, ups=0.93, wpb=110.2, bsz=40, num_updates=7970, lr=4.80249e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=21094
2023-01-05 02:57:52 - progress_bar.py[line:274] - INFO: epoch 001:   7992 / 102288 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.7, ups=0.92, wpb=108.2, bsz=40, num_updates=7980, lr=4.80198e-05, gnorm=0.744, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=21106
2023-01-05 02:58:03 - progress_bar.py[line:274] - INFO: epoch 001:   8002 / 102288 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=7990, lr=4.80147e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=21117
2023-01-05 02:58:15 - progress_bar.py[line:274] - INFO: epoch 001:   8012 / 102288 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=8000, lr=4.80096e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=21128
2023-01-05 02:58:15 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 02:58:16 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 02:58:16 - train.py[line:551] - INFO: load:0.92 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 03:00:48 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 03:00:48 - train.py[line:551] - INFO: load:0.94 valid_run:152.33 task_valid:148.19 collect_output:3.05
2023-01-05 03:03:17 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 03:03:17 - train.py[line:551] - INFO: load:0.97 valid_run:300.83 task_valid:290.94 collect_output:7.80
2023-01-05 03:05:50 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 03:05:50 - train.py[line:551] - INFO: load:0.99 valid_run:453.39 task_valid:433.80 collect_output:16.50
2023-01-05 03:08:19 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 03:08:19 - train.py[line:551] - INFO: load:1.02 valid_run:602.65 task_valid:578.48 collect_output:20.07
2023-01-05 03:10:52 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 03:10:52 - train.py[line:551] - INFO: load:1.04 valid_run:755.47 task_valid:725.89 collect_output:24.48
2023-01-05 03:13:24 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 03:13:24 - train.py[line:551] - INFO: load:1.07 valid_run:907.31 task_valid:871.20 collect_output:30.01
2023-01-05 03:15:58 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 03:15:58 - train.py[line:551] - INFO: load:1.09 valid_run:1061.06 task_valid:1017.09 collect_output:36.86
2023-01-05 03:18:29 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 03:18:29 - train.py[line:551] - INFO: load:1.12 valid_run:1212.71 task_valid:1158.11 collect_output:46.49
2023-01-05 03:20:59 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 03:20:59 - train.py[line:551] - INFO: load:1.14 valid_run:1362.66 task_valid:1302.79 collect_output:50.69
2023-01-05 03:23:28 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 03:23:28 - train.py[line:551] - INFO: load:1.17 valid_run:1511.44 task_valid:1445.70 collect_output:55.56
2023-01-05 03:25:58 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 03:25:58 - train.py[line:551] - INFO: load:1.20 valid_run:1661.23 task_valid:1590.43 collect_output:59.58
2023-01-05 03:28:29 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 03:28:29 - train.py[line:551] - INFO: load:1.22 valid_run:1811.77 task_valid:1735.34 collect_output:64.19
2023-01-05 03:30:59 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 03:30:59 - train.py[line:551] - INFO: load:1.25 valid_run:1962.09 task_valid:1877.00 collect_output:71.82
2023-01-05 03:33:30 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 03:33:30 - train.py[line:551] - INFO: load:1.27 valid_run:2112.66 task_valid:2022.21 collect_output:76.16
2023-01-05 03:36:00 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 03:36:00 - train.py[line:551] - INFO: load:1.30 valid_run:2262.80 task_valid:2168.54 collect_output:78.95
2023-01-05 03:38:30 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 03:38:30 - train.py[line:551] - INFO: load:1.33 valid_run:2413.15 task_valid:2312.57 collect_output:84.26
2023-01-05 03:41:02 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 03:41:02 - train.py[line:551] - INFO: load:1.35 valid_run:2564.99 task_valid:2457.84 collect_output:89.83
2023-01-05 03:43:33 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 03:43:33 - train.py[line:551] - INFO: load:1.38 valid_run:2715.57 task_valid:2604.65 collect_output:92.57
2023-01-05 03:46:02 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 03:46:02 - train.py[line:551] - INFO: load:1.40 valid_run:2864.22 task_valid:2746.00 collect_output:98.81
2023-01-05 03:48:33 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 03:48:33 - train.py[line:551] - INFO: load:1.43 valid_run:3015.05 task_valid:2891.17 collect_output:103.45
2023-01-05 03:51:05 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 03:51:05 - train.py[line:551] - INFO: load:1.46 valid_run:3167.38 task_valid:3035.51 collect_output:110.43
2023-01-05 03:53:35 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 03:53:35 - train.py[line:551] - INFO: load:1.48 valid_run:3316.93 task_valid:3179.86 collect_output:114.60
2023-01-05 03:56:06 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 03:56:06 - train.py[line:551] - INFO: load:1.51 valid_run:3468.26 task_valid:3325.79 collect_output:118.99
2023-01-05 03:58:38 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 03:58:38 - train.py[line:551] - INFO: load:1.54 valid_run:3619.93 task_valid:3472.24 collect_output:123.18

====================================================================================================
SGG eval:     R @ 50: 0.5194;     R @ 100: 0.5797;     R @ 500: 0.6240;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3731;    mR @ 100: 0.4263;    mR @ 500: 0.4777;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5268) (covered in:0.8750) (covering:0.2857) (eating:0.7353) (flying in:0.5000) (growing on:0.3750) (hanging from:0.4194) (lying on:0.0500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.9134) (says:0.0000) (sitting on:0.6933) (standing on:0.1250) (using:0.5500) (walking in:0.3333) (walking on:0.7973) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5194;     R @ 100: 0.5797;     R @ 500: 0.6240;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3731;    mR @ 100: 0.4263;    mR @ 500: 0.4777;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5268) (covered in:0.8750) (covering:0.2857) (eating:0.7353) (flying in:0.5000) (growing on:0.3750) (hanging from:0.4194) (lying on:0.0500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.9134) (says:0.0000) (sitting on:0.6933) (standing on:0.1250) (using:0.5500) (walking in:0.3333) (walking on:0.7973) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-01-05 04:01:09 - train.py[line:487] - INFO: 0.5797047619047618
2023-01-05 04:01:09 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 04:01:09 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.344 | loss_v1 0 | loss_v2 0 | nll_loss 0.187 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.579705 | ppl 1.14 | vqa_score 0.5146 | wps 118.9 | wpb 89.9 | bsz 30 | num_updates 8000 | best_R@100 0.611199
2023-01-05 04:01:09 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2023-01-05 04:01:09 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-05 04:01:54 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-05 04:03:24 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.5797047619047618) (writing took 135.04403838701546 seconds)
2023-01-05 04:03:36 - progress_bar.py[line:274] - INFO: epoch 001:   8022 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=0.3, ups=0, wpb=110.6, bsz=40, num_updates=8010, lr=4.80045e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25049
2023-01-05 04:03:47 - progress_bar.py[line:274] - INFO: epoch 001:   8032 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=8020, lr=4.79994e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25061
2023-01-05 04:03:59 - progress_bar.py[line:274] - INFO: epoch 001:   8042 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=8030, lr=4.79943e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=25072
2023-01-05 04:04:10 - progress_bar.py[line:274] - INFO: epoch 001:   8052 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.4, ups=0.91, wpb=110.8, bsz=40, num_updates=8040, lr=4.79892e-05, gnorm=0.627, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25084
2023-01-05 04:04:22 - progress_bar.py[line:274] - INFO: epoch 001:   8062 / 102288 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.7, ups=0.87, wpb=110, bsz=40, num_updates=8050, lr=4.79842e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25095
2023-01-05 04:04:34 - progress_bar.py[line:274] - INFO: epoch 001:   8072 / 102288 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=8060, lr=4.79791e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25107
2023-01-05 04:04:45 - progress_bar.py[line:274] - INFO: epoch 001:   8082 / 102288 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=8070, lr=4.7974e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25118
2023-01-05 04:04:57 - progress_bar.py[line:274] - INFO: epoch 001:   8092 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=8080, lr=4.79689e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25130
2023-01-05 04:05:08 - progress_bar.py[line:274] - INFO: epoch 001:   8102 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=8090, lr=4.79638e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25142
2023-01-05 04:05:20 - progress_bar.py[line:274] - INFO: epoch 001:   8112 / 102288 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.4, ups=0.92, wpb=109.7, bsz=40, num_updates=8100, lr=4.79587e-05, gnorm=0.607, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25153
2023-01-05 04:05:31 - progress_bar.py[line:274] - INFO: epoch 001:   8122 / 102288 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=8110, lr=4.79536e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25165
2023-01-05 04:05:43 - progress_bar.py[line:274] - INFO: epoch 001:   8132 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=8120, lr=4.79485e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25176
2023-01-05 04:05:55 - progress_bar.py[line:274] - INFO: epoch 001:   8142 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.4, ups=0.87, wpb=111, bsz=40, num_updates=8130, lr=4.79434e-05, gnorm=0.714, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25188
2023-01-05 04:06:07 - progress_bar.py[line:274] - INFO: epoch 001:   8152 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.4, ups=0.88, wpb=111.9, bsz=40, num_updates=8140, lr=4.79383e-05, gnorm=0.61, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25200
2023-01-05 04:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   8162 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.6, ups=0.89, wpb=108.4, bsz=40, num_updates=8150, lr=4.79332e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25212
2023-01-05 04:06:30 - progress_bar.py[line:274] - INFO: epoch 001:   8172 / 102288 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=8160, lr=4.79281e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25223
2023-01-05 04:06:41 - progress_bar.py[line:274] - INFO: epoch 001:   8182 / 102288 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.92, wpb=109.4, bsz=40, num_updates=8170, lr=4.79231e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25234
2023-01-05 04:06:53 - progress_bar.py[line:274] - INFO: epoch 001:   8192 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=8180, lr=4.7918e-05, gnorm=0.569, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25246
2023-01-05 04:07:05 - progress_bar.py[line:274] - INFO: epoch 001:   8202 / 102288 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=8190, lr=4.79129e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25258
2023-01-05 04:07:16 - progress_bar.py[line:274] - INFO: epoch 001:   8212 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.8, ups=0.93, wpb=110, bsz=40, num_updates=8200, lr=4.79078e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25269
2023-01-05 04:07:27 - progress_bar.py[line:274] - INFO: epoch 001:   8222 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=8210, lr=4.79027e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25280
2023-01-05 04:07:39 - progress_bar.py[line:274] - INFO: epoch 001:   8232 / 102288 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=94.6, ups=0.86, wpb=110.2, bsz=40, num_updates=8220, lr=4.78976e-05, gnorm=0.842, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=25292
2023-01-05 04:07:51 - progress_bar.py[line:274] - INFO: epoch 001:   8242 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100, ups=0.92, wpb=109.2, bsz=40, num_updates=8230, lr=4.78925e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25304
2023-01-05 04:08:02 - progress_bar.py[line:274] - INFO: epoch 001:   8252 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=8240, lr=4.78874e-05, gnorm=0.685, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25315
2023-01-05 04:08:14 - progress_bar.py[line:274] - INFO: epoch 001:   8262 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=8250, lr=4.78823e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25327
2023-01-05 04:08:26 - progress_bar.py[line:274] - INFO: epoch 001:   8272 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.9, ups=0.87, wpb=110.7, bsz=40, num_updates=8260, lr=4.78772e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25339
2023-01-05 04:08:38 - progress_bar.py[line:274] - INFO: epoch 001:   8282 / 102288 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.4, ups=0.89, wpb=109, bsz=40, num_updates=8270, lr=4.78721e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25351
2023-01-05 04:08:49 - progress_bar.py[line:274] - INFO: epoch 001:   8292 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=8280, lr=4.7867e-05, gnorm=0.591, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25362
2023-01-05 04:09:00 - progress_bar.py[line:274] - INFO: epoch 001:   8302 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=104.6, ups=0.94, wpb=111.7, bsz=40, num_updates=8290, lr=4.7862e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25373
2023-01-05 04:09:12 - progress_bar.py[line:274] - INFO: epoch 001:   8312 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=8300, lr=4.78569e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25385
2023-01-05 04:09:23 - progress_bar.py[line:274] - INFO: epoch 001:   8322 / 102288 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=8310, lr=4.78518e-05, gnorm=0.611, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25397
2023-01-05 04:09:35 - progress_bar.py[line:274] - INFO: epoch 001:   8332 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.3, ups=0.92, wpb=109.6, bsz=40, num_updates=8320, lr=4.78467e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25408
2023-01-05 04:09:47 - progress_bar.py[line:274] - INFO: epoch 001:   8342 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=8330, lr=4.78416e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25419
2023-01-05 04:09:58 - progress_bar.py[line:274] - INFO: epoch 001:   8352 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=8340, lr=4.78365e-05, gnorm=0.66, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25431
2023-01-05 04:10:09 - progress_bar.py[line:274] - INFO: epoch 001:   8362 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.4, ups=0.92, wpb=109.7, bsz=40, num_updates=8350, lr=4.78314e-05, gnorm=0.63, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25442
2023-01-05 04:10:21 - progress_bar.py[line:274] - INFO: epoch 001:   8372 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101, ups=0.91, wpb=111.2, bsz=40, num_updates=8360, lr=4.78263e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25454
2023-01-05 04:10:32 - progress_bar.py[line:274] - INFO: epoch 001:   8382 / 102288 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.8, ups=0.89, wpb=107.5, bsz=40, num_updates=8370, lr=4.78212e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25465
2023-01-05 04:10:44 - progress_bar.py[line:274] - INFO: epoch 001:   8392 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=8380, lr=4.78161e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25477
2023-01-05 04:10:55 - progress_bar.py[line:274] - INFO: epoch 001:   8402 / 102288 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=8390, lr=4.7811e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25489
2023-01-05 04:11:07 - progress_bar.py[line:274] - INFO: epoch 001:   8412 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=8400, lr=4.78059e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25500
2023-01-05 04:11:19 - progress_bar.py[line:274] - INFO: epoch 001:   8422 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.9, ups=0.87, wpb=110.2, bsz=40, num_updates=8410, lr=4.78008e-05, gnorm=0.8, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25512
2023-01-05 04:11:31 - progress_bar.py[line:274] - INFO: epoch 001:   8432 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.9, ups=0.89, wpb=110.1, bsz=40, num_updates=8420, lr=4.77958e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25524
2023-01-05 04:11:43 - progress_bar.py[line:274] - INFO: epoch 001:   8442 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.2, ups=0.88, wpb=109.5, bsz=40, num_updates=8430, lr=4.77907e-05, gnorm=0.677, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25536
2023-01-05 04:11:54 - progress_bar.py[line:274] - INFO: epoch 001:   8452 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=8440, lr=4.77856e-05, gnorm=0.801, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25547
2023-01-05 04:12:06 - progress_bar.py[line:274] - INFO: epoch 001:   8462 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.2, ups=0.88, wpb=111.6, bsz=40, num_updates=8450, lr=4.77805e-05, gnorm=0.67, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25559
2023-01-05 04:12:18 - progress_bar.py[line:274] - INFO: epoch 001:   8472 / 102288 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.5, ups=0.87, wpb=112.3, bsz=40, num_updates=8460, lr=4.77754e-05, gnorm=0.715, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25571
2023-01-05 04:12:30 - progress_bar.py[line:274] - INFO: epoch 001:   8482 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=8470, lr=4.77703e-05, gnorm=0.684, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25583
2023-01-05 04:12:41 - progress_bar.py[line:274] - INFO: epoch 001:   8492 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=8480, lr=4.77652e-05, gnorm=0.558, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25594
2023-01-05 04:12:53 - progress_bar.py[line:274] - INFO: epoch 001:   8502 / 102288 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=8490, lr=4.77601e-05, gnorm=0.589, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=25606
2023-01-05 04:13:05 - progress_bar.py[line:274] - INFO: epoch 001:   8512 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=93.6, ups=0.86, wpb=109, bsz=40, num_updates=8500, lr=4.7755e-05, gnorm=0.573, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=25618
2023-01-05 04:13:17 - progress_bar.py[line:274] - INFO: epoch 001:   8522 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=8510, lr=4.77499e-05, gnorm=0.663, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25630
2023-01-05 04:13:28 - progress_bar.py[line:274] - INFO: epoch 001:   8532 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.6, ups=0.93, wpb=108.9, bsz=40, num_updates=8520, lr=4.77448e-05, gnorm=0.699, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25641
2023-01-05 04:13:39 - progress_bar.py[line:274] - INFO: epoch 001:   8542 / 102288 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=8530, lr=4.77397e-05, gnorm=0.737, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25652
2023-01-05 04:13:51 - progress_bar.py[line:274] - INFO: epoch 001:   8552 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=8540, lr=4.77347e-05, gnorm=0.592, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25664
2023-01-05 04:14:02 - progress_bar.py[line:274] - INFO: epoch 001:   8562 / 102288 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=103.4, ups=0.94, wpb=109.8, bsz=40, num_updates=8550, lr=4.77296e-05, gnorm=0.716, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25675
2023-01-05 04:14:13 - progress_bar.py[line:274] - INFO: epoch 001:   8572 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=8560, lr=4.77245e-05, gnorm=0.572, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25686
2023-01-05 04:14:25 - progress_bar.py[line:274] - INFO: epoch 001:   8582 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=103.5, ups=0.93, wpb=111.6, bsz=40, num_updates=8570, lr=4.77194e-05, gnorm=0.673, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25698
2023-01-05 04:14:36 - progress_bar.py[line:274] - INFO: epoch 001:   8592 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.9, ups=0.9, wpb=109.6, bsz=40, num_updates=8580, lr=4.77143e-05, gnorm=0.614, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25709
2023-01-05 04:14:48 - progress_bar.py[line:274] - INFO: epoch 001:   8602 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=8590, lr=4.77092e-05, gnorm=0.681, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25721
2023-01-05 04:14:59 - progress_bar.py[line:274] - INFO: epoch 001:   8612 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=8600, lr=4.77041e-05, gnorm=0.653, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25732
2023-01-05 04:15:11 - progress_bar.py[line:274] - INFO: epoch 001:   8622 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=8610, lr=4.7699e-05, gnorm=0.584, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25744
2023-01-05 04:15:23 - progress_bar.py[line:274] - INFO: epoch 001:   8632 / 102288 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=8620, lr=4.76939e-05, gnorm=0.604, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25756
2023-01-05 04:15:34 - progress_bar.py[line:274] - INFO: epoch 001:   8642 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.1, ups=0.91, wpb=110.5, bsz=40, num_updates=8630, lr=4.76888e-05, gnorm=0.569, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25767
2023-01-05 04:15:46 - progress_bar.py[line:274] - INFO: epoch 001:   8652 / 102288 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=8640, lr=4.76837e-05, gnorm=0.609, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25779
2023-01-05 04:15:57 - progress_bar.py[line:274] - INFO: epoch 001:   8662 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=8650, lr=4.76786e-05, gnorm=0.636, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25790
2023-01-05 04:16:09 - progress_bar.py[line:274] - INFO: epoch 001:   8672 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=8660, lr=4.76736e-05, gnorm=0.777, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25802
2023-01-05 04:16:21 - progress_bar.py[line:274] - INFO: epoch 001:   8682 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.5, ups=0.88, wpb=109.6, bsz=40, num_updates=8670, lr=4.76685e-05, gnorm=0.695, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25814
2023-01-05 04:16:32 - progress_bar.py[line:274] - INFO: epoch 001:   8692 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=8680, lr=4.76634e-05, gnorm=0.756, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25825
2023-01-05 04:16:44 - progress_bar.py[line:274] - INFO: epoch 001:   8702 / 102288 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=8690, lr=4.76583e-05, gnorm=0.659, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25837
2023-01-05 04:16:55 - progress_bar.py[line:274] - INFO: epoch 001:   8712 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=8700, lr=4.76532e-05, gnorm=0.591, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25848
2023-01-05 04:17:06 - progress_bar.py[line:274] - INFO: epoch 001:   8722 / 102288 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=102.7, ups=0.93, wpb=110.9, bsz=40, num_updates=8710, lr=4.76481e-05, gnorm=0.725, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25860
2023-01-05 04:17:18 - progress_bar.py[line:274] - INFO: epoch 001:   8732 / 102288 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=8720, lr=4.7643e-05, gnorm=0.563, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25871
2023-01-05 04:17:30 - progress_bar.py[line:274] - INFO: epoch 001:   8742 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=8730, lr=4.76379e-05, gnorm=0.634, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25883
2023-01-05 04:17:41 - progress_bar.py[line:274] - INFO: epoch 001:   8752 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=8740, lr=4.76328e-05, gnorm=0.653, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25894
2023-01-05 04:17:53 - progress_bar.py[line:274] - INFO: epoch 001:   8762 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.9, ups=0.88, wpb=110.2, bsz=40, num_updates=8750, lr=4.76277e-05, gnorm=0.57, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25906
2023-01-05 04:18:05 - progress_bar.py[line:274] - INFO: epoch 001:   8772 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=8760, lr=4.76226e-05, gnorm=0.666, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25918
2023-01-05 04:18:16 - progress_bar.py[line:274] - INFO: epoch 001:   8782 / 102288 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.3, ups=0.91, wpb=108.5, bsz=40, num_updates=8770, lr=4.76175e-05, gnorm=0.58, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25930
2023-01-05 04:18:28 - progress_bar.py[line:274] - INFO: epoch 001:   8792 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=8780, lr=4.76125e-05, gnorm=0.699, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25941
2023-01-05 04:18:33 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 04:18:41 - progress_bar.py[line:274] - INFO: epoch 001:   8803 / 102288 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=90.7, ups=0.83, wpb=109.3, bsz=40, num_updates=8790, lr=4.76074e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=25954
2023-01-05 04:18:52 - progress_bar.py[line:274] - INFO: epoch 001:   8813 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.91, wpb=110.1, bsz=40, num_updates=8800, lr=4.76023e-05, gnorm=0.795, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25965
2023-01-05 04:19:03 - progress_bar.py[line:274] - INFO: epoch 001:   8823 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=8810, lr=4.75972e-05, gnorm=0.652, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25976
2023-01-05 04:19:15 - progress_bar.py[line:274] - INFO: epoch 001:   8833 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.2, ups=0.92, wpb=109.2, bsz=40, num_updates=8820, lr=4.75921e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25988
2023-01-05 04:19:26 - progress_bar.py[line:274] - INFO: epoch 001:   8843 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.2, ups=0.91, wpb=109.6, bsz=40, num_updates=8830, lr=4.7587e-05, gnorm=0.656, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25999
2023-01-05 04:19:37 - progress_bar.py[line:274] - INFO: epoch 001:   8853 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.7, ups=0.92, wpb=110, bsz=40, num_updates=8840, lr=4.75819e-05, gnorm=0.582, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26010
2023-01-05 04:19:49 - progress_bar.py[line:274] - INFO: epoch 001:   8863 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.88, wpb=111.6, bsz=40, num_updates=8850, lr=4.75768e-05, gnorm=0.765, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26022
2023-01-05 04:20:01 - progress_bar.py[line:274] - INFO: epoch 001:   8873 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.9, ups=0.9, wpb=109.6, bsz=40, num_updates=8860, lr=4.75717e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26034
2023-01-05 04:20:12 - progress_bar.py[line:274] - INFO: epoch 001:   8883 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.3, ups=0.88, wpb=109.4, bsz=40, num_updates=8870, lr=4.75666e-05, gnorm=0.658, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26045
2023-01-05 04:20:24 - progress_bar.py[line:274] - INFO: epoch 001:   8893 / 102288 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.7, ups=0.89, wpb=109.8, bsz=40, num_updates=8880, lr=4.75615e-05, gnorm=0.575, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26057
2023-01-05 04:20:36 - progress_bar.py[line:274] - INFO: epoch 001:   8903 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.1, ups=0.89, wpb=108.8, bsz=40, num_updates=8890, lr=4.75564e-05, gnorm=0.566, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26069
2023-01-05 04:20:47 - progress_bar.py[line:274] - INFO: epoch 001:   8913 / 102288 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.3, ups=0.87, wpb=110.6, bsz=40, num_updates=8900, lr=4.75514e-05, gnorm=0.826, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26081
2023-01-05 04:20:59 - progress_bar.py[line:274] - INFO: epoch 001:   8923 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=8910, lr=4.75463e-05, gnorm=0.697, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26092
2023-01-05 04:21:10 - progress_bar.py[line:274] - INFO: epoch 001:   8933 / 102288 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.7, ups=0.89, wpb=108.3, bsz=40, num_updates=8920, lr=4.75412e-05, gnorm=0.737, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26103
2023-01-05 04:21:22 - progress_bar.py[line:274] - INFO: epoch 001:   8943 / 102288 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.8, ups=0.9, wpb=110.7, bsz=40, num_updates=8930, lr=4.75361e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26115
2023-01-05 04:21:33 - progress_bar.py[line:274] - INFO: epoch 001:   8953 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.5, ups=0.91, wpb=111, bsz=40, num_updates=8940, lr=4.7531e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26126
2023-01-05 04:21:45 - progress_bar.py[line:274] - INFO: epoch 001:   8963 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=8950, lr=4.75259e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26138
2023-01-05 04:21:56 - progress_bar.py[line:274] - INFO: epoch 001:   8973 / 102288 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=8960, lr=4.75208e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26149
2023-01-05 04:22:08 - progress_bar.py[line:274] - INFO: epoch 001:   8983 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=8970, lr=4.75157e-05, gnorm=0.594, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26161
2023-01-05 04:22:20 - progress_bar.py[line:274] - INFO: epoch 001:   8993 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.6, ups=0.88, wpb=109.5, bsz=40, num_updates=8980, lr=4.75106e-05, gnorm=0.768, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26173
2023-01-05 04:22:31 - progress_bar.py[line:274] - INFO: epoch 001:   9003 / 102288 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.8, ups=0.9, wpb=109.2, bsz=40, num_updates=8990, lr=4.75055e-05, gnorm=0.702, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26184
2023-01-05 04:22:43 - progress_bar.py[line:274] - INFO: epoch 001:   9013 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=9000, lr=4.75004e-05, gnorm=0.755, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26196
2023-01-05 04:22:54 - progress_bar.py[line:274] - INFO: epoch 001:   9023 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.7, ups=0.91, wpb=111.2, bsz=40, num_updates=9010, lr=4.74953e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26207
2023-01-05 04:23:06 - progress_bar.py[line:274] - INFO: epoch 001:   9033 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.6, ups=0.92, wpb=108.8, bsz=40, num_updates=9020, lr=4.74902e-05, gnorm=0.96, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26219
2023-01-05 04:23:18 - progress_bar.py[line:274] - INFO: epoch 001:   9043 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=9030, lr=4.74852e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26230
2023-01-05 04:23:29 - progress_bar.py[line:274] - INFO: epoch 001:   9053 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.92, wpb=109.9, bsz=40, num_updates=9040, lr=4.74801e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26242
2023-01-05 04:23:41 - progress_bar.py[line:274] - INFO: epoch 001:   9063 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=9050, lr=4.7475e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26254
2023-01-05 04:23:52 - progress_bar.py[line:274] - INFO: epoch 001:   9073 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=9060, lr=4.74699e-05, gnorm=0.759, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26266
2023-01-05 04:24:04 - progress_bar.py[line:274] - INFO: epoch 001:   9083 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=9070, lr=4.74648e-05, gnorm=0.623, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26277
2023-01-05 04:24:16 - progress_bar.py[line:274] - INFO: epoch 001:   9093 / 102288 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.5, ups=0.91, wpb=108.7, bsz=40, num_updates=9080, lr=4.74597e-05, gnorm=0.754, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26289
2023-01-05 04:24:27 - progress_bar.py[line:274] - INFO: epoch 001:   9103 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=9090, lr=4.74546e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26300
2023-01-05 04:24:39 - progress_bar.py[line:274] - INFO: epoch 001:   9113 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=9100, lr=4.74495e-05, gnorm=0.775, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26312
2023-01-05 04:24:51 - progress_bar.py[line:274] - INFO: epoch 001:   9123 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.2, ups=0.9, wpb=110.7, bsz=40, num_updates=9110, lr=4.74444e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26324
2023-01-05 04:25:02 - progress_bar.py[line:274] - INFO: epoch 001:   9133 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=106.5, ups=0.96, wpb=111.4, bsz=40, num_updates=9120, lr=4.74393e-05, gnorm=0.537, clip=0, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=26335
2023-01-05 04:25:13 - progress_bar.py[line:274] - INFO: epoch 001:   9143 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=9130, lr=4.74342e-05, gnorm=0.561, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26346
2023-01-05 04:25:25 - progress_bar.py[line:274] - INFO: epoch 001:   9153 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=102.1, ups=0.93, wpb=109.7, bsz=40, num_updates=9140, lr=4.74291e-05, gnorm=0.646, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26358
2023-01-05 04:25:36 - progress_bar.py[line:274] - INFO: epoch 001:   9163 / 102288 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.2, ups=0.88, wpb=108.1, bsz=40, num_updates=9150, lr=4.74241e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26369
2023-01-05 04:25:48 - progress_bar.py[line:274] - INFO: epoch 001:   9173 / 102288 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=9160, lr=4.7419e-05, gnorm=0.606, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26381
2023-01-05 04:25:59 - progress_bar.py[line:274] - INFO: epoch 001:   9183 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=9170, lr=4.74139e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26392
2023-01-05 04:26:11 - progress_bar.py[line:274] - INFO: epoch 001:   9193 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103.1, ups=0.92, wpb=112.5, bsz=40, num_updates=9180, lr=4.74088e-05, gnorm=0.609, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26404
2023-01-05 04:26:22 - progress_bar.py[line:274] - INFO: epoch 001:   9203 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.6, ups=0.93, wpb=109.6, bsz=40, num_updates=9190, lr=4.74037e-05, gnorm=0.584, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26415
2023-01-05 04:26:33 - progress_bar.py[line:274] - INFO: epoch 001:   9213 / 102288 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.8, ups=0.91, wpb=110.2, bsz=40, num_updates=9200, lr=4.73986e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26426
2023-01-05 04:26:45 - progress_bar.py[line:274] - INFO: epoch 001:   9223 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=9210, lr=4.73935e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26438
2023-01-05 04:26:57 - progress_bar.py[line:274] - INFO: epoch 001:   9233 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=9220, lr=4.73884e-05, gnorm=0.664, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26450
2023-01-05 04:27:08 - progress_bar.py[line:274] - INFO: epoch 001:   9243 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.7, ups=0.91, wpb=109.9, bsz=40, num_updates=9230, lr=4.73833e-05, gnorm=0.717, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26461
2023-01-05 04:27:20 - progress_bar.py[line:274] - INFO: epoch 001:   9253 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.8, ups=0.89, wpb=108.7, bsz=40, num_updates=9240, lr=4.73782e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26473
2023-01-05 04:27:31 - progress_bar.py[line:274] - INFO: epoch 001:   9263 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.1, ups=0.9, wpb=111.9, bsz=40, num_updates=9250, lr=4.73731e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26484
2023-01-05 04:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   9273 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=9260, lr=4.7368e-05, gnorm=0.646, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26496
2023-01-05 04:27:54 - progress_bar.py[line:274] - INFO: epoch 001:   9283 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.2, ups=0.92, wpb=111.4, bsz=40, num_updates=9270, lr=4.7363e-05, gnorm=0.532, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26507
2023-01-05 04:28:06 - progress_bar.py[line:274] - INFO: epoch 001:   9293 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=9280, lr=4.73579e-05, gnorm=0.62, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26519
2023-01-05 04:28:17 - progress_bar.py[line:274] - INFO: epoch 001:   9303 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.7, ups=0.93, wpb=109.9, bsz=40, num_updates=9290, lr=4.73528e-05, gnorm=0.573, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26530
2023-01-05 04:28:28 - progress_bar.py[line:274] - INFO: epoch 001:   9313 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=9300, lr=4.73477e-05, gnorm=0.53, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26542
2023-01-05 04:28:40 - progress_bar.py[line:274] - INFO: epoch 001:   9323 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.2, ups=0.9, wpb=108.5, bsz=40, num_updates=9310, lr=4.73426e-05, gnorm=0.55, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26553
2023-01-05 04:28:51 - progress_bar.py[line:274] - INFO: epoch 001:   9333 / 102288 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=9320, lr=4.73375e-05, gnorm=0.643, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26564
2023-01-05 04:28:52 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 04:29:03 - progress_bar.py[line:274] - INFO: epoch 001:   9344 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=91.4, ups=0.83, wpb=110.7, bsz=40, num_updates=9330, lr=4.73324e-05, gnorm=0.587, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=26577
2023-01-05 04:29:15 - progress_bar.py[line:274] - INFO: epoch 001:   9354 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.9, ups=0.87, wpb=110.3, bsz=40, num_updates=9340, lr=4.73273e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26588
2023-01-05 04:29:26 - progress_bar.py[line:274] - INFO: epoch 001:   9364 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.3, ups=0.89, wpb=111.4, bsz=40, num_updates=9350, lr=4.73222e-05, gnorm=0.685, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26600
2023-01-05 04:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   9374 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.9, wpb=109.5, bsz=40, num_updates=9360, lr=4.73171e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26611
2023-01-05 04:29:49 - progress_bar.py[line:274] - INFO: epoch 001:   9384 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=9370, lr=4.7312e-05, gnorm=0.667, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26622
2023-01-05 04:30:01 - progress_bar.py[line:274] - INFO: epoch 001:   9394 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=9380, lr=4.73069e-05, gnorm=0.728, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26634
2023-01-05 04:30:12 - progress_bar.py[line:274] - INFO: epoch 001:   9404 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.91, wpb=110.1, bsz=40, num_updates=9390, lr=4.73019e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26645
2023-01-05 04:30:23 - progress_bar.py[line:274] - INFO: epoch 001:   9414 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=102.6, ups=0.93, wpb=110.7, bsz=40, num_updates=9400, lr=4.72968e-05, gnorm=0.57, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26656
2023-01-05 04:30:34 - progress_bar.py[line:274] - INFO: epoch 001:   9424 / 102288 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.3, ups=0.93, wpb=109.1, bsz=40, num_updates=9410, lr=4.72917e-05, gnorm=0.583, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26667
2023-01-05 04:30:45 - progress_bar.py[line:274] - INFO: epoch 001:   9434 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.1, ups=0.92, wpb=109.9, bsz=40, num_updates=9420, lr=4.72866e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26678
2023-01-05 04:30:56 - progress_bar.py[line:274] - INFO: epoch 001:   9444 / 102288 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=9430, lr=4.72815e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26690
2023-01-05 04:31:08 - progress_bar.py[line:274] - INFO: epoch 001:   9454 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.4, ups=0.91, wpb=111.6, bsz=40, num_updates=9440, lr=4.72764e-05, gnorm=0.599, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26701
2023-01-05 04:31:19 - progress_bar.py[line:274] - INFO: epoch 001:   9464 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.8, ups=0.91, wpb=109.1, bsz=40, num_updates=9450, lr=4.72713e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26712
2023-01-05 04:31:30 - progress_bar.py[line:274] - INFO: epoch 001:   9474 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.9, ups=0.88, wpb=112.2, bsz=40, num_updates=9460, lr=4.72662e-05, gnorm=0.588, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26724
2023-01-05 04:31:42 - progress_bar.py[line:274] - INFO: epoch 001:   9484 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.2, ups=0.9, wpb=111.2, bsz=40, num_updates=9470, lr=4.72611e-05, gnorm=0.796, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26735
2023-01-05 04:31:53 - progress_bar.py[line:274] - INFO: epoch 001:   9494 / 102288 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.9, ups=0.89, wpb=108.6, bsz=40, num_updates=9480, lr=4.7256e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26746
2023-01-05 04:32:05 - progress_bar.py[line:274] - INFO: epoch 001:   9504 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=9490, lr=4.72509e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26758
2023-01-05 04:32:16 - progress_bar.py[line:274] - INFO: epoch 001:   9514 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.88, wpb=111, bsz=40, num_updates=9500, lr=4.72458e-05, gnorm=0.633, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26769
2023-01-05 04:32:27 - progress_bar.py[line:274] - INFO: epoch 001:   9524 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.9, ups=0.91, wpb=109.2, bsz=40, num_updates=9510, lr=4.72408e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26781
2023-01-05 04:32:39 - progress_bar.py[line:274] - INFO: epoch 001:   9534 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=9520, lr=4.72357e-05, gnorm=0.749, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26792
2023-01-05 04:32:50 - progress_bar.py[line:274] - INFO: epoch 001:   9544 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=9530, lr=4.72306e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26803
2023-01-05 04:33:01 - progress_bar.py[line:274] - INFO: epoch 001:   9554 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=9540, lr=4.72255e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26815
2023-01-05 04:33:13 - progress_bar.py[line:274] - INFO: epoch 001:   9564 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.1, ups=0.89, wpb=108.9, bsz=40, num_updates=9550, lr=4.72204e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26826
2023-01-05 04:33:24 - progress_bar.py[line:274] - INFO: epoch 001:   9574 / 102288 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.6, ups=0.89, wpb=108.5, bsz=40, num_updates=9560, lr=4.72153e-05, gnorm=0.841, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26838
2023-01-05 04:33:36 - progress_bar.py[line:274] - INFO: epoch 001:   9584 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=9570, lr=4.72102e-05, gnorm=0.565, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26849
2023-01-05 04:33:47 - progress_bar.py[line:274] - INFO: epoch 001:   9594 / 102288 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.1, ups=0.92, wpb=109.2, bsz=40, num_updates=9580, lr=4.72051e-05, gnorm=0.67, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26860
2023-01-05 04:33:58 - progress_bar.py[line:274] - INFO: epoch 001:   9604 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=9590, lr=4.72e-05, gnorm=0.566, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26871
2023-01-05 04:34:10 - progress_bar.py[line:274] - INFO: epoch 001:   9614 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.6, ups=0.88, wpb=110.1, bsz=40, num_updates=9600, lr=4.71949e-05, gnorm=0.548, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26883
2023-01-05 04:34:21 - progress_bar.py[line:274] - INFO: epoch 001:   9624 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=9610, lr=4.71898e-05, gnorm=0.672, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26894
2023-01-05 04:34:32 - progress_bar.py[line:274] - INFO: epoch 001:   9634 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=9620, lr=4.71847e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26906
2023-01-05 04:34:44 - progress_bar.py[line:274] - INFO: epoch 001:   9644 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=9630, lr=4.71796e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26917
2023-01-05 04:34:55 - progress_bar.py[line:274] - INFO: epoch 001:   9654 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.7, ups=0.91, wpb=111.1, bsz=40, num_updates=9640, lr=4.71746e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26928
2023-01-05 04:35:06 - progress_bar.py[line:274] - INFO: epoch 001:   9664 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=9650, lr=4.71695e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26940
2023-01-05 04:35:18 - progress_bar.py[line:274] - INFO: epoch 001:   9674 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=9660, lr=4.71644e-05, gnorm=0.511, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26951
2023-01-05 04:35:29 - progress_bar.py[line:274] - INFO: epoch 001:   9684 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=9670, lr=4.71593e-05, gnorm=0.542, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26962
2023-01-05 04:35:40 - progress_bar.py[line:274] - INFO: epoch 001:   9694 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.9, ups=0.92, wpb=108, bsz=40, num_updates=9680, lr=4.71542e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26973
2023-01-05 04:35:52 - progress_bar.py[line:274] - INFO: epoch 001:   9704 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=9690, lr=4.71491e-05, gnorm=0.676, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26985
2023-01-05 04:36:03 - progress_bar.py[line:274] - INFO: epoch 001:   9714 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=9700, lr=4.7144e-05, gnorm=0.67, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26996
2023-01-05 04:36:14 - progress_bar.py[line:274] - INFO: epoch 001:   9724 / 102288 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.2, ups=0.9, wpb=109.9, bsz=40, num_updates=9710, lr=4.71389e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27008
2023-01-05 04:36:26 - progress_bar.py[line:274] - INFO: epoch 001:   9734 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.1, ups=0.9, wpb=111.9, bsz=40, num_updates=9720, lr=4.71338e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27019
2023-01-05 04:36:37 - progress_bar.py[line:274] - INFO: epoch 001:   9744 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=9730, lr=4.71287e-05, gnorm=0.608, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27030
2023-01-05 04:36:48 - progress_bar.py[line:274] - INFO: epoch 001:   9754 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=9740, lr=4.71236e-05, gnorm=0.709, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27042
2023-01-05 04:37:00 - progress_bar.py[line:274] - INFO: epoch 001:   9764 / 102288 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=9750, lr=4.71185e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27053
2023-01-05 04:37:11 - progress_bar.py[line:274] - INFO: epoch 001:   9774 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.5, ups=0.89, wpb=109.5, bsz=40, num_updates=9760, lr=4.71135e-05, gnorm=0.609, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27065
2023-01-05 04:37:23 - progress_bar.py[line:274] - INFO: epoch 001:   9784 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.8, ups=0.9, wpb=109.3, bsz=40, num_updates=9770, lr=4.71084e-05, gnorm=0.671, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=27076
2023-01-05 04:37:34 - progress_bar.py[line:274] - INFO: epoch 001:   9794 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.3, ups=0.92, wpb=110.2, bsz=40, num_updates=9780, lr=4.71033e-05, gnorm=0.75, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27087
2023-01-05 04:37:45 - progress_bar.py[line:274] - INFO: epoch 001:   9804 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96, ups=0.87, wpb=110.6, bsz=40, num_updates=9790, lr=4.70982e-05, gnorm=0.689, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27099
2023-01-05 04:37:57 - progress_bar.py[line:274] - INFO: epoch 001:   9814 / 102288 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=9800, lr=4.70931e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27110
2023-01-05 04:38:08 - progress_bar.py[line:274] - INFO: epoch 001:   9824 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=9810, lr=4.7088e-05, gnorm=0.551, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27121
2023-01-05 04:38:19 - progress_bar.py[line:274] - INFO: epoch 001:   9834 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=9820, lr=4.70829e-05, gnorm=0.577, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=27132
2023-01-05 04:38:30 - progress_bar.py[line:274] - INFO: epoch 001:   9844 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=9830, lr=4.70778e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27144
2023-01-05 04:38:42 - progress_bar.py[line:274] - INFO: epoch 001:   9854 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=9840, lr=4.70727e-05, gnorm=0.549, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27155
2023-01-05 04:38:53 - progress_bar.py[line:274] - INFO: epoch 001:   9864 / 102288 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=9850, lr=4.70676e-05, gnorm=0.647, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27167
2023-01-05 04:39:05 - progress_bar.py[line:274] - INFO: epoch 001:   9874 / 102288 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.1, ups=0.87, wpb=110.4, bsz=40, num_updates=9860, lr=4.70625e-05, gnorm=0.526, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27178
2023-01-05 04:39:14 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 04:39:17 - progress_bar.py[line:274] - INFO: epoch 001:   9885 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=89.7, ups=0.83, wpb=108.5, bsz=40, num_updates=9870, lr=4.70574e-05, gnorm=0.757, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=27191
2023-01-05 04:39:29 - progress_bar.py[line:274] - INFO: epoch 001:   9895 / 102288 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.3, ups=0.92, wpb=109.3, bsz=40, num_updates=9880, lr=4.70524e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27202
2023-01-05 04:39:40 - progress_bar.py[line:274] - INFO: epoch 001:   9905 / 102288 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=9890, lr=4.70473e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27213
2023-01-05 04:39:51 - progress_bar.py[line:274] - INFO: epoch 001:   9915 / 102288 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=9900, lr=4.70422e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27224
2023-01-05 04:40:03 - progress_bar.py[line:274] - INFO: epoch 001:   9925 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.89, wpb=108.6, bsz=40, num_updates=9910, lr=4.70371e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27236
2023-01-05 04:40:14 - progress_bar.py[line:274] - INFO: epoch 001:   9935 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=9920, lr=4.7032e-05, gnorm=0.634, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27247
2023-01-05 04:40:26 - progress_bar.py[line:274] - INFO: epoch 001:   9945 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.89, wpb=110.3, bsz=40, num_updates=9930, lr=4.70269e-05, gnorm=0.553, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=27259
2023-01-05 04:40:37 - progress_bar.py[line:274] - INFO: epoch 001:   9955 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.88, wpb=111, bsz=40, num_updates=9940, lr=4.70218e-05, gnorm=0.529, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27270
2023-01-05 04:40:48 - progress_bar.py[line:274] - INFO: epoch 001:   9965 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.3, ups=0.9, wpb=110.8, bsz=40, num_updates=9950, lr=4.70167e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27282
2023-01-05 04:40:59 - progress_bar.py[line:274] - INFO: epoch 001:   9975 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.5, ups=0.93, wpb=110.3, bsz=40, num_updates=9960, lr=4.70116e-05, gnorm=0.555, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27293
2023-01-05 04:41:11 - progress_bar.py[line:274] - INFO: epoch 001:   9985 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.8, ups=0.87, wpb=110, bsz=40, num_updates=9970, lr=4.70065e-05, gnorm=0.558, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27304
2023-01-05 04:41:22 - progress_bar.py[line:274] - INFO: epoch 001:   9995 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.2, ups=0.92, wpb=110.6, bsz=40, num_updates=9980, lr=4.70014e-05, gnorm=0.553, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=27315
2023-01-05 04:41:34 - progress_bar.py[line:274] - INFO: epoch 001:  10005 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=9990, lr=4.69963e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27327
2023-01-05 04:41:45 - progress_bar.py[line:274] - INFO: epoch 001:  10015 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=10000, lr=4.69913e-05, gnorm=0.552, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27338
2023-01-05 04:41:45 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 04:41:46 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 04:41:46 - train.py[line:551] - INFO: load:1.03 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 04:44:18 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 04:44:18 - train.py[line:551] - INFO: load:1.05 valid_run:151.43 task_valid:147.86 collect_output:2.50
2023-01-05 04:46:47 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 04:46:47 - train.py[line:551] - INFO: load:1.08 valid_run:300.26 task_valid:290.99 collect_output:7.16
2023-01-05 04:49:19 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 04:49:19 - train.py[line:551] - INFO: load:1.10 valid_run:452.75 task_valid:433.77 collect_output:15.84
2023-01-05 04:51:49 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 04:51:49 - train.py[line:551] - INFO: load:1.13 valid_run:602.12 task_valid:578.45 collect_output:19.54
2023-01-05 04:54:21 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 04:54:21 - train.py[line:551] - INFO: load:1.16 valid_run:754.43 task_valid:725.55 collect_output:23.75
2023-01-05 04:56:53 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 04:56:53 - train.py[line:551] - INFO: load:1.18 valid_run:906.48 task_valid:870.81 collect_output:29.52
2023-01-05 04:59:27 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 04:59:27 - train.py[line:551] - INFO: load:1.21 valid_run:1060.50 task_valid:1016.47 collect_output:36.88
2023-01-05 05:01:59 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 05:01:59 - train.py[line:551] - INFO: load:1.23 valid_run:1212.09 task_valid:1157.11 collect_output:46.81
2023-01-05 05:04:29 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 05:04:29 - train.py[line:551] - INFO: load:1.26 valid_run:1361.80 task_valid:1301.45 collect_output:51.17
2023-01-05 05:06:58 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 05:06:58 - train.py[line:551] - INFO: load:1.28 valid_run:1510.88 task_valid:1444.53 collect_output:56.11
2023-01-05 05:09:28 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 05:09:28 - train.py[line:551] - INFO: load:1.31 valid_run:1660.81 task_valid:1589.02 collect_output:60.56
2023-01-05 05:11:58 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 05:11:58 - train.py[line:551] - INFO: load:1.33 valid_run:1810.84 task_valid:1733.72 collect_output:64.88
2023-01-05 05:14:28 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 05:14:28 - train.py[line:551] - INFO: load:1.36 valid_run:1960.89 task_valid:1875.00 collect_output:72.63
2023-01-05 05:16:59 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 05:16:59 - train.py[line:551] - INFO: load:1.39 valid_run:2111.61 task_valid:2020.33 collect_output:77.01
2023-01-05 05:19:29 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 05:19:29 - train.py[line:551] - INFO: load:1.41 valid_run:2261.74 task_valid:2166.44 collect_output:80.03
2023-01-05 05:21:59 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 05:21:59 - train.py[line:551] - INFO: load:1.44 valid_run:2411.85 task_valid:2310.30 collect_output:85.26
2023-01-05 05:24:31 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 05:24:31 - train.py[line:551] - INFO: load:1.46 valid_run:2563.72 task_valid:2455.67 collect_output:90.73
2023-01-05 05:27:02 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 05:27:02 - train.py[line:551] - INFO: load:1.49 valid_run:2714.20 task_valid:2602.25 collect_output:93.61
2023-01-05 05:29:31 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 05:29:31 - train.py[line:551] - INFO: load:1.52 valid_run:2862.95 task_valid:2743.50 collect_output:100.07
2023-01-05 05:32:01 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 05:32:01 - train.py[line:551] - INFO: load:1.54 valid_run:3013.34 task_valid:2888.27 collect_output:104.68
2023-01-05 05:34:34 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 05:34:34 - train.py[line:551] - INFO: load:1.57 valid_run:3165.75 task_valid:3032.71 collect_output:111.61
2023-01-05 05:37:03 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 05:37:03 - train.py[line:551] - INFO: load:1.60 valid_run:3315.05 task_valid:3176.83 collect_output:115.78
2023-01-05 05:39:35 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 05:39:35 - train.py[line:551] - INFO: load:1.62 valid_run:3466.50 task_valid:3322.63 collect_output:120.38
2023-01-05 05:42:06 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 05:42:06 - train.py[line:551] - INFO: load:1.65 valid_run:3617.89 task_valid:3468.83 collect_output:124.55

====================================================================================================
SGG eval:     R @ 50: 0.4587;     R @ 100: 0.5448;     R @ 500: 0.5943;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3335;    mR @ 100: 0.3860;    mR @ 500: 0.4550;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5146) (covered in:0.6875) (covering:0.2143) (eating:0.7059) (flying in:0.5000) (growing on:0.3750) (hanging from:0.3871) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8750) (playing:0.0000) (riding:0.8546) (says:0.0000) (sitting on:0.6590) (standing on:0.1250) (using:0.5000) (walking in:0.3333) (walking on:0.5586) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4587;     R @ 100: 0.5448;     R @ 500: 0.5943;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3335;    mR @ 100: 0.3860;    mR @ 500: 0.4550;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5146) (covered in:0.6875) (covering:0.2143) (eating:0.7059) (flying in:0.5000) (growing on:0.3750) (hanging from:0.3871) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8750) (playing:0.0000) (riding:0.8546) (says:0.0000) (sitting on:0.6590) (standing on:0.1250) (using:0.5000) (walking in:0.3333) (walking on:0.5586) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-01-05 05:44:37 - train.py[line:487] - INFO: 0.5447523809523809
2023-01-05 05:44:37 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 05:44:38 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.35 | loss_v1 0 | loss_v2 0 | nll_loss 0.191 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.544752 | ppl 1.14 | vqa_score 0.4966 | wps 119 | wpb 89.9 | bsz 30 | num_updates 10000 | best_R@100 0.611199
2023-01-05 05:44:38 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-01-05 05:44:38 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-05 05:45:23 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-05 05:46:59 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.5447523809523809) (writing took 140.95979299023747 seconds)
2023-01-05 05:47:10 - progress_bar.py[line:274] - INFO: epoch 001:  10025 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=0.3, ups=0, wpb=111.2, bsz=40, num_updates=10010, lr=4.69862e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31263
2023-01-05 05:47:22 - progress_bar.py[line:274] - INFO: epoch 001:  10035 / 102288 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.9, ups=0.92, wpb=109.6, bsz=40, num_updates=10020, lr=4.69811e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31275
2023-01-05 05:47:33 - progress_bar.py[line:274] - INFO: epoch 001:  10045 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.6, ups=0.92, wpb=111.9, bsz=40, num_updates=10030, lr=4.6976e-05, gnorm=0.67, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31286
2023-01-05 05:47:45 - progress_bar.py[line:274] - INFO: epoch 001:  10055 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.1, ups=0.88, wpb=111.6, bsz=40, num_updates=10040, lr=4.69709e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31298
2023-01-05 05:47:56 - progress_bar.py[line:274] - INFO: epoch 001:  10065 / 102288 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=10050, lr=4.69658e-05, gnorm=0.703, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31309
2023-01-05 05:48:08 - progress_bar.py[line:274] - INFO: epoch 001:  10075 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=10060, lr=4.69607e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31321
2023-01-05 05:48:20 - progress_bar.py[line:274] - INFO: epoch 001:  10085 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=10070, lr=4.69556e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31333
2023-01-05 05:48:31 - progress_bar.py[line:274] - INFO: epoch 001:  10095 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=10080, lr=4.69505e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31344
2023-01-05 05:48:43 - progress_bar.py[line:274] - INFO: epoch 001:  10105 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=10090, lr=4.69454e-05, gnorm=0.559, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31356
2023-01-05 05:48:54 - progress_bar.py[line:274] - INFO: epoch 001:  10115 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.9, wpb=109.3, bsz=40, num_updates=10100, lr=4.69403e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31367
2023-01-05 05:49:06 - progress_bar.py[line:274] - INFO: epoch 001:  10125 / 102288 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=105, ups=0.94, wpb=111.4, bsz=40, num_updates=10110, lr=4.69352e-05, gnorm=0.732, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31379
2023-01-05 05:49:18 - progress_bar.py[line:274] - INFO: epoch 001:  10135 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.87, wpb=111.4, bsz=40, num_updates=10120, lr=4.69302e-05, gnorm=0.549, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31391
2023-01-05 05:49:29 - progress_bar.py[line:274] - INFO: epoch 001:  10145 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=10130, lr=4.69251e-05, gnorm=0.612, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31402
2023-01-05 05:49:40 - progress_bar.py[line:274] - INFO: epoch 001:  10155 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=10140, lr=4.692e-05, gnorm=0.583, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31414
2023-01-05 05:49:52 - progress_bar.py[line:274] - INFO: epoch 001:  10165 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.4, ups=0.88, wpb=110.8, bsz=40, num_updates=10150, lr=4.69149e-05, gnorm=0.633, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31425
2023-01-05 05:50:03 - progress_bar.py[line:274] - INFO: epoch 001:  10175 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.9, wpb=109.4, bsz=40, num_updates=10160, lr=4.69098e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31436
2023-01-05 05:50:14 - progress_bar.py[line:274] - INFO: epoch 001:  10185 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.7, ups=0.93, wpb=109.5, bsz=40, num_updates=10170, lr=4.69047e-05, gnorm=0.599, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31448
2023-01-05 05:50:26 - progress_bar.py[line:274] - INFO: epoch 001:  10195 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.4, ups=0.9, wpb=111.5, bsz=40, num_updates=10180, lr=4.68996e-05, gnorm=0.704, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31459
2023-01-05 05:50:37 - progress_bar.py[line:274] - INFO: epoch 001:  10205 / 102288 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.3, ups=0.9, wpb=109.9, bsz=40, num_updates=10190, lr=4.68945e-05, gnorm=0.625, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31470
2023-01-05 05:50:48 - progress_bar.py[line:274] - INFO: epoch 001:  10215 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.6, ups=0.9, wpb=110.2, bsz=40, num_updates=10200, lr=4.68894e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31482
2023-01-05 05:51:00 - progress_bar.py[line:274] - INFO: epoch 001:  10225 / 102288 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=10210, lr=4.68843e-05, gnorm=0.6, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31493
2023-01-05 05:51:11 - progress_bar.py[line:274] - INFO: epoch 001:  10235 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=10220, lr=4.68792e-05, gnorm=0.551, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31504
2023-01-05 05:51:22 - progress_bar.py[line:274] - INFO: epoch 001:  10245 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.1, ups=0.91, wpb=110.5, bsz=40, num_updates=10230, lr=4.68741e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31516
2023-01-05 05:51:34 - progress_bar.py[line:274] - INFO: epoch 001:  10255 / 102288 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.7, ups=0.88, wpb=108.5, bsz=40, num_updates=10240, lr=4.6869e-05, gnorm=0.567, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31527
2023-01-05 05:51:45 - progress_bar.py[line:274] - INFO: epoch 001:  10265 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.7, ups=0.92, wpb=111, bsz=40, num_updates=10250, lr=4.6864e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31538
2023-01-05 05:51:56 - progress_bar.py[line:274] - INFO: epoch 001:  10275 / 102288 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=10260, lr=4.68589e-05, gnorm=0.683, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31549
2023-01-05 05:52:08 - progress_bar.py[line:274] - INFO: epoch 001:  10285 / 102288 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97, ups=0.89, wpb=108.7, bsz=40, num_updates=10270, lr=4.68538e-05, gnorm=0.539, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31561
2023-01-05 05:52:19 - progress_bar.py[line:274] - INFO: epoch 001:  10295 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.7, ups=0.88, wpb=108.5, bsz=40, num_updates=10280, lr=4.68487e-05, gnorm=0.527, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31572
2023-01-05 05:52:31 - progress_bar.py[line:274] - INFO: epoch 001:  10305 / 102288 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=10290, lr=4.68436e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31584
2023-01-05 05:52:42 - progress_bar.py[line:274] - INFO: epoch 001:  10315 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.91, wpb=109.2, bsz=40, num_updates=10300, lr=4.68385e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31595
2023-01-05 05:52:53 - progress_bar.py[line:274] - INFO: epoch 001:  10325 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.5, ups=0.9, wpb=108.9, bsz=40, num_updates=10310, lr=4.68334e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31606
2023-01-05 05:53:04 - progress_bar.py[line:274] - INFO: epoch 001:  10335 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=10320, lr=4.68283e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31618
2023-01-05 05:53:16 - progress_bar.py[line:274] - INFO: epoch 001:  10345 / 102288 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.92, wpb=109.8, bsz=40, num_updates=10330, lr=4.68232e-05, gnorm=0.554, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31629
2023-01-05 05:53:27 - progress_bar.py[line:274] - INFO: epoch 001:  10355 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.8, ups=0.91, wpb=110.5, bsz=40, num_updates=10340, lr=4.68181e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31640
2023-01-05 05:53:38 - progress_bar.py[line:274] - INFO: epoch 001:  10365 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.1, ups=0.92, wpb=111.1, bsz=40, num_updates=10350, lr=4.6813e-05, gnorm=0.621, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31651
2023-01-05 05:53:49 - progress_bar.py[line:274] - INFO: epoch 001:  10375 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=10360, lr=4.68079e-05, gnorm=0.527, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31663
2023-01-05 05:54:00 - progress_bar.py[line:274] - INFO: epoch 001:  10385 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=102.3, ups=0.93, wpb=110.2, bsz=40, num_updates=10370, lr=4.68029e-05, gnorm=0.601, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31674
2023-01-05 05:54:12 - progress_bar.py[line:274] - INFO: epoch 001:  10395 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.1, ups=0.92, wpb=111.1, bsz=40, num_updates=10380, lr=4.67978e-05, gnorm=0.43, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31685
2023-01-05 05:54:23 - progress_bar.py[line:274] - INFO: epoch 001:  10405 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.6, ups=0.92, wpb=110.7, bsz=40, num_updates=10390, lr=4.67927e-05, gnorm=0.575, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31696
2023-01-05 05:54:34 - progress_bar.py[line:274] - INFO: epoch 001:  10415 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=10400, lr=4.67876e-05, gnorm=0.575, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31707
2023-01-05 05:54:45 - progress_bar.py[line:274] - INFO: epoch 001:  10425 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102, ups=0.92, wpb=111.3, bsz=40, num_updates=10410, lr=4.67825e-05, gnorm=0.574, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31719
2023-01-05 05:54:57 - progress_bar.py[line:274] - INFO: epoch 001:  10435 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=10420, lr=4.67774e-05, gnorm=0.637, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=31730
2023-01-05 05:55:09 - progress_bar.py[line:274] - INFO: epoch 001:  10445 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=10430, lr=4.67723e-05, gnorm=0.556, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31742
2023-01-05 05:55:20 - progress_bar.py[line:274] - INFO: epoch 001:  10455 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=10440, lr=4.67672e-05, gnorm=0.673, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31753
2023-01-05 05:55:31 - progress_bar.py[line:274] - INFO: epoch 001:  10465 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=10450, lr=4.67621e-05, gnorm=0.586, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31764
2023-01-05 05:55:43 - progress_bar.py[line:274] - INFO: epoch 001:  10475 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=10460, lr=4.6757e-05, gnorm=0.51, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31776
2023-01-05 05:55:54 - progress_bar.py[line:274] - INFO: epoch 001:  10485 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.89, wpb=110.8, bsz=40, num_updates=10470, lr=4.67519e-05, gnorm=0.496, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31787
2023-01-05 05:56:05 - progress_bar.py[line:274] - INFO: epoch 001:  10495 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=10480, lr=4.67468e-05, gnorm=0.628, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31799
2023-01-05 05:56:17 - progress_bar.py[line:274] - INFO: epoch 001:  10505 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=10490, lr=4.67418e-05, gnorm=0.573, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31810
2023-01-05 05:56:28 - progress_bar.py[line:274] - INFO: epoch 001:  10515 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=93.8, ups=0.87, wpb=107.7, bsz=40, num_updates=10500, lr=4.67367e-05, gnorm=0.571, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31822
2023-01-05 05:56:39 - progress_bar.py[line:274] - INFO: epoch 001:  10525 / 102288 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.9, ups=0.92, wpb=108.8, bsz=40, num_updates=10510, lr=4.67316e-05, gnorm=0.689, clip=20, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=31833
2023-01-05 05:56:51 - progress_bar.py[line:274] - INFO: epoch 001:  10535 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=10520, lr=4.67265e-05, gnorm=0.631, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31844
2023-01-05 05:57:02 - progress_bar.py[line:274] - INFO: epoch 001:  10545 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.3, ups=0.92, wpb=110.7, bsz=40, num_updates=10530, lr=4.67214e-05, gnorm=0.683, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31855
2023-01-05 05:57:13 - progress_bar.py[line:274] - INFO: epoch 001:  10555 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.3, ups=0.94, wpb=109.8, bsz=40, num_updates=10540, lr=4.67163e-05, gnorm=0.547, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31866
2023-01-05 05:57:24 - progress_bar.py[line:274] - INFO: epoch 001:  10565 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=104.4, ups=0.95, wpb=110.5, bsz=40, num_updates=10550, lr=4.67112e-05, gnorm=0.516, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31877
2023-01-05 05:57:35 - progress_bar.py[line:274] - INFO: epoch 001:  10575 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=10560, lr=4.67061e-05, gnorm=0.589, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31888
2023-01-05 05:57:46 - progress_bar.py[line:274] - INFO: epoch 001:  10585 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=10570, lr=4.6701e-05, gnorm=0.53, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31900
2023-01-05 05:57:58 - progress_bar.py[line:274] - INFO: epoch 001:  10595 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=10580, lr=4.66959e-05, gnorm=0.704, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31911
2023-01-05 05:58:09 - progress_bar.py[line:274] - INFO: epoch 001:  10605 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.6, ups=0.91, wpb=110, bsz=40, num_updates=10590, lr=4.66908e-05, gnorm=0.613, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31922
2023-01-05 05:58:20 - progress_bar.py[line:274] - INFO: epoch 001:  10615 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.8, ups=0.94, wpb=110.1, bsz=40, num_updates=10600, lr=4.66857e-05, gnorm=0.491, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31933
2023-01-05 05:58:31 - progress_bar.py[line:274] - INFO: epoch 001:  10625 / 102288 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=10610, lr=4.66807e-05, gnorm=0.636, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31945
2023-01-05 05:58:43 - progress_bar.py[line:274] - INFO: epoch 001:  10635 / 102288 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=10620, lr=4.66756e-05, gnorm=0.609, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31956
2023-01-05 05:58:54 - progress_bar.py[line:274] - INFO: epoch 001:  10645 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=10630, lr=4.66705e-05, gnorm=0.464, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31967
2023-01-05 05:59:05 - progress_bar.py[line:274] - INFO: epoch 001:  10655 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.6, ups=0.91, wpb=112.3, bsz=40, num_updates=10640, lr=4.66654e-05, gnorm=0.502, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31978
2023-01-05 05:59:16 - progress_bar.py[line:274] - INFO: epoch 001:  10665 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.89, wpb=109.3, bsz=40, num_updates=10650, lr=4.66603e-05, gnorm=0.479, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31990
2023-01-05 05:59:28 - progress_bar.py[line:274] - INFO: epoch 001:  10675 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.9, ups=0.91, wpb=111.5, bsz=40, num_updates=10660, lr=4.66552e-05, gnorm=0.551, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32001
2023-01-05 05:59:39 - progress_bar.py[line:274] - INFO: epoch 001:  10685 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.3, ups=0.9, wpb=109.7, bsz=40, num_updates=10670, lr=4.66501e-05, gnorm=0.566, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32012
2023-01-05 05:59:50 - progress_bar.py[line:274] - INFO: epoch 001:  10695 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=10680, lr=4.6645e-05, gnorm=0.537, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32024
2023-01-05 06:00:02 - progress_bar.py[line:274] - INFO: epoch 001:  10705 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=10690, lr=4.66399e-05, gnorm=0.509, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32035
2023-01-05 06:00:13 - progress_bar.py[line:274] - INFO: epoch 001:  10715 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103, ups=0.94, wpb=110.1, bsz=40, num_updates=10700, lr=4.66348e-05, gnorm=0.477, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32046
2023-01-05 06:00:24 - progress_bar.py[line:274] - INFO: epoch 001:  10725 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=10710, lr=4.66297e-05, gnorm=0.703, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32057
2023-01-05 06:00:35 - progress_bar.py[line:274] - INFO: epoch 001:  10735 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.1, ups=0.88, wpb=109.8, bsz=40, num_updates=10720, lr=4.66246e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32069
2023-01-05 06:00:46 - progress_bar.py[line:274] - INFO: epoch 001:  10745 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.5, ups=0.94, wpb=110.1, bsz=40, num_updates=10730, lr=4.66196e-05, gnorm=0.587, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32080
2023-01-05 06:00:58 - progress_bar.py[line:274] - INFO: epoch 001:  10755 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=10740, lr=4.66145e-05, gnorm=0.557, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=32091
2023-01-05 06:01:10 - progress_bar.py[line:274] - INFO: epoch 001:  10765 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.6, ups=0.87, wpb=111.1, bsz=40, num_updates=10750, lr=4.66094e-05, gnorm=0.597, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32103
2023-01-05 06:01:21 - progress_bar.py[line:274] - INFO: epoch 001:  10775 / 102288 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.2, ups=0.9, wpb=108.6, bsz=40, num_updates=10760, lr=4.66043e-05, gnorm=0.655, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32114
2023-01-05 06:01:32 - progress_bar.py[line:274] - INFO: epoch 001:  10785 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=10770, lr=4.65992e-05, gnorm=0.702, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32126
2023-01-05 06:01:44 - progress_bar.py[line:274] - INFO: epoch 001:  10795 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=10780, lr=4.65941e-05, gnorm=0.488, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32137
2023-01-05 06:01:55 - progress_bar.py[line:274] - INFO: epoch 001:  10805 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=10790, lr=4.6589e-05, gnorm=0.584, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32148
2023-01-05 06:02:06 - progress_bar.py[line:274] - INFO: epoch 001:  10815 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=10800, lr=4.65839e-05, gnorm=0.53, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32160
2023-01-05 06:02:18 - progress_bar.py[line:274] - INFO: epoch 001:  10825 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=10810, lr=4.65788e-05, gnorm=0.464, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32171
2023-01-05 06:02:29 - progress_bar.py[line:274] - INFO: epoch 001:  10835 / 102288 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.4, ups=0.9, wpb=109.8, bsz=40, num_updates=10820, lr=4.65737e-05, gnorm=0.569, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32182
2023-01-05 06:02:37 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 06:02:42 - progress_bar.py[line:274] - INFO: epoch 001:  10846 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=88.9, ups=0.81, wpb=110.2, bsz=40, num_updates=10830, lr=4.65686e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=32195
2023-01-05 06:02:53 - progress_bar.py[line:274] - INFO: epoch 001:  10856 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=10840, lr=4.65635e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32207
2023-01-05 06:03:04 - progress_bar.py[line:274] - INFO: epoch 001:  10866 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=106.3, ups=0.97, wpb=109.9, bsz=40, num_updates=10850, lr=4.65584e-05, gnorm=0.564, clip=0, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=32217
2023-01-05 06:03:16 - progress_bar.py[line:274] - INFO: epoch 001:  10876 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.6, ups=0.88, wpb=109.8, bsz=40, num_updates=10860, lr=4.65534e-05, gnorm=0.65, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32229
2023-01-05 06:03:27 - progress_bar.py[line:274] - INFO: epoch 001:  10886 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.9, wpb=110.9, bsz=40, num_updates=10870, lr=4.65483e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32240
2023-01-05 06:03:39 - progress_bar.py[line:274] - INFO: epoch 001:  10896 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=10880, lr=4.65432e-05, gnorm=0.471, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32252
2023-01-05 06:03:50 - progress_bar.py[line:274] - INFO: epoch 001:  10906 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=10890, lr=4.65381e-05, gnorm=0.565, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32263
2023-01-05 06:04:02 - progress_bar.py[line:274] - INFO: epoch 001:  10916 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99, ups=0.89, wpb=110.7, bsz=40, num_updates=10900, lr=4.6533e-05, gnorm=0.561, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32275
2023-01-05 06:04:13 - progress_bar.py[line:274] - INFO: epoch 001:  10926 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=10910, lr=4.65279e-05, gnorm=0.559, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32286
2023-01-05 06:04:25 - progress_bar.py[line:274] - INFO: epoch 001:  10936 / 102288 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.9, ups=0.9, wpb=109.4, bsz=40, num_updates=10920, lr=4.65228e-05, gnorm=0.553, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32298
2023-01-05 06:04:37 - progress_bar.py[line:274] - INFO: epoch 001:  10946 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.87, wpb=111.2, bsz=40, num_updates=10930, lr=4.65177e-05, gnorm=0.669, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32310
2023-01-05 06:04:48 - progress_bar.py[line:274] - INFO: epoch 001:  10956 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=10940, lr=4.65126e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=32321
2023-01-05 06:04:59 - progress_bar.py[line:274] - INFO: epoch 001:  10966 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.7, ups=0.94, wpb=110.1, bsz=40, num_updates=10950, lr=4.65075e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32332
2023-01-05 06:05:11 - progress_bar.py[line:274] - INFO: epoch 001:  10976 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.2, ups=0.9, wpb=108.7, bsz=40, num_updates=10960, lr=4.65024e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32344
2023-01-05 06:05:22 - progress_bar.py[line:274] - INFO: epoch 001:  10986 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.9, ups=0.9, wpb=109.3, bsz=40, num_updates=10970, lr=4.64973e-05, gnorm=0.523, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32355
2023-01-05 06:05:34 - progress_bar.py[line:274] - INFO: epoch 001:  10996 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.7, ups=0.88, wpb=110.1, bsz=40, num_updates=10980, lr=4.64923e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32367
2023-01-05 06:05:46 - progress_bar.py[line:274] - INFO: epoch 001:  11006 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.9, ups=0.87, wpb=110.5, bsz=40, num_updates=10990, lr=4.64872e-05, gnorm=0.59, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32379
2023-01-05 06:05:57 - progress_bar.py[line:274] - INFO: epoch 001:  11016 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=11000, lr=4.64821e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32390
2023-01-05 06:06:09 - progress_bar.py[line:274] - INFO: epoch 001:  11026 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=11010, lr=4.6477e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32402
2023-01-05 06:06:20 - progress_bar.py[line:274] - INFO: epoch 001:  11036 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=11020, lr=4.64719e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32414
2023-01-05 06:06:32 - progress_bar.py[line:274] - INFO: epoch 001:  11046 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.9, ups=0.92, wpb=109.1, bsz=40, num_updates=11030, lr=4.64668e-05, gnorm=0.531, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32425
2023-01-05 06:06:44 - progress_bar.py[line:274] - INFO: epoch 001:  11056 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.5, ups=0.89, wpb=110.6, bsz=40, num_updates=11040, lr=4.64617e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32437
2023-01-05 06:06:55 - progress_bar.py[line:274] - INFO: epoch 001:  11066 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.7, ups=0.92, wpb=111, bsz=40, num_updates=11050, lr=4.64566e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32448
2023-01-05 06:07:06 - progress_bar.py[line:274] - INFO: epoch 001:  11076 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.2, ups=0.9, wpb=112.3, bsz=40, num_updates=11060, lr=4.64515e-05, gnorm=0.663, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32459
2023-01-05 06:07:18 - progress_bar.py[line:274] - INFO: epoch 001:  11086 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.2, ups=0.91, wpb=110.8, bsz=40, num_updates=11070, lr=4.64464e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32471
2023-01-05 06:07:29 - progress_bar.py[line:274] - INFO: epoch 001:  11096 / 102288 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.9, ups=0.92, wpb=109.2, bsz=40, num_updates=11080, lr=4.64413e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32482
2023-01-05 06:07:41 - progress_bar.py[line:274] - INFO: epoch 001:  11106 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97, ups=0.89, wpb=108.7, bsz=40, num_updates=11090, lr=4.64362e-05, gnorm=0.556, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32494
2023-01-05 06:07:52 - progress_bar.py[line:274] - INFO: epoch 001:  11116 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=11100, lr=4.64312e-05, gnorm=0.573, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32505
2023-01-05 06:08:03 - progress_bar.py[line:274] - INFO: epoch 001:  11126 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.8, ups=0.92, wpb=111, bsz=40, num_updates=11110, lr=4.64261e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32517
2023-01-05 06:08:15 - progress_bar.py[line:274] - INFO: epoch 001:  11136 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=11120, lr=4.6421e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32528
2023-01-05 06:08:27 - progress_bar.py[line:274] - INFO: epoch 001:  11146 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.9, wpb=108.9, bsz=40, num_updates=11130, lr=4.64159e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32540
2023-01-05 06:08:38 - progress_bar.py[line:274] - INFO: epoch 001:  11156 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=11140, lr=4.64108e-05, gnorm=0.529, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=32551
2023-01-05 06:08:49 - progress_bar.py[line:274] - INFO: epoch 001:  11166 / 102288 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.2, ups=0.93, wpb=108, bsz=40, num_updates=11150, lr=4.64057e-05, gnorm=0.582, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32562
2023-01-05 06:09:01 - progress_bar.py[line:274] - INFO: epoch 001:  11176 / 102288 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.9, ups=0.88, wpb=110.3, bsz=40, num_updates=11160, lr=4.64006e-05, gnorm=0.627, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=32574
2023-01-05 06:09:12 - progress_bar.py[line:274] - INFO: epoch 001:  11186 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=11170, lr=4.63955e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32586
2023-01-05 06:09:24 - progress_bar.py[line:274] - INFO: epoch 001:  11196 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.88, wpb=109.8, bsz=40, num_updates=11180, lr=4.63904e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32597
2023-01-05 06:09:36 - progress_bar.py[line:274] - INFO: epoch 001:  11206 / 102288 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.3, ups=0.89, wpb=112.5, bsz=40, num_updates=11190, lr=4.63853e-05, gnorm=0.684, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32609
2023-01-05 06:09:47 - progress_bar.py[line:274] - INFO: epoch 001:  11216 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=11200, lr=4.63802e-05, gnorm=0.585, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32621
2023-01-05 06:09:59 - progress_bar.py[line:274] - INFO: epoch 001:  11226 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=11210, lr=4.63751e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32632
2023-01-05 06:10:11 - progress_bar.py[line:274] - INFO: epoch 001:  11236 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.5, ups=0.87, wpb=109.8, bsz=40, num_updates=11220, lr=4.63701e-05, gnorm=0.707, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32644
2023-01-05 06:10:22 - progress_bar.py[line:274] - INFO: epoch 001:  11246 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=11230, lr=4.6365e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32656
2023-01-05 06:10:34 - progress_bar.py[line:274] - INFO: epoch 001:  11256 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=11240, lr=4.63599e-05, gnorm=0.516, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32667
2023-01-05 06:10:45 - progress_bar.py[line:274] - INFO: epoch 001:  11266 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=11250, lr=4.63548e-05, gnorm=0.62, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32678
2023-01-05 06:10:57 - progress_bar.py[line:274] - INFO: epoch 001:  11276 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=11260, lr=4.63497e-05, gnorm=0.538, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32690
2023-01-05 06:11:08 - progress_bar.py[line:274] - INFO: epoch 001:  11286 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=11270, lr=4.63446e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32702
2023-01-05 06:11:20 - progress_bar.py[line:274] - INFO: epoch 001:  11296 / 102288 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.4, ups=0.89, wpb=109.5, bsz=40, num_updates=11280, lr=4.63395e-05, gnorm=0.765, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32713
2023-01-05 06:11:32 - progress_bar.py[line:274] - INFO: epoch 001:  11306 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.4, ups=0.9, wpb=108.9, bsz=40, num_updates=11290, lr=4.63344e-05, gnorm=0.594, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=32725
2023-01-05 06:11:43 - progress_bar.py[line:274] - INFO: epoch 001:  11316 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=11300, lr=4.63293e-05, gnorm=0.746, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32736
2023-01-05 06:11:54 - progress_bar.py[line:274] - INFO: epoch 001:  11326 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.9, ups=0.93, wpb=108.8, bsz=40, num_updates=11310, lr=4.63242e-05, gnorm=0.498, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32748
2023-01-05 06:12:06 - progress_bar.py[line:274] - INFO: epoch 001:  11336 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.6, ups=0.89, wpb=110.2, bsz=40, num_updates=11320, lr=4.63191e-05, gnorm=0.638, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32759
2023-01-05 06:12:18 - progress_bar.py[line:274] - INFO: epoch 001:  11346 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=11330, lr=4.6314e-05, gnorm=0.55, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32771
2023-01-05 06:12:28 - progress_bar.py[line:274] - INFO: epoch 001:  11356 / 102288 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=105.2, ups=0.96, wpb=109.7, bsz=40, num_updates=11340, lr=4.6309e-05, gnorm=0.605, clip=0, loss_scale=1024, train_wall=10, gb_free=10.5, ema_decay=0.9999, wall=32781
2023-01-05 06:12:40 - progress_bar.py[line:274] - INFO: epoch 001:  11366 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99, ups=0.9, wpb=109.5, bsz=40, num_updates=11350, lr=4.63039e-05, gnorm=0.699, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32793
2023-01-05 06:12:51 - progress_bar.py[line:274] - INFO: epoch 001:  11376 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=11360, lr=4.62988e-05, gnorm=0.581, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32805
2023-01-05 06:13:03 - progress_bar.py[line:274] - INFO: epoch 001:  11386 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.89, wpb=109.3, bsz=40, num_updates=11370, lr=4.62937e-05, gnorm=0.723, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=32816
2023-01-05 06:13:14 - progress_bar.py[line:274] - INFO: epoch 001:  11396 / 102288 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=102.2, ups=0.94, wpb=108.8, bsz=40, num_updates=11380, lr=4.62886e-05, gnorm=0.729, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32827
2023-01-05 06:13:26 - progress_bar.py[line:274] - INFO: epoch 001:  11406 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.2, ups=0.87, wpb=110.3, bsz=40, num_updates=11390, lr=4.62835e-05, gnorm=0.526, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32839
2023-01-05 06:13:37 - progress_bar.py[line:274] - INFO: epoch 001:  11416 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.7, ups=0.93, wpb=109.5, bsz=40, num_updates=11400, lr=4.62784e-05, gnorm=0.5, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32850
2023-01-05 06:13:48 - progress_bar.py[line:274] - INFO: epoch 001:  11426 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.1, ups=0.92, wpb=111.4, bsz=40, num_updates=11410, lr=4.62733e-05, gnorm=0.605, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32861
2023-01-05 06:14:00 - progress_bar.py[line:274] - INFO: epoch 001:  11436 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.9, wpb=109.5, bsz=40, num_updates=11420, lr=4.62682e-05, gnorm=0.488, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32873
2023-01-05 06:14:11 - progress_bar.py[line:274] - INFO: epoch 001:  11446 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=11430, lr=4.62631e-05, gnorm=0.651, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32884
2023-01-05 06:14:23 - progress_bar.py[line:274] - INFO: epoch 001:  11456 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=11440, lr=4.6258e-05, gnorm=0.583, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32896
2023-01-05 06:14:34 - progress_bar.py[line:274] - INFO: epoch 001:  11466 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=11450, lr=4.62529e-05, gnorm=0.56, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32908
2023-01-05 06:14:46 - progress_bar.py[line:274] - INFO: epoch 001:  11476 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.3, ups=0.87, wpb=112, bsz=40, num_updates=11460, lr=4.62478e-05, gnorm=0.693, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32919
2023-01-05 06:14:58 - progress_bar.py[line:274] - INFO: epoch 001:  11486 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.7, ups=0.89, wpb=108.6, bsz=40, num_updates=11470, lr=4.62428e-05, gnorm=0.516, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32931
2023-01-05 06:15:09 - progress_bar.py[line:274] - INFO: epoch 001:  11496 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.91, wpb=110.5, bsz=40, num_updates=11480, lr=4.62377e-05, gnorm=0.547, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32942
2023-01-05 06:15:11 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 06:15:22 - progress_bar.py[line:274] - INFO: epoch 001:  11507 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=91, ups=0.83, wpb=110, bsz=40, num_updates=11490, lr=4.62326e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=32955
2023-01-05 06:15:33 - progress_bar.py[line:274] - INFO: epoch 001:  11517 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99, ups=0.9, wpb=110.5, bsz=40, num_updates=11500, lr=4.62275e-05, gnorm=0.752, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32966
2023-01-05 06:15:45 - progress_bar.py[line:274] - INFO: epoch 001:  11527 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.7, ups=0.92, wpb=111.1, bsz=40, num_updates=11510, lr=4.62224e-05, gnorm=0.564, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32978
2023-01-05 06:15:56 - progress_bar.py[line:274] - INFO: epoch 001:  11537 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.8, ups=0.88, wpb=112.1, bsz=40, num_updates=11520, lr=4.62173e-05, gnorm=0.559, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32990
2023-01-05 06:16:08 - progress_bar.py[line:274] - INFO: epoch 001:  11547 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=11530, lr=4.62122e-05, gnorm=0.584, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33001
2023-01-05 06:16:19 - progress_bar.py[line:274] - INFO: epoch 001:  11557 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.8, ups=0.9, wpb=109.9, bsz=40, num_updates=11540, lr=4.62071e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33013
2023-01-05 06:16:31 - progress_bar.py[line:274] - INFO: epoch 001:  11567 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.87, wpb=111.1, bsz=40, num_updates=11550, lr=4.6202e-05, gnorm=0.581, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33024
2023-01-05 06:16:43 - progress_bar.py[line:274] - INFO: epoch 001:  11577 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.88, wpb=111.1, bsz=40, num_updates=11560, lr=4.61969e-05, gnorm=0.55, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33036
2023-01-05 06:16:54 - progress_bar.py[line:274] - INFO: epoch 001:  11587 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=104, ups=0.94, wpb=110.4, bsz=40, num_updates=11570, lr=4.61918e-05, gnorm=0.674, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33047
2023-01-05 06:17:05 - progress_bar.py[line:274] - INFO: epoch 001:  11597 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=11580, lr=4.61867e-05, gnorm=0.732, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33059
2023-01-05 06:17:17 - progress_bar.py[line:274] - INFO: epoch 001:  11607 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.8, ups=0.87, wpb=109.8, bsz=40, num_updates=11590, lr=4.61817e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33070
2023-01-05 06:17:29 - progress_bar.py[line:274] - INFO: epoch 001:  11617 / 102288 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=11600, lr=4.61766e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33082
2023-01-05 06:17:41 - progress_bar.py[line:274] - INFO: epoch 001:  11627 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=94.9, ups=0.88, wpb=107.8, bsz=40, num_updates=11610, lr=4.61715e-05, gnorm=0.496, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33094
2023-01-05 06:17:52 - progress_bar.py[line:274] - INFO: epoch 001:  11637 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=11620, lr=4.61664e-05, gnorm=0.577, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33106
2023-01-05 06:18:04 - progress_bar.py[line:274] - INFO: epoch 001:  11647 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.92, wpb=109.8, bsz=40, num_updates=11630, lr=4.61613e-05, gnorm=0.558, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33117
2023-01-05 06:18:15 - progress_bar.py[line:274] - INFO: epoch 001:  11657 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=11640, lr=4.61562e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33128
2023-01-05 06:18:27 - progress_bar.py[line:274] - INFO: epoch 001:  11667 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.8, ups=0.9, wpb=110.8, bsz=40, num_updates=11650, lr=4.61511e-05, gnorm=0.548, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33140
2023-01-05 06:18:38 - progress_bar.py[line:274] - INFO: epoch 001:  11677 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=11660, lr=4.6146e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33151
2023-01-05 06:18:50 - progress_bar.py[line:274] - INFO: epoch 001:  11687 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=11670, lr=4.61409e-05, gnorm=0.571, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33163
2023-01-05 06:19:01 - progress_bar.py[line:274] - INFO: epoch 001:  11697 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95, ups=0.87, wpb=109.1, bsz=40, num_updates=11680, lr=4.61358e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33175
2023-01-05 06:19:13 - progress_bar.py[line:274] - INFO: epoch 001:  11707 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=11690, lr=4.61307e-05, gnorm=0.54, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33186
2023-01-05 06:19:25 - progress_bar.py[line:274] - INFO: epoch 001:  11717 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=11700, lr=4.61256e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33198
2023-01-05 06:19:36 - progress_bar.py[line:274] - INFO: epoch 001:  11727 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.6, ups=0.92, wpb=109.4, bsz=40, num_updates=11710, lr=4.61206e-05, gnorm=0.573, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33209
2023-01-05 06:19:47 - progress_bar.py[line:274] - INFO: epoch 001:  11737 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=11720, lr=4.61155e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33220
2023-01-05 06:19:59 - progress_bar.py[line:274] - INFO: epoch 001:  11747 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=11730, lr=4.61104e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33232
2023-01-05 06:20:10 - progress_bar.py[line:274] - INFO: epoch 001:  11757 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=11740, lr=4.61053e-05, gnorm=0.831, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33244
2023-01-05 06:20:22 - progress_bar.py[line:274] - INFO: epoch 001:  11767 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=11750, lr=4.61002e-05, gnorm=0.862, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33255
2023-01-05 06:20:34 - progress_bar.py[line:274] - INFO: epoch 001:  11777 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.88, wpb=111.9, bsz=40, num_updates=11760, lr=4.60951e-05, gnorm=0.485, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33267
2023-01-05 06:20:45 - progress_bar.py[line:274] - INFO: epoch 001:  11787 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=105.5, ups=0.94, wpb=112.3, bsz=40, num_updates=11770, lr=4.609e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33278
2023-01-05 06:20:57 - progress_bar.py[line:274] - INFO: epoch 001:  11797 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.4, ups=0.88, wpb=109.5, bsz=40, num_updates=11780, lr=4.60849e-05, gnorm=0.46, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33290
2023-01-05 06:21:08 - progress_bar.py[line:274] - INFO: epoch 001:  11807 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.9, wpb=110.7, bsz=40, num_updates=11790, lr=4.60798e-05, gnorm=0.613, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33301
2023-01-05 06:21:20 - progress_bar.py[line:274] - INFO: epoch 001:  11817 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=11800, lr=4.60747e-05, gnorm=0.498, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33313
2023-01-05 06:21:31 - progress_bar.py[line:274] - INFO: epoch 001:  11827 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=11810, lr=4.60696e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33324
2023-01-05 06:21:43 - progress_bar.py[line:274] - INFO: epoch 001:  11837 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=11820, lr=4.60645e-05, gnorm=0.55, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33336
2023-01-05 06:21:54 - progress_bar.py[line:274] - INFO: epoch 001:  11847 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=11830, lr=4.60595e-05, gnorm=0.515, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33347
2023-01-05 06:22:05 - progress_bar.py[line:274] - INFO: epoch 001:  11857 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=104.2, ups=0.94, wpb=110.6, bsz=40, num_updates=11840, lr=4.60544e-05, gnorm=0.618, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33358
2023-01-05 06:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  11867 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=11850, lr=4.60493e-05, gnorm=0.541, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33370
2023-01-05 06:22:28 - progress_bar.py[line:274] - INFO: epoch 001:  11877 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=11860, lr=4.60442e-05, gnorm=0.588, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33382
2023-01-05 06:22:39 - progress_bar.py[line:274] - INFO: epoch 001:  11887 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=103.9, ups=0.94, wpb=110.2, bsz=40, num_updates=11870, lr=4.60391e-05, gnorm=0.73, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33392
2023-01-05 06:22:50 - progress_bar.py[line:274] - INFO: epoch 001:  11897 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.4, ups=0.92, wpb=110, bsz=40, num_updates=11880, lr=4.6034e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33404
2023-01-05 06:23:02 - progress_bar.py[line:274] - INFO: epoch 001:  11907 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.9, wpb=110.8, bsz=40, num_updates=11890, lr=4.60289e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33415
2023-01-05 06:23:14 - progress_bar.py[line:274] - INFO: epoch 001:  11917 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=11900, lr=4.60238e-05, gnorm=0.553, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33427
2023-01-05 06:23:25 - progress_bar.py[line:274] - INFO: epoch 001:  11927 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=11910, lr=4.60187e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33438
2023-01-05 06:23:36 - progress_bar.py[line:274] - INFO: epoch 001:  11937 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.9, wpb=109.6, bsz=40, num_updates=11920, lr=4.60136e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33450
2023-01-05 06:23:48 - progress_bar.py[line:274] - INFO: epoch 001:  11947 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.3, ups=0.91, wpb=110.8, bsz=40, num_updates=11930, lr=4.60085e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33461
2023-01-05 06:24:00 - progress_bar.py[line:274] - INFO: epoch 001:  11957 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.87, wpb=110.5, bsz=40, num_updates=11940, lr=4.60034e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33473
2023-01-05 06:24:11 - progress_bar.py[line:274] - INFO: epoch 001:  11967 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=11950, lr=4.59984e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33484
2023-01-05 06:24:23 - progress_bar.py[line:274] - INFO: epoch 001:  11977 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.6, ups=0.92, wpb=109.8, bsz=40, num_updates=11960, lr=4.59933e-05, gnorm=0.554, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33496
2023-01-05 06:24:34 - progress_bar.py[line:274] - INFO: epoch 001:  11987 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=11970, lr=4.59882e-05, gnorm=0.531, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33507
2023-01-05 06:24:46 - progress_bar.py[line:274] - INFO: epoch 001:  11997 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=103.5, ups=0.93, wpb=111.4, bsz=40, num_updates=11980, lr=4.59831e-05, gnorm=0.462, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=33519
2023-01-05 06:24:57 - progress_bar.py[line:274] - INFO: epoch 001:  12007 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.9, wpb=108.3, bsz=40, num_updates=11990, lr=4.5978e-05, gnorm=0.51, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33530
2023-01-05 06:25:09 - progress_bar.py[line:274] - INFO: epoch 001:  12017 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=12000, lr=4.59729e-05, gnorm=0.595, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33542
2023-01-05 06:25:09 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 06:25:10 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 06:25:10 - train.py[line:551] - INFO: load:0.93 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 06:27:42 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 06:27:42 - train.py[line:551] - INFO: load:0.96 valid_run:151.64 task_valid:148.05 collect_output:2.53
2023-01-05 06:30:10 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 06:30:10 - train.py[line:551] - INFO: load:0.98 valid_run:300.16 task_valid:290.80 collect_output:7.29
2023-01-05 06:32:43 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 06:32:43 - train.py[line:551] - INFO: load:1.01 valid_run:452.51 task_valid:433.39 collect_output:16.04
2023-01-05 06:35:12 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 06:35:12 - train.py[line:551] - INFO: load:1.03 valid_run:601.52 task_valid:578.05 collect_output:19.37
2023-01-05 06:37:44 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 06:37:45 - train.py[line:551] - INFO: load:1.06 valid_run:754.01 task_valid:725.41 collect_output:23.49
2023-01-05 06:40:16 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 06:40:16 - train.py[line:551] - INFO: load:1.09 valid_run:905.88 task_valid:870.70 collect_output:29.05
2023-01-05 06:42:50 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 06:42:50 - train.py[line:551] - INFO: load:1.11 valid_run:1059.39 task_valid:1016.45 collect_output:35.79
2023-01-05 06:45:21 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 06:45:21 - train.py[line:551] - INFO: load:1.14 valid_run:1210.75 task_valid:1157.20 collect_output:45.36
2023-01-05 06:47:51 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 06:47:51 - train.py[line:551] - INFO: load:1.16 valid_run:1360.49 task_valid:1301.69 collect_output:49.60
2023-01-05 06:50:20 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 06:50:20 - train.py[line:551] - INFO: load:1.19 valid_run:1509.13 task_valid:1444.46 collect_output:54.45
2023-01-05 06:52:50 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 06:52:50 - train.py[line:551] - INFO: load:1.22 valid_run:1658.85 task_valid:1589.20 collect_output:58.41
2023-01-05 06:55:20 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 06:55:20 - train.py[line:551] - INFO: load:1.24 valid_run:1808.78 task_valid:1733.87 collect_output:62.65
2023-01-05 06:57:50 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 06:57:50 - train.py[line:551] - INFO: load:1.27 valid_run:1958.95 task_valid:1875.57 collect_output:70.11
2023-01-05 07:00:21 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 07:00:21 - train.py[line:551] - INFO: load:1.30 valid_run:2109.61 task_valid:2020.89 collect_output:74.42
2023-01-05 07:02:51 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 07:02:51 - train.py[line:551] - INFO: load:1.32 valid_run:2259.53 task_valid:2167.10 collect_output:77.12
2023-01-05 07:05:21 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 07:05:21 - train.py[line:551] - INFO: load:1.35 valid_run:2409.71 task_valid:2311.21 collect_output:82.15
2023-01-05 07:07:53 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 07:07:53 - train.py[line:551] - INFO: load:1.38 valid_run:2561.54 task_valid:2456.38 collect_output:87.80
2023-01-05 07:10:23 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 07:10:23 - train.py[line:551] - INFO: load:1.40 valid_run:2711.86 task_valid:2602.92 collect_output:90.59
2023-01-05 07:12:52 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 07:12:52 - train.py[line:551] - INFO: load:1.43 valid_run:2860.36 task_valid:2744.28 collect_output:96.72
2023-01-05 07:15:22 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 07:15:22 - train.py[line:551] - INFO: load:1.46 valid_run:3010.90 task_valid:2889.24 collect_output:101.26
2023-01-05 07:17:55 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 07:17:55 - train.py[line:551] - INFO: load:1.48 valid_run:3163.23 task_valid:3033.49 collect_output:108.33
2023-01-05 07:20:24 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 07:20:24 - train.py[line:551] - INFO: load:1.51 valid_run:3312.42 task_valid:3177.64 collect_output:112.38
2023-01-05 07:22:55 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 07:22:55 - train.py[line:551] - INFO: load:1.54 valid_run:3463.59 task_valid:3323.46 collect_output:116.74
2023-01-05 07:25:27 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 07:25:27 - train.py[line:551] - INFO: load:1.56 valid_run:3615.01 task_valid:3469.83 collect_output:120.77

====================================================================================================
SGG eval:     R @ 50: 0.4004;     R @ 100: 0.5030;     R @ 500: 0.5527;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2738;    mR @ 100: 0.3351;    mR @ 500: 0.4050;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5756) (covered in:0.8125) (covering:0.2143) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3226) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.7917) (playing:0.0000) (riding:0.7918) (says:0.0000) (sitting on:0.6480) (standing on:0.0850) (using:0.4500) (walking in:0.3333) (walking on:0.3739) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4004;     R @ 100: 0.5030;     R @ 500: 0.5527;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2738;    mR @ 100: 0.3351;    mR @ 500: 0.4050;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5756) (covered in:0.8125) (covering:0.2143) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3226) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.7917) (playing:0.0000) (riding:0.7918) (says:0.0000) (sitting on:0.6480) (standing on:0.0850) (using:0.4500) (walking in:0.3333) (walking on:0.3739) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-01-05 07:27:58 - train.py[line:487] - INFO: 0.5030190476190476
2023-01-05 07:27:58 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 07:27:58 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.353 | loss_v1 0 | loss_v2 0 | nll_loss 0.199 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.503019 | ppl 1.15 | vqa_score 0.4696 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 12000 | best_R@100 0.611199
2023-01-05 07:27:59 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 12000 updates
2023-01-05 07:27:59 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-05 07:28:42 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-05 07:30:12 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 0.5030190476190476) (writing took 133.96572045981884 seconds)
2023-01-05 07:30:26 - progress_bar.py[line:274] - INFO: epoch 001:  12027 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=0.3, ups=0, wpb=112.2, bsz=40, num_updates=12010, lr=4.59678e-05, gnorm=0.522, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37457
2023-01-05 07:30:38 - progress_bar.py[line:274] - INFO: epoch 001:  12037 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.8, ups=0.92, wpb=110.7, bsz=40, num_updates=12020, lr=4.59627e-05, gnorm=0.505, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37470
2023-01-05 07:30:51 - progress_bar.py[line:274] - INFO: epoch 001:  12047 / 102288 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=12030, lr=4.59576e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37483
2023-01-05 07:31:04 - progress_bar.py[line:274] - INFO: epoch 001:  12057 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.9, wpb=110.1, bsz=40, num_updates=12040, lr=4.59525e-05, gnorm=0.481, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37496
2023-01-05 07:31:17 - progress_bar.py[line:274] - INFO: epoch 001:  12067 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=12050, lr=4.59474e-05, gnorm=0.587, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37508
2023-01-05 07:31:29 - progress_bar.py[line:274] - INFO: epoch 001:  12077 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.92, wpb=110.7, bsz=40, num_updates=12060, lr=4.59423e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37521
2023-01-05 07:31:42 - progress_bar.py[line:274] - INFO: epoch 001:  12087 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.7, ups=0.91, wpb=110.3, bsz=40, num_updates=12070, lr=4.59372e-05, gnorm=0.607, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37534
2023-01-05 07:31:55 - progress_bar.py[line:274] - INFO: epoch 001:  12097 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=12080, lr=4.59322e-05, gnorm=0.722, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37546
2023-01-05 07:32:08 - progress_bar.py[line:274] - INFO: epoch 001:  12107 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.3, ups=0.91, wpb=109.7, bsz=40, num_updates=12090, lr=4.59271e-05, gnorm=0.582, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37559
2023-01-05 07:32:20 - progress_bar.py[line:274] - INFO: epoch 001:  12117 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.5, ups=0.88, wpb=108.6, bsz=40, num_updates=12100, lr=4.5922e-05, gnorm=0.732, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37572
2023-01-05 07:32:33 - progress_bar.py[line:274] - INFO: epoch 001:  12127 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.1, ups=0.91, wpb=110.6, bsz=40, num_updates=12110, lr=4.59169e-05, gnorm=0.48, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=37585
2023-01-05 07:32:46 - progress_bar.py[line:274] - INFO: epoch 001:  12137 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.4, ups=0.89, wpb=109.2, bsz=40, num_updates=12120, lr=4.59118e-05, gnorm=0.524, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37598
2023-01-05 07:32:59 - progress_bar.py[line:274] - INFO: epoch 001:  12147 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.4, ups=0.91, wpb=110.8, bsz=40, num_updates=12130, lr=4.59067e-05, gnorm=0.628, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37611
2023-01-05 07:33:11 - progress_bar.py[line:274] - INFO: epoch 001:  12157 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99, ups=0.9, wpb=109.6, bsz=40, num_updates=12140, lr=4.59016e-05, gnorm=0.58, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37623
2023-01-05 07:33:25 - progress_bar.py[line:274] - INFO: epoch 001:  12167 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=12150, lr=4.58965e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37636
2023-01-05 07:33:34 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 07:33:39 - progress_bar.py[line:274] - INFO: epoch 001:  12178 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=87.9, ups=0.81, wpb=109.2, bsz=40, num_updates=12160, lr=4.58914e-05, gnorm=0.565, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=37650
2023-01-05 07:33:52 - progress_bar.py[line:274] - INFO: epoch 001:  12188 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.2, ups=0.88, wpb=110.6, bsz=40, num_updates=12170, lr=4.58863e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37664
2023-01-05 07:34:05 - progress_bar.py[line:274] - INFO: epoch 001:  12198 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.2, ups=0.87, wpb=109.8, bsz=40, num_updates=12180, lr=4.58812e-05, gnorm=0.573, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37677
2023-01-05 07:34:18 - progress_bar.py[line:274] - INFO: epoch 001:  12208 / 102288 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=12190, lr=4.58761e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37690
2023-01-05 07:34:31 - progress_bar.py[line:274] - INFO: epoch 001:  12218 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.4, ups=0.89, wpb=109.8, bsz=40, num_updates=12200, lr=4.58711e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37703
2023-01-05 07:34:44 - progress_bar.py[line:274] - INFO: epoch 001:  12228 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.1, ups=0.88, wpb=109.4, bsz=40, num_updates=12210, lr=4.5866e-05, gnorm=0.585, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=37716
2023-01-05 07:34:57 - progress_bar.py[line:274] - INFO: epoch 001:  12238 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.89, wpb=111.5, bsz=40, num_updates=12220, lr=4.58609e-05, gnorm=0.573, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37729
2023-01-05 07:35:10 - progress_bar.py[line:274] - INFO: epoch 001:  12248 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=12230, lr=4.58558e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37742
2023-01-05 07:35:23 - progress_bar.py[line:274] - INFO: epoch 001:  12258 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.89, wpb=110.3, bsz=40, num_updates=12240, lr=4.58507e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37755
2023-01-05 07:35:36 - progress_bar.py[line:274] - INFO: epoch 001:  12268 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.3, ups=0.91, wpb=111.2, bsz=40, num_updates=12250, lr=4.58456e-05, gnorm=0.609, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37768
2023-01-05 07:35:49 - progress_bar.py[line:274] - INFO: epoch 001:  12278 / 102288 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.7, ups=0.9, wpb=109.6, bsz=40, num_updates=12260, lr=4.58405e-05, gnorm=0.741, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37781
2023-01-05 07:36:02 - progress_bar.py[line:274] - INFO: epoch 001:  12288 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=12270, lr=4.58354e-05, gnorm=0.523, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37794
2023-01-05 07:36:15 - progress_bar.py[line:274] - INFO: epoch 001:  12298 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.9, ups=0.9, wpb=108.6, bsz=40, num_updates=12280, lr=4.58303e-05, gnorm=0.677, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37806
2023-01-05 07:36:27 - progress_bar.py[line:274] - INFO: epoch 001:  12308 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97, ups=0.89, wpb=109.3, bsz=40, num_updates=12290, lr=4.58252e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37819
2023-01-05 07:36:40 - progress_bar.py[line:274] - INFO: epoch 001:  12318 / 102288 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=12300, lr=4.58201e-05, gnorm=0.632, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37832
2023-01-05 07:36:53 - progress_bar.py[line:274] - INFO: epoch 001:  12328 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=12310, lr=4.5815e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37845
2023-01-05 07:37:06 - progress_bar.py[line:274] - INFO: epoch 001:  12338 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.7, ups=0.91, wpb=110.4, bsz=40, num_updates=12320, lr=4.581e-05, gnorm=0.623, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37857
2023-01-05 07:37:18 - progress_bar.py[line:274] - INFO: epoch 001:  12348 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.89, wpb=109.5, bsz=40, num_updates=12330, lr=4.58049e-05, gnorm=0.497, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37870
2023-01-05 07:37:30 - progress_bar.py[line:274] - INFO: epoch 001:  12358 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=12340, lr=4.57998e-05, gnorm=0.466, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37883
2023-01-05 07:37:42 - progress_bar.py[line:274] - INFO: epoch 001:  12368 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=12350, lr=4.57947e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37895
2023-01-05 07:37:53 - progress_bar.py[line:274] - INFO: epoch 001:  12378 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.6, ups=0.92, wpb=110.4, bsz=40, num_updates=12360, lr=4.57896e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37906
2023-01-05 07:38:04 - progress_bar.py[line:274] - INFO: epoch 001:  12388 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=12370, lr=4.57845e-05, gnorm=0.558, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37918
2023-01-05 07:38:16 - progress_bar.py[line:274] - INFO: epoch 001:  12398 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=12380, lr=4.57794e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37929
2023-01-05 07:38:28 - progress_bar.py[line:274] - INFO: epoch 001:  12408 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=12390, lr=4.57743e-05, gnorm=0.55, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37941
2023-01-05 07:38:40 - progress_bar.py[line:274] - INFO: epoch 001:  12418 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=12400, lr=4.57692e-05, gnorm=0.433, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37953
2023-01-05 07:38:51 - progress_bar.py[line:274] - INFO: epoch 001:  12428 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=12410, lr=4.57641e-05, gnorm=0.648, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37964
2023-01-05 07:39:02 - progress_bar.py[line:274] - INFO: epoch 001:  12438 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.92, wpb=110.5, bsz=40, num_updates=12420, lr=4.5759e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37976
2023-01-05 07:39:14 - progress_bar.py[line:274] - INFO: epoch 001:  12448 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=12430, lr=4.57539e-05, gnorm=0.535, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37987
2023-01-05 07:39:25 - progress_bar.py[line:274] - INFO: epoch 001:  12458 / 102288 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.3, ups=0.92, wpb=111.5, bsz=40, num_updates=12440, lr=4.57489e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37998
2023-01-05 07:39:37 - progress_bar.py[line:274] - INFO: epoch 001:  12468 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=12450, lr=4.57438e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38010
2023-01-05 07:39:49 - progress_bar.py[line:274] - INFO: epoch 001:  12478 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=12460, lr=4.57387e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38022
2023-01-05 07:40:00 - progress_bar.py[line:274] - INFO: epoch 001:  12488 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.5, ups=0.87, wpb=109.8, bsz=40, num_updates=12470, lr=4.57336e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38034
2023-01-05 07:40:12 - progress_bar.py[line:274] - INFO: epoch 001:  12498 / 102288 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.5, ups=0.88, wpb=109.4, bsz=40, num_updates=12480, lr=4.57285e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38045
2023-01-05 07:40:24 - progress_bar.py[line:274] - INFO: epoch 001:  12508 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=12490, lr=4.57234e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38057
2023-01-05 07:40:35 - progress_bar.py[line:274] - INFO: epoch 001:  12518 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.1, ups=0.92, wpb=110.2, bsz=40, num_updates=12500, lr=4.57183e-05, gnorm=0.548, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38068
2023-01-05 07:40:47 - progress_bar.py[line:274] - INFO: epoch 001:  12528 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=12510, lr=4.57132e-05, gnorm=0.733, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38080
2023-01-05 07:40:58 - progress_bar.py[line:274] - INFO: epoch 001:  12538 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.6, ups=0.91, wpb=110, bsz=40, num_updates=12520, lr=4.57081e-05, gnorm=0.736, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38091
2023-01-05 07:41:10 - progress_bar.py[line:274] - INFO: epoch 001:  12548 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.5, ups=0.9, wpb=111, bsz=40, num_updates=12530, lr=4.5703e-05, gnorm=0.546, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38103
2023-01-05 07:41:22 - progress_bar.py[line:274] - INFO: epoch 001:  12558 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=12540, lr=4.56979e-05, gnorm=0.487, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38115
2023-01-05 07:41:34 - progress_bar.py[line:274] - INFO: epoch 001:  12568 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.2, ups=0.88, wpb=110.1, bsz=40, num_updates=12550, lr=4.56928e-05, gnorm=0.564, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38127
2023-01-05 07:41:46 - progress_bar.py[line:274] - INFO: epoch 001:  12578 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=12560, lr=4.56878e-05, gnorm=0.573, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38139
2023-01-05 07:41:58 - progress_bar.py[line:274] - INFO: epoch 001:  12588 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=12570, lr=4.56827e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38151
2023-01-05 07:42:11 - progress_bar.py[line:274] - INFO: epoch 001:  12598 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=12580, lr=4.56776e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38163
2023-01-05 07:42:22 - progress_bar.py[line:274] - INFO: epoch 001:  12608 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.92, wpb=109.1, bsz=40, num_updates=12590, lr=4.56725e-05, gnorm=0.666, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38175
2023-01-05 07:42:35 - progress_bar.py[line:274] - INFO: epoch 001:  12618 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=12600, lr=4.56674e-05, gnorm=0.495, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38187
2023-01-05 07:42:47 - progress_bar.py[line:274] - INFO: epoch 001:  12628 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.9, ups=0.88, wpb=111.5, bsz=40, num_updates=12610, lr=4.56623e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38200
2023-01-05 07:42:58 - progress_bar.py[line:274] - INFO: epoch 001:  12638 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.7, ups=0.93, wpb=109.5, bsz=40, num_updates=12620, lr=4.56572e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38211
2023-01-05 07:43:10 - progress_bar.py[line:274] - INFO: epoch 001:  12648 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.92, wpb=109.1, bsz=40, num_updates=12630, lr=4.56521e-05, gnorm=0.628, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38223
2023-01-05 07:43:22 - progress_bar.py[line:274] - INFO: epoch 001:  12658 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102, ups=0.93, wpb=109.3, bsz=40, num_updates=12640, lr=4.5647e-05, gnorm=0.497, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38234
2023-01-05 07:43:33 - progress_bar.py[line:274] - INFO: epoch 001:  12668 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103, ups=0.93, wpb=110.9, bsz=40, num_updates=12650, lr=4.56419e-05, gnorm=0.566, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38246
2023-01-05 07:43:46 - progress_bar.py[line:274] - INFO: epoch 001:  12678 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=12660, lr=4.56368e-05, gnorm=0.526, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38258
2023-01-05 07:43:58 - progress_bar.py[line:274] - INFO: epoch 001:  12688 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.88, wpb=109.7, bsz=40, num_updates=12670, lr=4.56317e-05, gnorm=0.514, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38270
2023-01-05 07:44:10 - progress_bar.py[line:274] - INFO: epoch 001:  12698 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=12680, lr=4.56266e-05, gnorm=0.422, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38283
2023-01-05 07:44:22 - progress_bar.py[line:274] - INFO: epoch 001:  12708 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.7, ups=0.91, wpb=110, bsz=40, num_updates=12690, lr=4.56216e-05, gnorm=0.527, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38295
2023-01-05 07:44:34 - progress_bar.py[line:274] - INFO: epoch 001:  12718 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=12700, lr=4.56165e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38307
2023-01-05 07:44:46 - progress_bar.py[line:274] - INFO: epoch 001:  12728 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.89, wpb=110.4, bsz=40, num_updates=12710, lr=4.56114e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38319
2023-01-05 07:44:58 - progress_bar.py[line:274] - INFO: epoch 001:  12738 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=12720, lr=4.56063e-05, gnorm=0.468, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38331
2023-01-05 07:45:10 - progress_bar.py[line:274] - INFO: epoch 001:  12748 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=12730, lr=4.56012e-05, gnorm=0.581, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38343
2023-01-05 07:45:23 - progress_bar.py[line:274] - INFO: epoch 001:  12758 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.89, wpb=109.5, bsz=40, num_updates=12740, lr=4.55961e-05, gnorm=0.518, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38355
2023-01-05 07:45:35 - progress_bar.py[line:274] - INFO: epoch 001:  12768 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.3, ups=0.91, wpb=111.1, bsz=40, num_updates=12750, lr=4.5591e-05, gnorm=0.508, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38367
2023-01-05 07:45:47 - progress_bar.py[line:274] - INFO: epoch 001:  12778 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=12760, lr=4.55859e-05, gnorm=0.615, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38379
2023-01-05 07:45:59 - progress_bar.py[line:274] - INFO: epoch 001:  12788 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.8, ups=0.88, wpb=109.7, bsz=40, num_updates=12770, lr=4.55808e-05, gnorm=0.492, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38392
2023-01-05 07:46:11 - progress_bar.py[line:274] - INFO: epoch 001:  12798 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=12780, lr=4.55757e-05, gnorm=0.493, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=38403
2023-01-05 07:46:23 - progress_bar.py[line:274] - INFO: epoch 001:  12808 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.7, ups=0.92, wpb=109.9, bsz=40, num_updates=12790, lr=4.55706e-05, gnorm=0.522, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38415
2023-01-05 07:46:35 - progress_bar.py[line:274] - INFO: epoch 001:  12818 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=12800, lr=4.55655e-05, gnorm=0.562, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38427
2023-01-05 07:46:46 - progress_bar.py[line:274] - INFO: epoch 001:  12828 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.5, ups=0.94, wpb=110.6, bsz=40, num_updates=12810, lr=4.55605e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38439
2023-01-05 07:46:58 - progress_bar.py[line:274] - INFO: epoch 001:  12838 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=12820, lr=4.55554e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38451
2023-01-05 07:47:10 - progress_bar.py[line:274] - INFO: epoch 001:  12848 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.9, wpb=109, bsz=40, num_updates=12830, lr=4.55503e-05, gnorm=0.583, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38463
2023-01-05 07:47:22 - progress_bar.py[line:274] - INFO: epoch 001:  12858 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=12840, lr=4.55452e-05, gnorm=0.476, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=38475
2023-01-05 07:47:34 - progress_bar.py[line:274] - INFO: epoch 001:  12868 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.3, ups=0.9, wpb=110.7, bsz=40, num_updates=12850, lr=4.55401e-05, gnorm=0.53, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38487
2023-01-05 07:47:47 - progress_bar.py[line:274] - INFO: epoch 001:  12878 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.89, wpb=109, bsz=40, num_updates=12860, lr=4.5535e-05, gnorm=0.42, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=38499
2023-01-05 07:47:59 - progress_bar.py[line:274] - INFO: epoch 001:  12888 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=12870, lr=4.55299e-05, gnorm=0.465, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38511
2023-01-05 07:48:11 - progress_bar.py[line:274] - INFO: epoch 001:  12898 / 102288 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.4, ups=0.89, wpb=112.5, bsz=40, num_updates=12880, lr=4.55248e-05, gnorm=0.403, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38524
2023-01-05 07:48:23 - progress_bar.py[line:274] - INFO: epoch 001:  12908 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.91, wpb=110.6, bsz=40, num_updates=12890, lr=4.55197e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38536
2023-01-05 07:48:35 - progress_bar.py[line:274] - INFO: epoch 001:  12918 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.7, ups=0.88, wpb=111, bsz=40, num_updates=12900, lr=4.55146e-05, gnorm=0.452, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38548
2023-01-05 07:48:48 - progress_bar.py[line:274] - INFO: epoch 001:  12928 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.89, wpb=109.3, bsz=40, num_updates=12910, lr=4.55095e-05, gnorm=0.55, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38560
2023-01-05 07:49:00 - progress_bar.py[line:274] - INFO: epoch 001:  12938 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.88, wpb=111, bsz=40, num_updates=12920, lr=4.55044e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38572
2023-01-05 07:49:12 - progress_bar.py[line:274] - INFO: epoch 001:  12948 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=12930, lr=4.54994e-05, gnorm=0.614, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38584
2023-01-05 07:49:24 - progress_bar.py[line:274] - INFO: epoch 001:  12958 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=12940, lr=4.54943e-05, gnorm=0.451, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38596
2023-01-05 07:49:35 - progress_bar.py[line:274] - INFO: epoch 001:  12968 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.6, ups=0.91, wpb=111.1, bsz=40, num_updates=12950, lr=4.54892e-05, gnorm=0.432, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38608
2023-01-05 07:49:47 - progress_bar.py[line:274] - INFO: epoch 001:  12978 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=12960, lr=4.54841e-05, gnorm=0.42, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38620
2023-01-05 07:49:59 - progress_bar.py[line:274] - INFO: epoch 001:  12988 / 102288 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=12970, lr=4.5479e-05, gnorm=0.512, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38632
2023-01-05 07:50:11 - progress_bar.py[line:274] - INFO: epoch 001:  12998 / 102288 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=12980, lr=4.54739e-05, gnorm=0.608, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38644
2023-01-05 07:50:23 - progress_bar.py[line:274] - INFO: epoch 001:  13008 / 102288 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.8, ups=0.88, wpb=109.9, bsz=40, num_updates=12990, lr=4.54688e-05, gnorm=0.547, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38656
2023-01-05 07:50:35 - progress_bar.py[line:274] - INFO: epoch 001:  13018 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.91, wpb=111, bsz=40, num_updates=13000, lr=4.54637e-05, gnorm=0.541, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38668
2023-01-05 07:50:47 - progress_bar.py[line:274] - INFO: epoch 001:  13028 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=13010, lr=4.54586e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38680
2023-01-05 07:50:59 - progress_bar.py[line:274] - INFO: epoch 001:  13038 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=13020, lr=4.54535e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38692
2023-01-05 07:51:11 - progress_bar.py[line:274] - INFO: epoch 001:  13048 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.9, ups=0.92, wpb=110, bsz=40, num_updates=13030, lr=4.54484e-05, gnorm=0.538, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38704
2023-01-05 07:51:23 - progress_bar.py[line:274] - INFO: epoch 001:  13058 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.89, wpb=109.3, bsz=40, num_updates=13040, lr=4.54433e-05, gnorm=0.547, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38716
2023-01-05 07:51:35 - progress_bar.py[line:274] - INFO: epoch 001:  13068 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=13050, lr=4.54383e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=38728
2023-01-05 07:51:48 - progress_bar.py[line:274] - INFO: epoch 001:  13078 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.8, ups=0.88, wpb=110.7, bsz=40, num_updates=13060, lr=4.54332e-05, gnorm=0.492, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38740
2023-01-05 07:52:00 - progress_bar.py[line:274] - INFO: epoch 001:  13088 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=13070, lr=4.54281e-05, gnorm=0.431, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38753
2023-01-05 07:52:12 - progress_bar.py[line:274] - INFO: epoch 001:  13098 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.89, wpb=108.7, bsz=40, num_updates=13080, lr=4.5423e-05, gnorm=0.611, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38765
2023-01-05 07:52:24 - progress_bar.py[line:274] - INFO: epoch 001:  13108 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=13090, lr=4.54179e-05, gnorm=0.5, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38777
2023-01-05 07:52:37 - progress_bar.py[line:274] - INFO: epoch 001:  13118 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=13100, lr=4.54128e-05, gnorm=0.502, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38789
2023-01-05 07:52:48 - progress_bar.py[line:274] - INFO: epoch 001:  13128 / 102288 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.8, ups=0.92, wpb=110, bsz=40, num_updates=13110, lr=4.54077e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38801
2023-01-05 07:53:00 - progress_bar.py[line:274] - INFO: epoch 001:  13138 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.9, wpb=110.2, bsz=40, num_updates=13120, lr=4.54026e-05, gnorm=0.737, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38813
2023-01-05 07:53:12 - progress_bar.py[line:274] - INFO: epoch 001:  13148 / 102288 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.88, wpb=111.4, bsz=40, num_updates=13130, lr=4.53975e-05, gnorm=0.427, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38825
2023-01-05 07:53:25 - progress_bar.py[line:274] - INFO: epoch 001:  13158 / 102288 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=94.7, ups=0.87, wpb=108.9, bsz=40, num_updates=13140, lr=4.53924e-05, gnorm=0.556, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38837
2023-01-05 07:53:37 - progress_bar.py[line:274] - INFO: epoch 001:  13168 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.91, wpb=109.3, bsz=40, num_updates=13150, lr=4.53873e-05, gnorm=0.555, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38849
2023-01-05 07:53:49 - progress_bar.py[line:274] - INFO: epoch 001:  13178 / 102288 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.4, ups=0.87, wpb=110.4, bsz=40, num_updates=13160, lr=4.53822e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38862
2023-01-05 07:54:01 - progress_bar.py[line:274] - INFO: epoch 001:  13188 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.3, ups=0.87, wpb=109.5, bsz=40, num_updates=13170, lr=4.53772e-05, gnorm=0.44, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38874
2023-01-05 07:54:14 - progress_bar.py[line:274] - INFO: epoch 001:  13198 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=13180, lr=4.53721e-05, gnorm=0.567, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38886
2023-01-05 07:54:26 - progress_bar.py[line:274] - INFO: epoch 001:  13208 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.91, wpb=110.4, bsz=40, num_updates=13190, lr=4.5367e-05, gnorm=0.499, clip=10, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38898
2023-01-05 07:54:33 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-01-05 07:54:39 - progress_bar.py[line:274] - INFO: epoch 001:  13219 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=89.2, ups=0.82, wpb=109, bsz=40, num_updates=13200, lr=4.53619e-05, gnorm=0.577, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=38911
2023-01-05 07:54:44 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 07:54:52 - progress_bar.py[line:274] - INFO: epoch 001:  13230 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=89.6, ups=0.83, wpb=108.1, bsz=40, num_updates=13210, lr=4.53568e-05, gnorm=0.58, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=38924
2023-01-05 07:55:03 - progress_bar.py[line:274] - INFO: epoch 001:  13240 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.7, ups=0.92, wpb=109.8, bsz=40, num_updates=13220, lr=4.53517e-05, gnorm=0.418, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38936
2023-01-05 07:55:15 - progress_bar.py[line:274] - INFO: epoch 001:  13250 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.9, wpb=109.9, bsz=40, num_updates=13230, lr=4.53466e-05, gnorm=0.484, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38948
2023-01-05 07:55:28 - progress_bar.py[line:274] - INFO: epoch 001:  13260 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.5, ups=0.87, wpb=110.9, bsz=40, num_updates=13240, lr=4.53415e-05, gnorm=0.549, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38960
2023-01-05 07:55:40 - progress_bar.py[line:274] - INFO: epoch 001:  13270 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.7, ups=0.92, wpb=111.1, bsz=40, num_updates=13250, lr=4.53364e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38972
2023-01-05 07:55:52 - progress_bar.py[line:274] - INFO: epoch 001:  13280 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.1, ups=0.87, wpb=109.4, bsz=40, num_updates=13260, lr=4.53313e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38985
2023-01-05 07:56:04 - progress_bar.py[line:274] - INFO: epoch 001:  13290 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=13270, lr=4.53262e-05, gnorm=0.421, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38997
2023-01-05 07:56:16 - progress_bar.py[line:274] - INFO: epoch 001:  13300 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.91, wpb=109.7, bsz=40, num_updates=13280, lr=4.53211e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39009
2023-01-05 07:56:28 - progress_bar.py[line:274] - INFO: epoch 001:  13310 / 102288 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=13290, lr=4.5316e-05, gnorm=0.367, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39021
2023-01-05 07:56:40 - progress_bar.py[line:274] - INFO: epoch 001:  13320 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=13300, lr=4.5311e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39033
2023-01-05 07:56:52 - progress_bar.py[line:274] - INFO: epoch 001:  13330 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.4, ups=0.93, wpb=110.2, bsz=40, num_updates=13310, lr=4.53059e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39044
2023-01-05 07:57:04 - progress_bar.py[line:274] - INFO: epoch 001:  13340 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=13320, lr=4.53008e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39056
2023-01-05 07:57:16 - progress_bar.py[line:274] - INFO: epoch 001:  13350 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.8, ups=0.88, wpb=108.9, bsz=40, num_updates=13330, lr=4.52957e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39069
2023-01-05 07:57:28 - progress_bar.py[line:274] - INFO: epoch 001:  13360 / 102288 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101, ups=0.92, wpb=109.4, bsz=40, num_updates=13340, lr=4.52906e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39080
2023-01-05 07:57:40 - progress_bar.py[line:274] - INFO: epoch 001:  13370 / 102288 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=13350, lr=4.52855e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39092
2023-01-05 07:57:52 - progress_bar.py[line:274] - INFO: epoch 001:  13380 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.9, wpb=108.9, bsz=40, num_updates=13360, lr=4.52804e-05, gnorm=0.425, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39104
2023-01-05 07:58:03 - progress_bar.py[line:274] - INFO: epoch 001:  13390 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.9, ups=0.94, wpb=110.4, bsz=40, num_updates=13370, lr=4.52753e-05, gnorm=0.601, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39116
2023-01-05 07:58:15 - progress_bar.py[line:274] - INFO: epoch 001:  13400 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101, ups=0.92, wpb=110.1, bsz=40, num_updates=13380, lr=4.52702e-05, gnorm=0.505, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39128
2023-01-05 07:58:27 - progress_bar.py[line:274] - INFO: epoch 001:  13410 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.91, wpb=110.4, bsz=40, num_updates=13390, lr=4.52651e-05, gnorm=0.458, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39140
2023-01-05 07:58:39 - progress_bar.py[line:274] - INFO: epoch 001:  13420 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.88, wpb=111.3, bsz=40, num_updates=13400, lr=4.526e-05, gnorm=0.591, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39152
2023-01-05 07:58:51 - progress_bar.py[line:274] - INFO: epoch 001:  13430 / 102288 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=13410, lr=4.52549e-05, gnorm=0.496, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39164
2023-01-05 07:59:03 - progress_bar.py[line:274] - INFO: epoch 001:  13440 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=13420, lr=4.52499e-05, gnorm=0.5, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39176
2023-01-05 07:59:15 - progress_bar.py[line:274] - INFO: epoch 001:  13450 / 102288 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.6, ups=0.91, wpb=110.1, bsz=40, num_updates=13430, lr=4.52448e-05, gnorm=0.471, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39188
2023-01-05 07:59:27 - progress_bar.py[line:274] - INFO: epoch 001:  13460 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.1, ups=0.93, wpb=109.9, bsz=40, num_updates=13440, lr=4.52397e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39199
2023-01-05 07:59:39 - progress_bar.py[line:274] - INFO: epoch 001:  13470 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=13450, lr=4.52346e-05, gnorm=0.508, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39212
2023-01-05 07:59:51 - progress_bar.py[line:274] - INFO: epoch 001:  13480 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=13460, lr=4.52295e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39224
2023-01-05 08:00:03 - progress_bar.py[line:274] - INFO: epoch 001:  13490 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.88, wpb=111.5, bsz=40, num_updates=13470, lr=4.52244e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39236
2023-01-05 08:00:15 - progress_bar.py[line:274] - INFO: epoch 001:  13500 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103.8, ups=0.93, wpb=111.6, bsz=40, num_updates=13480, lr=4.52193e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39247
2023-01-05 08:00:27 - progress_bar.py[line:274] - INFO: epoch 001:  13510 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.3, ups=0.9, wpb=111.9, bsz=40, num_updates=13490, lr=4.52142e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39259
2023-01-05 08:00:39 - progress_bar.py[line:274] - INFO: epoch 001:  13520 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=13500, lr=4.52091e-05, gnorm=0.505, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39272
2023-01-05 08:00:52 - progress_bar.py[line:274] - INFO: epoch 001:  13530 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=13510, lr=4.5204e-05, gnorm=0.456, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39284
2023-01-05 08:01:04 - progress_bar.py[line:274] - INFO: epoch 001:  13540 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.9, wpb=109.4, bsz=40, num_updates=13520, lr=4.51989e-05, gnorm=0.436, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39296
2023-01-05 08:01:16 - progress_bar.py[line:274] - INFO: epoch 001:  13550 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.2, ups=0.91, wpb=111.6, bsz=40, num_updates=13530, lr=4.51938e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39308
2023-01-05 08:01:28 - progress_bar.py[line:274] - INFO: epoch 001:  13560 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=13540, lr=4.51888e-05, gnorm=0.426, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39320
2023-01-05 08:01:40 - progress_bar.py[line:274] - INFO: epoch 001:  13570 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.1, ups=0.9, wpb=111.2, bsz=40, num_updates=13550, lr=4.51837e-05, gnorm=0.547, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39333
2023-01-05 08:01:52 - progress_bar.py[line:274] - INFO: epoch 001:  13580 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.9, ups=0.88, wpb=111, bsz=40, num_updates=13560, lr=4.51786e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39345
2023-01-05 08:02:04 - progress_bar.py[line:274] - INFO: epoch 001:  13590 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=13570, lr=4.51735e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39357
2023-01-05 08:02:16 - progress_bar.py[line:274] - INFO: epoch 001:  13600 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=13580, lr=4.51684e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39369
2023-01-05 08:02:28 - progress_bar.py[line:274] - INFO: epoch 001:  13610 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=13590, lr=4.51633e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39381
2023-01-05 08:02:41 - progress_bar.py[line:274] - INFO: epoch 001:  13620 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=13600, lr=4.51582e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39393
2023-01-05 08:02:53 - progress_bar.py[line:274] - INFO: epoch 001:  13630 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.1, ups=0.89, wpb=110.3, bsz=40, num_updates=13610, lr=4.51531e-05, gnorm=0.364, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39405
2023-01-05 08:03:05 - progress_bar.py[line:274] - INFO: epoch 001:  13640 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=13620, lr=4.5148e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39418
2023-01-05 08:03:17 - progress_bar.py[line:274] - INFO: epoch 001:  13650 / 102288 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.6, ups=0.9, wpb=111.2, bsz=40, num_updates=13630, lr=4.51429e-05, gnorm=0.412, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39430
2023-01-05 08:03:29 - progress_bar.py[line:274] - INFO: epoch 001:  13660 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.6, ups=0.92, wpb=111.9, bsz=40, num_updates=13640, lr=4.51378e-05, gnorm=0.523, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39442
2023-01-05 08:03:41 - progress_bar.py[line:274] - INFO: epoch 001:  13670 / 102288 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=13650, lr=4.51327e-05, gnorm=0.507, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39454
2023-01-05 08:03:53 - progress_bar.py[line:274] - INFO: epoch 001:  13680 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.6, ups=0.91, wpb=109.9, bsz=40, num_updates=13660, lr=4.51277e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39466
2023-01-05 08:04:05 - progress_bar.py[line:274] - INFO: epoch 001:  13690 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=13670, lr=4.51226e-05, gnorm=0.581, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39478
2023-01-05 08:04:17 - progress_bar.py[line:274] - INFO: epoch 001:  13700 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.8, ups=0.88, wpb=109.8, bsz=40, num_updates=13680, lr=4.51175e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39490
2023-01-05 08:04:29 - progress_bar.py[line:274] - INFO: epoch 001:  13710 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.7, ups=0.91, wpb=111.1, bsz=40, num_updates=13690, lr=4.51124e-05, gnorm=0.417, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39502
2023-01-05 08:04:41 - progress_bar.py[line:274] - INFO: epoch 001:  13720 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=13700, lr=4.51073e-05, gnorm=0.454, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39514
2023-01-05 08:04:53 - progress_bar.py[line:274] - INFO: epoch 001:  13730 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=13710, lr=4.51022e-05, gnorm=0.392, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39525
2023-01-05 08:05:04 - progress_bar.py[line:274] - INFO: epoch 001:  13740 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=13720, lr=4.50971e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39537
2023-01-05 08:05:16 - progress_bar.py[line:274] - INFO: epoch 001:  13750 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=13730, lr=4.5092e-05, gnorm=0.439, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39549
2023-01-05 08:05:28 - progress_bar.py[line:274] - INFO: epoch 001:  13760 / 102288 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=13740, lr=4.50869e-05, gnorm=0.492, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39561
2023-01-05 08:05:40 - progress_bar.py[line:274] - INFO: epoch 001:  13770 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=13750, lr=4.50818e-05, gnorm=0.427, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39573
2023-01-05 08:05:53 - progress_bar.py[line:274] - INFO: epoch 001:  13780 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=13760, lr=4.50767e-05, gnorm=0.502, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39585
2023-01-05 08:06:05 - progress_bar.py[line:274] - INFO: epoch 001:  13790 / 102288 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=13770, lr=4.50716e-05, gnorm=0.373, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39597
2023-01-05 08:06:17 - progress_bar.py[line:274] - INFO: epoch 001:  13800 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.1, ups=0.88, wpb=109.2, bsz=40, num_updates=13780, lr=4.50665e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39609
2023-01-05 08:06:29 - progress_bar.py[line:274] - INFO: epoch 001:  13810 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.6, ups=0.89, wpb=109.1, bsz=40, num_updates=13790, lr=4.50615e-05, gnorm=0.683, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39621
2023-01-05 08:06:41 - progress_bar.py[line:274] - INFO: epoch 001:  13820 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.1, ups=0.88, wpb=112.1, bsz=40, num_updates=13800, lr=4.50564e-05, gnorm=0.491, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39634
2023-01-05 08:06:53 - progress_bar.py[line:274] - INFO: epoch 001:  13830 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.9, wpb=110, bsz=40, num_updates=13810, lr=4.50513e-05, gnorm=0.538, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39646
2023-01-05 08:07:05 - progress_bar.py[line:274] - INFO: epoch 001:  13840 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.1, ups=0.92, wpb=110.8, bsz=40, num_updates=13820, lr=4.50462e-05, gnorm=0.535, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39657
2023-01-05 08:07:17 - progress_bar.py[line:274] - INFO: epoch 001:  13850 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.2, ups=0.88, wpb=110.6, bsz=40, num_updates=13830, lr=4.50411e-05, gnorm=0.427, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39670
2023-01-05 08:07:29 - progress_bar.py[line:274] - INFO: epoch 001:  13860 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=13840, lr=4.5036e-05, gnorm=0.681, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39681
2023-01-05 08:07:40 - progress_bar.py[line:274] - INFO: epoch 001:  13870 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.7, ups=0.93, wpb=109.9, bsz=40, num_updates=13850, lr=4.50309e-05, gnorm=0.555, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39693
2023-01-05 08:07:52 - progress_bar.py[line:274] - INFO: epoch 001:  13880 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101, ups=0.92, wpb=109.6, bsz=40, num_updates=13860, lr=4.50258e-05, gnorm=0.462, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39705
2023-01-05 08:08:04 - progress_bar.py[line:274] - INFO: epoch 001:  13890 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=13870, lr=4.50207e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39717
2023-01-05 08:08:16 - progress_bar.py[line:274] - INFO: epoch 001:  13900 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=13880, lr=4.50156e-05, gnorm=0.52, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39729
2023-01-05 08:08:28 - progress_bar.py[line:274] - INFO: epoch 001:  13910 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.1, ups=0.9, wpb=111.5, bsz=40, num_updates=13890, lr=4.50105e-05, gnorm=0.445, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39741
2023-01-05 08:08:41 - progress_bar.py[line:274] - INFO: epoch 001:  13920 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.87, wpb=110.6, bsz=40, num_updates=13900, lr=4.50054e-05, gnorm=0.581, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39753
2023-01-05 08:08:53 - progress_bar.py[line:274] - INFO: epoch 001:  13930 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=13910, lr=4.50004e-05, gnorm=0.462, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39765
2023-01-05 08:09:05 - progress_bar.py[line:274] - INFO: epoch 001:  13940 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=13920, lr=4.49953e-05, gnorm=0.555, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39777
2023-01-05 08:09:17 - progress_bar.py[line:274] - INFO: epoch 001:  13950 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.6, ups=0.91, wpb=111.2, bsz=40, num_updates=13930, lr=4.49902e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39789
2023-01-05 08:09:29 - progress_bar.py[line:274] - INFO: epoch 001:  13960 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=94.5, ups=0.87, wpb=108.8, bsz=40, num_updates=13940, lr=4.49851e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=39802
2023-01-05 08:09:41 - progress_bar.py[line:274] - INFO: epoch 001:  13970 / 102288 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.1, ups=0.88, wpb=112.7, bsz=40, num_updates=13950, lr=4.498e-05, gnorm=0.519, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39814
2023-01-05 08:09:53 - progress_bar.py[line:274] - INFO: epoch 001:  13980 / 102288 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.1, ups=0.89, wpb=111.9, bsz=40, num_updates=13960, lr=4.49749e-05, gnorm=0.498, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39826
2023-01-05 08:10:05 - progress_bar.py[line:274] - INFO: epoch 001:  13990 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=13970, lr=4.49698e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39838
2023-01-05 08:10:17 - progress_bar.py[line:274] - INFO: epoch 001:  14000 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=13980, lr=4.49647e-05, gnorm=0.434, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39850
2023-01-05 08:10:29 - progress_bar.py[line:274] - INFO: epoch 001:  14010 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97, ups=0.87, wpb=111.2, bsz=40, num_updates=13990, lr=4.49596e-05, gnorm=0.466, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39862
2023-01-05 08:10:41 - progress_bar.py[line:274] - INFO: epoch 001:  14020 / 102288 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.9, ups=0.9, wpb=111.6, bsz=40, num_updates=14000, lr=4.49545e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39874
2023-01-05 08:10:41 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 08:10:42 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 08:10:42 - train.py[line:551] - INFO: load:0.84 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 08:13:14 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 08:13:14 - train.py[line:551] - INFO: load:0.86 valid_run:151.63 task_valid:147.81 collect_output:2.75
2023-01-05 08:15:43 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 08:15:43 - train.py[line:551] - INFO: load:0.89 valid_run:300.14 task_valid:290.49 collect_output:7.55
2023-01-05 08:18:15 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 08:18:15 - train.py[line:551] - INFO: load:0.92 valid_run:452.74 task_valid:433.52 collect_output:16.10
2023-01-05 08:20:45 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 08:20:45 - train.py[line:551] - INFO: load:0.94 valid_run:601.88 task_valid:578.09 collect_output:19.65
2023-01-05 08:23:17 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 08:23:17 - train.py[line:551] - INFO: load:0.97 valid_run:754.35 task_valid:725.19 collect_output:23.98
2023-01-05 08:25:49 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 08:25:49 - train.py[line:551] - INFO: load:1.00 valid_run:906.12 task_valid:870.46 collect_output:29.46
2023-01-05 08:28:23 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 08:28:23 - train.py[line:551] - INFO: load:1.02 valid_run:1059.96 task_valid:1016.23 collect_output:36.53
2023-01-05 08:30:54 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 08:30:54 - train.py[line:551] - INFO: load:1.05 valid_run:1211.44 task_valid:1156.96 collect_output:46.25
2023-01-05 08:33:25 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 08:33:25 - train.py[line:551] - INFO: load:1.08 valid_run:1361.47 task_valid:1301.80 collect_output:50.39
2023-01-05 08:35:53 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 08:35:53 - train.py[line:551] - INFO: load:1.10 valid_run:1510.21 task_valid:1444.84 collect_output:55.06
2023-01-05 08:38:24 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 08:38:24 - train.py[line:551] - INFO: load:1.13 valid_run:1660.27 task_valid:1589.70 collect_output:59.23
2023-01-05 08:40:54 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 08:40:54 - train.py[line:551] - INFO: load:1.16 valid_run:1810.34 task_valid:1734.32 collect_output:63.64
2023-01-05 08:43:24 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 08:43:24 - train.py[line:551] - INFO: load:1.18 valid_run:1960.54 task_valid:1875.87 collect_output:71.26
2023-01-05 08:45:55 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 08:45:55 - train.py[line:551] - INFO: load:1.21 valid_run:2111.09 task_valid:2021.14 collect_output:75.50
2023-01-05 08:48:25 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 08:48:25 - train.py[line:551] - INFO: load:1.24 valid_run:2261.24 task_valid:2167.60 collect_output:78.17
2023-01-05 08:50:55 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 08:50:55 - train.py[line:551] - INFO: load:1.27 valid_run:2411.38 task_valid:2311.52 collect_output:83.36
2023-01-05 08:53:27 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 08:53:27 - train.py[line:551] - INFO: load:1.29 valid_run:2563.20 task_valid:2456.76 collect_output:88.89
2023-01-05 08:55:57 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 08:55:57 - train.py[line:551] - INFO: load:1.32 valid_run:2713.61 task_valid:2603.33 collect_output:91.70
2023-01-05 08:58:26 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 08:58:26 - train.py[line:551] - INFO: load:1.35 valid_run:2862.40 task_valid:2744.72 collect_output:98.10
2023-01-05 09:00:57 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 09:00:57 - train.py[line:551] - INFO: load:1.37 valid_run:3013.02 task_valid:2889.48 collect_output:102.93
2023-01-05 09:03:29 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 09:03:29 - train.py[line:551] - INFO: load:1.40 valid_run:3165.34 task_valid:3033.75 collect_output:109.94
2023-01-05 09:05:59 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 09:05:59 - train.py[line:551] - INFO: load:1.43 valid_run:3314.75 task_valid:3178.07 collect_output:114.00
2023-01-05 09:08:30 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 09:08:30 - train.py[line:551] - INFO: load:1.45 valid_run:3466.05 task_valid:3324.07 collect_output:118.28
2023-01-05 09:11:02 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 09:11:02 - train.py[line:551] - INFO: load:1.48 valid_run:3617.47 task_valid:3470.08 collect_output:122.64

====================================================================================================
SGG eval:     R @ 50: 0.3675;     R @ 100: 0.4525;     R @ 500: 0.5013;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2556;    mR @ 100: 0.3066;    mR @ 500: 0.3744;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5220) (covered in:0.8125) (covering:0.2857) (eating:0.5882) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3065) (lying on:0.1000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.7083) (playing:0.0000) (riding:0.7255) (says:0.0000) (sitting on:0.5822) (standing on:0.1000) (using:0.3500) (walking in:0.3333) (walking on:0.1622) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 09:13:32 - train.py[line:487] - INFO: 0.45248571428571427

====================================================================================================
SGG eval:     R @ 50: 0.3675;     R @ 100: 0.4525;     R @ 500: 0.5013;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2556;    mR @ 100: 0.3066;    mR @ 500: 0.3744;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5220) (covered in:0.8125) (covering:0.2857) (eating:0.5882) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3065) (lying on:0.1000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.7083) (playing:0.0000) (riding:0.7255) (says:0.0000) (sitting on:0.5822) (standing on:0.1000) (using:0.3500) (walking in:0.3333) (walking on:0.1622) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 09:13:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 09:13:33 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.374 | loss_v1 0 | loss_v2 0 | nll_loss 0.221 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.452486 | ppl 1.17 | vqa_score 0.4245 | wps 119 | wpb 89.9 | bsz 30 | num_updates 14000 | best_R@100 0.611199
2023-01-05 09:13:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 14000 updates
2023-01-05 09:13:33 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-05 09:14:14 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-05 09:15:39 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 0.45248571428571427) (writing took 126.24210277199745 seconds)
2023-01-05 09:15:51 - progress_bar.py[line:274] - INFO: epoch 001:  14030 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=0.3, ups=0, wpb=109.4, bsz=40, num_updates=14010, lr=4.49494e-05, gnorm=0.439, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43784
2023-01-05 09:16:02 - progress_bar.py[line:274] - INFO: epoch 001:  14040 / 102288 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.6, ups=0.9, wpb=111.2, bsz=40, num_updates=14020, lr=4.49443e-05, gnorm=0.4, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43795
2023-01-05 09:16:13 - progress_bar.py[line:274] - INFO: epoch 001:  14050 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=14030, lr=4.49393e-05, gnorm=0.5, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43807
2023-01-05 09:16:25 - progress_bar.py[line:274] - INFO: epoch 001:  14060 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=14040, lr=4.49342e-05, gnorm=0.512, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43818
2023-01-05 09:16:36 - progress_bar.py[line:274] - INFO: epoch 001:  14070 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=14050, lr=4.49291e-05, gnorm=0.506, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43829
2023-01-05 09:16:47 - progress_bar.py[line:274] - INFO: epoch 001:  14080 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=14060, lr=4.4924e-05, gnorm=0.578, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43841
2023-01-05 09:16:59 - progress_bar.py[line:274] - INFO: epoch 001:  14090 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=14070, lr=4.49189e-05, gnorm=0.514, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43852
2023-01-05 09:17:10 - progress_bar.py[line:274] - INFO: epoch 001:  14100 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.2, ups=0.92, wpb=109.7, bsz=40, num_updates=14080, lr=4.49138e-05, gnorm=0.646, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43863
2023-01-05 09:17:21 - progress_bar.py[line:274] - INFO: epoch 001:  14110 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.9, ups=0.92, wpb=111.2, bsz=40, num_updates=14090, lr=4.49087e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43874
2023-01-05 09:17:32 - progress_bar.py[line:274] - INFO: epoch 001:  14120 / 102288 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=14100, lr=4.49036e-05, gnorm=0.423, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43886
2023-01-05 09:17:44 - progress_bar.py[line:274] - INFO: epoch 001:  14130 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=14110, lr=4.48985e-05, gnorm=0.445, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43897
2023-01-05 09:17:56 - progress_bar.py[line:274] - INFO: epoch 001:  14140 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95, ups=0.87, wpb=109.2, bsz=40, num_updates=14120, lr=4.48934e-05, gnorm=0.56, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43909
2023-01-05 09:18:07 - progress_bar.py[line:274] - INFO: epoch 001:  14150 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.88, wpb=111.8, bsz=40, num_updates=14130, lr=4.48883e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43920
2023-01-05 09:18:18 - progress_bar.py[line:274] - INFO: epoch 001:  14160 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.6, ups=0.93, wpb=110.4, bsz=40, num_updates=14140, lr=4.48832e-05, gnorm=0.461, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43931
2023-01-05 09:18:29 - progress_bar.py[line:274] - INFO: epoch 001:  14170 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=14150, lr=4.48782e-05, gnorm=0.5, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43943
2023-01-05 09:18:41 - progress_bar.py[line:274] - INFO: epoch 001:  14180 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.5, ups=0.88, wpb=111, bsz=40, num_updates=14160, lr=4.48731e-05, gnorm=0.576, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43954
2023-01-05 09:18:52 - progress_bar.py[line:274] - INFO: epoch 001:  14190 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=14170, lr=4.4868e-05, gnorm=0.46, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43966
2023-01-05 09:19:04 - progress_bar.py[line:274] - INFO: epoch 001:  14200 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=14180, lr=4.48629e-05, gnorm=0.466, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43977
2023-01-05 09:19:15 - progress_bar.py[line:274] - INFO: epoch 001:  14210 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.4, ups=0.9, wpb=108.9, bsz=40, num_updates=14190, lr=4.48578e-05, gnorm=0.712, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43988
2023-01-05 09:19:26 - progress_bar.py[line:274] - INFO: epoch 001:  14220 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.9, wpb=109.4, bsz=40, num_updates=14200, lr=4.48527e-05, gnorm=0.597, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43999
2023-01-05 09:19:38 - progress_bar.py[line:274] - INFO: epoch 001:  14230 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.88, wpb=110.7, bsz=40, num_updates=14210, lr=4.48476e-05, gnorm=0.492, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44011
2023-01-05 09:19:49 - progress_bar.py[line:274] - INFO: epoch 001:  14240 / 102288 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.6, ups=0.89, wpb=108.3, bsz=40, num_updates=14220, lr=4.48425e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44022
2023-01-05 09:20:00 - progress_bar.py[line:274] - INFO: epoch 001:  14250 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=14230, lr=4.48374e-05, gnorm=0.54, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44034
2023-01-05 09:20:11 - progress_bar.py[line:274] - INFO: epoch 001:  14260 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.2, ups=0.92, wpb=110.2, bsz=40, num_updates=14240, lr=4.48323e-05, gnorm=0.489, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44045
2023-01-05 09:20:13 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-01-05 09:20:23 - progress_bar.py[line:274] - INFO: epoch 001:  14271 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=91.8, ups=0.85, wpb=108.3, bsz=40, num_updates=14250, lr=4.48272e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44057
2023-01-05 09:20:35 - progress_bar.py[line:274] - INFO: epoch 001:  14281 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98, ups=0.88, wpb=111.2, bsz=40, num_updates=14260, lr=4.48221e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44068
2023-01-05 09:20:46 - progress_bar.py[line:274] - INFO: epoch 001:  14291 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=14270, lr=4.48171e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44079
2023-01-05 09:20:58 - progress_bar.py[line:274] - INFO: epoch 001:  14301 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=14280, lr=4.4812e-05, gnorm=0.484, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44091
2023-01-05 09:21:08 - progress_bar.py[line:274] - INFO: epoch 001:  14311 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.4, ups=0.94, wpb=109.1, bsz=40, num_updates=14290, lr=4.48069e-05, gnorm=0.506, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44102
2023-01-05 09:21:20 - progress_bar.py[line:274] - INFO: epoch 001:  14321 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.88, wpb=110.4, bsz=40, num_updates=14300, lr=4.48018e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44113
2023-01-05 09:21:31 - progress_bar.py[line:274] - INFO: epoch 001:  14331 / 102288 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=103.7, ups=0.93, wpb=111.5, bsz=40, num_updates=14310, lr=4.47967e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44124
2023-01-05 09:21:42 - progress_bar.py[line:274] - INFO: epoch 001:  14341 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=14320, lr=4.47916e-05, gnorm=0.524, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44136
2023-01-05 09:21:54 - progress_bar.py[line:274] - INFO: epoch 001:  14351 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=14330, lr=4.47865e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44147
2023-01-05 09:22:05 - progress_bar.py[line:274] - INFO: epoch 001:  14361 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=14340, lr=4.47814e-05, gnorm=0.378, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44159
2023-01-05 09:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  14371 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.7, ups=0.88, wpb=110, bsz=40, num_updates=14350, lr=4.47763e-05, gnorm=0.545, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44170
2023-01-05 09:22:28 - progress_bar.py[line:274] - INFO: epoch 001:  14381 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.9, wpb=109.9, bsz=40, num_updates=14360, lr=4.47712e-05, gnorm=0.479, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44181
2023-01-05 09:22:40 - progress_bar.py[line:274] - INFO: epoch 001:  14391 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=14370, lr=4.47661e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44193
2023-01-05 09:22:51 - progress_bar.py[line:274] - INFO: epoch 001:  14401 / 102288 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.6, ups=0.88, wpb=109.9, bsz=40, num_updates=14380, lr=4.4761e-05, gnorm=0.495, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44205
2023-01-05 09:23:02 - progress_bar.py[line:274] - INFO: epoch 001:  14411 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.91, wpb=110.8, bsz=40, num_updates=14390, lr=4.47559e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44216
2023-01-05 09:23:14 - progress_bar.py[line:274] - INFO: epoch 001:  14421 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.9, wpb=109.1, bsz=40, num_updates=14400, lr=4.47509e-05, gnorm=0.484, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44227
2023-01-05 09:23:25 - progress_bar.py[line:274] - INFO: epoch 001:  14431 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=14410, lr=4.47458e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44239
2023-01-05 09:23:36 - progress_bar.py[line:274] - INFO: epoch 001:  14441 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=104.2, ups=0.94, wpb=110.6, bsz=40, num_updates=14420, lr=4.47407e-05, gnorm=0.749, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44249
2023-01-05 09:23:47 - progress_bar.py[line:274] - INFO: epoch 001:  14451 / 102288 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.6, ups=0.92, wpb=110.6, bsz=40, num_updates=14430, lr=4.47356e-05, gnorm=0.593, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44261
2023-01-05 09:23:59 - progress_bar.py[line:274] - INFO: epoch 001:  14461 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=14440, lr=4.47305e-05, gnorm=0.511, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44272
2023-01-05 09:24:10 - progress_bar.py[line:274] - INFO: epoch 001:  14471 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.9, ups=0.88, wpb=109.1, bsz=40, num_updates=14450, lr=4.47254e-05, gnorm=0.515, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44284
2023-01-05 09:24:22 - progress_bar.py[line:274] - INFO: epoch 001:  14481 / 102288 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=14460, lr=4.47203e-05, gnorm=0.408, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44295
2023-01-05 09:24:33 - progress_bar.py[line:274] - INFO: epoch 001:  14491 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=14470, lr=4.47152e-05, gnorm=0.504, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44306
2023-01-05 09:24:44 - progress_bar.py[line:274] - INFO: epoch 001:  14501 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.88, wpb=111.1, bsz=40, num_updates=14480, lr=4.47101e-05, gnorm=0.5, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44318
2023-01-05 09:24:55 - progress_bar.py[line:274] - INFO: epoch 001:  14511 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.3, ups=0.93, wpb=110.5, bsz=40, num_updates=14490, lr=4.4705e-05, gnorm=0.549, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44329
2023-01-05 09:25:07 - progress_bar.py[line:274] - INFO: epoch 001:  14521 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.88, wpb=109.4, bsz=40, num_updates=14500, lr=4.46999e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44340
2023-01-05 09:25:18 - progress_bar.py[line:274] - INFO: epoch 001:  14531 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.4, ups=0.92, wpb=110.5, bsz=40, num_updates=14510, lr=4.46948e-05, gnorm=0.472, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44351
2023-01-05 09:25:29 - progress_bar.py[line:274] - INFO: epoch 001:  14541 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.92, wpb=110.3, bsz=40, num_updates=14520, lr=4.46898e-05, gnorm=0.465, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44363
2023-01-05 09:25:41 - progress_bar.py[line:274] - INFO: epoch 001:  14551 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.89, wpb=109.1, bsz=40, num_updates=14530, lr=4.46847e-05, gnorm=0.567, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44374
2023-01-05 09:25:52 - progress_bar.py[line:274] - INFO: epoch 001:  14561 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=14540, lr=4.46796e-05, gnorm=0.41, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44385
2023-01-05 09:26:04 - progress_bar.py[line:274] - INFO: epoch 001:  14571 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.88, wpb=110.4, bsz=40, num_updates=14550, lr=4.46745e-05, gnorm=0.403, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44397
2023-01-05 09:26:15 - progress_bar.py[line:274] - INFO: epoch 001:  14581 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.89, wpb=110.8, bsz=40, num_updates=14560, lr=4.46694e-05, gnorm=0.485, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44408
2023-01-05 09:26:26 - progress_bar.py[line:274] - INFO: epoch 001:  14591 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.92, wpb=109.8, bsz=40, num_updates=14570, lr=4.46643e-05, gnorm=0.571, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44419
2023-01-05 09:26:37 - progress_bar.py[line:274] - INFO: epoch 001:  14601 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=14580, lr=4.46592e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44431
2023-01-05 09:26:49 - progress_bar.py[line:274] - INFO: epoch 001:  14611 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.4, ups=0.87, wpb=111.1, bsz=40, num_updates=14590, lr=4.46541e-05, gnorm=0.517, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44442
2023-01-05 09:27:00 - progress_bar.py[line:274] - INFO: epoch 001:  14621 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.8, ups=0.91, wpb=109.1, bsz=40, num_updates=14600, lr=4.4649e-05, gnorm=0.513, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44454
2023-01-05 09:27:12 - progress_bar.py[line:274] - INFO: epoch 001:  14631 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=14610, lr=4.46439e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=44465
2023-01-05 09:27:23 - progress_bar.py[line:274] - INFO: epoch 001:  14641 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=14620, lr=4.46388e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44476
2023-01-05 09:27:34 - progress_bar.py[line:274] - INFO: epoch 001:  14651 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.9, ups=0.9, wpb=110.2, bsz=40, num_updates=14630, lr=4.46337e-05, gnorm=0.477, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44488
2023-01-05 09:27:41 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 09:27:46 - progress_bar.py[line:274] - INFO: epoch 001:  14662 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=93.6, ups=0.85, wpb=110.2, bsz=40, num_updates=14640, lr=4.46287e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=44500
2023-01-05 09:27:58 - progress_bar.py[line:274] - INFO: epoch 001:  14672 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=14650, lr=4.46236e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44511
2023-01-05 09:28:09 - progress_bar.py[line:274] - INFO: epoch 001:  14682 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.91, wpb=109.4, bsz=40, num_updates=14660, lr=4.46185e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44522
2023-01-05 09:28:20 - progress_bar.py[line:274] - INFO: epoch 001:  14692 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.9, wpb=108.9, bsz=40, num_updates=14670, lr=4.46134e-05, gnorm=0.526, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44534
2023-01-05 09:28:32 - progress_bar.py[line:274] - INFO: epoch 001:  14702 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=14680, lr=4.46083e-05, gnorm=0.572, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44545
2023-01-05 09:28:43 - progress_bar.py[line:274] - INFO: epoch 001:  14712 / 102288 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.6, ups=0.92, wpb=108.8, bsz=40, num_updates=14690, lr=4.46032e-05, gnorm=0.537, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44556
2023-01-05 09:28:54 - progress_bar.py[line:274] - INFO: epoch 001:  14722 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.9, ups=0.93, wpb=109.7, bsz=40, num_updates=14700, lr=4.45981e-05, gnorm=0.516, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44567
2023-01-05 09:29:04 - progress_bar.py[line:274] - INFO: epoch 001:  14732 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=104, ups=0.95, wpb=108.9, bsz=40, num_updates=14710, lr=4.4593e-05, gnorm=0.412, clip=0, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=44578
2023-01-05 09:29:16 - progress_bar.py[line:274] - INFO: epoch 001:  14742 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97, ups=0.88, wpb=110.2, bsz=40, num_updates=14720, lr=4.45879e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44589
2023-01-05 09:29:27 - progress_bar.py[line:274] - INFO: epoch 001:  14752 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=14730, lr=4.45828e-05, gnorm=0.41, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44601
2023-01-05 09:29:39 - progress_bar.py[line:274] - INFO: epoch 001:  14762 / 102288 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=14740, lr=4.45777e-05, gnorm=0.437, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44612
2023-01-05 09:29:51 - progress_bar.py[line:274] - INFO: epoch 001:  14772 / 102288 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.9, ups=0.88, wpb=111.2, bsz=40, num_updates=14750, lr=4.45726e-05, gnorm=0.434, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44624
2023-01-05 09:30:02 - progress_bar.py[line:274] - INFO: epoch 001:  14782 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.9, wpb=111, bsz=40, num_updates=14760, lr=4.45676e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44635
2023-01-05 09:30:14 - progress_bar.py[line:274] - INFO: epoch 001:  14792 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.8, ups=0.87, wpb=110, bsz=40, num_updates=14770, lr=4.45625e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44647
2023-01-05 09:30:25 - progress_bar.py[line:274] - INFO: epoch 001:  14802 / 102288 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103.3, ups=0.93, wpb=111.2, bsz=40, num_updates=14780, lr=4.45574e-05, gnorm=0.412, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44658
2023-01-05 09:30:36 - progress_bar.py[line:274] - INFO: epoch 001:  14812 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.8, ups=0.89, wpb=111.7, bsz=40, num_updates=14790, lr=4.45523e-05, gnorm=0.511, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44669
2023-01-05 09:30:47 - progress_bar.py[line:274] - INFO: epoch 001:  14822 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.8, ups=0.92, wpb=109, bsz=40, num_updates=14800, lr=4.45472e-05, gnorm=0.501, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44680
2023-01-05 09:30:59 - progress_bar.py[line:274] - INFO: epoch 001:  14832 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.89, wpb=110.2, bsz=40, num_updates=14810, lr=4.45421e-05, gnorm=0.521, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44692
2023-01-05 09:31:10 - progress_bar.py[line:274] - INFO: epoch 001:  14842 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=14820, lr=4.4537e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44703
2023-01-05 09:31:21 - progress_bar.py[line:274] - INFO: epoch 001:  14852 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.6, ups=0.92, wpb=109.2, bsz=40, num_updates=14830, lr=4.45319e-05, gnorm=0.526, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44714
2023-01-05 09:31:32 - progress_bar.py[line:274] - INFO: epoch 001:  14862 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=103.2, ups=0.93, wpb=110.6, bsz=40, num_updates=14840, lr=4.45268e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44725
2023-01-05 09:31:43 - progress_bar.py[line:274] - INFO: epoch 001:  14872 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.2, ups=0.9, wpb=109.9, bsz=40, num_updates=14850, lr=4.45217e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44736
2023-01-05 09:31:55 - progress_bar.py[line:274] - INFO: epoch 001:  14882 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.89, wpb=109.1, bsz=40, num_updates=14860, lr=4.45166e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44748
2023-01-05 09:32:06 - progress_bar.py[line:274] - INFO: epoch 001:  14892 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.3, ups=0.86, wpb=111, bsz=40, num_updates=14870, lr=4.45115e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=44760
2023-01-05 09:32:18 - progress_bar.py[line:274] - INFO: epoch 001:  14902 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.1, ups=0.87, wpb=109.2, bsz=40, num_updates=14880, lr=4.45065e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44771
2023-01-05 09:32:30 - progress_bar.py[line:274] - INFO: epoch 001:  14912 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.89, wpb=111.1, bsz=40, num_updates=14890, lr=4.45014e-05, gnorm=0.555, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44783
2023-01-05 09:32:41 - progress_bar.py[line:274] - INFO: epoch 001:  14922 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.1, ups=0.91, wpb=108.3, bsz=40, num_updates=14900, lr=4.44963e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44794
2023-01-05 09:32:52 - progress_bar.py[line:274] - INFO: epoch 001:  14932 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=105.2, ups=0.94, wpb=111.7, bsz=40, num_updates=14910, lr=4.44912e-05, gnorm=0.376, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44805
2023-01-05 09:33:03 - progress_bar.py[line:274] - INFO: epoch 001:  14942 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=14920, lr=4.44861e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44816
2023-01-05 09:33:14 - progress_bar.py[line:274] - INFO: epoch 001:  14952 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=14930, lr=4.4481e-05, gnorm=0.391, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44828
2023-01-05 09:33:26 - progress_bar.py[line:274] - INFO: epoch 001:  14962 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=94.7, ups=0.87, wpb=108.6, bsz=40, num_updates=14940, lr=4.44759e-05, gnorm=0.539, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44839
2023-01-05 09:33:37 - progress_bar.py[line:274] - INFO: epoch 001:  14972 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101, ups=0.91, wpb=111.4, bsz=40, num_updates=14950, lr=4.44708e-05, gnorm=0.416, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44851
2023-01-05 09:33:49 - progress_bar.py[line:274] - INFO: epoch 001:  14982 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.9, ups=0.91, wpb=109.1, bsz=40, num_updates=14960, lr=4.44657e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44862
2023-01-05 09:34:00 - progress_bar.py[line:274] - INFO: epoch 001:  14992 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=14970, lr=4.44606e-05, gnorm=0.428, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44873
2023-01-05 09:34:12 - progress_bar.py[line:274] - INFO: epoch 001:  15002 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.4, ups=0.93, wpb=110.1, bsz=40, num_updates=14980, lr=4.44555e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44884
2023-01-05 09:34:24 - progress_bar.py[line:274] - INFO: epoch 001:  15012 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=14990, lr=4.44504e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44896
2023-01-05 09:34:36 - progress_bar.py[line:274] - INFO: epoch 001:  15022 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98, ups=0.88, wpb=111.3, bsz=40, num_updates=15000, lr=4.44453e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44909
2023-01-05 09:34:49 - progress_bar.py[line:274] - INFO: epoch 001:  15032 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.8, ups=0.88, wpb=109.9, bsz=40, num_updates=15010, lr=4.44403e-05, gnorm=0.373, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44921
2023-01-05 09:35:01 - progress_bar.py[line:274] - INFO: epoch 001:  15042 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.3, ups=0.9, wpb=110.9, bsz=40, num_updates=15020, lr=4.44352e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44933
2023-01-05 09:35:13 - progress_bar.py[line:274] - INFO: epoch 001:  15052 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97, ups=0.89, wpb=108.5, bsz=40, num_updates=15030, lr=4.44301e-05, gnorm=0.672, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44946
2023-01-05 09:35:26 - progress_bar.py[line:274] - INFO: epoch 001:  15062 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96, ups=0.87, wpb=110.5, bsz=40, num_updates=15040, lr=4.4425e-05, gnorm=0.483, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44958
2023-01-05 09:35:38 - progress_bar.py[line:274] - INFO: epoch 001:  15072 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.91, wpb=109.7, bsz=40, num_updates=15050, lr=4.44199e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44971
2023-01-05 09:35:51 - progress_bar.py[line:274] - INFO: epoch 001:  15082 / 102288 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.6, ups=0.89, wpb=110, bsz=40, num_updates=15060, lr=4.44148e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44983
2023-01-05 09:36:04 - progress_bar.py[line:274] - INFO: epoch 001:  15092 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.2, ups=0.87, wpb=111.9, bsz=40, num_updates=15070, lr=4.44097e-05, gnorm=0.421, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44996
2023-01-05 09:36:16 - progress_bar.py[line:274] - INFO: epoch 001:  15102 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=15080, lr=4.44046e-05, gnorm=0.472, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45008
2023-01-05 09:36:29 - progress_bar.py[line:274] - INFO: epoch 001:  15112 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=15090, lr=4.43995e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45021
2023-01-05 09:36:41 - progress_bar.py[line:274] - INFO: epoch 001:  15122 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=15100, lr=4.43944e-05, gnorm=0.375, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45033
2023-01-05 09:36:54 - progress_bar.py[line:274] - INFO: epoch 001:  15132 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=15110, lr=4.43893e-05, gnorm=0.334, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45046
2023-01-05 09:37:06 - progress_bar.py[line:274] - INFO: epoch 001:  15142 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=15120, lr=4.43842e-05, gnorm=0.435, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=45058
2023-01-05 09:37:19 - progress_bar.py[line:274] - INFO: epoch 001:  15152 / 102288 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.5, ups=0.89, wpb=111, bsz=40, num_updates=15130, lr=4.43792e-05, gnorm=0.373, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45071
2023-01-05 09:37:31 - progress_bar.py[line:274] - INFO: epoch 001:  15162 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=15140, lr=4.43741e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45084
2023-01-05 09:37:44 - progress_bar.py[line:274] - INFO: epoch 001:  15172 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.8, ups=0.89, wpb=109.8, bsz=40, num_updates=15150, lr=4.4369e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45096
2023-01-05 09:37:56 - progress_bar.py[line:274] - INFO: epoch 001:  15182 / 102288 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.7, ups=0.88, wpb=108.9, bsz=40, num_updates=15160, lr=4.43639e-05, gnorm=0.483, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45109
2023-01-05 09:38:09 - progress_bar.py[line:274] - INFO: epoch 001:  15192 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=15170, lr=4.43588e-05, gnorm=0.494, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45121
2023-01-05 09:38:12 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 09:38:22 - progress_bar.py[line:274] - INFO: epoch 001:  15203 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=90.4, ups=0.82, wpb=110.5, bsz=40, num_updates=15180, lr=4.43537e-05, gnorm=0.471, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45135
2023-01-05 09:38:35 - progress_bar.py[line:274] - INFO: epoch 001:  15213 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=15190, lr=4.43486e-05, gnorm=0.399, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45147
2023-01-05 09:38:47 - progress_bar.py[line:274] - INFO: epoch 001:  15223 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=15200, lr=4.43435e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45160
2023-01-05 09:39:00 - progress_bar.py[line:274] - INFO: epoch 001:  15233 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.9, wpb=109.3, bsz=40, num_updates=15210, lr=4.43384e-05, gnorm=0.376, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=45172
2023-01-05 09:39:12 - progress_bar.py[line:274] - INFO: epoch 001:  15243 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=103.5, ups=0.94, wpb=110.4, bsz=40, num_updates=15220, lr=4.43333e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45184
2023-01-05 09:39:24 - progress_bar.py[line:274] - INFO: epoch 001:  15253 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.2, ups=0.91, wpb=109.6, bsz=40, num_updates=15230, lr=4.43282e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45197
2023-01-05 09:39:37 - progress_bar.py[line:274] - INFO: epoch 001:  15263 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.89, wpb=109.1, bsz=40, num_updates=15240, lr=4.43231e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45209
2023-01-05 09:39:49 - progress_bar.py[line:274] - INFO: epoch 001:  15273 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=15250, lr=4.43181e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45221
2023-01-05 09:40:01 - progress_bar.py[line:274] - INFO: epoch 001:  15283 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.92, wpb=109.6, bsz=40, num_updates=15260, lr=4.4313e-05, gnorm=0.517, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45233
2023-01-05 09:40:13 - progress_bar.py[line:274] - INFO: epoch 001:  15293 / 102288 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.6, ups=0.92, wpb=109.8, bsz=40, num_updates=15270, lr=4.43079e-05, gnorm=0.377, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45246
2023-01-05 09:40:26 - progress_bar.py[line:274] - INFO: epoch 001:  15303 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.3, ups=0.92, wpb=111.5, bsz=40, num_updates=15280, lr=4.43028e-05, gnorm=0.48, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45258
2023-01-05 09:40:38 - progress_bar.py[line:274] - INFO: epoch 001:  15313 / 102288 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.3, ups=0.87, wpb=112, bsz=40, num_updates=15290, lr=4.42977e-05, gnorm=0.511, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45271
2023-01-05 09:40:51 - progress_bar.py[line:274] - INFO: epoch 001:  15323 / 102288 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.2, ups=0.88, wpb=110.5, bsz=40, num_updates=15300, lr=4.42926e-05, gnorm=0.515, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45283
2023-01-05 09:41:03 - progress_bar.py[line:274] - INFO: epoch 001:  15333 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.6, ups=0.9, wpb=110, bsz=40, num_updates=15310, lr=4.42875e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45295
2023-01-05 09:41:16 - progress_bar.py[line:274] - INFO: epoch 001:  15343 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.1, ups=0.88, wpb=109.1, bsz=40, num_updates=15320, lr=4.42824e-05, gnorm=0.421, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45308
2023-01-05 09:41:28 - progress_bar.py[line:274] - INFO: epoch 001:  15353 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.5, ups=0.9, wpb=110.5, bsz=40, num_updates=15330, lr=4.42773e-05, gnorm=0.5, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45321
2023-01-05 09:41:41 - progress_bar.py[line:274] - INFO: epoch 001:  15363 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.8, ups=0.92, wpb=109.8, bsz=40, num_updates=15340, lr=4.42722e-05, gnorm=0.558, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45333
2023-01-05 09:41:54 - progress_bar.py[line:274] - INFO: epoch 001:  15373 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97, ups=0.88, wpb=110.3, bsz=40, num_updates=15350, lr=4.42671e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45346
2023-01-05 09:42:06 - progress_bar.py[line:274] - INFO: epoch 001:  15383 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=15360, lr=4.4262e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45359
2023-01-05 09:42:19 - progress_bar.py[line:274] - INFO: epoch 001:  15393 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=15370, lr=4.4257e-05, gnorm=0.472, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=45371
2023-01-05 09:42:31 - progress_bar.py[line:274] - INFO: epoch 001:  15403 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.7, ups=0.9, wpb=110.2, bsz=40, num_updates=15380, lr=4.42519e-05, gnorm=0.466, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45383
2023-01-05 09:42:43 - progress_bar.py[line:274] - INFO: epoch 001:  15413 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=15390, lr=4.42468e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45396
2023-01-05 09:42:55 - progress_bar.py[line:274] - INFO: epoch 001:  15423 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=104.9, ups=0.95, wpb=109.9, bsz=40, num_updates=15400, lr=4.42417e-05, gnorm=0.605, clip=10, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=45407
2023-01-05 09:43:07 - progress_bar.py[line:274] - INFO: epoch 001:  15433 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=15410, lr=4.42366e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45420
2023-01-05 09:43:20 - progress_bar.py[line:274] - INFO: epoch 001:  15443 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=15420, lr=4.42315e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45432
2023-01-05 09:43:32 - progress_bar.py[line:274] - INFO: epoch 001:  15453 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.1, ups=0.89, wpb=110.5, bsz=40, num_updates=15430, lr=4.42264e-05, gnorm=0.545, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=45445
2023-01-05 09:43:45 - progress_bar.py[line:274] - INFO: epoch 001:  15463 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.9, wpb=109, bsz=40, num_updates=15440, lr=4.42213e-05, gnorm=0.532, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45457
2023-01-05 09:43:58 - progress_bar.py[line:274] - INFO: epoch 001:  15473 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=15450, lr=4.42162e-05, gnorm=0.414, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45469
2023-01-05 09:44:10 - progress_bar.py[line:274] - INFO: epoch 001:  15483 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.89, wpb=110.7, bsz=40, num_updates=15460, lr=4.42111e-05, gnorm=0.456, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45482
2023-01-05 09:44:23 - progress_bar.py[line:274] - INFO: epoch 001:  15493 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.4, ups=0.89, wpb=109.3, bsz=40, num_updates=15470, lr=4.4206e-05, gnorm=0.432, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45495
2023-01-05 09:44:35 - progress_bar.py[line:274] - INFO: epoch 001:  15503 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101, ups=0.92, wpb=110, bsz=40, num_updates=15480, lr=4.42009e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45507
2023-01-05 09:44:47 - progress_bar.py[line:274] - INFO: epoch 001:  15513 / 102288 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.9, ups=0.89, wpb=112, bsz=40, num_updates=15490, lr=4.41959e-05, gnorm=0.593, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45520
2023-01-05 09:45:00 - progress_bar.py[line:274] - INFO: epoch 001:  15523 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=15500, lr=4.41908e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45532
2023-01-05 09:45:12 - progress_bar.py[line:274] - INFO: epoch 001:  15533 / 102288 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.4, ups=0.87, wpb=109.7, bsz=40, num_updates=15510, lr=4.41857e-05, gnorm=0.598, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45545
2023-01-05 09:45:25 - progress_bar.py[line:274] - INFO: epoch 001:  15543 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100, ups=0.91, wpb=109.9, bsz=40, num_updates=15520, lr=4.41806e-05, gnorm=0.503, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45557
2023-01-05 09:45:37 - progress_bar.py[line:274] - INFO: epoch 001:  15553 / 102288 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.9, ups=0.9, wpb=108.5, bsz=40, num_updates=15530, lr=4.41755e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45569
2023-01-05 09:45:49 - progress_bar.py[line:274] - INFO: epoch 001:  15563 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.3, ups=0.91, wpb=109.8, bsz=40, num_updates=15540, lr=4.41704e-05, gnorm=0.42, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45582
2023-01-05 09:46:02 - progress_bar.py[line:274] - INFO: epoch 001:  15573 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=15550, lr=4.41653e-05, gnorm=0.448, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45594
2023-01-05 09:46:17 - progress_bar.py[line:274] - INFO: epoch 001:  15583 / 102288 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=81.9, ups=0.75, wpb=108.8, bsz=40, num_updates=15560, lr=4.41602e-05, gnorm=0.434, clip=10, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=45609
2023-01-05 09:46:33 - progress_bar.py[line:274] - INFO: epoch 001:  15593 / 102288 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=72.9, ups=0.67, wpb=108.7, bsz=40, num_updates=15570, lr=4.41551e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=15, gb_free=10.8, ema_decay=0.9999, wall=45625
2023-01-05 09:46:45 - progress_bar.py[line:274] - INFO: epoch 001:  15603 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.8, ups=0.88, wpb=109.8, bsz=40, num_updates=15580, lr=4.415e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45638
2023-01-05 09:46:58 - progress_bar.py[line:274] - INFO: epoch 001:  15613 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.89, wpb=110.6, bsz=40, num_updates=15590, lr=4.41449e-05, gnorm=0.412, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45650
2023-01-05 09:47:10 - progress_bar.py[line:274] - INFO: epoch 001:  15623 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.89, wpb=109.3, bsz=40, num_updates=15600, lr=4.41398e-05, gnorm=0.501, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45663
2023-01-05 09:47:23 - progress_bar.py[line:274] - INFO: epoch 001:  15633 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.8, ups=0.88, wpb=109.8, bsz=40, num_updates=15610, lr=4.41347e-05, gnorm=0.549, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45675
2023-01-05 09:47:35 - progress_bar.py[line:274] - INFO: epoch 001:  15643 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.1, ups=0.91, wpb=111.6, bsz=40, num_updates=15620, lr=4.41297e-05, gnorm=0.501, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45687
2023-01-05 09:47:47 - progress_bar.py[line:274] - INFO: epoch 001:  15653 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.9, wpb=110.9, bsz=40, num_updates=15630, lr=4.41246e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45700
2023-01-05 09:47:59 - progress_bar.py[line:274] - INFO: epoch 001:  15663 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=15640, lr=4.41195e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45712
2023-01-05 09:48:12 - progress_bar.py[line:274] - INFO: epoch 001:  15673 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=15650, lr=4.41144e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45724
2023-01-05 09:48:24 - progress_bar.py[line:274] - INFO: epoch 001:  15683 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=15660, lr=4.41093e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=45736
2023-01-05 09:48:36 - progress_bar.py[line:274] - INFO: epoch 001:  15693 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101, ups=0.92, wpb=109.8, bsz=40, num_updates=15670, lr=4.41042e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45748
2023-01-05 09:48:48 - progress_bar.py[line:274] - INFO: epoch 001:  15703 / 102288 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=15680, lr=4.40991e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=45760
2023-01-05 09:49:00 - progress_bar.py[line:274] - INFO: epoch 001:  15713 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.8, ups=0.93, wpb=110.6, bsz=40, num_updates=15690, lr=4.4094e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45772
2023-01-05 09:49:12 - progress_bar.py[line:274] - INFO: epoch 001:  15723 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=15700, lr=4.40889e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45785
2023-01-05 09:49:24 - progress_bar.py[line:274] - INFO: epoch 001:  15733 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.5, ups=0.92, wpb=109.8, bsz=40, num_updates=15710, lr=4.40838e-05, gnorm=0.533, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45796
2023-01-05 09:49:36 - progress_bar.py[line:274] - INFO: epoch 001:  15743 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.89, wpb=110.3, bsz=40, num_updates=15720, lr=4.40787e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45809
2023-01-05 09:49:49 - progress_bar.py[line:274] - INFO: epoch 001:  15753 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=15730, lr=4.40736e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45821
2023-01-05 09:50:02 - progress_bar.py[line:274] - INFO: epoch 001:  15763 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=15740, lr=4.40686e-05, gnorm=0.465, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45834
2023-01-05 09:50:13 - progress_bar.py[line:274] - INFO: epoch 001:  15773 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=15750, lr=4.40635e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45846
2023-01-05 09:50:24 - progress_bar.py[line:274] - INFO: epoch 001:  15783 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=15760, lr=4.40584e-05, gnorm=0.433, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45857
2023-01-05 09:50:35 - progress_bar.py[line:274] - INFO: epoch 001:  15793 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.5, ups=0.93, wpb=109.5, bsz=40, num_updates=15770, lr=4.40533e-05, gnorm=0.442, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45868
2023-01-05 09:50:46 - progress_bar.py[line:274] - INFO: epoch 001:  15803 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.9, ups=0.92, wpb=111.2, bsz=40, num_updates=15780, lr=4.40482e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45880
2023-01-05 09:50:58 - progress_bar.py[line:274] - INFO: epoch 001:  15813 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=15790, lr=4.40431e-05, gnorm=0.551, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45891
2023-01-05 09:51:09 - progress_bar.py[line:274] - INFO: epoch 001:  15823 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=15800, lr=4.4038e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45902
2023-01-05 09:51:20 - progress_bar.py[line:274] - INFO: epoch 001:  15833 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.91, wpb=108.5, bsz=40, num_updates=15810, lr=4.40329e-05, gnorm=0.664, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45914
2023-01-05 09:51:32 - progress_bar.py[line:274] - INFO: epoch 001:  15843 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.6, ups=0.91, wpb=110.7, bsz=40, num_updates=15820, lr=4.40278e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45925
2023-01-05 09:51:43 - progress_bar.py[line:274] - INFO: epoch 001:  15853 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.89, wpb=109.8, bsz=40, num_updates=15830, lr=4.40227e-05, gnorm=0.593, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45936
2023-01-05 09:51:54 - progress_bar.py[line:274] - INFO: epoch 001:  15863 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.7, ups=0.92, wpb=110.8, bsz=40, num_updates=15840, lr=4.40176e-05, gnorm=0.438, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45948
2023-01-05 09:52:06 - progress_bar.py[line:274] - INFO: epoch 001:  15873 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=15850, lr=4.40125e-05, gnorm=0.545, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45959
2023-01-05 09:52:17 - progress_bar.py[line:274] - INFO: epoch 001:  15883 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.91, wpb=110.5, bsz=40, num_updates=15860, lr=4.40075e-05, gnorm=0.374, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45970
2023-01-05 09:52:28 - progress_bar.py[line:274] - INFO: epoch 001:  15893 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=15870, lr=4.40024e-05, gnorm=0.544, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45982
2023-01-05 09:52:40 - progress_bar.py[line:274] - INFO: epoch 001:  15903 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.1, ups=0.87, wpb=109.8, bsz=40, num_updates=15880, lr=4.39973e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=45994
2023-01-05 09:52:51 - progress_bar.py[line:274] - INFO: epoch 001:  15913 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.5, ups=0.92, wpb=110.6, bsz=40, num_updates=15890, lr=4.39922e-05, gnorm=0.373, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46005
2023-01-05 09:53:03 - progress_bar.py[line:274] - INFO: epoch 001:  15923 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.5, ups=0.89, wpb=108.2, bsz=40, num_updates=15900, lr=4.39871e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46016
2023-01-05 09:53:14 - progress_bar.py[line:274] - INFO: epoch 001:  15933 / 102288 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=15910, lr=4.3982e-05, gnorm=0.376, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46028
2023-01-05 09:53:26 - progress_bar.py[line:274] - INFO: epoch 001:  15943 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=15920, lr=4.39769e-05, gnorm=0.446, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46039
2023-01-05 09:53:37 - progress_bar.py[line:274] - INFO: epoch 001:  15953 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=15930, lr=4.39718e-05, gnorm=0.443, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=46050
2023-01-05 09:53:49 - progress_bar.py[line:274] - INFO: epoch 001:  15963 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.4, ups=0.89, wpb=109.5, bsz=40, num_updates=15940, lr=4.39667e-05, gnorm=0.538, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46062
2023-01-05 09:54:00 - progress_bar.py[line:274] - INFO: epoch 001:  15973 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=15950, lr=4.39616e-05, gnorm=0.513, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46073
2023-01-05 09:54:11 - progress_bar.py[line:274] - INFO: epoch 001:  15983 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.9, wpb=110.1, bsz=40, num_updates=15960, lr=4.39565e-05, gnorm=0.413, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=46085
2023-01-05 09:54:23 - progress_bar.py[line:274] - INFO: epoch 001:  15993 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=15970, lr=4.39514e-05, gnorm=0.539, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=46096
2023-01-05 09:54:34 - progress_bar.py[line:274] - INFO: epoch 001:  16003 / 102288 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.6, ups=0.88, wpb=111.9, bsz=40, num_updates=15980, lr=4.39464e-05, gnorm=0.446, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=46108
2023-01-05 09:54:46 - progress_bar.py[line:274] - INFO: epoch 001:  16013 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.2, ups=0.89, wpb=109, bsz=40, num_updates=15990, lr=4.39413e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=46119
2023-01-05 09:54:57 - progress_bar.py[line:274] - INFO: epoch 001:  16023 / 102288 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.7, ups=0.89, wpb=112.7, bsz=40, num_updates=16000, lr=4.39362e-05, gnorm=0.431, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46130
2023-01-05 09:54:57 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 09:54:58 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 09:54:58 - train.py[line:551] - INFO: load:0.93 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 09:57:31 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 09:57:31 - train.py[line:551] - INFO: load:0.95 valid_run:152.29 task_valid:148.12 collect_output:3.05
2023-01-05 09:59:59 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 10:00:00 - train.py[line:551] - INFO: load:0.98 valid_run:300.96 task_valid:291.21 collect_output:7.60
2023-01-05 10:02:32 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 10:02:32 - train.py[line:551] - INFO: load:1.00 valid_run:453.48 task_valid:434.40 collect_output:15.86
2023-01-05 10:05:02 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 10:05:02 - train.py[line:551] - INFO: load:1.03 valid_run:603.08 task_valid:579.54 collect_output:19.26
2023-01-05 10:07:34 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 10:07:34 - train.py[line:551] - INFO: load:1.05 valid_run:755.54 task_valid:727.01 collect_output:23.21
2023-01-05 10:10:06 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 10:10:06 - train.py[line:551] - INFO: load:1.08 valid_run:907.34 task_valid:872.49 collect_output:28.49
2023-01-05 10:12:40 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 10:12:40 - train.py[line:551] - INFO: load:1.10 valid_run:1060.97 task_valid:1018.57 collect_output:34.95
2023-01-05 10:15:12 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 10:15:12 - train.py[line:551] - INFO: load:1.13 valid_run:1212.71 task_valid:1159.96 collect_output:44.22
2023-01-05 10:17:41 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 10:17:41 - train.py[line:551] - INFO: load:1.15 valid_run:1362.45 task_valid:1304.62 collect_output:48.26
2023-01-05 10:20:10 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 10:20:10 - train.py[line:551] - INFO: load:1.18 valid_run:1511.11 task_valid:1447.65 collect_output:52.88
2023-01-05 10:22:40 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 10:22:40 - train.py[line:551] - INFO: load:1.20 valid_run:1660.99 task_valid:1592.54 collect_output:56.84
2023-01-05 10:25:11 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 10:25:11 - train.py[line:551] - INFO: load:1.23 valid_run:1811.41 task_valid:1737.63 collect_output:61.09
2023-01-05 10:27:41 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 10:27:41 - train.py[line:551] - INFO: load:1.26 valid_run:1961.61 task_valid:1879.37 collect_output:68.49
2023-01-05 10:30:12 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 10:30:12 - train.py[line:551] - INFO: load:1.28 valid_run:2112.27 task_valid:2024.72 collect_output:72.72
2023-01-05 10:32:42 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 10:32:42 - train.py[line:551] - INFO: load:1.31 valid_run:2262.43 task_valid:2171.05 collect_output:75.46
2023-01-05 10:35:12 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 10:35:12 - train.py[line:551] - INFO: load:1.33 valid_run:2412.98 task_valid:2315.33 collect_output:80.65
2023-01-05 10:37:45 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 10:37:45 - train.py[line:551] - INFO: load:1.36 valid_run:2564.94 task_valid:2460.94 collect_output:85.94
2023-01-05 10:40:15 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 10:40:15 - train.py[line:551] - INFO: load:1.39 valid_run:2715.80 task_valid:2607.99 collect_output:88.68
2023-01-05 10:42:44 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 10:42:44 - train.py[line:551] - INFO: load:1.41 valid_run:2864.44 task_valid:2749.69 collect_output:94.52
2023-01-05 10:45:15 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 10:45:15 - train.py[line:551] - INFO: load:1.44 valid_run:3015.36 task_valid:2895.02 collect_output:99.05
2023-01-05 10:47:48 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 10:47:48 - train.py[line:551] - INFO: load:1.46 valid_run:3167.79 task_valid:3039.69 collect_output:105.75
2023-01-05 10:50:17 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 10:50:17 - train.py[line:551] - INFO: load:1.49 valid_run:3317.54 task_valid:3184.46 collect_output:109.62
2023-01-05 10:52:49 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 10:52:49 - train.py[line:551] - INFO: load:1.52 valid_run:3469.10 task_valid:3330.73 collect_output:113.87
2023-01-05 10:55:21 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 10:55:21 - train.py[line:551] - INFO: load:1.54 valid_run:3621.05 task_valid:3477.40 collect_output:118.08

====================================================================================================
SGG eval:     R @ 50: 0.3515;     R @ 100: 0.4107;     R @ 500: 0.4501;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2712;    mR @ 100: 0.3194;    mR @ 500: 0.3727;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.4902) (covered in:0.8125) (covering:0.3714) (eating:0.5882) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3032) (lying on:0.1667) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7083) (playing:0.0000) (riding:0.6487) (says:0.0000) (sitting on:0.4734) (standing on:0.1083) (using:0.4500) (walking in:0.3333) (walking on:0.2523) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.3515;     R @ 100: 0.4107;     R @ 500: 0.4501;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2712;    mR @ 100: 0.3194;    mR @ 500: 0.3727;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.4902) (covered in:0.8125) (covering:0.3714) (eating:0.5882) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3032) (lying on:0.1667) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7083) (playing:0.0000) (riding:0.6487) (says:0.0000) (sitting on:0.4734) (standing on:0.1083) (using:0.4500) (walking in:0.3333) (walking on:0.2523) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-01-05 10:57:52 - train.py[line:487] - INFO: 0.4106761904761905
2023-01-05 10:57:52 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 10:57:53 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.367 | loss_v1 0 | loss_v2 0 | nll_loss 0.213 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.410676 | ppl 1.16 | vqa_score 0.3727 | wps 118.9 | wpb 89.9 | bsz 30 | num_updates 16000 | best_R@100 0.611199
2023-01-05 10:57:53 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 16000 updates
2023-01-05 10:57:53 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-05 10:58:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-05 11:00:42 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_BERT_v1_data/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt (epoch 1 @ 16000 updates, score 0.4106761904761905) (writing took 169.44770113751292 seconds)
2023-01-05 11:00:54 - progress_bar.py[line:274] - INFO: epoch 001:  16033 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=0.3, ups=0, wpb=109.6, bsz=40, num_updates=16010, lr=4.39311e-05, gnorm=0.49, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50086
2023-01-05 11:01:05 - progress_bar.py[line:274] - INFO: epoch 001:  16043 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.9, ups=0.91, wpb=109.6, bsz=40, num_updates=16020, lr=4.3926e-05, gnorm=0.467, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50098
2023-01-05 11:01:16 - progress_bar.py[line:274] - INFO: epoch 001:  16053 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.9, ups=0.93, wpb=110.8, bsz=40, num_updates=16030, lr=4.39209e-05, gnorm=0.361, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50109
2023-01-05 11:01:28 - progress_bar.py[line:274] - INFO: epoch 001:  16063 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=16040, lr=4.39158e-05, gnorm=0.543, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50121
2023-01-05 11:01:39 - progress_bar.py[line:274] - INFO: epoch 001:  16073 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.91, wpb=108.9, bsz=40, num_updates=16050, lr=4.39107e-05, gnorm=0.571, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50132
2023-01-05 11:01:50 - progress_bar.py[line:274] - INFO: epoch 001:  16083 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.1, ups=0.92, wpb=109.3, bsz=40, num_updates=16060, lr=4.39056e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50143
2023-01-05 11:02:02 - progress_bar.py[line:274] - INFO: epoch 001:  16093 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.1, ups=0.87, wpb=110.9, bsz=40, num_updates=16070, lr=4.39005e-05, gnorm=0.438, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50155
2023-01-05 11:02:14 - progress_bar.py[line:274] - INFO: epoch 001:  16103 / 102288 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.2, ups=0.9, wpb=111.8, bsz=40, num_updates=16080, lr=4.38954e-05, gnorm=0.495, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50167
2023-01-05 11:02:25 - progress_bar.py[line:274] - INFO: epoch 001:  16113 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=16090, lr=4.38903e-05, gnorm=0.533, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50179
2023-01-05 11:02:37 - progress_bar.py[line:274] - INFO: epoch 001:  16123 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.92, wpb=108.8, bsz=40, num_updates=16100, lr=4.38853e-05, gnorm=0.449, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50190
2023-01-05 11:02:48 - progress_bar.py[line:274] - INFO: epoch 001:  16133 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.89, wpb=109.9, bsz=40, num_updates=16110, lr=4.38802e-05, gnorm=0.625, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50202
2023-01-05 11:03:00 - progress_bar.py[line:274] - INFO: epoch 001:  16143 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=16120, lr=4.38751e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50213
2023-01-05 11:03:11 - progress_bar.py[line:274] - INFO: epoch 001:  16153 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=16130, lr=4.387e-05, gnorm=0.394, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50225
2023-01-05 11:03:23 - progress_bar.py[line:274] - INFO: epoch 001:  16163 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.9, wpb=108.5, bsz=40, num_updates=16140, lr=4.38649e-05, gnorm=0.579, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50236
2023-01-05 11:03:34 - progress_bar.py[line:274] - INFO: epoch 001:  16173 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=16150, lr=4.38598e-05, gnorm=0.609, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50247
2023-01-05 11:03:46 - progress_bar.py[line:274] - INFO: epoch 001:  16183 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=16160, lr=4.38547e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50259
2023-01-05 11:03:57 - progress_bar.py[line:274] - INFO: epoch 001:  16193 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=16170, lr=4.38496e-05, gnorm=0.496, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50270
2023-01-05 11:04:08 - progress_bar.py[line:274] - INFO: epoch 001:  16203 / 102288 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.6, ups=0.92, wpb=110.7, bsz=40, num_updates=16180, lr=4.38445e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50281
2023-01-05 11:04:20 - progress_bar.py[line:274] - INFO: epoch 001:  16213 / 102288 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=16190, lr=4.38394e-05, gnorm=0.333, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50293
2023-01-05 11:04:31 - progress_bar.py[line:274] - INFO: epoch 001:  16223 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=16200, lr=4.38343e-05, gnorm=0.406, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50305
2023-01-05 11:04:43 - progress_bar.py[line:274] - INFO: epoch 001:  16233 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100, ups=0.9, wpb=110.5, bsz=40, num_updates=16210, lr=4.38292e-05, gnorm=0.462, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=50316
2023-01-05 11:04:54 - progress_bar.py[line:274] - INFO: epoch 001:  16243 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=16220, lr=4.38241e-05, gnorm=0.413, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50328
2023-01-05 11:05:06 - progress_bar.py[line:274] - INFO: epoch 001:  16253 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=16230, lr=4.38191e-05, gnorm=0.389, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50339
2023-01-05 11:05:18 - progress_bar.py[line:274] - INFO: epoch 001:  16263 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.88, wpb=109.6, bsz=40, num_updates=16240, lr=4.3814e-05, gnorm=0.507, clip=10, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50351
2023-01-05 11:05:29 - progress_bar.py[line:274] - INFO: epoch 001:  16273 / 102288 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.89, wpb=110.3, bsz=40, num_updates=16250, lr=4.38089e-05, gnorm=0.361, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50362
2023-01-05 11:05:41 - progress_bar.py[line:274] - INFO: epoch 001:  16283 / 102288 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=16260, lr=4.38038e-05, gnorm=0.389, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50374
2023-01-05 11:05:44 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-01-05 11:05:53 - progress_bar.py[line:274] - INFO: epoch 001:  16294 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=92.9, ups=0.83, wpb=112.2, bsz=40, num_updates=16270, lr=4.37987e-05, gnorm=0.474, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=50386
2023-01-05 11:06:05 - progress_bar.py[line:274] - INFO: epoch 001:  16304 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.91, wpb=110, bsz=40, num_updates=16280, lr=4.37936e-05, gnorm=0.399, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50398
2023-01-05 11:06:16 - progress_bar.py[line:274] - INFO: epoch 001:  16314 / 102288 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.8, ups=0.92, wpb=111.2, bsz=40, num_updates=16290, lr=4.37885e-05, gnorm=0.384, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50409
2023-01-05 11:06:28 - progress_bar.py[line:274] - INFO: epoch 001:  16324 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.8, ups=0.88, wpb=110.2, bsz=40, num_updates=16300, lr=4.37834e-05, gnorm=0.407, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50421
2023-01-05 11:06:39 - progress_bar.py[line:274] - INFO: epoch 001:  16334 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.89, wpb=108.3, bsz=40, num_updates=16310, lr=4.37783e-05, gnorm=0.611, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50432
2023-01-05 11:06:50 - progress_bar.py[line:274] - INFO: epoch 001:  16344 / 102288 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.7, ups=0.93, wpb=109.5, bsz=40, num_updates=16320, lr=4.37732e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50444
2023-01-05 11:07:02 - progress_bar.py[line:274] - INFO: epoch 001:  16354 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=16330, lr=4.37681e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50455
2023-01-05 11:07:14 - progress_bar.py[line:274] - INFO: epoch 001:  16364 / 102288 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=16340, lr=4.3763e-05, gnorm=0.462, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50467
2023-01-05 11:07:25 - progress_bar.py[line:274] - INFO: epoch 001:  16374 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=16350, lr=4.3758e-05, gnorm=0.523, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50478
2023-01-05 11:07:37 - progress_bar.py[line:274] - INFO: epoch 001:  16384 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=16360, lr=4.37529e-05, gnorm=0.559, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50490
2023-01-05 11:07:48 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 11:07:49 - progress_bar.py[line:274] - INFO: epoch 001:  16395 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=91.2, ups=0.82, wpb=111, bsz=40, num_updates=16370, lr=4.37478e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=50502
2023-01-05 11:08:01 - progress_bar.py[line:274] - INFO: epoch 001:  16405 / 102288 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=102.3, ups=0.91, wpb=111.9, bsz=40, num_updates=16380, lr=4.37427e-05, gnorm=0.342, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50514
2023-01-05 11:08:13 - progress_bar.py[line:274] - INFO: epoch 001:  16415 / 102288 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=94.7, ups=0.86, wpb=109.5, bsz=40, num_updates=16390, lr=4.37376e-05, gnorm=0.31, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=50526
2023-01-05 11:08:24 - progress_bar.py[line:274] - INFO: epoch 001:  16425 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=16400, lr=4.37325e-05, gnorm=0.431, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50537
2023-01-05 11:08:36 - progress_bar.py[line:274] - INFO: epoch 001:  16435 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.2, ups=0.87, wpb=110.7, bsz=40, num_updates=16410, lr=4.37274e-05, gnorm=0.379, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50549
2023-01-05 11:08:47 - progress_bar.py[line:274] - INFO: epoch 001:  16445 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=16420, lr=4.37223e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50561
2023-01-05 11:08:59 - progress_bar.py[line:274] - INFO: epoch 001:  16455 / 102288 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.8, ups=0.87, wpb=111.1, bsz=40, num_updates=16430, lr=4.37172e-05, gnorm=0.506, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50573
2023-01-05 11:09:11 - progress_bar.py[line:274] - INFO: epoch 001:  16465 / 102288 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.7, ups=0.88, wpb=110, bsz=40, num_updates=16440, lr=4.37121e-05, gnorm=0.428, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50584
2023-01-05 11:09:23 - progress_bar.py[line:274] - INFO: epoch 001:  16475 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=94.9, ups=0.88, wpb=108.1, bsz=40, num_updates=16450, lr=4.3707e-05, gnorm=0.505, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50596
2023-01-05 11:09:34 - progress_bar.py[line:274] - INFO: epoch 001:  16485 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.9, ups=0.93, wpb=110.5, bsz=40, num_updates=16460, lr=4.37019e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50607
2023-01-05 11:09:45 - progress_bar.py[line:274] - INFO: epoch 001:  16495 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101, ups=0.91, wpb=110.5, bsz=40, num_updates=16470, lr=4.36969e-05, gnorm=0.53, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50618
2023-01-05 11:09:57 - progress_bar.py[line:274] - INFO: epoch 001:  16505 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=16480, lr=4.36918e-05, gnorm=0.416, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50630
2023-01-05 11:10:08 - progress_bar.py[line:274] - INFO: epoch 001:  16515 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=16490, lr=4.36867e-05, gnorm=0.492, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50642
2023-01-05 11:10:20 - progress_bar.py[line:274] - INFO: epoch 001:  16525 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=16500, lr=4.36816e-05, gnorm=0.685, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50653
2023-01-05 11:10:32 - progress_bar.py[line:274] - INFO: epoch 001:  16535 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.88, wpb=111.6, bsz=40, num_updates=16510, lr=4.36765e-05, gnorm=0.501, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50665
2023-01-05 11:10:43 - progress_bar.py[line:274] - INFO: epoch 001:  16545 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.6, ups=0.94, wpb=108.9, bsz=40, num_updates=16520, lr=4.36714e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50676
2023-01-05 11:10:54 - progress_bar.py[line:274] - INFO: epoch 001:  16555 / 102288 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=16530, lr=4.36663e-05, gnorm=0.376, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50687
2023-01-05 11:11:06 - progress_bar.py[line:274] - INFO: epoch 001:  16565 / 102288 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.4, ups=0.88, wpb=110, bsz=40, num_updates=16540, lr=4.36612e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50699
2023-01-05 11:11:18 - progress_bar.py[line:274] - INFO: epoch 001:  16575 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.88, wpb=109.9, bsz=40, num_updates=16550, lr=4.36561e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=50711
2023-01-05 11:11:29 - progress_bar.py[line:274] - INFO: epoch 001:  16585 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.1, ups=0.9, wpb=109.9, bsz=40, num_updates=16560, lr=4.3651e-05, gnorm=0.369, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50722
2023-01-05 11:11:41 - progress_bar.py[line:274] - INFO: epoch 001:  16595 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.9, wpb=110.2, bsz=40, num_updates=16570, lr=4.36459e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50734
2023-01-05 11:11:52 - progress_bar.py[line:274] - INFO: epoch 001:  16605 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.1, ups=0.89, wpb=110.2, bsz=40, num_updates=16580, lr=4.36408e-05, gnorm=0.6, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50745
2023-01-05 11:12:04 - progress_bar.py[line:274] - INFO: epoch 001:  16615 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=16590, lr=4.36358e-05, gnorm=0.412, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50757
2023-01-05 11:12:15 - progress_bar.py[line:274] - INFO: epoch 001:  16625 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=16600, lr=4.36307e-05, gnorm=0.488, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50768
2023-01-05 11:12:27 - progress_bar.py[line:274] - INFO: epoch 001:  16635 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96, ups=0.87, wpb=110.3, bsz=40, num_updates=16610, lr=4.36256e-05, gnorm=0.395, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50780
2023-01-05 11:12:38 - progress_bar.py[line:274] - INFO: epoch 001:  16645 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.2, ups=0.92, wpb=110.6, bsz=40, num_updates=16620, lr=4.36205e-05, gnorm=0.388, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50791
2023-01-05 11:12:50 - progress_bar.py[line:274] - INFO: epoch 001:  16655 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=16630, lr=4.36154e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50803
2023-01-05 11:13:01 - progress_bar.py[line:274] - INFO: epoch 001:  16665 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103, ups=0.93, wpb=110.5, bsz=40, num_updates=16640, lr=4.36103e-05, gnorm=0.5, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50814
2023-01-05 11:13:12 - progress_bar.py[line:274] - INFO: epoch 001:  16675 / 102288 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=16650, lr=4.36052e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50826
2023-01-05 11:13:24 - progress_bar.py[line:274] - INFO: epoch 001:  16685 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=16660, lr=4.36001e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50837
2023-01-05 11:13:36 - progress_bar.py[line:274] - INFO: epoch 001:  16695 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=16670, lr=4.3595e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50849
2023-01-05 11:13:47 - progress_bar.py[line:274] - INFO: epoch 001:  16705 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.2, ups=0.93, wpb=110.2, bsz=40, num_updates=16680, lr=4.35899e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50860
2023-01-05 11:13:59 - progress_bar.py[line:274] - INFO: epoch 001:  16715 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.9, ups=0.87, wpb=111.1, bsz=40, num_updates=16690, lr=4.35848e-05, gnorm=0.35, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50872
2023-01-05 11:14:10 - progress_bar.py[line:274] - INFO: epoch 001:  16725 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=16700, lr=4.35797e-05, gnorm=0.399, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50884
2023-01-05 11:14:22 - progress_bar.py[line:274] - INFO: epoch 001:  16735 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.7, ups=0.93, wpb=109.9, bsz=40, num_updates=16710, lr=4.35747e-05, gnorm=0.529, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50895
2023-01-05 11:14:33 - progress_bar.py[line:274] - INFO: epoch 001:  16745 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.5, ups=0.9, wpb=110.2, bsz=40, num_updates=16720, lr=4.35696e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50906
2023-01-05 11:14:45 - progress_bar.py[line:274] - INFO: epoch 001:  16755 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.7, ups=0.9, wpb=109.1, bsz=40, num_updates=16730, lr=4.35645e-05, gnorm=0.494, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50918
2023-01-05 11:14:56 - progress_bar.py[line:274] - INFO: epoch 001:  16765 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.1, ups=0.91, wpb=109.4, bsz=40, num_updates=16740, lr=4.35594e-05, gnorm=0.596, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50929
2023-01-05 11:15:07 - progress_bar.py[line:274] - INFO: epoch 001:  16775 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.7, ups=0.94, wpb=109.4, bsz=40, num_updates=16750, lr=4.35543e-05, gnorm=0.351, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50940
2023-01-05 11:15:18 - progress_bar.py[line:274] - INFO: epoch 001:  16785 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102, ups=0.93, wpb=109.9, bsz=40, num_updates=16760, lr=4.35492e-05, gnorm=0.37, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50951
2023-01-05 11:15:30 - progress_bar.py[line:274] - INFO: epoch 001:  16795 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=16770, lr=4.35441e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50963
2023-01-05 11:15:41 - progress_bar.py[line:274] - INFO: epoch 001:  16805 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=16780, lr=4.3539e-05, gnorm=0.525, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50974
2023-01-05 11:15:53 - progress_bar.py[line:274] - INFO: epoch 001:  16815 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=16790, lr=4.35339e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50986
2023-01-05 11:16:04 - progress_bar.py[line:274] - INFO: epoch 001:  16825 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.88, wpb=110.3, bsz=40, num_updates=16800, lr=4.35288e-05, gnorm=0.576, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50998
2023-01-05 11:16:16 - progress_bar.py[line:274] - INFO: epoch 001:  16835 / 102288 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=16810, lr=4.35237e-05, gnorm=0.327, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51009
2023-01-05 11:16:27 - progress_bar.py[line:274] - INFO: epoch 001:  16845 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=16820, lr=4.35186e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51021
2023-01-05 11:16:39 - progress_bar.py[line:274] - INFO: epoch 001:  16855 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=16830, lr=4.35135e-05, gnorm=0.413, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51032
2023-01-05 11:16:51 - progress_bar.py[line:274] - INFO: epoch 001:  16865 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=16840, lr=4.35085e-05, gnorm=0.47, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51044
2023-01-05 11:17:02 - progress_bar.py[line:274] - INFO: epoch 001:  16875 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=16850, lr=4.35034e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51055
2023-01-05 11:17:14 - progress_bar.py[line:274] - INFO: epoch 001:  16885 / 102288 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=16860, lr=4.34983e-05, gnorm=0.381, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51067
2023-01-05 11:17:25 - progress_bar.py[line:274] - INFO: epoch 001:  16895 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=16870, lr=4.34932e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51078
2023-01-05 11:17:37 - progress_bar.py[line:274] - INFO: epoch 001:  16905 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.5, ups=0.9, wpb=110.9, bsz=40, num_updates=16880, lr=4.34881e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51090
2023-01-05 11:17:48 - progress_bar.py[line:274] - INFO: epoch 001:  16915 / 102288 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=16890, lr=4.3483e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51101
2023-01-05 11:17:59 - progress_bar.py[line:274] - INFO: epoch 001:  16925 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.92, wpb=108.2, bsz=40, num_updates=16900, lr=4.34779e-05, gnorm=0.551, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51113
2023-01-05 11:18:11 - progress_bar.py[line:274] - INFO: epoch 001:  16935 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=16910, lr=4.34728e-05, gnorm=0.425, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51124
2023-01-05 11:18:22 - progress_bar.py[line:274] - INFO: epoch 001:  16945 / 102288 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=105.3, ups=0.96, wpb=109.7, bsz=40, num_updates=16920, lr=4.34677e-05, gnorm=0.398, clip=0, loss_scale=1024, train_wall=10, gb_free=10.5, ema_decay=0.9999, wall=51135
2023-01-05 11:18:33 - progress_bar.py[line:274] - INFO: epoch 001:  16955 / 102288 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=105.9, ups=0.95, wpb=111.1, bsz=40, num_updates=16930, lr=4.34626e-05, gnorm=0.427, clip=0, loss_scale=1024, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=51146
2023-01-05 11:18:44 - progress_bar.py[line:274] - INFO: epoch 001:  16965 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.4, ups=0.9, wpb=110.2, bsz=40, num_updates=16940, lr=4.34575e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51157
2023-01-05 11:18:55 - progress_bar.py[line:274] - INFO: epoch 001:  16975 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=16950, lr=4.34524e-05, gnorm=0.372, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=51168
2023-01-05 11:19:07 - progress_bar.py[line:274] - INFO: epoch 001:  16985 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.88, wpb=110.4, bsz=40, num_updates=16960, lr=4.34474e-05, gnorm=0.518, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51180
2023-01-05 11:19:19 - progress_bar.py[line:274] - INFO: epoch 001:  16995 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.89, wpb=109.3, bsz=40, num_updates=16970, lr=4.34423e-05, gnorm=0.438, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51192
2023-01-05 11:19:30 - progress_bar.py[line:274] - INFO: epoch 001:  17005 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=16980, lr=4.34372e-05, gnorm=0.541, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51203
2023-01-05 11:19:42 - progress_bar.py[line:274] - INFO: epoch 001:  17015 / 102288 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.7, ups=0.89, wpb=107.4, bsz=40, num_updates=16990, lr=4.34321e-05, gnorm=0.547, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51215
2023-01-05 11:19:53 - progress_bar.py[line:274] - INFO: epoch 001:  17025 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.3, ups=0.88, wpb=108.5, bsz=40, num_updates=17000, lr=4.3427e-05, gnorm=0.46, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51227
2023-01-05 11:20:05 - progress_bar.py[line:274] - INFO: epoch 001:  17035 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.9, wpb=110.8, bsz=40, num_updates=17010, lr=4.34219e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51238
2023-01-05 11:20:16 - progress_bar.py[line:274] - INFO: epoch 001:  17045 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=17020, lr=4.34168e-05, gnorm=0.452, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51250
2023-01-05 11:20:28 - progress_bar.py[line:274] - INFO: epoch 001:  17055 / 102288 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=17030, lr=4.34117e-05, gnorm=0.459, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51261
2023-01-05 11:20:39 - progress_bar.py[line:274] - INFO: epoch 001:  17065 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.89, wpb=110.7, bsz=40, num_updates=17040, lr=4.34066e-05, gnorm=0.354, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51272
2023-01-05 11:20:50 - progress_bar.py[line:274] - INFO: epoch 001:  17075 / 102288 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.9, wpb=108.8, bsz=40, num_updates=17050, lr=4.34015e-05, gnorm=0.452, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51284
2023-01-05 11:21:02 - progress_bar.py[line:274] - INFO: epoch 001:  17085 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=17060, lr=4.33964e-05, gnorm=0.432, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51295
2023-01-05 11:21:13 - progress_bar.py[line:274] - INFO: epoch 001:  17095 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=17070, lr=4.33913e-05, gnorm=0.362, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=51307
2023-01-05 11:21:25 - progress_bar.py[line:274] - INFO: epoch 001:  17105 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=17080, lr=4.33863e-05, gnorm=0.382, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=51318
2023-01-05 11:21:36 - progress_bar.py[line:274] - INFO: epoch 001:  17115 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=17090, lr=4.33812e-05, gnorm=0.403, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51330
2023-01-05 11:21:48 - progress_bar.py[line:274] - INFO: epoch 001:  17125 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.5, ups=0.91, wpb=110.9, bsz=40, num_updates=17100, lr=4.33761e-05, gnorm=0.43, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51341
2023-01-05 11:21:59 - progress_bar.py[line:274] - INFO: epoch 001:  17135 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=17110, lr=4.3371e-05, gnorm=0.494, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51352
2023-01-05 11:22:10 - progress_bar.py[line:274] - INFO: epoch 001:  17145 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.9, ups=0.9, wpb=109.6, bsz=40, num_updates=17120, lr=4.33659e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51363
2023-01-05 11:22:21 - progress_bar.py[line:274] - INFO: epoch 001:  17155 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.3, ups=0.91, wpb=109.6, bsz=40, num_updates=17130, lr=4.33608e-05, gnorm=0.562, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51374
2023-01-05 11:22:32 - progress_bar.py[line:274] - INFO: epoch 001:  17165 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.9, ups=0.9, wpb=109.7, bsz=40, num_updates=17140, lr=4.33557e-05, gnorm=0.579, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=51386
2023-01-05 11:22:44 - progress_bar.py[line:274] - INFO: epoch 001:  17175 / 102288 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=17150, lr=4.33506e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51397
2023-01-05 11:22:55 - progress_bar.py[line:274] - INFO: epoch 001:  17185 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=17160, lr=4.33455e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51409
2023-01-05 11:23:07 - progress_bar.py[line:274] - INFO: epoch 001:  17195 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=17170, lr=4.33404e-05, gnorm=0.51, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51420
2023-01-05 11:23:18 - progress_bar.py[line:274] - INFO: epoch 001:  17205 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=17180, lr=4.33353e-05, gnorm=0.545, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51432
2023-01-05 11:23:30 - progress_bar.py[line:274] - INFO: epoch 001:  17215 / 102288 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.2, ups=0.91, wpb=109.6, bsz=40, num_updates=17190, lr=4.33302e-05, gnorm=0.502, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51443
2023-01-05 11:23:41 - progress_bar.py[line:274] - INFO: epoch 001:  17225 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=17200, lr=4.33252e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=51454
2023-01-05 11:23:53 - progress_bar.py[line:274] - INFO: epoch 001:  17235 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.9, ups=0.88, wpb=110.3, bsz=40, num_updates=17210, lr=4.33201e-05, gnorm=0.494, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51466
2023-01-05 11:24:04 - progress_bar.py[line:274] - INFO: epoch 001:  17245 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.6, ups=0.88, wpb=111.1, bsz=40, num_updates=17220, lr=4.3315e-05, gnorm=0.47, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51478
2023-01-05 11:24:16 - progress_bar.py[line:274] - INFO: epoch 001:  17255 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.5, ups=0.9, wpb=110.7, bsz=40, num_updates=17230, lr=4.33099e-05, gnorm=0.471, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51489
2023-01-05 11:24:27 - progress_bar.py[line:274] - INFO: epoch 001:  17265 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.4, ups=0.9, wpb=111.2, bsz=40, num_updates=17240, lr=4.33048e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51500
2023-01-05 11:24:39 - progress_bar.py[line:274] - INFO: epoch 001:  17275 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.9, ups=0.87, wpb=110.6, bsz=40, num_updates=17250, lr=4.32997e-05, gnorm=0.532, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51512
2023-01-05 11:24:50 - progress_bar.py[line:274] - INFO: epoch 001:  17285 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.9, ups=0.89, wpb=110.6, bsz=40, num_updates=17260, lr=4.32946e-05, gnorm=0.389, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51523
2023-01-05 11:25:01 - progress_bar.py[line:274] - INFO: epoch 001:  17295 / 102288 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=17270, lr=4.32895e-05, gnorm=0.36, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51535
2023-01-05 11:25:13 - progress_bar.py[line:274] - INFO: epoch 001:  17305 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.88, wpb=109.6, bsz=40, num_updates=17280, lr=4.32844e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51546
2023-01-05 11:25:24 - progress_bar.py[line:274] - INFO: epoch 001:  17315 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=17290, lr=4.32793e-05, gnorm=0.476, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51558
2023-01-05 11:25:36 - progress_bar.py[line:274] - INFO: epoch 001:  17325 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=17300, lr=4.32742e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51569
2023-01-05 11:25:40 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 11:25:48 - progress_bar.py[line:274] - INFO: epoch 001:  17336 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=93.4, ups=0.85, wpb=110.5, bsz=40, num_updates=17310, lr=4.32691e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=51581
2023-01-05 11:25:59 - progress_bar.py[line:274] - INFO: epoch 001:  17346 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.91, wpb=109.9, bsz=40, num_updates=17320, lr=4.32641e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51592
2023-01-05 11:26:11 - progress_bar.py[line:274] - INFO: epoch 001:  17356 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=17330, lr=4.3259e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51604
2023-01-05 11:26:22 - progress_bar.py[line:274] - INFO: epoch 001:  17366 / 102288 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=17340, lr=4.32539e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51615
2023-01-05 11:26:33 - progress_bar.py[line:274] - INFO: epoch 001:  17376 / 102288 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=17350, lr=4.32488e-05, gnorm=0.388, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51627
2023-01-05 11:26:45 - progress_bar.py[line:274] - INFO: epoch 001:  17386 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.1, ups=0.9, wpb=112.1, bsz=40, num_updates=17360, lr=4.32437e-05, gnorm=0.452, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51638
2023-01-05 11:26:56 - progress_bar.py[line:274] - INFO: epoch 001:  17396 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.8, ups=0.93, wpb=110.4, bsz=40, num_updates=17370, lr=4.32386e-05, gnorm=0.388, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51649
2023-01-05 11:27:07 - progress_bar.py[line:274] - INFO: epoch 001:  17406 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=17380, lr=4.32335e-05, gnorm=0.294, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51660
2023-01-05 11:27:18 - progress_bar.py[line:274] - INFO: epoch 001:  17416 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.6, ups=0.89, wpb=109.7, bsz=40, num_updates=17390, lr=4.32284e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51672
2023-01-05 11:27:30 - progress_bar.py[line:274] - INFO: epoch 001:  17426 / 102288 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.6, ups=0.9, wpb=111.8, bsz=40, num_updates=17400, lr=4.32233e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51683
2023-01-05 11:27:41 - progress_bar.py[line:274] - INFO: epoch 001:  17436 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.9, wpb=109, bsz=40, num_updates=17410, lr=4.32182e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51694
2023-01-05 11:27:52 - progress_bar.py[line:274] - INFO: epoch 001:  17446 / 102288 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.9, ups=0.91, wpb=109.3, bsz=40, num_updates=17420, lr=4.32131e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51705
2023-01-05 11:28:04 - progress_bar.py[line:274] - INFO: epoch 001:  17456 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=94.9, ups=0.87, wpb=109, bsz=40, num_updates=17430, lr=4.3208e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51717
2023-01-05 11:28:15 - progress_bar.py[line:274] - INFO: epoch 001:  17466 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.9, ups=0.9, wpb=108.3, bsz=40, num_updates=17440, lr=4.32029e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51728
2023-01-05 11:28:27 - progress_bar.py[line:274] - INFO: epoch 001:  17476 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=17450, lr=4.31979e-05, gnorm=0.377, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51740
2023-01-05 11:28:38 - progress_bar.py[line:274] - INFO: epoch 001:  17486 / 102288 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.91, wpb=111, bsz=40, num_updates=17460, lr=4.31928e-05, gnorm=0.451, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51751
2023-01-05 11:28:49 - progress_bar.py[line:274] - INFO: epoch 001:  17496 / 102288 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.8, ups=0.91, wpb=111.4, bsz=40, num_updates=17470, lr=4.31877e-05, gnorm=0.432, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51762
2023-01-05 11:29:00 - progress_bar.py[line:274] - INFO: epoch 001:  17506 / 102288 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.9, wpb=109.5, bsz=40, num_updates=17480, lr=4.31826e-05, gnorm=0.443, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51774
2023-01-05 11:29:12 - progress_bar.py[line:274] - INFO: epoch 001:  17516 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=17490, lr=4.31775e-05, gnorm=0.511, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51785
2023-01-05 11:29:23 - progress_bar.py[line:274] - INFO: epoch 001:  17526 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=17500, lr=4.31724e-05, gnorm=0.41, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51796
2023-01-05 11:29:35 - progress_bar.py[line:274] - INFO: epoch 001:  17536 / 102288 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.6, ups=0.87, wpb=110.1, bsz=40, num_updates=17510, lr=4.31673e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51808
2023-01-05 11:29:46 - progress_bar.py[line:274] - INFO: epoch 001:  17546 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.3, ups=0.95, wpb=108, bsz=40, num_updates=17520, lr=4.31622e-05, gnorm=0.394, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51819
2023-01-05 11:29:57 - progress_bar.py[line:274] - INFO: epoch 001:  17556 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.3, ups=0.91, wpb=110, bsz=40, num_updates=17530, lr=4.31571e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51830
2023-01-05 11:30:08 - progress_bar.py[line:274] - INFO: epoch 001:  17566 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.89, wpb=108.7, bsz=40, num_updates=17540, lr=4.3152e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51842
2023-01-05 11:30:19 - progress_bar.py[line:274] - INFO: epoch 001:  17576 / 102288 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.9, ups=0.93, wpb=111.2, bsz=40, num_updates=17550, lr=4.31469e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51853
2023-01-05 11:30:31 - progress_bar.py[line:274] - INFO: epoch 001:  17586 / 102288 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.88, wpb=110.9, bsz=40, num_updates=17560, lr=4.31418e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51864
2023-01-05 11:30:42 - progress_bar.py[line:274] - INFO: epoch 001:  17596 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.2, ups=0.9, wpb=109.2, bsz=40, num_updates=17570, lr=4.31368e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51875
2023-01-05 11:30:54 - progress_bar.py[line:274] - INFO: epoch 001:  17606 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=17580, lr=4.31317e-05, gnorm=0.363, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51887
2023-01-05 11:31:05 - progress_bar.py[line:274] - INFO: epoch 001:  17616 / 102288 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=17590, lr=4.31266e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51898
2023-01-05 11:31:16 - progress_bar.py[line:274] - INFO: epoch 001:  17626 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=17600, lr=4.31215e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51910
2023-01-05 11:31:28 - progress_bar.py[line:274] - INFO: epoch 001:  17636 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.5, ups=0.91, wpb=108.9, bsz=40, num_updates=17610, lr=4.31164e-05, gnorm=0.367, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51921
2023-01-05 11:31:39 - progress_bar.py[line:274] - INFO: epoch 001:  17646 / 102288 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96, ups=0.88, wpb=109.5, bsz=40, num_updates=17620, lr=4.31113e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51933
2023-01-05 11:31:51 - progress_bar.py[line:274] - INFO: epoch 001:  17656 / 102288 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.9, wpb=110.4, bsz=40, num_updates=17630, lr=4.31062e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=51944
2023-01-05 11:32:02 - progress_bar.py[line:274] - INFO: epoch 001:  17666 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.5, ups=0.89, wpb=109.9, bsz=40, num_updates=17640, lr=4.31011e-05, gnorm=0.351, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51955
2023-01-05 11:32:13 - progress_bar.py[line:274] - INFO: epoch 001:  17676 / 102288 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.9, ups=0.92, wpb=110.5, bsz=40, num_updates=17650, lr=4.3096e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51966
2023-01-05 11:32:24 - progress_bar.py[line:274] - INFO: epoch 001:  17686 / 102288 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=17660, lr=4.30909e-05, gnorm=0.439, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51978
2023-01-05 11:32:36 - progress_bar.py[line:274] - INFO: epoch 001:  17696 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.89, wpb=110.8, bsz=40, num_updates=17670, lr=4.30858e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51989
2023-01-05 11:32:47 - progress_bar.py[line:274] - INFO: epoch 001:  17706 / 102288 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=17680, lr=4.30807e-05, gnorm=0.543, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=52000
2023-01-05 11:32:59 - progress_bar.py[line:274] - INFO: epoch 001:  17716 / 102288 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.9, wpb=110.8, bsz=40, num_updates=17690, lr=4.30757e-05, gnorm=0.34, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52012
2023-01-05 11:33:10 - progress_bar.py[line:274] - INFO: epoch 001:  17726 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.4, ups=0.91, wpb=110.9, bsz=40, num_updates=17700, lr=4.30706e-05, gnorm=0.52, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52023
2023-01-05 11:33:22 - progress_bar.py[line:274] - INFO: epoch 001:  17736 / 102288 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=17710, lr=4.30655e-05, gnorm=0.534, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52035
2023-01-05 11:33:34 - progress_bar.py[line:274] - INFO: epoch 001:  17746 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.9, wpb=109.6, bsz=40, num_updates=17720, lr=4.30604e-05, gnorm=0.514, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52047
2023-01-05 11:33:44 - progress_bar.py[line:274] - INFO: epoch 001:  17756 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.7, ups=0.94, wpb=108.7, bsz=40, num_updates=17730, lr=4.30553e-05, gnorm=0.4, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52058
2023-01-05 11:33:56 - progress_bar.py[line:274] - INFO: epoch 001:  17766 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=17740, lr=4.30502e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52069
2023-01-05 11:34:07 - progress_bar.py[line:274] - INFO: epoch 001:  17776 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.9, ups=0.91, wpb=110.5, bsz=40, num_updates=17750, lr=4.30451e-05, gnorm=0.405, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52080
2023-01-05 11:34:18 - progress_bar.py[line:274] - INFO: epoch 001:  17786 / 102288 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=17760, lr=4.304e-05, gnorm=0.46, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52092
2023-01-05 11:34:30 - progress_bar.py[line:274] - INFO: epoch 001:  17796 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.6, ups=0.91, wpb=110.4, bsz=40, num_updates=17770, lr=4.30349e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52103
2023-01-05 11:34:41 - progress_bar.py[line:274] - INFO: epoch 001:  17806 / 102288 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.9, wpb=109.6, bsz=40, num_updates=17780, lr=4.30298e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=52114
2023-01-05 11:34:52 - progress_bar.py[line:274] - INFO: epoch 001:  17816 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.8, ups=0.89, wpb=110.4, bsz=40, num_updates=17790, lr=4.30247e-05, gnorm=0.467, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52126
2023-01-05 11:35:04 - progress_bar.py[line:274] - INFO: epoch 001:  17826 / 102288 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101, ups=0.89, wpb=113.1, bsz=40, num_updates=17800, lr=4.30196e-05, gnorm=0.429, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52137
2023-01-05 11:35:15 - progress_bar.py[line:274] - INFO: epoch 001:  17836 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.2, ups=0.92, wpb=110.6, bsz=40, num_updates=17810, lr=4.30146e-05, gnorm=0.568, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52148
2023-01-05 11:35:27 - progress_bar.py[line:274] - INFO: epoch 001:  17846 / 102288 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.1, ups=0.87, wpb=110.4, bsz=40, num_updates=17820, lr=4.30095e-05, gnorm=0.5, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=52160
2023-01-05 11:35:38 - progress_bar.py[line:274] - INFO: epoch 001:  17856 / 102288 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.9, ups=0.93, wpb=110.1, bsz=40, num_updates=17830, lr=4.30044e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52171
2023-01-05 11:35:48 - progress_bar.py[line:274] - INFO: epoch 001:  17866 / 102288 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.9, ups=0.94, wpb=110.3, bsz=40, num_updates=17840, lr=4.29993e-05, gnorm=0.438, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52182
2023-01-05 11:36:00 - progress_bar.py[line:274] - INFO: epoch 001:  17876 / 102288 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.88, wpb=111.1, bsz=40, num_updates=17850, lr=4.29942e-05, gnorm=0.525, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52193
2023-01-05 11:36:07 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 11:36:13 - progress_bar.py[line:274] - INFO: epoch 001:  17887 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=87, ups=0.8, wpb=109.3, bsz=40, num_updates=17860, lr=4.29891e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=52206
2023-01-05 11:36:24 - progress_bar.py[line:274] - INFO: epoch 001:  17897 / 102288 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=17870, lr=4.2984e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=52218
2023-01-05 11:36:36 - progress_bar.py[line:274] - INFO: epoch 001:  17907 / 102288 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.4, ups=0.89, wpb=108.4, bsz=40, num_updates=17880, lr=4.29789e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52229
2023-01-05 11:36:47 - progress_bar.py[line:274] - INFO: epoch 001:  17917 / 102288 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=17890, lr=4.29738e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52241
2023-01-05 11:36:59 - progress_bar.py[line:274] - INFO: epoch 001:  17927 / 102288 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=17900, lr=4.29687e-05, gnorm=0.368, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=52252
2023-01-05 11:37:10 - progress_bar.py[line:274] - INFO: epoch 001:  17937 / 102288 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=17910, lr=4.29636e-05, gnorm=0.392, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=52263
2023-01-05 11:37:21 - progress_bar.py[line:274] - INFO: epoch 001:  17947 / 102288 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.1, ups=0.91, wpb=109.6, bsz=40, num_updates=17920, lr=4.29585e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52274
2023-01-05 11:37:32 - progress_bar.py[line:274] - INFO: epoch 001:  17957 / 102288 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.5, ups=0.91, wpb=109, bsz=40, num_updates=17930, lr=4.29535e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52286
2023-01-05 11:37:44 - progress_bar.py[line:274] - INFO: epoch 001:  17967 / 102288 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.6, ups=0.89, wpb=109.6, bsz=40, num_updates=17940, lr=4.29484e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52297
2023-01-05 11:37:56 - progress_bar.py[line:274] - INFO: epoch 001:  17977 / 102288 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.9, ups=0.87, wpb=110.3, bsz=40, num_updates=17950, lr=4.29433e-05, gnorm=0.347, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52309
2023-01-05 11:38:07 - progress_bar.py[line:274] - INFO: epoch 001:  17987 / 102288 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.1, ups=0.91, wpb=111.4, bsz=40, num_updates=17960, lr=4.29382e-05, gnorm=0.429, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52320
2023-01-05 11:38:18 - progress_bar.py[line:274] - INFO: epoch 001:  17997 / 102288 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.2, ups=0.88, wpb=108.7, bsz=40, num_updates=17970, lr=4.29331e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52332
2023-01-05 11:38:30 - progress_bar.py[line:274] - INFO: epoch 001:  18007 / 102288 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=17980, lr=4.2928e-05, gnorm=0.46, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=52343
2023-01-05 11:38:41 - progress_bar.py[line:274] - INFO: epoch 001:  18017 / 102288 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.1, ups=0.89, wpb=108.5, bsz=40, num_updates=17990, lr=4.29229e-05, gnorm=0.489, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=52354
2023-01-05 11:38:53 - progress_bar.py[line:274] - INFO: epoch 001:  18027 / 102288 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=18000, lr=4.29178e-05, gnorm=0.368, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=52366
2023-01-05 11:38:53 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 11:38:54 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 11:38:54 - train.py[line:551] - INFO: load:0.85 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 11:39:09 - trainer.py[line:1409] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.21 GiB (GPU 0; 39.59 GiB total capacity; 8.89 GiB already allocated; 1.26 GiB free; 35.84 GiB reserved in total by PyTorch)
2023-01-05 11:39:09 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9103 MB |   14332 MB |    7373 TB |    7373 TB |
|       from large pool |    8958 MB |   14188 MB |    7371 TB |    7371 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9103 MB |   14332 MB |    7373 TB |    7373 TB |
|       from large pool |    8958 MB |   14188 MB |    7371 TB |    7371 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   36702 MB |   37622 MB |  168302 MB |  131600 MB |
|       from large pool |   36556 MB |   37470 MB |  168004 MB |  131448 MB |
|       from small pool |     146 MB |     152 MB |     298 MB |     152 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27598 MB |   27598 MB |    7601 TB |    7601 TB |
|       from large pool |   27597 MB |   27597 MB |    7598 TB |    7598 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |  318718 K  |  318715 K  |
|       from large pool |     563    |     575    |  111608 K  |  111607 K  |
|       from small pool |    3106    |    3116    |  207110 K  |  207107 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |  318718 K  |  318715 K  |
|       from large pool |     563    |     575    |  111608 K  |  111607 K  |
|       from small pool |    3106    |    3116    |  207110 K  |  207107 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     164    |     427    |     267    |
|       from large pool |      87    |      88    |     278    |     191    |
|       from small pool |      73    |      76    |     149    |      76    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     121    |  211285 K  |  211285 K  |
|       from large pool |      76    |      80    |   40961 K  |   40960 K  |
|       from small pool |      35    |      45    |  170324 K  |  170324 K  |
|===========================================================================|

2023-01-05 11:39:09 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-05 11:39:09 - trainer.py[line:1158] - WARNING: ran out of memory in validation step, retrying batch
2023-01-05 11:41:27 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 11:41:27 - train.py[line:551] - INFO: load:0.88 valid_run:153.34 task_valid:148.22 collect_output:2.88
2023-01-05 11:43:57 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 11:43:57 - train.py[line:551] - INFO: load:0.90 valid_run:303.23 task_valid:291.56 collect_output:8.32
2023-01-05 11:46:30 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 11:46:30 - train.py[line:551] - INFO: load:0.93 valid_run:456.25 task_valid:435.04 collect_output:16.73
2023-01-05 11:49:00 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 11:49:00 - train.py[line:551] - INFO: load:0.95 valid_run:606.03 task_valid:580.19 collect_output:20.31
2023-01-05 11:51:33 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 11:51:33 - train.py[line:551] - INFO: load:0.98 valid_run:759.00 task_valid:727.90 collect_output:24.49
2023-01-05 11:54:06 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 11:54:06 - train.py[line:551] - INFO: load:1.00 valid_run:911.44 task_valid:873.76 collect_output:29.96
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1641434
Killing subprocess 1641435
Main process received SIGINT, exiting
