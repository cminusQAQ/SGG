2023-02-21 20:12:18 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-21 20:12:18 - utils.py[line:261] - INFO: Start init
2023-02-21 20:12:18 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-21 20:12:18 - utils.py[line:261] - INFO: Start init
2023-02-21 20:12:19 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-21 20:12:19 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-21 20:12:19 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-21 20:12:19 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-21 20:12:33 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_caption_opt_new_pretrained_with_coco', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=50, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_caption_opt_new_pretrained_with_coco', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-21 20:12:33 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-21 20:12:33 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-21 20:12:38 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-21 20:12:38 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-21 20:12:38 - train.py[line:119] - INFO: model: OFAModel
2023-02-21 20:12:38 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-21 20:12:38 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-21 20:12:38 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-21 20:12:39 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-21 20:12:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-21 20:12:39 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-21 20:12:39 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 20:12:39 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 20:12:39 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-21 20:12:40 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-21 20:12:40 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-21 20:12:40 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt
2023-02-21 20:13:06 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-21 20:13:07 - trainer.py[line:656] - INFO: Loading EMA from checkpoint
2023-02-21 20:13:07 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-21 20:13:07 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 20:13:07 - trainer.py[line:663] - INFO: Loading EMA fp32 params from checkpoint
2023-02-21 20:13:08 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt (epoch 6 @ 0 updates)
2023-02-21 20:13:08 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 20:13:08 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 20600, warmup steps 824, warmup_factor 0.0012135922330097086
Total steps 20600, warmup steps 824, warmup_factor 0.0012135922330097086
2023-02-21 20:13:09 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-21 20:13:09 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 20:13:30 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 412 loss=0.714, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=81.4, ups=0.74, wpb=109.8, bsz=40, num_updates=10, lr=6.06796e-07, gnorm=3.386, clip=100, loss_scale=128, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=50
2023-02-21 20:13:42 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 412 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.6, ups=0.87, wpb=111.9, bsz=40, num_updates=20, lr=1.21359e-06, gnorm=2.415, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=63
2023-02-21 20:13:54 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 412 loss=0.813, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95, ups=0.85, wpb=111.5, bsz=40, num_updates=30, lr=1.82039e-06, gnorm=3.315, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=75
2023-02-21 20:14:06 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 412 loss=0.739, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.88, wpb=110.8, bsz=40, num_updates=40, lr=2.42718e-06, gnorm=3.084, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=86
2023-02-21 20:14:17 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 412 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.8, ups=0.87, wpb=111.4, bsz=40, num_updates=50, lr=3.03398e-06, gnorm=2.702, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=98
2023-02-21 20:14:29 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 412 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.7, ups=0.87, wpb=110.7, bsz=40, num_updates=60, lr=3.64078e-06, gnorm=2.531, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=110
2023-02-21 20:14:40 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 412 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.6, ups=0.87, wpb=111.3, bsz=40, num_updates=70, lr=4.24757e-06, gnorm=2.36, clip=100, loss_scale=128, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=121
2023-02-21 20:14:51 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 412 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=80, lr=4.85437e-06, gnorm=2.019, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=132
2023-02-21 20:15:03 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 412 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=90, lr=5.46117e-06, gnorm=2.17, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=144
2023-02-21 20:15:14 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 412 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.1, ups=0.9, wpb=111.2, bsz=40, num_updates=100, lr=6.06796e-06, gnorm=2.295, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=155
2023-02-21 20:15:25 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 412 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=110, lr=6.67476e-06, gnorm=1.906, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=166
2023-02-21 20:15:36 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 412 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=120, lr=7.28155e-06, gnorm=1.811, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=178
2023-02-21 20:15:48 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 412 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.8, ups=0.89, wpb=111.5, bsz=40, num_updates=130, lr=7.88835e-06, gnorm=1.602, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=189
2023-02-21 20:15:59 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 412 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.4, ups=0.88, wpb=112.2, bsz=40, num_updates=140, lr=8.49515e-06, gnorm=1.46, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=200
2023-02-21 20:16:10 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 412 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.8, ups=0.88, wpb=111.2, bsz=40, num_updates=150, lr=9.10194e-06, gnorm=1.351, clip=80, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=212
2023-02-21 20:16:22 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 412 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.8, ups=0.87, wpb=112.2, bsz=40, num_updates=160, lr=9.70874e-06, gnorm=1.243, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=223
2023-02-21 20:16:33 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 412 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.5, ups=0.89, wpb=111.3, bsz=40, num_updates=170, lr=1.03155e-05, gnorm=1.181, clip=60, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=234
2023-02-21 20:16:44 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 412 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.7, ups=0.91, wpb=111.1, bsz=40, num_updates=180, lr=1.09223e-05, gnorm=1.148, clip=70, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=245
2023-02-21 20:16:55 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 412 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.4, ups=0.92, wpb=112.8, bsz=40, num_updates=190, lr=1.15291e-05, gnorm=1.028, clip=70, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=256
2023-02-21 20:17:07 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 412 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.1, ups=0.87, wpb=111.5, bsz=40, num_updates=200, lr=1.21359e-05, gnorm=0.955, clip=50, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=268
2023-02-21 20:17:18 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 412 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.4, ups=0.88, wpb=111.3, bsz=40, num_updates=210, lr=1.27427e-05, gnorm=1.08, clip=60, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=279
2023-02-21 20:17:29 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 412 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=220, lr=1.33495e-05, gnorm=0.859, clip=20, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=290
2023-02-21 20:17:40 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 412 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.8, ups=0.94, wpb=110.8, bsz=40, num_updates=230, lr=1.39563e-05, gnorm=0.921, clip=30, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=301
2023-02-21 20:17:51 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 412 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=240, lr=1.45631e-05, gnorm=0.853, clip=20, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=312
2023-02-21 20:18:02 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 412 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.9, ups=0.91, wpb=110.4, bsz=40, num_updates=250, lr=1.51699e-05, gnorm=0.969, clip=40, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=323
2023-02-21 20:18:13 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 412 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=260, lr=1.57767e-05, gnorm=0.904, clip=30, loss_scale=128, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=334
2023-02-21 20:18:24 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 412 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.6, ups=0.91, wpb=111.1, bsz=40, num_updates=270, lr=1.63835e-05, gnorm=0.839, clip=40, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=345
2023-02-21 20:18:35 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 412 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=280, lr=1.69903e-05, gnorm=0.849, clip=20, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=356
2023-02-21 20:18:47 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 412 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.3, ups=0.9, wpb=109.8, bsz=40, num_updates=290, lr=1.75971e-05, gnorm=0.998, clip=40, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=368
2023-02-21 20:18:58 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 412 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.6, ups=0.87, wpb=110.8, bsz=40, num_updates=300, lr=1.82039e-05, gnorm=0.983, clip=60, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=379
2023-02-21 20:19:09 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 412 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.6, ups=0.9, wpb=111.1, bsz=40, num_updates=310, lr=1.88107e-05, gnorm=0.811, clip=20, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=390
2023-02-21 20:19:21 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=320, lr=1.94175e-05, gnorm=0.948, clip=30, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=402
2023-02-21 20:19:31 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.7, ups=0.92, wpb=112.8, bsz=40, num_updates=330, lr=2.00243e-05, gnorm=0.89, clip=20, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=412
2023-02-21 20:19:42 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 412 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.2, ups=0.91, wpb=110.1, bsz=40, num_updates=340, lr=2.06311e-05, gnorm=0.988, clip=50, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=423
2023-02-21 20:19:54 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 412 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.3, ups=0.9, wpb=111.7, bsz=40, num_updates=350, lr=2.12379e-05, gnorm=0.915, clip=40, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=435
2023-02-21 20:20:05 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 412 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.6, ups=0.88, wpb=110.3, bsz=40, num_updates=360, lr=2.18447e-05, gnorm=0.988, clip=30, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=446
2023-02-21 20:20:16 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 412 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=370, lr=2.24515e-05, gnorm=0.948, clip=50, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=457
2023-02-21 20:20:27 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 412 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=380, lr=2.30583e-05, gnorm=0.869, clip=30, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=468
2023-02-21 20:20:38 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 412 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.1, ups=0.91, wpb=111.4, bsz=40, num_updates=390, lr=2.3665e-05, gnorm=0.926, clip=10, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=480
2023-02-21 20:20:50 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 412 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.8, ups=0.9, wpb=111.4, bsz=40, num_updates=400, lr=2.42718e-05, gnorm=0.884, clip=20, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=491
2023-02-21 20:21:01 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 412 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.5, ups=0.92, wpb=111.6, bsz=40, num_updates=410, lr=2.48786e-05, gnorm=0.981, clip=50, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=502
2023-02-21 20:21:03 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-02-21 20:21:03 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.436 | loss_v1 0 | loss_v2 0 | nll_loss 0.18 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.13 | wps 98.1 | ups 0.88 | wpb 111.1 | bsz 40 | num_updates 412 | lr 2.5e-05 | gnorm 1.446 | clip 61.7 | loss_scale 128 | train_wall 465 | gb_free 10.7 | ema_decay 0.9999 | wall 504
2023-02-21 20:21:03 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 20:21:03 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 20:21:03 - trainer.py[line:758] - INFO: begin training epoch 2
2023-02-21 20:21:03 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 20:21:14 - progress_bar.py[line:274] - INFO: epoch 002:      8 / 412 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=82, ups=0.75, wpb=110, bsz=40, num_updates=420, lr=2.54854e-05, gnorm=0.82, clip=30, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=515
2023-02-21 20:21:25 - progress_bar.py[line:274] - INFO: epoch 002:     18 / 412 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.2, ups=0.91, wpb=111.2, bsz=40, num_updates=430, lr=2.60922e-05, gnorm=0.862, clip=40, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=526
2023-02-21 20:21:36 - progress_bar.py[line:274] - INFO: epoch 002:     28 / 412 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.9, wpb=110.3, bsz=40, num_updates=440, lr=2.6699e-05, gnorm=0.573, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=537
2023-02-21 20:21:48 - progress_bar.py[line:274] - INFO: epoch 002:     38 / 412 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.87, wpb=111.7, bsz=40, num_updates=450, lr=2.73058e-05, gnorm=0.942, clip=20, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=549
2023-02-21 20:21:58 - progress_bar.py[line:274] - INFO: epoch 002:     48 / 412 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.93, wpb=109.4, bsz=40, num_updates=460, lr=2.79126e-05, gnorm=0.864, clip=30, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=559
2023-02-21 20:22:09 - progress_bar.py[line:274] - INFO: epoch 002:     58 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=111.1, bsz=40, num_updates=470, lr=2.85194e-05, gnorm=0.893, clip=40, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=571
2023-02-21 20:22:21 - progress_bar.py[line:274] - INFO: epoch 002:     68 / 412 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.2, ups=0.86, wpb=111.3, bsz=40, num_updates=480, lr=2.91262e-05, gnorm=1.042, clip=50, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=582
2023-02-21 20:22:32 - progress_bar.py[line:274] - INFO: epoch 002:     78 / 412 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.4, ups=0.9, wpb=111.1, bsz=40, num_updates=490, lr=2.9733e-05, gnorm=1.044, clip=60, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=593
2023-02-21 20:22:43 - progress_bar.py[line:274] - INFO: epoch 002:     88 / 412 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=500, lr=3.03398e-05, gnorm=0.765, clip=10, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=605
2023-02-21 20:22:55 - progress_bar.py[line:274] - INFO: epoch 002:     98 / 412 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=95.6, ups=0.87, wpb=109.8, bsz=40, num_updates=510, lr=3.09466e-05, gnorm=0.885, clip=10, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=616
2023-02-21 20:23:06 - progress_bar.py[line:274] - INFO: epoch 002:    108 / 412 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=520, lr=3.15534e-05, gnorm=0.907, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=627
2023-02-21 20:23:17 - progress_bar.py[line:274] - INFO: epoch 002:    118 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.91, wpb=111, bsz=40, num_updates=530, lr=3.21602e-05, gnorm=0.989, clip=50, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=638
2023-02-21 20:23:28 - progress_bar.py[line:274] - INFO: epoch 002:    128 / 412 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.4, ups=0.92, wpb=111.5, bsz=40, num_updates=540, lr=3.2767e-05, gnorm=1.021, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=649
2023-02-21 20:23:39 - progress_bar.py[line:274] - INFO: epoch 002:    138 / 412 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.8, ups=0.91, wpb=111.3, bsz=40, num_updates=550, lr=3.33738e-05, gnorm=1.03, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=660
2023-02-21 20:23:50 - progress_bar.py[line:274] - INFO: epoch 002:    148 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.7, ups=0.92, wpb=112.4, bsz=40, num_updates=560, lr=3.39806e-05, gnorm=0.923, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=671
2023-02-21 20:24:01 - progress_bar.py[line:274] - INFO: epoch 002:    158 / 412 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=570, lr=3.45874e-05, gnorm=0.969, clip=30, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=682
2023-02-21 20:24:12 - progress_bar.py[line:274] - INFO: epoch 002:    168 / 412 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.88, wpb=112.5, bsz=40, num_updates=580, lr=3.51942e-05, gnorm=0.955, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=693
2023-02-21 20:24:23 - progress_bar.py[line:274] - INFO: epoch 002:    178 / 412 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=590, lr=3.5801e-05, gnorm=1.007, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=705
2023-02-21 20:24:35 - progress_bar.py[line:274] - INFO: epoch 002:    188 / 412 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.91, wpb=110, bsz=40, num_updates=600, lr=3.64078e-05, gnorm=1.051, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=716
2023-02-21 20:24:46 - progress_bar.py[line:274] - INFO: epoch 002:    198 / 412 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.9, wpb=112.2, bsz=40, num_updates=610, lr=3.70146e-05, gnorm=0.942, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=727
2023-02-21 20:24:57 - progress_bar.py[line:274] - INFO: epoch 002:    208 / 412 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=620, lr=3.76214e-05, gnorm=1.156, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=738
2023-02-21 20:25:08 - progress_bar.py[line:274] - INFO: epoch 002:    218 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.5, ups=0.87, wpb=110.8, bsz=40, num_updates=630, lr=3.82282e-05, gnorm=1.024, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=749
2023-02-21 20:25:20 - progress_bar.py[line:274] - INFO: epoch 002:    228 / 412 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.89, wpb=110.1, bsz=40, num_updates=640, lr=3.8835e-05, gnorm=0.796, clip=20, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=761
2023-02-21 20:25:31 - progress_bar.py[line:274] - INFO: epoch 002:    238 / 412 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.88, wpb=111.8, bsz=40, num_updates=650, lr=3.94417e-05, gnorm=0.933, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=772
2023-02-21 20:25:42 - progress_bar.py[line:274] - INFO: epoch 002:    248 / 412 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111.1, bsz=40, num_updates=660, lr=4.00485e-05, gnorm=0.879, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=783
2023-02-21 20:25:53 - progress_bar.py[line:274] - INFO: epoch 002:    258 / 412 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.91, wpb=111.1, bsz=40, num_updates=670, lr=4.06553e-05, gnorm=1.224, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=794
2023-02-21 20:26:05 - progress_bar.py[line:274] - INFO: epoch 002:    268 / 412 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=110.6, bsz=40, num_updates=680, lr=4.12621e-05, gnorm=0.797, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=806
2023-02-21 20:26:16 - progress_bar.py[line:274] - INFO: epoch 002:    278 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.9, wpb=111.4, bsz=40, num_updates=690, lr=4.18689e-05, gnorm=0.887, clip=20, loss_scale=256, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=817
2023-02-21 20:26:27 - progress_bar.py[line:274] - INFO: epoch 002:    288 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.88, wpb=112, bsz=40, num_updates=700, lr=4.24757e-05, gnorm=1.012, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=828
2023-02-21 20:26:38 - progress_bar.py[line:274] - INFO: epoch 002:    298 / 412 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=710, lr=4.30825e-05, gnorm=0.897, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=840
2023-02-21 20:26:50 - progress_bar.py[line:274] - INFO: epoch 002:    308 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=720, lr=4.36893e-05, gnorm=0.879, clip=20, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=851
2023-02-21 20:27:01 - progress_bar.py[line:274] - INFO: epoch 002:    318 / 412 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.9, wpb=111.3, bsz=40, num_updates=730, lr=4.42961e-05, gnorm=0.932, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=862
2023-02-21 20:27:12 - progress_bar.py[line:274] - INFO: epoch 002:    328 / 412 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.1, ups=0.88, wpb=110.5, bsz=40, num_updates=740, lr=4.49029e-05, gnorm=0.947, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=873
2023-02-21 20:27:23 - progress_bar.py[line:274] - INFO: epoch 002:    338 / 412 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.5, ups=0.92, wpb=111.4, bsz=40, num_updates=750, lr=4.55097e-05, gnorm=0.857, clip=30, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=884
2023-02-21 20:27:34 - progress_bar.py[line:274] - INFO: epoch 002:    348 / 412 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.3, ups=0.9, wpb=111.8, bsz=40, num_updates=760, lr=4.61165e-05, gnorm=0.992, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=895
2023-02-21 20:27:46 - progress_bar.py[line:274] - INFO: epoch 002:    358 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=770, lr=4.67233e-05, gnorm=0.841, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=907
2023-02-21 20:27:57 - progress_bar.py[line:274] - INFO: epoch 002:    368 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=780, lr=4.73301e-05, gnorm=0.847, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=918
2023-02-21 20:28:08 - progress_bar.py[line:274] - INFO: epoch 002:    378 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.87, wpb=111.3, bsz=40, num_updates=790, lr=4.79369e-05, gnorm=0.849, clip=20, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=929
2023-02-21 20:28:19 - progress_bar.py[line:274] - INFO: epoch 002:    388 / 412 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.1, ups=0.91, wpb=110.3, bsz=40, num_updates=800, lr=4.85437e-05, gnorm=1.054, clip=50, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=940
2023-02-21 20:28:30 - progress_bar.py[line:274] - INFO: epoch 002:    398 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.91, wpb=112.4, bsz=40, num_updates=810, lr=4.91505e-05, gnorm=1.041, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=951
2023-02-21 20:28:41 - progress_bar.py[line:274] - INFO: epoch 002:    408 / 412 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.2, ups=0.91, wpb=111.4, bsz=40, num_updates=820, lr=4.97573e-05, gnorm=1.301, clip=80, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=962
2023-02-21 20:28:46 - train.py[line:339] - INFO: end of epoch 2 (average epoch stats below)
2023-02-21 20:28:46 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.3 | loss_v1 0 | loss_v2 0 | nll_loss 0.119 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.09 | wps 98.8 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 824 | lr 5e-05 | gnorm 0.944 | clip 38.1 | loss_scale 256 | train_wall 459 | gb_free 10.5 | ema_decay 0.9999 | wall 967
2023-02-21 20:28:46 - trainer.py[line:694] - INFO: loading train data for epoch 3
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 20:28:46 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 20:28:46 - trainer.py[line:758] - INFO: begin training epoch 3
2023-02-21 20:28:46 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 20:28:55 - progress_bar.py[line:274] - INFO: epoch 003:      6 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=81.2, ups=0.72, wpb=112.5, bsz=40, num_updates=830, lr=4.99848e-05, gnorm=0.806, clip=50, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=976
2023-02-21 20:29:07 - progress_bar.py[line:274] - INFO: epoch 003:     16 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96, ups=0.87, wpb=110.5, bsz=40, num_updates=840, lr=4.99595e-05, gnorm=0.856, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=988
2023-02-21 20:29:18 - progress_bar.py[line:274] - INFO: epoch 003:     26 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.2, bsz=40, num_updates=850, lr=4.99343e-05, gnorm=0.893, clip=50, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=999
2023-02-21 20:29:29 - progress_bar.py[line:274] - INFO: epoch 003:     36 / 412 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=860, lr=4.9909e-05, gnorm=0.946, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1010
2023-02-21 20:29:40 - progress_bar.py[line:274] - INFO: epoch 003:     46 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.89, wpb=110.3, bsz=40, num_updates=870, lr=4.98837e-05, gnorm=0.785, clip=30, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1021
2023-02-21 20:29:51 - progress_bar.py[line:274] - INFO: epoch 003:     56 / 412 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.92, wpb=111.3, bsz=40, num_updates=880, lr=4.98584e-05, gnorm=1.055, clip=50, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1032
2023-02-21 20:30:02 - progress_bar.py[line:274] - INFO: epoch 003:     66 / 412 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.91, wpb=111.2, bsz=40, num_updates=890, lr=4.98331e-05, gnorm=0.94, clip=50, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1043
2023-02-21 20:30:13 - progress_bar.py[line:274] - INFO: epoch 003:     76 / 412 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.92, wpb=110.8, bsz=40, num_updates=900, lr=4.98078e-05, gnorm=0.835, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1054
2023-02-21 20:30:24 - progress_bar.py[line:274] - INFO: epoch 003:     86 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.9, wpb=112.1, bsz=40, num_updates=910, lr=4.97826e-05, gnorm=0.741, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1065
2023-02-21 20:30:36 - progress_bar.py[line:274] - INFO: epoch 003:     96 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.87, wpb=112.1, bsz=40, num_updates=920, lr=4.97573e-05, gnorm=0.585, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1077
2023-02-21 20:30:47 - progress_bar.py[line:274] - INFO: epoch 003:    106 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.92, wpb=111.5, bsz=40, num_updates=930, lr=4.9732e-05, gnorm=0.946, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1088
2023-02-21 20:30:58 - progress_bar.py[line:274] - INFO: epoch 003:    116 / 412 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.9, wpb=112.3, bsz=40, num_updates=940, lr=4.97067e-05, gnorm=0.919, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1099
2023-02-21 20:31:09 - progress_bar.py[line:274] - INFO: epoch 003:    126 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=950, lr=4.96814e-05, gnorm=0.879, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1110
2023-02-21 20:31:20 - progress_bar.py[line:274] - INFO: epoch 003:    136 / 412 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.89, wpb=110.1, bsz=40, num_updates=960, lr=4.96561e-05, gnorm=0.956, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1121
2023-02-21 20:31:32 - progress_bar.py[line:274] - INFO: epoch 003:    146 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.88, wpb=110.8, bsz=40, num_updates=970, lr=4.96309e-05, gnorm=0.652, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1133
2023-02-21 20:31:43 - progress_bar.py[line:274] - INFO: epoch 003:    156 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=111.9, bsz=40, num_updates=980, lr=4.96056e-05, gnorm=0.772, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1144
2023-02-21 20:31:54 - progress_bar.py[line:274] - INFO: epoch 003:    166 / 412 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=990, lr=4.95803e-05, gnorm=0.974, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1155
2023-02-21 20:32:05 - progress_bar.py[line:274] - INFO: epoch 003:    176 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=1000, lr=4.9555e-05, gnorm=0.786, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1166
2023-02-21 20:32:05 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 20:32:05 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 20:32:07 - train.py[line:549] - INFO: 0 / 6234
2023-02-21 20:32:07 - train.py[line:551] - INFO: load:1.55 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 20:34:12 - train.py[line:549] - INFO: 200 / 6234
2023-02-21 20:34:12 - train.py[line:551] - INFO: load:1.58 valid_run:125.07 task_valid:122.32 collect_output:1.61
2023-02-21 20:36:12 - train.py[line:549] - INFO: 400 / 6234
2023-02-21 20:36:12 - train.py[line:551] - INFO: load:1.60 valid_run:245.17 task_valid:238.50 collect_output:4.50
2023-02-21 20:38:15 - train.py[line:549] - INFO: 600 / 6234
2023-02-21 20:38:15 - train.py[line:551] - INFO: load:1.63 valid_run:367.76 task_valid:355.19 collect_output:9.37
2023-02-21 20:40:17 - train.py[line:549] - INFO: 800 / 6234
2023-02-21 20:40:17 - train.py[line:551] - INFO: load:1.65 valid_run:489.84 task_valid:469.32 collect_output:16.28
2023-02-21 20:42:19 - train.py[line:549] - INFO: 1000 / 6234
2023-02-21 20:42:19 - train.py[line:551] - INFO: load:1.68 valid_run:611.16 task_valid:587.54 collect_output:18.34
2023-02-21 20:44:22 - train.py[line:549] - INFO: 1200 / 6234
2023-02-21 20:44:22 - train.py[line:551] - INFO: load:1.70 valid_run:734.27 task_valid:706.49 collect_output:21.44
2023-02-21 20:46:25 - train.py[line:549] - INFO: 1400 / 6234
2023-02-21 20:46:25 - train.py[line:551] - INFO: load:1.73 valid_run:857.60 task_valid:824.99 collect_output:25.26
2023-02-21 20:48:28 - train.py[line:549] - INFO: 1600 / 6234
2023-02-21 20:48:28 - train.py[line:551] - INFO: load:1.75 valid_run:979.93 task_valid:942.31 collect_output:29.25
2023-02-21 20:50:31 - train.py[line:549] - INFO: 1800 / 6234
2023-02-21 20:50:31 - train.py[line:551] - INFO: load:1.78 valid_run:1103.51 task_valid:1059.62 collect_output:34.50
2023-02-21 20:52:33 - train.py[line:549] - INFO: 2000 / 6234
2023-02-21 20:52:33 - train.py[line:551] - INFO: load:1.80 valid_run:1225.26 task_valid:1173.00 collect_output:41.84
2023-02-21 20:54:34 - train.py[line:549] - INFO: 2200 / 6234
2023-02-21 20:54:34 - train.py[line:551] - INFO: load:1.83 valid_run:1345.81 task_valid:1289.31 collect_output:45.04
2023-02-21 20:56:35 - train.py[line:549] - INFO: 2400 / 6234
2023-02-21 20:56:35 - train.py[line:551] - INFO: load:1.85 valid_run:1467.55 task_valid:1406.79 collect_output:48.27
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/test_caption_opt_new_pretrained_with_coco', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=50', '--warmup-ratio=0.04', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=1000', '--validate-interval-updates=1000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' died with <Signals.SIGKILL: 9>.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3921339
Killing subprocess 3921340
2023-02-21 21:12:10 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-21 21:12:10 - utils.py[line:261] - INFO: Start init
2023-02-21 21:12:10 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-21 21:12:10 - utils.py[line:261] - INFO: Start init
2023-02-21 21:12:10 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-21 21:12:10 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-21 21:12:10 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-21 21:12:10 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-21 21:12:14 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_caption_opt_new_pretrained_with_coco', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=50, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=6, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_caption_opt_new_pretrained_with_coco', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-21 21:12:14 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-21 21:12:14 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-21 21:12:18 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-21 21:12:18 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-21 21:12:18 - train.py[line:119] - INFO: model: OFAModel
2023-02-21 21:12:18 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-21 21:12:18 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-21 21:12:18 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-21 21:12:18 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-21 21:12:19 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-21 21:12:19 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-21 21:12:19 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 21:12:19 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 21:12:19 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-21 21:12:20 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-21 21:12:20 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-21 21:12:20 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt
2023-02-21 21:12:45 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-21 21:12:46 - trainer.py[line:656] - INFO: Loading EMA from checkpoint
2023-02-21 21:12:46 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-21 21:12:46 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-21 21:12:46 - trainer.py[line:663] - INFO: Loading EMA fp32 params from checkpoint
2023-02-21 21:12:46 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/coco_out_of_VG_best_checkpoint.pt (epoch 6 @ 0 updates)
2023-02-21 21:12:46 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 21:12:47 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv slice_id 1 row count 8240 total row count 16480
Total steps 20600, warmup steps 824, warmup_factor 0.0012135922330097086
2023-02-21 21:12:47 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-21 21:12:47 - train.py[line:312] - INFO: Start iterating over samples
Total steps 20600, warmup steps 824, warmup_factor 0.0012135922330097086
2023-02-21 21:13:03 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 412 loss=0.714, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=89, ups=0.81, wpb=109.8, bsz=40, num_updates=10, lr=6.06796e-07, gnorm=3.383, clip=100, loss_scale=128, train_wall=15, gb_free=10.8, ema_decay=0.9999, wall=44
2023-02-21 21:13:15 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 412 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.5, ups=0.87, wpb=111.9, bsz=40, num_updates=20, lr=1.21359e-06, gnorm=2.42, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56
2023-02-21 21:13:26 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 412 loss=0.813, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.9, ups=0.87, wpb=111.5, bsz=40, num_updates=30, lr=1.82039e-06, gnorm=3.297, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=67
2023-02-21 21:13:38 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 412 loss=0.739, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96, ups=0.87, wpb=110.8, bsz=40, num_updates=40, lr=2.42718e-06, gnorm=3.075, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=79
2023-02-21 21:13:49 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 412 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.1, ups=0.87, wpb=111.4, bsz=40, num_updates=50, lr=3.03398e-06, gnorm=2.702, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=90
2023-02-21 21:14:01 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 412 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=60, lr=3.64078e-06, gnorm=2.541, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=102
2023-02-21 21:14:12 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 412 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.2, ups=0.87, wpb=111.3, bsz=40, num_updates=70, lr=4.24757e-06, gnorm=2.381, clip=100, loss_scale=128, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=113
2023-02-21 21:14:23 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 412 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.3, ups=0.9, wpb=110.4, bsz=40, num_updates=80, lr=4.85437e-06, gnorm=2.056, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=124
2023-02-21 21:14:35 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 412 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.5, ups=0.88, wpb=111.5, bsz=40, num_updates=90, lr=5.46117e-06, gnorm=2.186, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=136
2023-02-21 21:14:46 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 412 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.8, ups=0.9, wpb=111.2, bsz=40, num_updates=100, lr=6.06796e-06, gnorm=2.245, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=147
2023-02-21 21:14:57 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 412 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.7, ups=0.88, wpb=110.1, bsz=40, num_updates=110, lr=6.67476e-06, gnorm=1.905, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=158
2023-02-21 21:15:09 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 412 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=93.1, ups=0.85, wpb=110, bsz=40, num_updates=120, lr=7.28155e-06, gnorm=1.799, clip=90, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=170
2023-02-21 21:15:20 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 412 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.2, ups=0.9, wpb=111.5, bsz=40, num_updates=130, lr=7.88835e-06, gnorm=1.6, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=181
2023-02-21 21:15:32 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 412 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.3, ups=0.87, wpb=112.2, bsz=40, num_updates=140, lr=8.49515e-06, gnorm=1.473, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=193
2023-02-21 21:15:43 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 412 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=150, lr=9.10194e-06, gnorm=1.365, clip=80, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=204
2023-02-21 21:15:55 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 412 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.6, ups=0.87, wpb=112.2, bsz=40, num_updates=160, lr=9.70874e-06, gnorm=1.24, clip=90, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=216
2023-02-21 21:16:06 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 412 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=170, lr=1.03155e-05, gnorm=1.145, clip=60, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=227
2023-02-21 21:16:17 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 412 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.1, ups=0.9, wpb=111.1, bsz=40, num_updates=180, lr=1.09223e-05, gnorm=1.135, clip=70, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=238
2023-02-21 21:16:28 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 412 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.5, ups=0.92, wpb=112.8, bsz=40, num_updates=190, lr=1.15291e-05, gnorm=1.02, clip=70, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=249
2023-02-21 21:16:40 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 412 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.5, ups=0.87, wpb=111.5, bsz=40, num_updates=200, lr=1.21359e-05, gnorm=0.943, clip=50, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=261
2023-02-21 21:16:51 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 412 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=210, lr=1.27427e-05, gnorm=1.093, clip=70, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=272
2023-02-21 21:17:02 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 412 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.5, ups=0.89, wpb=111.5, bsz=40, num_updates=220, lr=1.33495e-05, gnorm=0.869, clip=20, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=283
2023-02-21 21:17:13 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 412 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.2, ups=0.93, wpb=110.8, bsz=40, num_updates=230, lr=1.39563e-05, gnorm=0.929, clip=30, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=294
2023-02-21 21:17:24 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 412 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102, ups=0.91, wpb=111.8, bsz=40, num_updates=240, lr=1.45631e-05, gnorm=0.849, clip=20, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=305
2023-02-21 21:17:35 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 412 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=250, lr=1.51699e-05, gnorm=0.975, clip=50, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=316
2023-02-21 21:17:46 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 412 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=260, lr=1.57767e-05, gnorm=0.913, clip=30, loss_scale=128, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=327
2023-02-21 21:17:57 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 412 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.2, ups=0.9, wpb=111.1, bsz=40, num_updates=270, lr=1.63835e-05, gnorm=0.843, clip=40, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=338
2023-02-21 21:18:09 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 412 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=280, lr=1.69903e-05, gnorm=0.853, clip=20, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=349
2023-02-21 21:18:20 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 412 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.7, ups=0.89, wpb=109.8, bsz=40, num_updates=290, lr=1.75971e-05, gnorm=0.996, clip=40, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=361
2023-02-21 21:18:31 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 412 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.2, ups=0.87, wpb=110.8, bsz=40, num_updates=300, lr=1.82039e-05, gnorm=0.976, clip=50, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=372
2023-02-21 21:18:43 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 412 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=310, lr=1.88107e-05, gnorm=0.811, clip=20, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=383
2023-02-21 21:18:54 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=320, lr=1.94175e-05, gnorm=0.954, clip=30, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=395
2023-02-21 21:19:05 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.2, ups=0.92, wpb=112.8, bsz=40, num_updates=330, lr=2.00243e-05, gnorm=0.896, clip=30, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=406
2023-02-21 21:19:16 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 412 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.9, ups=0.91, wpb=110.1, bsz=40, num_updates=340, lr=2.06311e-05, gnorm=0.988, clip=50, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=417
2023-02-21 21:19:27 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 412 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.8, ups=0.89, wpb=111.7, bsz=40, num_updates=350, lr=2.12379e-05, gnorm=0.915, clip=40, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=428
2023-02-21 21:19:38 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 412 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=360, lr=2.18447e-05, gnorm=0.998, clip=30, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=439
2023-02-21 21:19:50 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 412 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=370, lr=2.24515e-05, gnorm=0.958, clip=60, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=451
2023-02-21 21:20:01 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 412 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.5, ups=0.89, wpb=109.7, bsz=40, num_updates=380, lr=2.30583e-05, gnorm=0.86, clip=30, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=462
2023-02-21 21:20:12 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 412 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.7, ups=0.9, wpb=111.4, bsz=40, num_updates=390, lr=2.3665e-05, gnorm=0.905, clip=10, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=473
2023-02-21 21:20:23 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 412 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=400, lr=2.42718e-05, gnorm=0.889, clip=20, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=484
2023-02-21 21:20:34 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 412 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.8, ups=0.92, wpb=111.6, bsz=40, num_updates=410, lr=2.48786e-05, gnorm=0.943, clip=40, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=495
2023-02-21 21:20:36 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-02-21 21:20:36 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.436 | loss_v1 0 | loss_v2 0 | nll_loss 0.18 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.13 | wps 98.5 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 412 | lr 2.5e-05 | gnorm 1.445 | clip 61.9 | loss_scale 128 | train_wall 465 | gb_free 10.7 | ema_decay 0.9999 | wall 497
2023-02-21 21:20:36 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 21:20:36 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 21:20:37 - trainer.py[line:758] - INFO: begin training epoch 2
2023-02-21 21:20:37 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 21:20:47 - progress_bar.py[line:274] - INFO: epoch 002:      8 / 412 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=82.2, ups=0.75, wpb=110, bsz=40, num_updates=420, lr=2.54854e-05, gnorm=0.828, clip=30, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=508
2023-02-21 21:20:58 - progress_bar.py[line:274] - INFO: epoch 002:     18 / 412 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.9, ups=0.91, wpb=111.2, bsz=40, num_updates=430, lr=2.60922e-05, gnorm=0.895, clip=40, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=519
2023-02-21 21:21:10 - progress_bar.py[line:274] - INFO: epoch 002:     28 / 412 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=440, lr=2.6699e-05, gnorm=0.577, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=531
2023-02-21 21:21:21 - progress_bar.py[line:274] - INFO: epoch 002:     38 / 412 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.6, ups=0.87, wpb=111.7, bsz=40, num_updates=450, lr=2.73058e-05, gnorm=0.938, clip=40, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=542
2023-02-21 21:21:32 - progress_bar.py[line:274] - INFO: epoch 002:     48 / 412 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.92, wpb=109.4, bsz=40, num_updates=460, lr=2.79126e-05, gnorm=0.861, clip=30, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=553
2023-02-21 21:21:43 - progress_bar.py[line:274] - INFO: epoch 002:     58 / 412 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.9, wpb=111.1, bsz=40, num_updates=470, lr=2.85194e-05, gnorm=0.868, clip=30, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=564
2023-02-21 21:21:55 - progress_bar.py[line:274] - INFO: epoch 002:     68 / 412 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.6, ups=0.87, wpb=111.3, bsz=40, num_updates=480, lr=2.91262e-05, gnorm=1.009, clip=40, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=576
2023-02-21 21:22:06 - progress_bar.py[line:274] - INFO: epoch 002:     78 / 412 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=490, lr=2.9733e-05, gnorm=1.048, clip=60, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=587
2023-02-21 21:22:17 - progress_bar.py[line:274] - INFO: epoch 002:     88 / 412 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=500, lr=3.03398e-05, gnorm=0.789, clip=20, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=598
2023-02-21 21:22:29 - progress_bar.py[line:274] - INFO: epoch 002:     98 / 412 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=95.5, ups=0.87, wpb=109.8, bsz=40, num_updates=510, lr=3.09466e-05, gnorm=0.853, clip=10, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=610
2023-02-21 21:22:40 - progress_bar.py[line:274] - INFO: epoch 002:    108 / 412 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=520, lr=3.15534e-05, gnorm=0.92, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=621
2023-02-21 21:22:51 - progress_bar.py[line:274] - INFO: epoch 002:    118 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=111, bsz=40, num_updates=530, lr=3.21602e-05, gnorm=0.994, clip=50, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=632
2023-02-21 21:23:02 - progress_bar.py[line:274] - INFO: epoch 002:    128 / 412 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.91, wpb=111.5, bsz=40, num_updates=540, lr=3.2767e-05, gnorm=0.975, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=643
2023-02-21 21:23:13 - progress_bar.py[line:274] - INFO: epoch 002:    138 / 412 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.9, wpb=111.3, bsz=40, num_updates=550, lr=3.33738e-05, gnorm=1.034, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=654
2023-02-21 21:23:24 - progress_bar.py[line:274] - INFO: epoch 002:    148 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.8, ups=0.91, wpb=112.4, bsz=40, num_updates=560, lr=3.39806e-05, gnorm=0.868, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=665
2023-02-21 21:23:35 - progress_bar.py[line:274] - INFO: epoch 002:    158 / 412 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.2, ups=0.9, wpb=111.4, bsz=40, num_updates=570, lr=3.45874e-05, gnorm=1, clip=30, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=676
2023-02-21 21:23:47 - progress_bar.py[line:274] - INFO: epoch 002:    168 / 412 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.88, wpb=112.5, bsz=40, num_updates=580, lr=3.51942e-05, gnorm=0.97, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=688
2023-02-21 21:23:58 - progress_bar.py[line:274] - INFO: epoch 002:    178 / 412 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.89, wpb=111.1, bsz=40, num_updates=590, lr=3.5801e-05, gnorm=0.992, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=699
2023-02-21 21:24:09 - progress_bar.py[line:274] - INFO: epoch 002:    188 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=600, lr=3.64078e-05, gnorm=1.042, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=710
2023-02-21 21:24:20 - progress_bar.py[line:274] - INFO: epoch 002:    198 / 412 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=610, lr=3.70146e-05, gnorm=0.906, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=721
2023-02-21 21:24:31 - progress_bar.py[line:274] - INFO: epoch 002:    208 / 412 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=620, lr=3.76214e-05, gnorm=1.064, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=732
2023-02-21 21:24:43 - progress_bar.py[line:274] - INFO: epoch 002:    218 / 412 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=630, lr=3.82282e-05, gnorm=0.905, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=744
2023-02-21 21:24:54 - progress_bar.py[line:274] - INFO: epoch 002:    228 / 412 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.89, wpb=110.1, bsz=40, num_updates=640, lr=3.8835e-05, gnorm=0.793, clip=20, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=755
2023-02-21 21:25:06 - progress_bar.py[line:274] - INFO: epoch 002:    238 / 412 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111.8, bsz=40, num_updates=650, lr=3.94417e-05, gnorm=0.885, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=766
2023-02-21 21:25:17 - progress_bar.py[line:274] - INFO: epoch 002:    248 / 412 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111.1, bsz=40, num_updates=660, lr=4.00485e-05, gnorm=0.902, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=778
2023-02-21 21:25:28 - progress_bar.py[line:274] - INFO: epoch 002:    258 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=670, lr=4.06553e-05, gnorm=1.278, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=789
2023-02-21 21:25:39 - progress_bar.py[line:274] - INFO: epoch 002:    268 / 412 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=680, lr=4.12621e-05, gnorm=0.799, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=800
2023-02-21 21:25:50 - progress_bar.py[line:274] - INFO: epoch 002:    278 / 412 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=690, lr=4.18689e-05, gnorm=0.862, clip=10, loss_scale=256, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=811
2023-02-21 21:26:02 - progress_bar.py[line:274] - INFO: epoch 002:    288 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=700, lr=4.24757e-05, gnorm=0.918, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=823
2023-02-21 21:26:13 - progress_bar.py[line:274] - INFO: epoch 002:    298 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.6, ups=0.88, wpb=110.1, bsz=40, num_updates=710, lr=4.30825e-05, gnorm=0.781, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=834
2023-02-21 21:26:24 - progress_bar.py[line:274] - INFO: epoch 002:    308 / 412 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.7, bsz=40, num_updates=720, lr=4.36893e-05, gnorm=0.903, clip=20, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=845
2023-02-21 21:26:36 - progress_bar.py[line:274] - INFO: epoch 002:    318 / 412 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=730, lr=4.42961e-05, gnorm=0.857, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=857
2023-02-21 21:26:47 - progress_bar.py[line:274] - INFO: epoch 002:    328 / 412 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.9, ups=0.88, wpb=110.5, bsz=40, num_updates=740, lr=4.49029e-05, gnorm=0.899, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=868
2023-02-21 21:26:58 - progress_bar.py[line:274] - INFO: epoch 002:    338 / 412 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.91, wpb=111.4, bsz=40, num_updates=750, lr=4.55097e-05, gnorm=0.841, clip=30, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=879
2023-02-21 21:27:09 - progress_bar.py[line:274] - INFO: epoch 002:    348 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.9, wpb=111.8, bsz=40, num_updates=760, lr=4.61165e-05, gnorm=1.093, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=890
2023-02-21 21:27:20 - progress_bar.py[line:274] - INFO: epoch 002:    358 / 412 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.89, wpb=110.6, bsz=40, num_updates=770, lr=4.67233e-05, gnorm=0.927, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=901
2023-02-21 21:27:32 - progress_bar.py[line:274] - INFO: epoch 002:    368 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.89, wpb=111.1, bsz=40, num_updates=780, lr=4.73301e-05, gnorm=0.828, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=913
2023-02-21 21:27:43 - progress_bar.py[line:274] - INFO: epoch 002:    378 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.87, wpb=111.3, bsz=40, num_updates=790, lr=4.79369e-05, gnorm=0.904, clip=30, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=924
2023-02-21 21:27:54 - progress_bar.py[line:274] - INFO: epoch 002:    388 / 412 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.3, ups=0.9, wpb=110.3, bsz=40, num_updates=800, lr=4.85437e-05, gnorm=0.999, clip=40, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=935
2023-02-21 21:28:05 - progress_bar.py[line:274] - INFO: epoch 002:    398 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=810, lr=4.91505e-05, gnorm=1.091, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=946
2023-02-21 21:28:16 - progress_bar.py[line:274] - INFO: epoch 002:    408 / 412 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=820, lr=4.97573e-05, gnorm=1.171, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=957
2023-02-21 21:28:21 - train.py[line:339] - INFO: end of epoch 2 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 21:28:21 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.3 | loss_v1 0 | loss_v2 0 | nll_loss 0.118 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.09 | wps 98.5 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 824 | lr 5e-05 | gnorm 0.93 | clip 35.2 | loss_scale 256 | train_wall 461 | gb_free 10.5 | ema_decay 0.9999 | wall 962
2023-02-21 21:28:21 - trainer.py[line:694] - INFO: loading train data for epoch 3
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 21:28:21 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 21:28:22 - trainer.py[line:758] - INFO: begin training epoch 3
2023-02-21 21:28:22 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 21:28:30 - progress_bar.py[line:274] - INFO: epoch 003:      6 / 412 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=82.7, ups=0.74, wpb=112.5, bsz=40, num_updates=830, lr=4.99848e-05, gnorm=0.806, clip=40, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=971
2023-02-21 21:28:42 - progress_bar.py[line:274] - INFO: epoch 003:     16 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.6, ups=0.87, wpb=110.5, bsz=40, num_updates=840, lr=4.99595e-05, gnorm=0.882, clip=30, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=983
2023-02-21 21:28:53 - progress_bar.py[line:274] - INFO: epoch 003:     26 / 412 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=112.2, bsz=40, num_updates=850, lr=4.99343e-05, gnorm=1.157, clip=50, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=994
2023-02-21 21:29:04 - progress_bar.py[line:274] - INFO: epoch 003:     36 / 412 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.6, bsz=40, num_updates=860, lr=4.9909e-05, gnorm=0.867, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1005
2023-02-21 21:29:15 - progress_bar.py[line:274] - INFO: epoch 003:     46 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.5, ups=0.88, wpb=110.3, bsz=40, num_updates=870, lr=4.98837e-05, gnorm=0.729, clip=30, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1016
2023-02-21 21:29:26 - progress_bar.py[line:274] - INFO: epoch 003:     56 / 412 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.91, wpb=111.3, bsz=40, num_updates=880, lr=4.98584e-05, gnorm=0.986, clip=50, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1027
2023-02-21 21:29:37 - progress_bar.py[line:274] - INFO: epoch 003:     66 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.91, wpb=111.2, bsz=40, num_updates=890, lr=4.98331e-05, gnorm=1.022, clip=60, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1038
2023-02-21 21:29:48 - progress_bar.py[line:274] - INFO: epoch 003:     76 / 412 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.92, wpb=110.8, bsz=40, num_updates=900, lr=4.98078e-05, gnorm=0.793, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1049
2023-02-21 21:29:59 - progress_bar.py[line:274] - INFO: epoch 003:     86 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=112.1, bsz=40, num_updates=910, lr=4.97826e-05, gnorm=0.839, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1060
2023-02-21 21:30:11 - progress_bar.py[line:274] - INFO: epoch 003:     96 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.87, wpb=112.1, bsz=40, num_updates=920, lr=4.97573e-05, gnorm=0.62, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1072
2023-02-21 21:30:22 - progress_bar.py[line:274] - INFO: epoch 003:    106 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=930, lr=4.9732e-05, gnorm=0.836, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1083
2023-02-21 21:30:33 - progress_bar.py[line:274] - INFO: epoch 003:    116 / 412 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.9, wpb=112.3, bsz=40, num_updates=940, lr=4.97067e-05, gnorm=0.926, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1094
2023-02-21 21:30:44 - progress_bar.py[line:274] - INFO: epoch 003:    126 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=950, lr=4.96814e-05, gnorm=0.969, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1105
2023-02-21 21:30:56 - progress_bar.py[line:274] - INFO: epoch 003:    136 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=960, lr=4.96561e-05, gnorm=0.837, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1117
2023-02-21 21:31:07 - progress_bar.py[line:274] - INFO: epoch 003:    146 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.89, wpb=110.8, bsz=40, num_updates=970, lr=4.96309e-05, gnorm=0.658, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1128
2023-02-21 21:31:18 - progress_bar.py[line:274] - INFO: epoch 003:    156 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=980, lr=4.96056e-05, gnorm=0.933, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1139
2023-02-21 21:31:29 - progress_bar.py[line:274] - INFO: epoch 003:    166 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=990, lr=4.95803e-05, gnorm=1.105, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1150
2023-02-21 21:31:40 - progress_bar.py[line:274] - INFO: epoch 003:    176 / 412 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=1000, lr=4.9555e-05, gnorm=0.891, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1161
2023-02-21 21:31:40 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 21:31:40 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 21:31:42 - train.py[line:549] - INFO: 0 / 6234
2023-02-21 21:31:42 - train.py[line:551] - INFO: load:0.84 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 21:34:35 - train.py[line:549] - INFO: 200 / 6234
2023-02-21 21:34:35 - train.py[line:551] - INFO: load:0.86 valid_run:173.30 task_valid:121.38 collect_output:50.84
2023-02-21 21:36:35 - train.py[line:549] - INFO: 400 / 6234
2023-02-21 21:36:35 - train.py[line:551] - INFO: load:0.89 valid_run:293.43 task_valid:236.26 collect_output:55.06
2023-02-21 21:38:38 - train.py[line:549] - INFO: 600 / 6234
2023-02-21 21:38:38 - train.py[line:551] - INFO: load:0.91 valid_run:416.23 task_valid:351.90 collect_output:61.18
2023-02-21 21:40:41 - train.py[line:549] - INFO: 800 / 6234
2023-02-21 21:40:41 - train.py[line:551] - INFO: load:0.94 valid_run:538.81 task_valid:465.27 collect_output:69.33
2023-02-21 21:42:41 - train.py[line:549] - INFO: 1000 / 6234
2023-02-21 21:42:41 - train.py[line:551] - INFO: load:0.96 valid_run:659.58 task_valid:582.13 collect_output:72.15
2023-02-21 21:44:45 - train.py[line:549] - INFO: 1200 / 6234
2023-02-21 21:44:45 - train.py[line:551] - INFO: load:0.99 valid_run:782.65 task_valid:700.20 collect_output:76.12
2023-02-21 21:46:48 - train.py[line:549] - INFO: 1400 / 6234
2023-02-21 21:46:48 - train.py[line:551] - INFO: load:1.01 valid_run:906.00 task_valid:817.55 collect_output:81.12
2023-02-21 21:48:50 - train.py[line:549] - INFO: 1600 / 6234
2023-02-21 21:48:50 - train.py[line:551] - INFO: load:1.04 valid_run:1028.13 task_valid:933.32 collect_output:86.46
2023-02-21 21:50:55 - train.py[line:549] - INFO: 1800 / 6234
2023-02-21 21:50:55 - train.py[line:551] - INFO: load:1.06 valid_run:1152.39 task_valid:1049.93 collect_output:93.08
2023-02-21 21:52:57 - train.py[line:549] - INFO: 2000 / 6234
2023-02-21 21:52:57 - train.py[line:551] - INFO: load:1.09 valid_run:1274.62 task_valid:1161.69 collect_output:102.52
2023-02-21 21:54:57 - train.py[line:549] - INFO: 2200 / 6234
2023-02-21 21:54:57 - train.py[line:551] - INFO: load:1.11 valid_run:1394.94 task_valid:1276.40 collect_output:107.08
2023-02-21 21:56:59 - train.py[line:549] - INFO: 2400 / 6234
2023-02-21 21:56:59 - train.py[line:551] - INFO: load:1.14 valid_run:1516.85 task_valid:1392.44 collect_output:111.94
2023-02-21 21:58:58 - train.py[line:549] - INFO: 2600 / 6234
2023-02-21 21:58:58 - train.py[line:551] - INFO: load:1.16 valid_run:1635.97 task_valid:1505.44 collect_output:117.04
2023-02-21 22:00:59 - train.py[line:549] - INFO: 2800 / 6234
2023-02-21 22:00:59 - train.py[line:551] - INFO: load:1.19 valid_run:1757.00 task_valid:1622.69 collect_output:119.80
2023-02-21 22:03:01 - train.py[line:549] - INFO: 3000 / 6234
2023-02-21 22:03:01 - train.py[line:551] - INFO: load:1.21 valid_run:1878.38 task_valid:1738.17 collect_output:124.67
2023-02-21 22:05:02 - train.py[line:549] - INFO: 3200 / 6234
2023-02-21 22:05:02 - train.py[line:551] - INFO: load:1.24 valid_run:1999.79 task_valid:1851.41 collect_output:131.80
2023-02-21 22:07:04 - train.py[line:549] - INFO: 3400 / 6234
2023-02-21 22:07:04 - train.py[line:551] - INFO: load:1.26 valid_run:2121.39 task_valid:1966.75 collect_output:137.04
2023-02-21 22:09:05 - train.py[line:549] - INFO: 3600 / 6234
2023-02-21 22:09:05 - train.py[line:551] - INFO: load:1.29 valid_run:2242.12 task_valid:2084.09 collect_output:139.42
2023-02-21 22:11:06 - train.py[line:549] - INFO: 3800 / 6234
2023-02-21 22:11:06 - train.py[line:551] - INFO: load:1.31 valid_run:2363.22 task_valid:2200.28 collect_output:143.31
2023-02-21 22:13:06 - train.py[line:549] - INFO: 4000 / 6234
2023-02-21 22:13:06 - train.py[line:551] - INFO: load:1.34 valid_run:2483.44 task_valid:2316.00 collect_output:146.81
2023-02-21 22:15:08 - train.py[line:549] - INFO: 4200 / 6234
2023-02-21 22:15:08 - train.py[line:551] - INFO: load:1.36 valid_run:2605.31 task_valid:2431.76 collect_output:151.92
2023-02-21 22:17:10 - train.py[line:549] - INFO: 4400 / 6234
2023-02-21 22:17:10 - train.py[line:551] - INFO: load:1.39 valid_run:2727.17 task_valid:2549.69 collect_output:154.83
2023-02-21 22:19:10 - train.py[line:549] - INFO: 4600 / 6234
2023-02-21 22:19:10 - train.py[line:551] - INFO: load:1.41 valid_run:2847.49 task_valid:2663.12 collect_output:160.71
2023-02-21 22:21:10 - train.py[line:549] - INFO: 4800 / 6234
2023-02-21 22:21:10 - train.py[line:551] - INFO: load:1.44 valid_run:2967.29 task_valid:2778.51 collect_output:164.09
2023-02-21 22:23:12 - train.py[line:549] - INFO: 5000 / 6234
2023-02-21 22:23:12 - train.py[line:551] - INFO: load:1.46 valid_run:3089.09 task_valid:2893.71 collect_output:169.68
2023-02-21 22:25:15 - train.py[line:549] - INFO: 5200 / 6234
2023-02-21 22:25:15 - train.py[line:551] - INFO: load:1.49 valid_run:3212.29 task_valid:3008.83 collect_output:176.73
2023-02-21 22:27:15 - train.py[line:549] - INFO: 5400 / 6234
2023-02-21 22:27:15 - train.py[line:551] - INFO: load:1.51 valid_run:3332.02 task_valid:3122.04 collect_output:182.25
2023-02-21 22:29:17 - train.py[line:549] - INFO: 5600 / 6234
2023-02-21 22:29:17 - train.py[line:551] - INFO: load:1.54 valid_run:3453.80 task_valid:3240.85 collect_output:184.19
2023-02-21 22:31:19 - train.py[line:549] - INFO: 5800 / 6234
2023-02-21 22:31:19 - train.py[line:551] - INFO: load:1.57 valid_run:3575.92 task_valid:3355.75 collect_output:190.40
2023-02-21 22:33:21 - train.py[line:549] - INFO: 6000 / 6234
2023-02-21 22:33:21 - train.py[line:551] - INFO: load:1.59 valid_run:3697.55 task_valid:3473.34 collect_output:193.43
2023-02-21 22:35:22 - train.py[line:549] - INFO: 6200 / 6234
2023-02-21 22:35:22 - train.py[line:551] - INFO: load:1.62 valid_run:3818.15 task_valid:3590.90 collect_output:195.46

====================================================================================================
SGG eval:     R @ 50: 0.6344;     R @ 100: 0.6613;     R @ 500: 0.6796;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4149;    mR @ 100: 0.4371;    mR @ 500: 0.4553;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3323) (lying on:0.4333) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8333) (playing:0.0000) (riding:0.9461) (says:0.0000) (sitting on:0.7188) (standing on:0.5310) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6344;     R @ 100: 0.6613;     R @ 500: 0.6796;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4149;    mR @ 100: 0.4371;    mR @ 500: 0.4553;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3323) (lying on:0.4333) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8333) (playing:0.0000) (riding:0.9461) (says:0.0000) (sitting on:0.7188) (standing on:0.5310) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================

2023-02-21 22:35:52 - train.py[line:487] - INFO: 0.6612753501400559
2023-02-21 22:35:52 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-21 22:35:52 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.249 | loss_v1 0 | loss_v2 0 | nll_loss 0.078 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.661275 | ppl 1.06 | vqa_score 0.5676 | wps 116.5 | wpb 72 | bsz 24 | num_updates 1000
2023-02-21 22:35:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 1000 updates
2023-02-21 22:35:52 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_3_1000.pt
2023-02-21 22:35:58 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_3_1000.pt
2023-02-21 22:36:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_3_1000.pt (epoch 3 @ 1000 updates, score 0.6612753501400559) (writing took 10.695948027074337 seconds)
2023-02-21 22:36:15 - progress_bar.py[line:274] - INFO: epoch 003:    186 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=112.3, bsz=40, num_updates=1010, lr=4.95297e-05, gnorm=0.823, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5036
2023-02-21 22:36:26 - progress_bar.py[line:274] - INFO: epoch 003:    196 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=1020, lr=4.95044e-05, gnorm=0.653, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5047
2023-02-21 22:36:37 - progress_bar.py[line:274] - INFO: epoch 003:    206 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=1030, lr=4.94792e-05, gnorm=0.786, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5058
2023-02-21 22:36:48 - progress_bar.py[line:274] - INFO: epoch 003:    216 / 412 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.5, bsz=40, num_updates=1040, lr=4.94539e-05, gnorm=0.788, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5069
2023-02-21 22:36:59 - progress_bar.py[line:274] - INFO: epoch 003:    226 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.4, bsz=40, num_updates=1050, lr=4.94286e-05, gnorm=0.796, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5080
2023-02-21 22:37:10 - progress_bar.py[line:274] - INFO: epoch 003:    236 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=1060, lr=4.94033e-05, gnorm=0.787, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5091
2023-02-21 22:37:21 - progress_bar.py[line:274] - INFO: epoch 003:    246 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=1070, lr=4.9378e-05, gnorm=0.755, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5102
2023-02-21 22:37:32 - progress_bar.py[line:274] - INFO: epoch 003:    256 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=1080, lr=4.93528e-05, gnorm=0.873, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5113
2023-02-21 22:37:43 - progress_bar.py[line:274] - INFO: epoch 003:    266 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=111.5, bsz=40, num_updates=1090, lr=4.93275e-05, gnorm=0.699, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5124
2023-02-21 22:37:55 - progress_bar.py[line:274] - INFO: epoch 003:    276 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.88, wpb=112.1, bsz=40, num_updates=1100, lr=4.93022e-05, gnorm=0.9, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5136
2023-02-21 22:38:06 - progress_bar.py[line:274] - INFO: epoch 003:    286 / 412 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=1110, lr=4.92769e-05, gnorm=0.964, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5147
2023-02-21 22:38:17 - progress_bar.py[line:274] - INFO: epoch 003:    296 / 412 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=1120, lr=4.92516e-05, gnorm=0.853, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5158
2023-02-21 22:38:28 - progress_bar.py[line:274] - INFO: epoch 003:    306 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.9, wpb=110.2, bsz=40, num_updates=1130, lr=4.92263e-05, gnorm=0.708, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5169
2023-02-21 22:38:39 - progress_bar.py[line:274] - INFO: epoch 003:    316 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.92, wpb=111.4, bsz=40, num_updates=1140, lr=4.92011e-05, gnorm=0.844, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5180
2023-02-21 22:38:50 - progress_bar.py[line:274] - INFO: epoch 003:    326 / 412 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.9, wpb=110.5, bsz=40, num_updates=1150, lr=4.91758e-05, gnorm=0.887, clip=40, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=5191
2023-02-21 22:39:01 - progress_bar.py[line:274] - INFO: epoch 003:    336 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.92, wpb=111.3, bsz=40, num_updates=1160, lr=4.91505e-05, gnorm=0.915, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5202
2023-02-21 22:39:12 - progress_bar.py[line:274] - INFO: epoch 003:    346 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=1170, lr=4.91252e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5213
2023-02-21 22:39:23 - progress_bar.py[line:274] - INFO: epoch 003:    356 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=1180, lr=4.90999e-05, gnorm=0.704, clip=30, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5224
2023-02-21 22:39:34 - progress_bar.py[line:274] - INFO: epoch 003:    366 / 412 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.91, wpb=110.5, bsz=40, num_updates=1190, lr=4.90746e-05, gnorm=0.951, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5235
2023-02-21 22:39:45 - progress_bar.py[line:274] - INFO: epoch 003:    376 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.9, wpb=110.3, bsz=40, num_updates=1200, lr=4.90494e-05, gnorm=0.93, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5246
2023-02-21 22:39:57 - progress_bar.py[line:274] - INFO: epoch 003:    386 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=1210, lr=4.90241e-05, gnorm=0.937, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5258
2023-02-21 22:40:08 - progress_bar.py[line:274] - INFO: epoch 003:    396 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.1, ups=0.93, wpb=112.1, bsz=40, num_updates=1220, lr=4.89988e-05, gnorm=0.725, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5269
2023-02-21 22:40:19 - progress_bar.py[line:274] - INFO: epoch 003:    406 / 412 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.89, wpb=109.6, bsz=40, num_updates=1230, lr=4.89735e-05, gnorm=0.973, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5280
2023-02-21 22:40:26 - train.py[line:339] - INFO: end of epoch 3 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 22:40:26 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.273 | loss_v1 0 | loss_v2 0 | nll_loss 0.09 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.06 | wps 10.6 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 1236 | lr 4.89583e-05 | gnorm 0.849 | clip 28.9 | loss_scale 512 | train_wall 458 | gb_free 10.7 | ema_decay 0.9999 | wall 5286
2023-02-21 22:40:26 - trainer.py[line:694] - INFO: loading train data for epoch 4
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 22:40:26 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 22:40:26 - trainer.py[line:758] - INFO: begin training epoch 4
2023-02-21 22:40:26 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 22:40:32 - progress_bar.py[line:274] - INFO: epoch 004:      4 / 412 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=83, ups=0.75, wpb=110, bsz=40, num_updates=1240, lr=4.89482e-05, gnorm=0.846, clip=40, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5293
2023-02-21 22:40:44 - progress_bar.py[line:274] - INFO: epoch 004:     14 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=112.7, bsz=40, num_updates=1250, lr=4.89229e-05, gnorm=0.741, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5305
2023-02-21 22:40:55 - progress_bar.py[line:274] - INFO: epoch 004:     24 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.91, wpb=110.5, bsz=40, num_updates=1260, lr=4.88977e-05, gnorm=0.681, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5316
2023-02-21 22:41:06 - progress_bar.py[line:274] - INFO: epoch 004:     34 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=1270, lr=4.88724e-05, gnorm=0.975, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5327
2023-02-21 22:41:17 - progress_bar.py[line:274] - INFO: epoch 004:     44 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.1, ups=0.87, wpb=109.9, bsz=40, num_updates=1280, lr=4.88471e-05, gnorm=0.595, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5338
2023-02-21 22:41:28 - progress_bar.py[line:274] - INFO: epoch 004:     54 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.1, bsz=40, num_updates=1290, lr=4.88218e-05, gnorm=0.797, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5349
2023-02-21 22:41:40 - progress_bar.py[line:274] - INFO: epoch 004:     64 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=111.9, bsz=40, num_updates=1300, lr=4.87965e-05, gnorm=0.692, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5361
2023-02-21 22:41:51 - progress_bar.py[line:274] - INFO: epoch 004:     74 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=1310, lr=4.87712e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5372
2023-02-21 22:42:02 - progress_bar.py[line:274] - INFO: epoch 004:     84 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=1320, lr=4.8746e-05, gnorm=0.593, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5383
2023-02-21 22:42:13 - progress_bar.py[line:274] - INFO: epoch 004:     94 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=1330, lr=4.87207e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5394
2023-02-21 22:42:24 - progress_bar.py[line:274] - INFO: epoch 004:    104 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.2, ups=0.94, wpb=110.7, bsz=40, num_updates=1340, lr=4.86954e-05, gnorm=0.71, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5405
2023-02-21 22:42:35 - progress_bar.py[line:274] - INFO: epoch 004:    114 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=1350, lr=4.86701e-05, gnorm=0.611, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5416
2023-02-21 22:42:46 - progress_bar.py[line:274] - INFO: epoch 004:    124 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.88, wpb=112, bsz=40, num_updates=1360, lr=4.86448e-05, gnorm=0.641, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5427
2023-02-21 22:42:58 - progress_bar.py[line:274] - INFO: epoch 004:    134 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=1370, lr=4.86195e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5438
2023-02-21 22:43:08 - progress_bar.py[line:274] - INFO: epoch 004:    144 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.92, wpb=110.7, bsz=40, num_updates=1380, lr=4.85943e-05, gnorm=0.974, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5449
2023-02-21 22:43:19 - progress_bar.py[line:274] - INFO: epoch 004:    154 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=1390, lr=4.8569e-05, gnorm=0.829, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5460
2023-02-21 22:43:31 - progress_bar.py[line:274] - INFO: epoch 004:    164 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=112, bsz=40, num_updates=1400, lr=4.85437e-05, gnorm=0.711, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5472
2023-02-21 22:43:42 - progress_bar.py[line:274] - INFO: epoch 004:    174 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=112.1, bsz=40, num_updates=1410, lr=4.85184e-05, gnorm=0.717, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5483
2023-02-21 22:43:53 - progress_bar.py[line:274] - INFO: epoch 004:    184 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.5, bsz=40, num_updates=1420, lr=4.84931e-05, gnorm=0.71, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5494
2023-02-21 22:44:05 - progress_bar.py[line:274] - INFO: epoch 004:    194 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=112.3, bsz=40, num_updates=1430, lr=4.84678e-05, gnorm=0.807, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=5506
2023-02-21 22:44:16 - progress_bar.py[line:274] - INFO: epoch 004:    204 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=1440, lr=4.84426e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5517
2023-02-21 22:44:27 - progress_bar.py[line:274] - INFO: epoch 004:    214 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112.5, bsz=40, num_updates=1450, lr=4.84173e-05, gnorm=0.781, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5528
2023-02-21 22:44:38 - progress_bar.py[line:274] - INFO: epoch 004:    224 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.6, bsz=40, num_updates=1460, lr=4.8392e-05, gnorm=1.083, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5539
2023-02-21 22:44:49 - progress_bar.py[line:274] - INFO: epoch 004:    234 / 412 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.88, wpb=110.1, bsz=40, num_updates=1470, lr=4.83667e-05, gnorm=0.745, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5550
2023-02-21 22:45:01 - progress_bar.py[line:274] - INFO: epoch 004:    244 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=1480, lr=4.83414e-05, gnorm=0.737, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5561
2023-02-21 22:45:12 - progress_bar.py[line:274] - INFO: epoch 004:    254 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=1490, lr=4.83161e-05, gnorm=0.598, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5573
2023-02-21 22:45:23 - progress_bar.py[line:274] - INFO: epoch 004:    264 / 412 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=1500, lr=4.82909e-05, gnorm=0.755, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5584
2023-02-21 22:45:34 - progress_bar.py[line:274] - INFO: epoch 004:    274 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.4, ups=0.87, wpb=110.4, bsz=40, num_updates=1510, lr=4.82656e-05, gnorm=0.721, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5595
2023-02-21 22:45:45 - progress_bar.py[line:274] - INFO: epoch 004:    284 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=1520, lr=4.82403e-05, gnorm=0.75, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5606
2023-02-21 22:45:56 - progress_bar.py[line:274] - INFO: epoch 004:    294 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.9, ups=0.93, wpb=111.9, bsz=40, num_updates=1530, lr=4.8215e-05, gnorm=0.685, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5617
2023-02-21 22:46:08 - progress_bar.py[line:274] - INFO: epoch 004:    304 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.7, bsz=40, num_updates=1540, lr=4.81897e-05, gnorm=0.868, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5628
2023-02-21 22:46:19 - progress_bar.py[line:274] - INFO: epoch 004:    314 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=1550, lr=4.81644e-05, gnorm=0.827, clip=40, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5640
2023-02-21 22:46:30 - progress_bar.py[line:274] - INFO: epoch 004:    324 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.89, wpb=110.9, bsz=40, num_updates=1560, lr=4.81392e-05, gnorm=0.628, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5651
2023-02-21 22:46:41 - progress_bar.py[line:274] - INFO: epoch 004:    334 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=1570, lr=4.81139e-05, gnorm=0.648, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5662
2023-02-21 22:46:52 - progress_bar.py[line:274] - INFO: epoch 004:    344 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=111.2, bsz=40, num_updates=1580, lr=4.80886e-05, gnorm=0.596, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5673
2023-02-21 22:47:03 - progress_bar.py[line:274] - INFO: epoch 004:    354 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=111.4, bsz=40, num_updates=1590, lr=4.80633e-05, gnorm=0.686, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5684
2023-02-21 22:47:15 - progress_bar.py[line:274] - INFO: epoch 004:    364 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.87, wpb=111.2, bsz=40, num_updates=1600, lr=4.8038e-05, gnorm=0.714, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5696
2023-02-21 22:47:26 - progress_bar.py[line:274] - INFO: epoch 004:    374 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=1610, lr=4.80127e-05, gnorm=0.684, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5707
2023-02-21 22:47:37 - progress_bar.py[line:274] - INFO: epoch 004:    384 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.9, wpb=109.3, bsz=40, num_updates=1620, lr=4.79875e-05, gnorm=0.947, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5718
2023-02-21 22:47:49 - progress_bar.py[line:274] - INFO: epoch 004:    394 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=1630, lr=4.79622e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5730
2023-02-21 22:48:00 - progress_bar.py[line:274] - INFO: epoch 004:    404 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=1640, lr=4.79369e-05, gnorm=1.05, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5741
2023-02-21 22:48:09 - train.py[line:339] - INFO: end of epoch 4 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 22:48:09 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.262 | loss_v1 0 | loss_v2 0 | nll_loss 0.077 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.06 | wps 98.8 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 1648 | lr 4.79167e-05 | gnorm 0.732 | clip 21.1 | loss_scale 1024 | train_wall 459 | gb_free 10.9 | ema_decay 0.9999 | wall 5750
2023-02-21 22:48:09 - trainer.py[line:694] - INFO: loading train data for epoch 5
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 22:48:09 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 22:48:09 - trainer.py[line:758] - INFO: begin training epoch 5
2023-02-21 22:48:09 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 22:48:13 - progress_bar.py[line:274] - INFO: epoch 005:      2 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=82.4, ups=0.74, wpb=110.6, bsz=40, num_updates=1650, lr=4.79116e-05, gnorm=0.809, clip=40, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=5754
2023-02-21 22:48:25 - progress_bar.py[line:274] - INFO: epoch 005:     12 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.9, bsz=40, num_updates=1660, lr=4.78863e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5765
2023-02-21 22:48:36 - progress_bar.py[line:274] - INFO: epoch 005:     22 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=1670, lr=4.7861e-05, gnorm=0.827, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5777
2023-02-21 22:48:47 - progress_bar.py[line:274] - INFO: epoch 005:     32 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.88, wpb=110.6, bsz=40, num_updates=1680, lr=4.78358e-05, gnorm=0.419, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5788
2023-02-21 22:48:58 - progress_bar.py[line:274] - INFO: epoch 005:     42 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.93, wpb=111.3, bsz=40, num_updates=1690, lr=4.78105e-05, gnorm=0.631, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5799
2023-02-21 22:49:09 - progress_bar.py[line:274] - INFO: epoch 005:     52 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.92, wpb=110.1, bsz=40, num_updates=1700, lr=4.77852e-05, gnorm=0.668, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5810
2023-02-21 22:49:20 - progress_bar.py[line:274] - INFO: epoch 005:     62 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.3, bsz=40, num_updates=1710, lr=4.77599e-05, gnorm=0.598, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5821
2023-02-21 22:49:31 - progress_bar.py[line:274] - INFO: epoch 005:     72 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=1720, lr=4.77346e-05, gnorm=0.732, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5832
2023-02-21 22:49:42 - progress_bar.py[line:274] - INFO: epoch 005:     82 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.92, wpb=111.2, bsz=40, num_updates=1730, lr=4.77093e-05, gnorm=0.871, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5843
2023-02-21 22:49:54 - progress_bar.py[line:274] - INFO: epoch 005:     92 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=1740, lr=4.76841e-05, gnorm=0.496, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5854
2023-02-21 22:50:05 - progress_bar.py[line:274] - INFO: epoch 005:    102 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=1750, lr=4.76588e-05, gnorm=0.601, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5866
2023-02-21 22:50:16 - progress_bar.py[line:274] - INFO: epoch 005:    112 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.93, wpb=111.2, bsz=40, num_updates=1760, lr=4.76335e-05, gnorm=0.881, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5877
2023-02-21 22:50:27 - progress_bar.py[line:274] - INFO: epoch 005:    122 / 412 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.6, ups=0.87, wpb=111.5, bsz=40, num_updates=1770, lr=4.76082e-05, gnorm=0.731, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5888
2023-02-21 22:50:39 - progress_bar.py[line:274] - INFO: epoch 005:    132 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.7, ups=0.87, wpb=111.4, bsz=40, num_updates=1780, lr=4.75829e-05, gnorm=0.523, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5900
2023-02-21 22:50:50 - progress_bar.py[line:274] - INFO: epoch 005:    142 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.91, wpb=112.7, bsz=40, num_updates=1790, lr=4.75576e-05, gnorm=0.477, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5911
2023-02-21 22:51:01 - progress_bar.py[line:274] - INFO: epoch 005:    152 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=110.1, bsz=40, num_updates=1800, lr=4.75324e-05, gnorm=0.529, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5922
2023-02-21 22:51:12 - progress_bar.py[line:274] - INFO: epoch 005:    162 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=1810, lr=4.75071e-05, gnorm=0.893, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5933
2023-02-21 22:51:23 - progress_bar.py[line:274] - INFO: epoch 005:    172 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=1820, lr=4.74818e-05, gnorm=0.799, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5944
2023-02-21 22:51:34 - progress_bar.py[line:274] - INFO: epoch 005:    182 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=1830, lr=4.74565e-05, gnorm=0.482, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5955
2023-02-21 22:51:45 - progress_bar.py[line:274] - INFO: epoch 005:    192 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.91, wpb=110.1, bsz=40, num_updates=1840, lr=4.74312e-05, gnorm=0.764, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5966
2023-02-21 22:51:57 - progress_bar.py[line:274] - INFO: epoch 005:    202 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=1850, lr=4.74059e-05, gnorm=0.621, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5977
2023-02-21 22:52:07 - progress_bar.py[line:274] - INFO: epoch 005:    212 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=112.4, bsz=40, num_updates=1860, lr=4.73807e-05, gnorm=0.681, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5988
2023-02-21 22:52:19 - progress_bar.py[line:274] - INFO: epoch 005:    222 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=1870, lr=4.73554e-05, gnorm=0.327, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6000
2023-02-21 22:52:30 - progress_bar.py[line:274] - INFO: epoch 005:    232 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.91, wpb=110.9, bsz=40, num_updates=1880, lr=4.73301e-05, gnorm=0.67, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6011
2023-02-21 22:52:41 - progress_bar.py[line:274] - INFO: epoch 005:    242 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.7, bsz=40, num_updates=1890, lr=4.73048e-05, gnorm=0.812, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6022
2023-02-21 22:52:52 - progress_bar.py[line:274] - INFO: epoch 005:    252 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=1900, lr=4.72795e-05, gnorm=0.89, clip=40, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6033
2023-02-21 22:53:04 - progress_bar.py[line:274] - INFO: epoch 005:    262 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.7, ups=0.87, wpb=110, bsz=40, num_updates=1910, lr=4.72542e-05, gnorm=0.985, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6045
2023-02-21 22:53:15 - progress_bar.py[line:274] - INFO: epoch 005:    272 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.3, bsz=40, num_updates=1920, lr=4.7229e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6056
2023-02-21 22:53:26 - progress_bar.py[line:274] - INFO: epoch 005:    282 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.5, ups=0.87, wpb=110.3, bsz=40, num_updates=1930, lr=4.72037e-05, gnorm=0.47, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6067
2023-02-21 22:53:38 - progress_bar.py[line:274] - INFO: epoch 005:    292 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111.1, bsz=40, num_updates=1940, lr=4.71784e-05, gnorm=0.842, clip=30, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6079
2023-02-21 22:53:49 - progress_bar.py[line:274] - INFO: epoch 005:    302 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=1950, lr=4.71531e-05, gnorm=0.655, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6090
2023-02-21 22:54:00 - progress_bar.py[line:274] - INFO: epoch 005:    312 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=1960, lr=4.71278e-05, gnorm=0.495, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6101
2023-02-21 22:54:11 - progress_bar.py[line:274] - INFO: epoch 005:    322 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=1970, lr=4.71025e-05, gnorm=0.653, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6112
2023-02-21 22:54:22 - progress_bar.py[line:274] - INFO: epoch 005:    332 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.9, ups=0.87, wpb=109.7, bsz=40, num_updates=1980, lr=4.70773e-05, gnorm=0.525, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6123
2023-02-21 22:54:34 - progress_bar.py[line:274] - INFO: epoch 005:    342 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=1990, lr=4.7052e-05, gnorm=0.835, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6135
2023-02-21 22:54:45 - progress_bar.py[line:274] - INFO: epoch 005:    352 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=2000, lr=4.70267e-05, gnorm=0.721, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6146
2023-02-21 22:54:45 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 22:54:46 - train.py[line:549] - INFO: 0 / 6234
2023-02-21 22:54:46 - train.py[line:551] - INFO: load:0.85 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 22:56:49 - train.py[line:549] - INFO: 200 / 6234
2023-02-21 22:56:49 - train.py[line:551] - INFO: load:0.88 valid_run:123.01 task_valid:119.07 collect_output:2.89
2023-02-21 22:58:49 - train.py[line:549] - INFO: 400 / 6234
2023-02-21 22:58:49 - train.py[line:551] - INFO: load:0.91 valid_run:243.03 task_valid:234.21 collect_output:6.75
2023-02-21 23:00:51 - train.py[line:549] - INFO: 600 / 6234
2023-02-21 23:00:51 - train.py[line:551] - INFO: load:0.93 valid_run:364.99 task_valid:349.72 collect_output:12.19
2023-02-21 23:02:53 - train.py[line:549] - INFO: 800 / 6234
2023-02-21 23:02:53 - train.py[line:551] - INFO: load:0.95 valid_run:487.06 task_valid:462.57 collect_output:20.40
2023-02-21 23:04:54 - train.py[line:549] - INFO: 1000 / 6234
2023-02-21 23:04:54 - train.py[line:551] - INFO: load:0.98 valid_run:607.55 task_valid:579.29 collect_output:23.17
2023-02-21 23:06:57 - train.py[line:549] - INFO: 1200 / 6234
2023-02-21 23:06:57 - train.py[line:551] - INFO: load:1.01 valid_run:730.48 task_valid:697.31 collect_output:27.06
2023-02-21 23:09:00 - train.py[line:549] - INFO: 1400 / 6234
2023-02-21 23:09:00 - train.py[line:551] - INFO: load:1.03 valid_run:853.74 task_valid:814.91 collect_output:31.69
2023-02-21 23:11:02 - train.py[line:549] - INFO: 1600 / 6234
2023-02-21 23:11:02 - train.py[line:551] - INFO: load:1.06 valid_run:975.64 task_valid:930.73 collect_output:36.74
2023-02-21 23:13:06 - train.py[line:549] - INFO: 1800 / 6234
2023-02-21 23:13:06 - train.py[line:551] - INFO: load:1.08 valid_run:1099.65 task_valid:1047.11 collect_output:43.37
2023-02-21 23:15:08 - train.py[line:549] - INFO: 2000 / 6234
2023-02-21 23:15:08 - train.py[line:551] - INFO: load:1.11 valid_run:1221.57 task_valid:1158.96 collect_output:52.45
2023-02-21 23:17:09 - train.py[line:549] - INFO: 2200 / 6234
2023-02-21 23:17:09 - train.py[line:551] - INFO: load:1.13 valid_run:1341.85 task_valid:1273.91 collect_output:56.77
2023-02-21 23:19:10 - train.py[line:549] - INFO: 2400 / 6234
2023-02-21 23:19:10 - train.py[line:551] - INFO: load:1.16 valid_run:1463.65 task_valid:1390.10 collect_output:61.39
2023-02-21 23:21:10 - train.py[line:549] - INFO: 2600 / 6234
2023-02-21 23:21:10 - train.py[line:551] - INFO: load:1.18 valid_run:1582.99 task_valid:1503.45 collect_output:66.35
2023-02-21 23:23:11 - train.py[line:549] - INFO: 2800 / 6234
2023-02-21 23:23:11 - train.py[line:551] - INFO: load:1.21 valid_run:1703.98 task_valid:1620.55 collect_output:69.22
2023-02-21 23:25:12 - train.py[line:549] - INFO: 3000 / 6234
2023-02-21 23:25:12 - train.py[line:551] - INFO: load:1.24 valid_run:1825.21 task_valid:1735.96 collect_output:74.03
2023-02-21 23:27:14 - train.py[line:549] - INFO: 3200 / 6234
2023-02-21 23:27:14 - train.py[line:551] - INFO: load:1.26 valid_run:1946.66 task_valid:1849.31 collect_output:81.11
2023-02-21 23:29:15 - train.py[line:549] - INFO: 3400 / 6234
2023-02-21 23:29:15 - train.py[line:551] - INFO: load:1.29 valid_run:2068.08 task_valid:1964.75 collect_output:86.09
2023-02-21 23:31:16 - train.py[line:549] - INFO: 3600 / 6234
2023-02-21 23:31:16 - train.py[line:551] - INFO: load:1.31 valid_run:2188.70 task_valid:2082.00 collect_output:88.46
2023-02-21 23:33:17 - train.py[line:549] - INFO: 3800 / 6234
2023-02-21 23:33:17 - train.py[line:551] - INFO: load:1.34 valid_run:2309.88 task_valid:2198.18 collect_output:92.44
2023-02-21 23:35:18 - train.py[line:549] - INFO: 4000 / 6234
2023-02-21 23:35:18 - train.py[line:551] - INFO: load:1.36 valid_run:2430.28 task_valid:2314.18 collect_output:95.83
2023-02-21 23:37:19 - train.py[line:549] - INFO: 4200 / 6234
2023-02-21 23:37:19 - train.py[line:551] - INFO: load:1.39 valid_run:2552.15 task_valid:2430.00 collect_output:100.86
2023-02-21 23:39:21 - train.py[line:549] - INFO: 4400 / 6234
2023-02-21 23:39:21 - train.py[line:551] - INFO: load:1.41 valid_run:2674.07 task_valid:2548.19 collect_output:103.58
2023-02-21 23:41:22 - train.py[line:549] - INFO: 4600 / 6234
2023-02-21 23:41:22 - train.py[line:551] - INFO: load:1.44 valid_run:2794.71 task_valid:2661.84 collect_output:109.57
2023-02-21 23:43:22 - train.py[line:549] - INFO: 4800 / 6234
2023-02-21 23:43:22 - train.py[line:551] - INFO: load:1.46 valid_run:2914.66 task_valid:2777.42 collect_output:112.94
2023-02-21 23:45:24 - train.py[line:549] - INFO: 5000 / 6234
2023-02-21 23:45:24 - train.py[line:551] - INFO: load:1.49 valid_run:3036.55 task_valid:2892.82 collect_output:118.39
2023-02-21 23:47:27 - train.py[line:549] - INFO: 5200 / 6234
2023-02-21 23:47:27 - train.py[line:551] - INFO: load:1.51 valid_run:3159.75 task_valid:3008.24 collect_output:125.15
2023-02-21 23:49:27 - train.py[line:549] - INFO: 5400 / 6234
2023-02-21 23:49:27 - train.py[line:551] - INFO: load:1.54 valid_run:3279.50 task_valid:3121.85 collect_output:130.27
2023-02-21 23:51:29 - train.py[line:549] - INFO: 5600 / 6234
2023-02-21 23:51:29 - train.py[line:551] - INFO: load:1.56 valid_run:3401.29 task_valid:3240.53 collect_output:132.36
2023-02-21 23:53:31 - train.py[line:549] - INFO: 5800 / 6234
2023-02-21 23:53:31 - train.py[line:551] - INFO: load:1.59 valid_run:3523.23 task_valid:3355.53 collect_output:138.28
2023-02-21 23:55:33 - train.py[line:549] - INFO: 6000 / 6234
2023-02-21 23:55:33 - train.py[line:551] - INFO: load:1.62 valid_run:3645.03 task_valid:3473.34 collect_output:141.23
2023-02-21 23:57:34 - train.py[line:549] - INFO: 6200 / 6234
2023-02-21 23:57:34 - train.py[line:551] - INFO: load:1.64 valid_run:3766.13 task_valid:3591.13 collect_output:143.51

====================================================================================================
SGG eval:     R @ 50: 0.6481;     R @ 100: 0.6686;     R @ 500: 0.6913;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4220;    mR @ 100: 0.4423;    mR @ 500: 0.4629;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8229) (playing:0.0000) (riding:0.9461) (says:0.0000) (sitting on:0.7188) (standing on:0.5580) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================

2023-02-21 23:58:05 - train.py[line:487] - INFO: 0.668626330532213

====================================================================================================
SGG eval:     R @ 50: 0.6481;     R @ 100: 0.6686;     R @ 500: 0.6913;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4220;    mR @ 100: 0.4423;    mR @ 500: 0.4629;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8229) (playing:0.0000) (riding:0.9461) (says:0.0000) (sitting on:0.7188) (standing on:0.5580) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================

2023-02-21 23:58:05 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-21 23:58:05 - progress_bar.py[line:282] - INFO: epoch 005 | valid on 'valid' subset | loss 0.238 | loss_v1 0 | loss_v2 0 | nll_loss 0.065 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.668626 | ppl 1.05 | vqa_score 0.5653 | wps 118.1 | wpb 72 | bsz 24 | num_updates 2000 | best_R@100 0.668626
2023-02-21 23:58:05 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 5 @ 2000 updates
2023-02-21 23:58:05 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_5_2000.pt
2023-02-21 23:58:11 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_5_2000.pt
2023-02-21 23:58:16 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_5_2000.pt (epoch 5 @ 2000 updates, score 0.668626330532213) (writing took 10.552574349567294 seconds)
2023-02-21 23:58:27 - progress_bar.py[line:274] - INFO: epoch 005:    362 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=110.8, bsz=40, num_updates=2010, lr=4.70014e-05, gnorm=1.074, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9968
2023-02-21 23:58:38 - progress_bar.py[line:274] - INFO: epoch 005:    372 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=2020, lr=4.69761e-05, gnorm=0.915, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9979
2023-02-21 23:58:49 - progress_bar.py[line:274] - INFO: epoch 005:    382 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=2030, lr=4.69508e-05, gnorm=0.8, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9990
2023-02-21 23:59:00 - progress_bar.py[line:274] - INFO: epoch 005:    392 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=2040, lr=4.69256e-05, gnorm=0.852, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10001
2023-02-21 23:59:12 - progress_bar.py[line:274] - INFO: epoch 005:    402 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=2050, lr=4.69003e-05, gnorm=0.736, clip=20, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10013
2023-02-21 23:59:23 - progress_bar.py[line:274] - INFO: epoch 005:    412 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=2060, lr=4.6875e-05, gnorm=0.668, clip=20, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10024
2023-02-21 23:59:23 - train.py[line:339] - INFO: end of epoch 5 (average epoch stats below)
2023-02-21 23:59:23 - progress_bar.py[line:282] - INFO: epoch 005 | loss 0.254 | loss_v1 0 | loss_v2 0 | nll_loss 0.069 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 2060 | lr 4.6875e-05 | gnorm 0.686 | clip 19.2 | loss_scale 2048 | train_wall 459 | gb_free 10.7 | ema_decay 0.9999 | wall 10024
2023-02-21 23:59:23 - trainer.py[line:694] - INFO: loading train data for epoch 6
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 23:59:23 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 23:59:24 - trainer.py[line:758] - INFO: begin training epoch 6
2023-02-21 23:59:24 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 23:59:36 - progress_bar.py[line:274] - INFO: epoch 006:     10 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=83.8, ups=0.75, wpb=112, bsz=40, num_updates=2070, lr=4.68497e-05, gnorm=0.711, clip=10, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10037
2023-02-21 23:59:48 - progress_bar.py[line:274] - INFO: epoch 006:     20 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=110.3, bsz=40, num_updates=2080, lr=4.68244e-05, gnorm=0.527, clip=10, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10049
2023-02-21 23:59:59 - progress_bar.py[line:274] - INFO: epoch 006:     30 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=2090, lr=4.67992e-05, gnorm=0.512, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10060
2023-02-22 00:00:10 - progress_bar.py[line:274] - INFO: epoch 006:     40 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.94, wpb=111.3, bsz=40, num_updates=2100, lr=4.67739e-05, gnorm=0.496, clip=10, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10071
2023-02-22 00:00:12 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 00:00:22 - progress_bar.py[line:274] - INFO: epoch 006:     51 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=92.8, ups=0.83, wpb=112.4, bsz=40, num_updates=2110, lr=4.67486e-05, gnorm=0.566, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=10083
2023-02-22 00:00:33 - progress_bar.py[line:274] - INFO: epoch 006:     61 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=111.9, bsz=40, num_updates=2120, lr=4.67233e-05, gnorm=0.714, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10094
2023-02-22 00:00:44 - progress_bar.py[line:274] - INFO: epoch 006:     71 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=2130, lr=4.6698e-05, gnorm=0.66, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10105
2023-02-22 00:00:56 - progress_bar.py[line:274] - INFO: epoch 006:     81 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.7, ups=0.87, wpb=109.5, bsz=40, num_updates=2140, lr=4.66727e-05, gnorm=0.7, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10117
2023-02-22 00:01:07 - progress_bar.py[line:274] - INFO: epoch 006:     91 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=110.4, bsz=40, num_updates=2150, lr=4.66475e-05, gnorm=0.715, clip=20, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10128
2023-02-22 00:01:18 - progress_bar.py[line:274] - INFO: epoch 006:    101 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=2160, lr=4.66222e-05, gnorm=0.472, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10139
2023-02-22 00:01:29 - progress_bar.py[line:274] - INFO: epoch 006:    111 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.5, bsz=40, num_updates=2170, lr=4.65969e-05, gnorm=0.616, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10150
2023-02-22 00:01:41 - progress_bar.py[line:274] - INFO: epoch 006:    121 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=112.8, bsz=40, num_updates=2180, lr=4.65716e-05, gnorm=0.729, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10162
2023-02-22 00:01:52 - progress_bar.py[line:274] - INFO: epoch 006:    131 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=2190, lr=4.65463e-05, gnorm=0.791, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10173
2023-02-22 00:02:03 - progress_bar.py[line:274] - INFO: epoch 006:    141 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=2200, lr=4.6521e-05, gnorm=0.484, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10184
2023-02-22 00:02:15 - progress_bar.py[line:274] - INFO: epoch 006:    151 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.88, wpb=110.7, bsz=40, num_updates=2210, lr=4.64958e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10196
2023-02-22 00:02:26 - progress_bar.py[line:274] - INFO: epoch 006:    161 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=111.7, bsz=40, num_updates=2220, lr=4.64705e-05, gnorm=0.606, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10207
2023-02-22 00:02:37 - progress_bar.py[line:274] - INFO: epoch 006:    171 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.91, wpb=110.7, bsz=40, num_updates=2230, lr=4.64452e-05, gnorm=0.507, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10218
2023-02-22 00:02:48 - progress_bar.py[line:274] - INFO: epoch 006:    181 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=111.7, bsz=40, num_updates=2240, lr=4.64199e-05, gnorm=0.597, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10229
2023-02-22 00:03:00 - progress_bar.py[line:274] - INFO: epoch 006:    191 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.8, ups=0.88, wpb=109.2, bsz=40, num_updates=2250, lr=4.63946e-05, gnorm=0.601, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10241
2023-02-22 00:03:11 - progress_bar.py[line:274] - INFO: epoch 006:    201 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=2260, lr=4.63693e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10252
2023-02-22 00:03:22 - progress_bar.py[line:274] - INFO: epoch 006:    211 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.89, wpb=110.9, bsz=40, num_updates=2270, lr=4.63441e-05, gnorm=0.364, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10263
2023-02-22 00:03:33 - progress_bar.py[line:274] - INFO: epoch 006:    221 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.8, bsz=40, num_updates=2280, lr=4.63188e-05, gnorm=0.593, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10274
2023-02-22 00:03:45 - progress_bar.py[line:274] - INFO: epoch 006:    231 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.6, bsz=40, num_updates=2290, lr=4.62935e-05, gnorm=0.541, clip=20, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10286
2023-02-22 00:03:56 - progress_bar.py[line:274] - INFO: epoch 006:    241 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.8, bsz=40, num_updates=2300, lr=4.62682e-05, gnorm=0.594, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10297
2023-02-22 00:04:07 - progress_bar.py[line:274] - INFO: epoch 006:    251 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.89, wpb=109.7, bsz=40, num_updates=2310, lr=4.62429e-05, gnorm=0.769, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10308
2023-02-22 00:04:18 - progress_bar.py[line:274] - INFO: epoch 006:    261 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.91, wpb=110.9, bsz=40, num_updates=2320, lr=4.62176e-05, gnorm=0.452, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10319
2023-02-22 00:04:29 - progress_bar.py[line:274] - INFO: epoch 006:    271 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.9, wpb=110.6, bsz=40, num_updates=2330, lr=4.61924e-05, gnorm=0.475, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10330
2023-02-22 00:04:41 - progress_bar.py[line:274] - INFO: epoch 006:    281 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=2340, lr=4.61671e-05, gnorm=0.565, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10342
2023-02-22 00:04:52 - progress_bar.py[line:274] - INFO: epoch 006:    291 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=111.2, bsz=40, num_updates=2350, lr=4.61418e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10353
2023-02-22 00:05:03 - progress_bar.py[line:274] - INFO: epoch 006:    301 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=2360, lr=4.61165e-05, gnorm=0.671, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10364
2023-02-22 00:05:14 - progress_bar.py[line:274] - INFO: epoch 006:    311 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111.6, bsz=40, num_updates=2370, lr=4.60912e-05, gnorm=0.552, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10375
2023-02-22 00:05:26 - progress_bar.py[line:274] - INFO: epoch 006:    321 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=2380, lr=4.60659e-05, gnorm=0.5, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10387
2023-02-22 00:05:37 - progress_bar.py[line:274] - INFO: epoch 006:    331 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=2390, lr=4.60407e-05, gnorm=0.493, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10398
2023-02-22 00:05:48 - progress_bar.py[line:274] - INFO: epoch 006:    341 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.91, wpb=110, bsz=40, num_updates=2400, lr=4.60154e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10409
2023-02-22 00:05:59 - progress_bar.py[line:274] - INFO: epoch 006:    351 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.4, ups=0.88, wpb=109.6, bsz=40, num_updates=2410, lr=4.59901e-05, gnorm=0.846, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10420
2023-02-22 00:06:10 - progress_bar.py[line:274] - INFO: epoch 006:    361 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=2420, lr=4.59648e-05, gnorm=0.681, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10431
2023-02-22 00:06:21 - progress_bar.py[line:274] - INFO: epoch 006:    371 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.93, wpb=111.4, bsz=40, num_updates=2430, lr=4.59395e-05, gnorm=0.554, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10442
2023-02-22 00:06:33 - progress_bar.py[line:274] - INFO: epoch 006:    381 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=112.1, bsz=40, num_updates=2440, lr=4.59142e-05, gnorm=0.524, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10454
2023-02-22 00:06:44 - progress_bar.py[line:274] - INFO: epoch 006:    391 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=2450, lr=4.5889e-05, gnorm=0.484, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10465
2023-02-22 00:06:56 - progress_bar.py[line:274] - INFO: epoch 006:    401 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=2460, lr=4.58637e-05, gnorm=0.508, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10476
2023-02-22 00:07:07 - progress_bar.py[line:274] - INFO: epoch 006:    411 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=2470, lr=4.58384e-05, gnorm=0.726, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10488
2023-02-22 00:07:08 - train.py[line:339] - INFO: end of epoch 6 (average epoch stats below)
2023-02-22 00:07:08 - progress_bar.py[line:282] - INFO: epoch 006 | loss 0.248 | loss_v1 0 | loss_v2 0 | nll_loss 0.062 | ntokens 111.129 | nsentences 40 | sample_size 111.129 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.3 | ups 0.88 | wpb 111.1 | bsz 40 | num_updates 2471 | lr 4.58359e-05 | gnorm 0.573 | clip 12.2 | loss_scale 1024 | train_wall 459 | gb_free 10 | ema_decay 0.9999 | wall 10489
2023-02-22 00:07:08 - trainer.py[line:694] - INFO: loading train data for epoch 7
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 00:07:08 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 00:07:08 - trainer.py[line:758] - INFO: begin training epoch 7
2023-02-22 00:07:08 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 00:07:20 - progress_bar.py[line:274] - INFO: epoch 007:      9 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=84, ups=0.74, wpb=112.8, bsz=40, num_updates=2480, lr=4.58131e-05, gnorm=0.537, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10501
2023-02-22 00:07:23 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 00:07:32 - progress_bar.py[line:274] - INFO: epoch 007:     20 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=93.5, ups=0.83, wpb=112.5, bsz=40, num_updates=2490, lr=4.57878e-05, gnorm=0.565, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=10513
2023-02-22 00:07:44 - progress_bar.py[line:274] - INFO: epoch 007:     30 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=110.5, bsz=40, num_updates=2500, lr=4.57625e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10524
2023-02-22 00:07:55 - progress_bar.py[line:274] - INFO: epoch 007:     40 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.3, bsz=40, num_updates=2510, lr=4.57373e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=10536
2023-02-22 00:08:06 - progress_bar.py[line:274] - INFO: epoch 007:     50 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.89, wpb=111.1, bsz=40, num_updates=2520, lr=4.5712e-05, gnorm=0.51, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10547
2023-02-22 00:08:17 - progress_bar.py[line:274] - INFO: epoch 007:     60 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=2530, lr=4.56867e-05, gnorm=0.45, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=10558
2023-02-22 00:08:29 - progress_bar.py[line:274] - INFO: epoch 007:     70 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.9, wpb=109.8, bsz=40, num_updates=2540, lr=4.56614e-05, gnorm=0.448, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10569
2023-02-22 00:08:40 - progress_bar.py[line:274] - INFO: epoch 007:     80 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=2550, lr=4.56361e-05, gnorm=0.429, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=10581
2023-02-22 00:08:51 - progress_bar.py[line:274] - INFO: epoch 007:     90 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=2560, lr=4.56108e-05, gnorm=0.374, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10592
2023-02-22 00:09:02 - progress_bar.py[line:274] - INFO: epoch 007:    100 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.6, ups=0.88, wpb=110.2, bsz=40, num_updates=2570, lr=4.55856e-05, gnorm=0.372, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10603
2023-02-22 00:09:14 - progress_bar.py[line:274] - INFO: epoch 007:    110 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=2580, lr=4.55603e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10614
2023-02-22 00:09:25 - progress_bar.py[line:274] - INFO: epoch 007:    120 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=2590, lr=4.5535e-05, gnorm=0.603, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10626
2023-02-22 00:09:36 - progress_bar.py[line:274] - INFO: epoch 007:    130 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=110.6, bsz=40, num_updates=2600, lr=4.55097e-05, gnorm=0.552, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10637
2023-02-22 00:09:47 - progress_bar.py[line:274] - INFO: epoch 007:    140 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=111.6, bsz=40, num_updates=2610, lr=4.54844e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10648
2023-02-22 00:09:58 - progress_bar.py[line:274] - INFO: epoch 007:    150 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=2620, lr=4.54591e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10659
2023-02-22 00:10:09 - progress_bar.py[line:274] - INFO: epoch 007:    160 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=2630, lr=4.54339e-05, gnorm=0.479, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10670
2023-02-22 00:10:20 - progress_bar.py[line:274] - INFO: epoch 007:    170 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=2640, lr=4.54086e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10681
2023-02-22 00:10:31 - progress_bar.py[line:274] - INFO: epoch 007:    180 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.3, bsz=40, num_updates=2650, lr=4.53833e-05, gnorm=0.513, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10692
2023-02-22 00:10:43 - progress_bar.py[line:274] - INFO: epoch 007:    190 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=2660, lr=4.5358e-05, gnorm=0.603, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10703
2023-02-22 00:10:54 - progress_bar.py[line:274] - INFO: epoch 007:    200 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.5, bsz=40, num_updates=2670, lr=4.53327e-05, gnorm=0.341, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10715
2023-02-22 00:11:04 - progress_bar.py[line:274] - INFO: epoch 007:    210 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.92, wpb=110.6, bsz=40, num_updates=2680, lr=4.53074e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10725
2023-02-22 00:11:16 - progress_bar.py[line:274] - INFO: epoch 007:    220 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.91, wpb=110.9, bsz=40, num_updates=2690, lr=4.52822e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10736
2023-02-22 00:11:27 - progress_bar.py[line:274] - INFO: epoch 007:    230 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.4, ups=0.87, wpb=109.9, bsz=40, num_updates=2700, lr=4.52569e-05, gnorm=0.584, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10748
2023-02-22 00:11:38 - progress_bar.py[line:274] - INFO: epoch 007:    240 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.89, wpb=109.5, bsz=40, num_updates=2710, lr=4.52316e-05, gnorm=0.595, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10759
2023-02-22 00:11:50 - progress_bar.py[line:274] - INFO: epoch 007:    250 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.2, ups=0.89, wpb=108.7, bsz=40, num_updates=2720, lr=4.52063e-05, gnorm=0.633, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10770
2023-02-22 00:12:00 - progress_bar.py[line:274] - INFO: epoch 007:    260 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.92, wpb=110.6, bsz=40, num_updates=2730, lr=4.5181e-05, gnorm=0.374, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10781
2023-02-22 00:12:12 - progress_bar.py[line:274] - INFO: epoch 007:    270 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=2740, lr=4.51557e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10793
2023-02-22 00:12:23 - progress_bar.py[line:274] - INFO: epoch 007:    280 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=2750, lr=4.51305e-05, gnorm=0.476, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10804
2023-02-22 00:12:34 - progress_bar.py[line:274] - INFO: epoch 007:    290 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=2760, lr=4.51052e-05, gnorm=0.513, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10815
2023-02-22 00:12:46 - progress_bar.py[line:274] - INFO: epoch 007:    300 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.88, wpb=111.6, bsz=40, num_updates=2770, lr=4.50799e-05, gnorm=0.443, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10827
2023-02-22 00:12:56 - progress_bar.py[line:274] - INFO: epoch 007:    310 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.2, ups=0.94, wpb=112.2, bsz=40, num_updates=2780, lr=4.50546e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10837
2023-02-22 00:13:08 - progress_bar.py[line:274] - INFO: epoch 007:    320 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=111.4, bsz=40, num_updates=2790, lr=4.50293e-05, gnorm=0.631, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10849
2023-02-22 00:13:19 - progress_bar.py[line:274] - INFO: epoch 007:    330 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.8, ups=0.88, wpb=110.3, bsz=40, num_updates=2800, lr=4.5004e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10860
2023-02-22 00:13:30 - progress_bar.py[line:274] - INFO: epoch 007:    340 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.7, bsz=40, num_updates=2810, lr=4.49788e-05, gnorm=0.584, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10871
2023-02-22 00:13:41 - progress_bar.py[line:274] - INFO: epoch 007:    350 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=110.8, bsz=40, num_updates=2820, lr=4.49535e-05, gnorm=0.466, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10882
2023-02-22 00:13:53 - progress_bar.py[line:274] - INFO: epoch 007:    360 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.88, wpb=112, bsz=40, num_updates=2830, lr=4.49282e-05, gnorm=0.502, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10894
2023-02-22 00:14:04 - progress_bar.py[line:274] - INFO: epoch 007:    370 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.7, ups=0.87, wpb=111.4, bsz=40, num_updates=2840, lr=4.49029e-05, gnorm=0.501, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10905
2023-02-22 00:14:16 - progress_bar.py[line:274] - INFO: epoch 007:    380 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.7, ups=0.88, wpb=110, bsz=40, num_updates=2850, lr=4.48776e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10916
2023-02-22 00:14:27 - progress_bar.py[line:274] - INFO: epoch 007:    390 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=113, bsz=40, num_updates=2860, lr=4.48523e-05, gnorm=0.647, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10928
2023-02-22 00:14:38 - progress_bar.py[line:274] - INFO: epoch 007:    400 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105, ups=0.94, wpb=111.8, bsz=40, num_updates=2870, lr=4.48271e-05, gnorm=0.889, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10939
2023-02-22 00:14:49 - progress_bar.py[line:274] - INFO: epoch 007:    410 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=2880, lr=4.48018e-05, gnorm=0.615, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10950
2023-02-22 00:14:51 - train.py[line:339] - INFO: end of epoch 7 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 00:14:51 - progress_bar.py[line:282] - INFO: epoch 007 | loss 0.247 | loss_v1 0 | loss_v2 0 | nll_loss 0.062 | ntokens 111.139 | nsentences 40 | sample_size 111.139 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.6 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 2882 | lr 4.47967e-05 | gnorm 0.538 | clip 9.5 | loss_scale 512 | train_wall 459 | gb_free 11.1 | ema_decay 0.9999 | wall 10952
2023-02-22 00:14:51 - trainer.py[line:694] - INFO: loading train data for epoch 8
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 00:14:51 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 00:14:52 - trainer.py[line:758] - INFO: begin training epoch 8
2023-02-22 00:14:52 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 00:15:03 - progress_bar.py[line:274] - INFO: epoch 008:      8 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=81, ups=0.73, wpb=111.3, bsz=40, num_updates=2890, lr=4.47765e-05, gnorm=0.321, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=10964
2023-02-22 00:15:14 - progress_bar.py[line:274] - INFO: epoch 008:     18 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.9, wpb=111.4, bsz=40, num_updates=2900, lr=4.47512e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10975
2023-02-22 00:15:25 - progress_bar.py[line:274] - INFO: epoch 008:     28 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=2910, lr=4.47259e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10986
2023-02-22 00:15:36 - progress_bar.py[line:274] - INFO: epoch 008:     38 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.88, wpb=110.4, bsz=40, num_updates=2920, lr=4.47006e-05, gnorm=0.65, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10997
2023-02-22 00:15:47 - progress_bar.py[line:274] - INFO: epoch 008:     48 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.91, wpb=108.5, bsz=40, num_updates=2930, lr=4.46754e-05, gnorm=0.604, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11008
2023-02-22 00:15:59 - progress_bar.py[line:274] - INFO: epoch 008:     58 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.89, wpb=110.5, bsz=40, num_updates=2940, lr=4.46501e-05, gnorm=0.632, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=11020
2023-02-22 00:16:10 - progress_bar.py[line:274] - INFO: epoch 008:     68 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=111.3, bsz=40, num_updates=2950, lr=4.46248e-05, gnorm=0.392, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11031
2023-02-22 00:16:21 - progress_bar.py[line:274] - INFO: epoch 008:     78 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.7, bsz=40, num_updates=2960, lr=4.45995e-05, gnorm=0.363, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=11042
2023-02-22 00:16:32 - progress_bar.py[line:274] - INFO: epoch 008:     88 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.1, bsz=40, num_updates=2970, lr=4.45742e-05, gnorm=0.509, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11053
2023-02-22 00:16:43 - progress_bar.py[line:274] - INFO: epoch 008:     98 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=111.6, bsz=40, num_updates=2980, lr=4.45489e-05, gnorm=0.558, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=11064
2023-02-22 00:16:54 - progress_bar.py[line:274] - INFO: epoch 008:    108 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=2990, lr=4.45237e-05, gnorm=0.434, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=11075
2023-02-22 00:17:06 - progress_bar.py[line:274] - INFO: epoch 008:    118 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=111.8, bsz=40, num_updates=3000, lr=4.44984e-05, gnorm=0.806, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=11087
2023-02-22 00:17:06 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 00:17:07 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 00:17:07 - train.py[line:551] - INFO: load:0.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 00:19:09 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 00:19:09 - train.py[line:551] - INFO: load:0.90 valid_run:122.33 task_valid:118.08 collect_output:3.22
2023-02-22 00:21:09 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 00:21:09 - train.py[line:551] - INFO: load:0.93 valid_run:242.36 task_valid:233.03 collect_output:7.28
2023-02-22 00:23:11 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 00:23:11 - train.py[line:551] - INFO: load:0.95 valid_run:364.55 task_valid:348.62 collect_output:12.86
2023-02-22 00:25:14 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 00:25:14 - train.py[line:551] - INFO: load:0.97 valid_run:486.56 task_valid:461.42 collect_output:21.06
2023-02-22 00:27:14 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 00:27:14 - train.py[line:551] - INFO: load:1.00 valid_run:606.95 task_valid:577.95 collect_output:23.89
2023-02-22 00:29:17 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 00:29:17 - train.py[line:551] - INFO: load:1.03 valid_run:729.64 task_valid:695.88 collect_output:27.63
2023-02-22 00:31:20 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 00:31:20 - train.py[line:551] - INFO: load:1.05 valid_run:852.84 task_valid:813.26 collect_output:32.44
2023-02-22 00:33:22 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 00:33:22 - train.py[line:551] - INFO: load:1.08 valid_run:974.74 task_valid:929.02 collect_output:37.58
2023-02-22 00:35:26 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 00:35:26 - train.py[line:551] - INFO: load:1.10 valid_run:1098.97 task_valid:1045.39 collect_output:44.43
2023-02-22 00:37:28 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 00:37:28 - train.py[line:551] - INFO: load:1.13 valid_run:1221.09 task_valid:1157.28 collect_output:53.65
2023-02-22 00:39:29 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 00:39:29 - train.py[line:551] - INFO: load:1.15 valid_run:1341.56 task_valid:1272.06 collect_output:58.33
2023-02-22 00:41:31 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 00:41:31 - train.py[line:551] - INFO: load:1.18 valid_run:1463.50 task_valid:1388.51 collect_output:62.79
2023-02-22 00:43:30 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 00:43:30 - train.py[line:551] - INFO: load:1.20 valid_run:1582.55 task_valid:1501.74 collect_output:67.58
2023-02-22 00:45:31 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 00:45:31 - train.py[line:551] - INFO: load:1.23 valid_run:1703.55 task_valid:1618.76 collect_output:70.55
2023-02-22 00:47:32 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 00:47:32 - train.py[line:551] - INFO: load:1.25 valid_run:1824.69 task_valid:1734.27 collect_output:75.14
2023-02-22 00:49:34 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 00:49:34 - train.py[line:551] - INFO: load:1.28 valid_run:1945.90 task_valid:1847.44 collect_output:82.14
2023-02-22 00:51:35 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 00:51:35 - train.py[line:551] - INFO: load:1.30 valid_run:2067.20 task_valid:1962.80 collect_output:87.08
2023-02-22 00:53:36 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 00:53:36 - train.py[line:551] - INFO: load:1.33 valid_run:2187.69 task_valid:2080.02 collect_output:89.32
2023-02-22 00:55:37 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 00:55:37 - train.py[line:551] - INFO: load:1.35 valid_run:2309.12 task_valid:2196.37 collect_output:93.39
2023-02-22 00:57:38 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 00:57:38 - train.py[line:551] - INFO: load:1.38 valid_run:2429.66 task_valid:2312.41 collect_output:96.85
2023-02-22 00:59:39 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 00:59:39 - train.py[line:551] - INFO: load:1.41 valid_run:2551.49 task_valid:2428.10 collect_output:101.99
2023-02-22 01:01:41 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 01:01:41 - train.py[line:551] - INFO: load:1.43 valid_run:2673.39 task_valid:2546.09 collect_output:104.90
2023-02-22 01:03:42 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 01:03:42 - train.py[line:551] - INFO: load:1.46 valid_run:2793.82 task_valid:2659.70 collect_output:110.71
2023-02-22 01:05:42 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 01:05:42 - train.py[line:551] - INFO: load:1.48 valid_run:2913.78 task_valid:2775.16 collect_output:114.17
2023-02-22 01:07:44 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 01:07:44 - train.py[line:551] - INFO: load:1.51 valid_run:3035.50 task_valid:2890.58 collect_output:119.45
2023-02-22 01:09:47 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 01:09:47 - train.py[line:551] - INFO: load:1.53 valid_run:3158.81 task_valid:3005.77 collect_output:126.55
2023-02-22 01:11:47 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 01:11:47 - train.py[line:551] - INFO: load:1.56 valid_run:3278.31 task_valid:3119.13 collect_output:131.63
2023-02-22 01:13:48 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 01:13:48 - train.py[line:551] - INFO: load:1.59 valid_run:3399.93 task_valid:3237.76 collect_output:133.61
2023-02-22 01:15:50 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 01:15:50 - train.py[line:551] - INFO: load:1.62 valid_run:3521.97 task_valid:3352.53 collect_output:139.87
2023-02-22 01:17:52 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 01:17:52 - train.py[line:551] - INFO: load:1.64 valid_run:3643.77 task_valid:3470.12 collect_output:143.07
2023-02-22 01:19:53 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 01:19:53 - train.py[line:551] - INFO: load:1.67 valid_run:3764.62 task_valid:3587.70 collect_output:145.31

====================================================================================================
SGG eval:     R @ 50: 0.6565;     R @ 100: 0.6760;     R @ 500: 0.7020;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4328;    mR @ 100: 0.4521;    mR @ 500: 0.4822;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3581) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8646) (playing:0.0000) (riding:0.9461) (says:0.0000) (sitting on:0.7120) (standing on:0.5947) (using:0.6000) (walking in:0.0000) (walking on:0.3423) (watching:0.5903) 
--------------------------------------------------------
====================================================================================================

2023-02-22 01:20:24 - train.py[line:487] - INFO: 0.6759929971988795
2023-02-22 01:20:24 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 01:20:24 - progress_bar.py[line:282] - INFO: epoch 008 | valid on 'valid' subset | loss 0.229 | loss_v1 0 | loss_v2 0 | nll_loss 0.06 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.675993 | ppl 1.04 | vqa_score 0.5631 | wps 118.1 | wpb 72 | bsz 24 | num_updates 3000 | best_R@100 0.675993
2023-02-22 01:20:24 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 8 @ 3000 updates
2023-02-22 01:20:24 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_8_3000.pt

====================================================================================================
SGG eval:     R @ 50: 0.6565;     R @ 100: 0.6760;     R @ 500: 0.7020;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4328;    mR @ 100: 0.4521;    mR @ 500: 0.4822;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3581) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8646) (playing:0.0000) (riding:0.9461) (says:0.0000) (sitting on:0.7120) (standing on:0.5947) (using:0.6000) (walking in:0.0000) (walking on:0.3423) (watching:0.5903) 
--------------------------------------------------------
====================================================================================================

2023-02-22 01:20:30 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_8_3000.pt
2023-02-22 01:20:35 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_8_3000.pt (epoch 8 @ 3000 updates, score 0.6759929971988795) (writing took 10.495506590232253 seconds)
2023-02-22 01:20:46 - progress_bar.py[line:274] - INFO: epoch 008:    128 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=111.5, bsz=40, num_updates=3010, lr=4.44731e-05, gnorm=0.601, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14907
2023-02-22 01:20:58 - progress_bar.py[line:274] - INFO: epoch 008:    138 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=3020, lr=4.44478e-05, gnorm=0.653, clip=20, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14918
2023-02-22 01:21:09 - progress_bar.py[line:274] - INFO: epoch 008:    148 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.9, wpb=110.4, bsz=40, num_updates=3030, lr=4.44225e-05, gnorm=0.658, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14930
2023-02-22 01:21:20 - progress_bar.py[line:274] - INFO: epoch 008:    158 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.89, wpb=110.7, bsz=40, num_updates=3040, lr=4.43972e-05, gnorm=0.67, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14941
2023-02-22 01:21:31 - progress_bar.py[line:274] - INFO: epoch 008:    168 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.4, ups=0.9, wpb=110.7, bsz=40, num_updates=3050, lr=4.4372e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14952
2023-02-22 01:21:42 - progress_bar.py[line:274] - INFO: epoch 008:    178 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.91, wpb=111.4, bsz=40, num_updates=3060, lr=4.43467e-05, gnorm=0.448, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14963
2023-02-22 01:21:53 - progress_bar.py[line:274] - INFO: epoch 008:    188 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=3070, lr=4.43214e-05, gnorm=0.797, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14974
2023-02-22 01:22:05 - progress_bar.py[line:274] - INFO: epoch 008:    198 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=3080, lr=4.42961e-05, gnorm=0.441, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14985
2023-02-22 01:22:16 - progress_bar.py[line:274] - INFO: epoch 008:    208 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.91, wpb=110.9, bsz=40, num_updates=3090, lr=4.42708e-05, gnorm=0.671, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14996
2023-02-22 01:22:26 - progress_bar.py[line:274] - INFO: epoch 008:    218 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.92, wpb=110.3, bsz=40, num_updates=3100, lr=4.42456e-05, gnorm=0.581, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=15007
2023-02-22 01:22:38 - progress_bar.py[line:274] - INFO: epoch 008:    228 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.9, wpb=111.6, bsz=40, num_updates=3110, lr=4.42203e-05, gnorm=0.642, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15019
2023-02-22 01:22:49 - progress_bar.py[line:274] - INFO: epoch 008:    238 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=111.8, bsz=40, num_updates=3120, lr=4.4195e-05, gnorm=0.759, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15030
2023-02-22 01:23:00 - progress_bar.py[line:274] - INFO: epoch 008:    248 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.2, bsz=40, num_updates=3130, lr=4.41697e-05, gnorm=0.869, clip=30, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15041
2023-02-22 01:23:11 - progress_bar.py[line:274] - INFO: epoch 008:    258 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.4, bsz=40, num_updates=3140, lr=4.41444e-05, gnorm=0.74, clip=30, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=15052
2023-02-22 01:23:22 - progress_bar.py[line:274] - INFO: epoch 008:    268 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.93, wpb=110.4, bsz=40, num_updates=3150, lr=4.41191e-05, gnorm=0.536, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15063
2023-02-22 01:23:33 - progress_bar.py[line:274] - INFO: epoch 008:    278 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.1, bsz=40, num_updates=3160, lr=4.40939e-05, gnorm=0.449, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15074
2023-02-22 01:23:44 - progress_bar.py[line:274] - INFO: epoch 008:    288 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.92, wpb=110.9, bsz=40, num_updates=3170, lr=4.40686e-05, gnorm=0.757, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15085
2023-02-22 01:23:55 - progress_bar.py[line:274] - INFO: epoch 008:    298 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=3180, lr=4.40433e-05, gnorm=0.538, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15096
2023-02-22 01:24:06 - progress_bar.py[line:274] - INFO: epoch 008:    308 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.93, wpb=111.5, bsz=40, num_updates=3190, lr=4.4018e-05, gnorm=0.456, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15107
2023-02-22 01:24:17 - progress_bar.py[line:274] - INFO: epoch 008:    318 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.9, ups=0.87, wpb=110.7, bsz=40, num_updates=3200, lr=4.39927e-05, gnorm=0.469, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15118
2023-02-22 01:24:29 - progress_bar.py[line:274] - INFO: epoch 008:    328 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.4, bsz=40, num_updates=3210, lr=4.39674e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15130
2023-02-22 01:24:40 - progress_bar.py[line:274] - INFO: epoch 008:    338 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=3220, lr=4.39422e-05, gnorm=0.534, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15141
2023-02-22 01:24:51 - progress_bar.py[line:274] - INFO: epoch 008:    348 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=111, bsz=40, num_updates=3230, lr=4.39169e-05, gnorm=0.779, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15152
2023-02-22 01:25:02 - progress_bar.py[line:274] - INFO: epoch 008:    358 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.91, wpb=110.3, bsz=40, num_updates=3240, lr=4.38916e-05, gnorm=0.591, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15163
2023-02-22 01:25:14 - progress_bar.py[line:274] - INFO: epoch 008:    368 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.2, ups=0.87, wpb=110.4, bsz=40, num_updates=3250, lr=4.38663e-05, gnorm=0.592, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15175
2023-02-22 01:25:25 - progress_bar.py[line:274] - INFO: epoch 008:    378 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=3260, lr=4.3841e-05, gnorm=0.573, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=15186
2023-02-22 01:25:36 - progress_bar.py[line:274] - INFO: epoch 008:    388 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.91, wpb=111.3, bsz=40, num_updates=3270, lr=4.38157e-05, gnorm=0.557, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15197
2023-02-22 01:25:48 - progress_bar.py[line:274] - INFO: epoch 008:    398 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=111.6, bsz=40, num_updates=3280, lr=4.37905e-05, gnorm=0.45, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15208
2023-02-22 01:25:59 - progress_bar.py[line:274] - INFO: epoch 008:    408 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=3290, lr=4.37652e-05, gnorm=0.569, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15220
2023-02-22 01:26:03 - train.py[line:339] - INFO: end of epoch 8 (average epoch stats below)
2023-02-22 01:26:03 - progress_bar.py[line:282] - INFO: epoch 008 | loss 0.245 | loss_v1 0 | loss_v2 0 | nll_loss 0.06 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 3294 | lr 4.37551e-05 | gnorm 0.568 | clip 14.3 | loss_scale 1024 | train_wall 458 | gb_free 10.6 | ema_decay 0.9999 | wall 15224
2023-02-22 01:26:03 - trainer.py[line:694] - INFO: loading train data for epoch 9
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 01:26:03 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 01:26:04 - trainer.py[line:758] - INFO: begin training epoch 9
2023-02-22 01:26:04 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 01:26:12 - progress_bar.py[line:274] - INFO: epoch 009:      6 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=85.5, ups=0.76, wpb=113.1, bsz=40, num_updates=3300, lr=4.37399e-05, gnorm=0.608, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15233
2023-02-22 01:26:23 - progress_bar.py[line:274] - INFO: epoch 009:     16 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.9, wpb=110.1, bsz=40, num_updates=3310, lr=4.37146e-05, gnorm=0.397, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15244
2023-02-22 01:26:34 - progress_bar.py[line:274] - INFO: epoch 009:     26 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=111.5, bsz=40, num_updates=3320, lr=4.36893e-05, gnorm=0.424, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15255
2023-02-22 01:26:46 - progress_bar.py[line:274] - INFO: epoch 009:     36 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.6, bsz=40, num_updates=3330, lr=4.3664e-05, gnorm=0.554, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15267
2023-02-22 01:26:57 - progress_bar.py[line:274] - INFO: epoch 009:     46 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.5, bsz=40, num_updates=3340, lr=4.36388e-05, gnorm=0.493, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15278
2023-02-22 01:27:08 - progress_bar.py[line:274] - INFO: epoch 009:     56 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=3350, lr=4.36135e-05, gnorm=0.65, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15289
2023-02-22 01:27:19 - progress_bar.py[line:274] - INFO: epoch 009:     66 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.89, wpb=111.2, bsz=40, num_updates=3360, lr=4.35882e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15300
2023-02-22 01:27:30 - progress_bar.py[line:274] - INFO: epoch 009:     76 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.2, bsz=40, num_updates=3370, lr=4.35629e-05, gnorm=0.351, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15311
2023-02-22 01:27:42 - progress_bar.py[line:274] - INFO: epoch 009:     86 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=3380, lr=4.35376e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15323
2023-02-22 01:27:53 - progress_bar.py[line:274] - INFO: epoch 009:     96 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.87, wpb=112.1, bsz=40, num_updates=3390, lr=4.35123e-05, gnorm=0.352, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15334
2023-02-22 01:28:04 - progress_bar.py[line:274] - INFO: epoch 009:    106 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=111.8, bsz=40, num_updates=3400, lr=4.34871e-05, gnorm=0.412, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15345
2023-02-22 01:28:16 - progress_bar.py[line:274] - INFO: epoch 009:    116 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.88, wpb=110.3, bsz=40, num_updates=3410, lr=4.34618e-05, gnorm=0.418, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15357
2023-02-22 01:28:27 - progress_bar.py[line:274] - INFO: epoch 009:    126 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.92, wpb=110.2, bsz=40, num_updates=3420, lr=4.34365e-05, gnorm=0.845, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15368
2023-02-22 01:28:37 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 01:28:39 - progress_bar.py[line:274] - INFO: epoch 009:    137 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=90, ups=0.8, wpb=112.3, bsz=40, num_updates=3430, lr=4.34112e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15380
2023-02-22 01:28:50 - progress_bar.py[line:274] - INFO: epoch 009:    147 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.5, ups=0.93, wpb=112.3, bsz=40, num_updates=3440, lr=4.33859e-05, gnorm=0.575, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15391
2023-02-22 01:29:01 - progress_bar.py[line:274] - INFO: epoch 009:    157 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=3450, lr=4.33606e-05, gnorm=0.494, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15402
2023-02-22 01:29:12 - progress_bar.py[line:274] - INFO: epoch 009:    167 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=3460, lr=4.33354e-05, gnorm=0.425, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15413
2023-02-22 01:29:24 - progress_bar.py[line:274] - INFO: epoch 009:    177 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.9, wpb=111.7, bsz=40, num_updates=3470, lr=4.33101e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15425
2023-02-22 01:29:34 - progress_bar.py[line:274] - INFO: epoch 009:    187 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.93, wpb=112.7, bsz=40, num_updates=3480, lr=4.32848e-05, gnorm=0.648, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15435
2023-02-22 01:29:46 - progress_bar.py[line:274] - INFO: epoch 009:    197 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=111.1, bsz=40, num_updates=3490, lr=4.32595e-05, gnorm=0.907, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15447
2023-02-22 01:29:57 - progress_bar.py[line:274] - INFO: epoch 009:    207 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.9, wpb=111, bsz=40, num_updates=3500, lr=4.32342e-05, gnorm=0.393, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15458
2023-02-22 01:30:09 - progress_bar.py[line:274] - INFO: epoch 009:    217 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.88, wpb=110.5, bsz=40, num_updates=3510, lr=4.32089e-05, gnorm=0.49, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=15469
2023-02-22 01:30:20 - progress_bar.py[line:274] - INFO: epoch 009:    227 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=3520, lr=4.31837e-05, gnorm=0.354, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15481
2023-02-22 01:30:31 - progress_bar.py[line:274] - INFO: epoch 009:    237 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=3530, lr=4.31584e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15492
2023-02-22 01:30:42 - progress_bar.py[line:274] - INFO: epoch 009:    247 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97, ups=0.87, wpb=111.1, bsz=40, num_updates=3540, lr=4.31331e-05, gnorm=0.92, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15503
2023-02-22 01:30:54 - progress_bar.py[line:274] - INFO: epoch 009:    257 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.89, wpb=110.1, bsz=40, num_updates=3550, lr=4.31078e-05, gnorm=0.648, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=15515
2023-02-22 01:31:05 - progress_bar.py[line:274] - INFO: epoch 009:    267 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.7, ups=0.89, wpb=108.8, bsz=40, num_updates=3560, lr=4.30825e-05, gnorm=0.525, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15526
2023-02-22 01:31:16 - progress_bar.py[line:274] - INFO: epoch 009:    277 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=3570, lr=4.30572e-05, gnorm=0.548, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15537
2023-02-22 01:31:27 - progress_bar.py[line:274] - INFO: epoch 009:    287 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=111.8, bsz=40, num_updates=3580, lr=4.3032e-05, gnorm=0.543, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15548
2023-02-22 01:31:38 - progress_bar.py[line:274] - INFO: epoch 009:    297 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.94, wpb=110.4, bsz=40, num_updates=3590, lr=4.30067e-05, gnorm=0.447, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15559
2023-02-22 01:31:49 - progress_bar.py[line:274] - INFO: epoch 009:    307 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.9, wpb=109.5, bsz=40, num_updates=3600, lr=4.29814e-05, gnorm=0.277, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15570
2023-02-22 01:32:01 - progress_bar.py[line:274] - INFO: epoch 009:    317 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.5, ups=0.87, wpb=109.8, bsz=40, num_updates=3610, lr=4.29561e-05, gnorm=0.505, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15582
2023-02-22 01:32:12 - progress_bar.py[line:274] - INFO: epoch 009:    327 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.8, ups=0.87, wpb=110, bsz=40, num_updates=3620, lr=4.29308e-05, gnorm=0.752, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15593
2023-02-22 01:32:24 - progress_bar.py[line:274] - INFO: epoch 009:    337 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.9, wpb=111.5, bsz=40, num_updates=3630, lr=4.29055e-05, gnorm=0.554, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15605
2023-02-22 01:32:35 - progress_bar.py[line:274] - INFO: epoch 009:    347 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=110.6, bsz=40, num_updates=3640, lr=4.28803e-05, gnorm=0.481, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15616
2023-02-22 01:32:46 - progress_bar.py[line:274] - INFO: epoch 009:    357 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=3650, lr=4.2855e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=15627
2023-02-22 01:32:58 - progress_bar.py[line:274] - INFO: epoch 009:    367 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.9, ups=0.87, wpb=110.7, bsz=40, num_updates=3660, lr=4.28297e-05, gnorm=0.766, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15639
2023-02-22 01:33:09 - progress_bar.py[line:274] - INFO: epoch 009:    377 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=111.2, bsz=40, num_updates=3670, lr=4.28044e-05, gnorm=0.64, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15650
2023-02-22 01:33:20 - progress_bar.py[line:274] - INFO: epoch 009:    387 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=3680, lr=4.27791e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15661
2023-02-22 01:33:32 - progress_bar.py[line:274] - INFO: epoch 009:    397 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.7, bsz=40, num_updates=3690, lr=4.27538e-05, gnorm=0.331, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15673
2023-02-22 01:33:43 - progress_bar.py[line:274] - INFO: epoch 009:    407 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.92, wpb=111.6, bsz=40, num_updates=3700, lr=4.27286e-05, gnorm=0.412, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15684
2023-02-22 01:33:48 - train.py[line:339] - INFO: end of epoch 9 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 01:33:48 - progress_bar.py[line:282] - INFO: epoch 009 | loss 0.243 | loss_v1 0 | loss_v2 0 | nll_loss 0.057 | ntokens 111.119 | nsentences 40 | sample_size 111.119 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.2 | ups 0.88 | wpb 111.1 | bsz 40 | num_updates 3705 | lr 4.27159e-05 | gnorm 0.523 | clip 12.2 | loss_scale 512 | train_wall 460 | gb_free 10.6 | ema_decay 0.9999 | wall 15689
2023-02-22 01:33:48 - trainer.py[line:694] - INFO: loading train data for epoch 10
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 01:33:48 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 01:33:49 - trainer.py[line:758] - INFO: begin training epoch 10
2023-02-22 01:33:49 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 01:33:57 - progress_bar.py[line:274] - INFO: epoch 010:      5 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=81.7, ups=0.73, wpb=111.6, bsz=40, num_updates=3710, lr=4.27033e-05, gnorm=0.495, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15697
2023-02-22 01:34:08 - progress_bar.py[line:274] - INFO: epoch 010:     15 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.4, bsz=40, num_updates=3720, lr=4.2678e-05, gnorm=0.351, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15709
2023-02-22 01:34:19 - progress_bar.py[line:274] - INFO: epoch 010:     25 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=3730, lr=4.26527e-05, gnorm=0.757, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15720
2023-02-22 01:34:30 - progress_bar.py[line:274] - INFO: epoch 010:     35 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=3740, lr=4.26274e-05, gnorm=0.458, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15731
2023-02-22 01:34:41 - progress_bar.py[line:274] - INFO: epoch 010:     45 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=3750, lr=4.26021e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15742
2023-02-22 01:34:53 - progress_bar.py[line:274] - INFO: epoch 010:     55 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=3760, lr=4.25769e-05, gnorm=0.542, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15754
2023-02-22 01:35:04 - progress_bar.py[line:274] - INFO: epoch 010:     65 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.89, wpb=109.6, bsz=40, num_updates=3770, lr=4.25516e-05, gnorm=0.471, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15765
2023-02-22 01:35:15 - progress_bar.py[line:274] - INFO: epoch 010:     75 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=3780, lr=4.25263e-05, gnorm=0.363, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15776
2023-02-22 01:35:26 - progress_bar.py[line:274] - INFO: epoch 010:     85 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=3790, lr=4.2501e-05, gnorm=0.657, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15787
2023-02-22 01:35:37 - progress_bar.py[line:274] - INFO: epoch 010:     95 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=3800, lr=4.24757e-05, gnorm=0.502, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15798
2023-02-22 01:35:48 - progress_bar.py[line:274] - INFO: epoch 010:    105 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.92, wpb=110.9, bsz=40, num_updates=3810, lr=4.24504e-05, gnorm=0.519, clip=20, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=15809
2023-02-22 01:35:59 - progress_bar.py[line:274] - INFO: epoch 010:    115 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.2, ups=0.93, wpb=112.4, bsz=40, num_updates=3820, lr=4.24252e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15820
2023-02-22 01:36:10 - progress_bar.py[line:274] - INFO: epoch 010:    125 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=3830, lr=4.23999e-05, gnorm=0.991, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15831
2023-02-22 01:36:21 - progress_bar.py[line:274] - INFO: epoch 010:    135 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=3840, lr=4.23746e-05, gnorm=0.381, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15842
2023-02-22 01:36:33 - progress_bar.py[line:274] - INFO: epoch 010:    145 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.6, bsz=40, num_updates=3850, lr=4.23493e-05, gnorm=0.641, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15854
2023-02-22 01:36:44 - progress_bar.py[line:274] - INFO: epoch 010:    155 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.8, ups=0.88, wpb=111.1, bsz=40, num_updates=3860, lr=4.2324e-05, gnorm=0.37, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15865
2023-02-22 01:36:55 - progress_bar.py[line:274] - INFO: epoch 010:    165 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.92, wpb=111.6, bsz=40, num_updates=3870, lr=4.22987e-05, gnorm=0.587, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15876
2023-02-22 01:37:07 - progress_bar.py[line:274] - INFO: epoch 010:    175 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=3880, lr=4.22735e-05, gnorm=0.489, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15887
2023-02-22 01:37:18 - progress_bar.py[line:274] - INFO: epoch 010:    185 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=3890, lr=4.22482e-05, gnorm=0.38, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15899
2023-02-22 01:37:29 - progress_bar.py[line:274] - INFO: epoch 010:    195 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111.4, bsz=40, num_updates=3900, lr=4.22229e-05, gnorm=0.307, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15910
2023-02-22 01:37:40 - progress_bar.py[line:274] - INFO: epoch 010:    205 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.4, bsz=40, num_updates=3910, lr=4.21976e-05, gnorm=0.42, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15921
2023-02-22 01:37:51 - progress_bar.py[line:274] - INFO: epoch 010:    215 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=3920, lr=4.21723e-05, gnorm=0.365, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15932
2023-02-22 01:38:02 - progress_bar.py[line:274] - INFO: epoch 010:    225 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.2, ups=0.91, wpb=109.6, bsz=40, num_updates=3930, lr=4.2147e-05, gnorm=0.312, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15943
2023-02-22 01:38:14 - progress_bar.py[line:274] - INFO: epoch 010:    235 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=3940, lr=4.21218e-05, gnorm=0.89, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15954
2023-02-22 01:38:25 - progress_bar.py[line:274] - INFO: epoch 010:    245 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.88, wpb=111, bsz=40, num_updates=3950, lr=4.20965e-05, gnorm=0.568, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15966
2023-02-22 01:38:36 - progress_bar.py[line:274] - INFO: epoch 010:    255 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.8, bsz=40, num_updates=3960, lr=4.20712e-05, gnorm=0.693, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15977
2023-02-22 01:38:47 - progress_bar.py[line:274] - INFO: epoch 010:    265 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=112.2, bsz=40, num_updates=3970, lr=4.20459e-05, gnorm=0.569, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15988
2023-02-22 01:38:58 - progress_bar.py[line:274] - INFO: epoch 010:    275 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=3980, lr=4.20206e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15999
2023-02-22 01:39:09 - progress_bar.py[line:274] - INFO: epoch 010:    285 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=3990, lr=4.19953e-05, gnorm=0.624, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=16010
2023-02-22 01:39:21 - progress_bar.py[line:274] - INFO: epoch 010:    295 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.89, wpb=111.8, bsz=40, num_updates=4000, lr=4.19701e-05, gnorm=0.515, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=16022
2023-02-22 01:39:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 01:39:22 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 01:39:22 - train.py[line:551] - INFO: load:0.96 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 01:41:24 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 01:41:24 - train.py[line:551] - INFO: load:0.98 valid_run:122.12 task_valid:117.86 collect_output:3.24
2023-02-22 01:43:24 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 01:43:24 - train.py[line:551] - INFO: load:1.00 valid_run:241.90 task_valid:232.61 collect_output:7.26
2023-02-22 01:45:26 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 01:45:26 - train.py[line:551] - INFO: load:1.03 valid_run:363.89 task_valid:348.07 collect_output:12.77
2023-02-22 01:47:28 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 01:47:28 - train.py[line:551] - INFO: load:1.05 valid_run:485.85 task_valid:460.70 collect_output:21.10
2023-02-22 01:49:28 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 01:49:28 - train.py[line:551] - INFO: load:1.08 valid_run:606.08 task_valid:577.02 collect_output:24.01
2023-02-22 01:51:31 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 01:51:31 - train.py[line:551] - INFO: load:1.10 valid_run:728.55 task_valid:694.77 collect_output:27.73
2023-02-22 01:53:34 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 01:53:34 - train.py[line:551] - INFO: load:1.13 valid_run:851.38 task_valid:811.73 collect_output:32.61
2023-02-22 01:55:36 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 01:55:36 - train.py[line:551] - INFO: load:1.15 valid_run:973.21 task_valid:927.10 collect_output:38.07
2023-02-22 01:57:40 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 01:57:40 - train.py[line:551] - INFO: load:1.18 valid_run:1097.28 task_valid:1043.26 collect_output:45.00
2023-02-22 01:59:42 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 01:59:42 - train.py[line:551] - INFO: load:1.20 valid_run:1219.33 task_valid:1154.84 collect_output:54.46
2023-02-22 02:01:42 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 02:01:42 - train.py[line:551] - INFO: load:1.23 valid_run:1339.37 task_valid:1269.31 collect_output:59.05
2023-02-22 02:03:43 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 02:03:43 - train.py[line:551] - INFO: load:1.25 valid_run:1460.87 task_valid:1385.20 collect_output:63.66
2023-02-22 02:05:42 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 02:05:42 - train.py[line:551] - INFO: load:1.28 valid_run:1579.57 task_valid:1497.78 collect_output:68.78
2023-02-22 02:07:43 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 02:07:43 - train.py[line:551] - INFO: load:1.30 valid_run:1700.19 task_valid:1614.47 collect_output:71.72
2023-02-22 02:09:44 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 02:09:44 - train.py[line:551] - INFO: load:1.33 valid_run:1821.34 task_valid:1729.63 collect_output:76.70
2023-02-22 02:11:45 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 02:11:45 - train.py[line:551] - INFO: load:1.35 valid_run:1942.60 task_valid:1842.46 collect_output:84.10
2023-02-22 02:13:47 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 02:13:47 - train.py[line:551] - INFO: load:1.38 valid_run:2063.63 task_valid:1957.38 collect_output:89.23
2023-02-22 02:15:47 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 02:15:47 - train.py[line:551] - INFO: load:1.40 valid_run:2183.86 task_valid:2074.19 collect_output:91.67
2023-02-22 02:17:48 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 02:17:48 - train.py[line:551] - INFO: load:1.43 valid_run:2304.82 task_valid:2190.16 collect_output:95.66
2023-02-22 02:19:48 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 02:19:48 - train.py[line:551] - INFO: load:1.45 valid_run:2425.04 task_valid:2305.81 collect_output:99.22
2023-02-22 02:21:50 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 02:21:50 - train.py[line:551] - INFO: load:1.48 valid_run:2546.83 task_valid:2421.33 collect_output:104.48
2023-02-22 02:23:52 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 02:23:52 - train.py[line:551] - INFO: load:1.50 valid_run:2668.70 task_valid:2539.30 collect_output:107.37
2023-02-22 02:25:52 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 02:25:52 - train.py[line:551] - INFO: load:1.53 valid_run:2789.03 task_valid:2652.69 collect_output:113.30
2023-02-22 02:27:52 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 02:27:52 - train.py[line:551] - INFO: load:1.55 valid_run:2908.75 task_valid:2767.81 collect_output:116.89
2023-02-22 02:29:54 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 02:29:54 - train.py[line:551] - INFO: load:1.58 valid_run:3030.44 task_valid:2883.09 collect_output:122.30
2023-02-22 02:31:57 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 02:31:57 - train.py[line:551] - INFO: load:1.60 valid_run:3153.77 task_valid:2997.96 collect_output:129.75
2023-02-22 02:33:57 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 02:33:57 - train.py[line:551] - INFO: load:1.63 valid_run:3273.28 task_valid:3111.11 collect_output:135.10
2023-02-22 02:35:58 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 02:35:58 - train.py[line:551] - INFO: load:1.65 valid_run:3394.81 task_valid:3229.79 collect_output:136.94
2023-02-22 02:38:00 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 02:38:00 - train.py[line:551] - INFO: load:1.68 valid_run:3516.67 task_valid:3344.26 collect_output:143.33
2023-02-22 02:40:02 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 02:40:02 - train.py[line:551] - INFO: load:1.71 valid_run:3638.18 task_valid:3461.69 collect_output:146.40
2023-02-22 02:42:03 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 02:42:03 - train.py[line:551] - INFO: load:1.73 valid_run:3758.84 task_valid:3579.18 collect_output:148.56

====================================================================================================
SGG eval:     R @ 50: 0.6597;     R @ 100: 0.6817;     R @ 500: 0.7040;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4487;    mR @ 100: 0.4701;    mR @ 500: 0.4998;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5417) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1562) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8854) (playing:0.0000) (riding:0.9510) (says:0.0000) (sitting on:0.7166) (standing on:0.5847) (using:0.6000) (walking in:0.3333) (walking on:0.3378) (watching:0.5417) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6597;     R @ 100: 0.6817;     R @ 500: 0.7040;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4487;    mR @ 100: 0.4701;    mR @ 500: 0.4998;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5417) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1562) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8854) (playing:0.0000) (riding:0.9510) (says:0.0000) (sitting on:0.7166) (standing on:0.5847) (using:0.6000) (walking in:0.3333) (walking on:0.3378) (watching:0.5417) 
--------------------------------------------------------
====================================================================================================

2023-02-22 02:42:33 - train.py[line:487] - INFO: 0.6816929971988795
2023-02-22 02:42:33 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 02:42:33 - progress_bar.py[line:282] - INFO: epoch 010 | valid on 'valid' subset | loss 0.229 | loss_v1 0 | loss_v2 0 | nll_loss 0.055 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.681693 | ppl 1.04 | vqa_score 0.5586 | wps 118.3 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.681693
2023-02-22 02:42:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 4000 updates
2023-02-22 02:42:33 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_10_4000.pt
2023-02-22 02:42:39 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_10_4000.pt
2023-02-22 02:42:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_10_4000.pt (epoch 10 @ 4000 updates, score 0.6816929971988795) (writing took 10.6439796872437 seconds)
2023-02-22 02:42:55 - progress_bar.py[line:274] - INFO: epoch 010:    305 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=111.2, bsz=40, num_updates=4010, lr=4.19448e-05, gnorm=0.838, clip=50, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19836
2023-02-22 02:43:06 - progress_bar.py[line:274] - INFO: epoch 010:    315 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.93, wpb=110.3, bsz=40, num_updates=4020, lr=4.19195e-05, gnorm=0.43, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19847
2023-02-22 02:43:17 - progress_bar.py[line:274] - INFO: epoch 010:    325 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.1, ups=0.87, wpb=111.3, bsz=40, num_updates=4030, lr=4.18942e-05, gnorm=0.452, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19858
2023-02-22 02:43:29 - progress_bar.py[line:274] - INFO: epoch 010:    335 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=4040, lr=4.18689e-05, gnorm=0.479, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19870
2023-02-22 02:43:40 - progress_bar.py[line:274] - INFO: epoch 010:    345 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=4050, lr=4.18436e-05, gnorm=0.515, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19881
2023-02-22 02:43:51 - progress_bar.py[line:274] - INFO: epoch 010:    355 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.3, bsz=40, num_updates=4060, lr=4.18184e-05, gnorm=0.711, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19892
2023-02-22 02:44:02 - progress_bar.py[line:274] - INFO: epoch 010:    365 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.9, wpb=111.8, bsz=40, num_updates=4070, lr=4.17931e-05, gnorm=0.308, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19903
2023-02-22 02:44:14 - progress_bar.py[line:274] - INFO: epoch 010:    375 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.5, ups=0.88, wpb=109.6, bsz=40, num_updates=4080, lr=4.17678e-05, gnorm=0.485, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19915
2023-02-22 02:44:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 02:44:26 - progress_bar.py[line:274] - INFO: epoch 010:    386 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=94, ups=0.84, wpb=112.5, bsz=40, num_updates=4090, lr=4.17425e-05, gnorm=0.535, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=19927
2023-02-22 02:44:37 - progress_bar.py[line:274] - INFO: epoch 010:    396 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.92, wpb=112.4, bsz=40, num_updates=4100, lr=4.17172e-05, gnorm=0.569, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19938
2023-02-22 02:44:48 - progress_bar.py[line:274] - INFO: epoch 010:    406 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=4110, lr=4.16919e-05, gnorm=0.46, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19949
2023-02-22 02:44:55 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 02:44:56 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 02:44:56 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 02:46:58 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 02:46:58 - train.py[line:551] - INFO: load:1.07 valid_run:121.95 task_valid:118.27 collect_output:2.61
2023-02-22 02:48:58 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 02:48:58 - train.py[line:551] - INFO: load:1.10 valid_run:241.93 task_valid:233.21 collect_output:6.61
2023-02-22 02:51:00 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 02:51:00 - train.py[line:551] - INFO: load:1.12 valid_run:364.04 task_valid:348.76 collect_output:12.13
2023-02-22 02:53:02 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 02:53:02 - train.py[line:551] - INFO: load:1.15 valid_run:486.10 task_valid:461.44 collect_output:20.50
2023-02-22 02:55:03 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 02:55:03 - train.py[line:551] - INFO: load:1.17 valid_run:606.63 task_valid:577.84 collect_output:23.62
2023-02-22 02:57:06 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 02:57:06 - train.py[line:551] - INFO: load:1.20 valid_run:729.48 task_valid:695.72 collect_output:27.59
2023-02-22 02:59:09 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 02:59:09 - train.py[line:551] - INFO: load:1.23 valid_run:852.83 task_valid:813.02 collect_output:32.62
2023-02-22 03:01:11 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 03:01:11 - train.py[line:551] - INFO: load:1.25 valid_run:974.80 task_valid:928.52 collect_output:38.09
2023-02-22 03:03:16 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 03:03:16 - train.py[line:551] - INFO: load:1.28 valid_run:1099.07 task_valid:1044.87 collect_output:44.98
2023-02-22 03:05:18 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 03:05:18 - train.py[line:551] - INFO: load:1.30 valid_run:1221.30 task_valid:1156.59 collect_output:54.47
2023-02-22 03:07:18 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 03:07:18 - train.py[line:551] - INFO: load:1.33 valid_run:1341.46 task_valid:1271.23 collect_output:59.00
2023-02-22 03:09:20 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 03:09:20 - train.py[line:551] - INFO: load:1.35 valid_run:1463.16 task_valid:1387.30 collect_output:63.62
2023-02-22 03:11:19 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 03:11:19 - train.py[line:551] - INFO: load:1.38 valid_run:1582.27 task_valid:1500.14 collect_output:68.87
2023-02-22 03:13:20 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 03:13:20 - train.py[line:551] - INFO: load:1.41 valid_run:1703.31 task_valid:1617.26 collect_output:71.77
2023-02-22 03:15:21 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 03:15:21 - train.py[line:551] - INFO: load:1.43 valid_run:1824.37 task_valid:1732.48 collect_output:76.60
2023-02-22 03:17:23 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 03:17:23 - train.py[line:551] - INFO: load:1.46 valid_run:1945.85 task_valid:1845.47 collect_output:84.09
2023-02-22 03:19:24 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 03:19:24 - train.py[line:551] - INFO: load:1.49 valid_run:2067.23 task_valid:1960.77 collect_output:89.15
2023-02-22 03:21:25 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 03:21:25 - train.py[line:551] - INFO: load:1.51 valid_run:2187.58 task_valid:2077.85 collect_output:91.41
2023-02-22 03:23:26 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 03:23:26 - train.py[line:551] - INFO: load:1.54 valid_run:2308.64 task_valid:2193.92 collect_output:95.38
2023-02-22 03:25:26 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 03:25:26 - train.py[line:551] - INFO: load:1.57 valid_run:2428.90 task_valid:2309.65 collect_output:98.90
2023-02-22 03:27:28 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 03:27:28 - train.py[line:551] - INFO: load:1.59 valid_run:2550.59 task_valid:2425.28 collect_output:103.97
2023-02-22 03:29:30 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 03:29:30 - train.py[line:551] - INFO: load:1.62 valid_run:2672.59 task_valid:2543.40 collect_output:106.86
2023-02-22 03:31:31 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 03:31:31 - train.py[line:551] - INFO: load:1.65 valid_run:2793.05 task_valid:2656.82 collect_output:112.91
2023-02-22 03:33:30 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 03:33:30 - train.py[line:551] - INFO: load:1.67 valid_run:2912.78 task_valid:2771.98 collect_output:116.45
2023-02-22 03:35:32 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 03:35:32 - train.py[line:551] - INFO: load:1.70 valid_run:3034.43 task_valid:2887.26 collect_output:121.80
2023-02-22 03:37:35 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 03:37:35 - train.py[line:551] - INFO: load:1.73 valid_run:3157.66 task_valid:3002.38 collect_output:128.91
2023-02-22 03:39:35 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 03:39:35 - train.py[line:551] - INFO: load:1.75 valid_run:3277.46 task_valid:3115.69 collect_output:134.40
2023-02-22 03:41:37 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 03:41:37 - train.py[line:551] - INFO: load:1.78 valid_run:3398.99 task_valid:3234.15 collect_output:136.46
2023-02-22 03:43:39 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 03:43:39 - train.py[line:551] - INFO: load:1.80 valid_run:3520.92 task_valid:3348.81 collect_output:142.74
2023-02-22 03:45:40 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 03:45:40 - train.py[line:551] - INFO: load:1.83 valid_run:3642.59 task_valid:3466.49 collect_output:145.72
2023-02-22 03:47:42 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 03:47:42 - train.py[line:551] - INFO: load:1.86 valid_run:3763.60 task_valid:3584.24 collect_output:147.97

====================================================================================================
SGG eval:     R @ 50: 0.6605;     R @ 100: 0.6819;     R @ 500: 0.7047;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4490;    mR @ 100: 0.4681;    mR @ 500: 0.5000;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5000) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1562) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8854) (playing:0.0000) (riding:0.9510) (says:0.0000) (sitting on:0.7200) (standing on:0.5847) (using:0.6000) (walking in:0.3333) (walking on:0.3378) (watching:0.5417) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6605;     R @ 100: 0.6819;     R @ 500: 0.7047;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4490;    mR @ 100: 0.4681;    mR @ 500: 0.5000;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5000) (covering:0.0000) (eating:0.8235) (flying in:1.0000) (growing on:0.1562) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8854) (playing:0.0000) (riding:0.9510) (says:0.0000) (sitting on:0.7200) (standing on:0.5847) (using:0.6000) (walking in:0.3333) (walking on:0.3378) (watching:0.5417) 
--------------------------------------------------------
====================================================================================================

2023-02-22 03:48:12 - train.py[line:487] - INFO: 0.6818596638655463
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 03:48:13 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 03:48:13 - progress_bar.py[line:282] - INFO: epoch 010 | valid on 'valid' subset | loss 0.232 | loss_v1 0 | loss_v2 0 | nll_loss 0.058 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.68186 | ppl 1.04 | vqa_score 0.5586 | wps 118.2 | wpb 72 | bsz 24 | num_updates 4116 | best_R@100 0.68186
2023-02-22 03:48:13 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 4116 updates
2023-02-22 03:48:13 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint10.pt
2023-02-22 03:48:18 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint10.pt
2023-02-22 03:48:23 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint10.pt (epoch 10 @ 4116 updates, score 0.6818596638655463) (writing took 10.678596336394548 seconds)
2023-02-22 03:48:23 - train.py[line:339] - INFO: end of epoch 10 (average epoch stats below)
2023-02-22 03:48:23 - progress_bar.py[line:282] - INFO: epoch 010 | loss 0.242 | loss_v1 0 | loss_v2 0 | nll_loss 0.056 | ntokens 111.136 | nsentences 40 | sample_size 111.136 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 5.7 | ups 0.05 | wpb 111.1 | bsz 40 | num_updates 4116 | lr 4.16768e-05 | gnorm 0.522 | clip 14.8 | loss_scale 512 | train_wall 458 | gb_free 10.7 | ema_decay 0.9999 | wall 23764
2023-02-22 03:48:24 - trainer.py[line:694] - INFO: loading train data for epoch 11
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 03:48:24 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 03:48:24 - trainer.py[line:758] - INFO: begin training epoch 11
2023-02-22 03:48:24 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 03:48:30 - progress_bar.py[line:274] - INFO: epoch 011:      4 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=110.3, bsz=40, num_updates=4120, lr=4.16667e-05, gnorm=0.731, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=23771
2023-02-22 03:48:41 - progress_bar.py[line:274] - INFO: epoch 011:     14 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.9, bsz=40, num_updates=4130, lr=4.16414e-05, gnorm=0.383, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23782
2023-02-22 03:48:52 - progress_bar.py[line:274] - INFO: epoch 011:     24 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.91, wpb=109.6, bsz=40, num_updates=4140, lr=4.16161e-05, gnorm=0.279, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23793
2023-02-22 03:49:03 - progress_bar.py[line:274] - INFO: epoch 011:     34 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.5, bsz=40, num_updates=4150, lr=4.15908e-05, gnorm=0.471, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23804
2023-02-22 03:49:15 - progress_bar.py[line:274] - INFO: epoch 011:     44 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=4160, lr=4.15655e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=23816
2023-02-22 03:49:26 - progress_bar.py[line:274] - INFO: epoch 011:     54 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=4170, lr=4.15403e-05, gnorm=0.645, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23827
2023-02-22 03:49:36 - progress_bar.py[line:274] - INFO: epoch 011:     64 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.93, wpb=110.2, bsz=40, num_updates=4180, lr=4.1515e-05, gnorm=0.541, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=23837
2023-02-22 03:49:48 - progress_bar.py[line:274] - INFO: epoch 011:     74 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.89, wpb=110.2, bsz=40, num_updates=4190, lr=4.14897e-05, gnorm=0.616, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23849
2023-02-22 03:49:59 - progress_bar.py[line:274] - INFO: epoch 011:     84 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=4200, lr=4.14644e-05, gnorm=0.545, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23860
2023-02-22 03:50:10 - progress_bar.py[line:274] - INFO: epoch 011:     94 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.9, wpb=111.9, bsz=40, num_updates=4210, lr=4.14391e-05, gnorm=0.212, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=23871
2023-02-22 03:50:21 - progress_bar.py[line:274] - INFO: epoch 011:    104 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111.3, bsz=40, num_updates=4220, lr=4.14138e-05, gnorm=0.318, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23882
2023-02-22 03:50:33 - progress_bar.py[line:274] - INFO: epoch 011:    114 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=4230, lr=4.13886e-05, gnorm=0.591, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=23893
2023-02-22 03:50:44 - progress_bar.py[line:274] - INFO: epoch 011:    124 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=4240, lr=4.13633e-05, gnorm=0.327, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23905
2023-02-22 03:50:55 - progress_bar.py[line:274] - INFO: epoch 011:    134 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=4250, lr=4.1338e-05, gnorm=0.501, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23916
2023-02-22 03:51:07 - progress_bar.py[line:274] - INFO: epoch 011:    144 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.9, ups=0.87, wpb=111.7, bsz=40, num_updates=4260, lr=4.13127e-05, gnorm=0.809, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23928
2023-02-22 03:51:18 - progress_bar.py[line:274] - INFO: epoch 011:    154 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=4270, lr=4.12874e-05, gnorm=0.544, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=23939
2023-02-22 03:51:29 - progress_bar.py[line:274] - INFO: epoch 011:    164 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112, bsz=40, num_updates=4280, lr=4.12621e-05, gnorm=0.576, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23950
2023-02-22 03:51:40 - progress_bar.py[line:274] - INFO: epoch 011:    174 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=4290, lr=4.12369e-05, gnorm=0.421, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23961
2023-02-22 03:51:51 - progress_bar.py[line:274] - INFO: epoch 011:    184 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.9, wpb=113.2, bsz=40, num_updates=4300, lr=4.12116e-05, gnorm=0.691, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23972
2023-02-22 03:52:02 - progress_bar.py[line:274] - INFO: epoch 011:    194 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=111.6, bsz=40, num_updates=4310, lr=4.11863e-05, gnorm=0.476, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23983
2023-02-22 03:52:14 - progress_bar.py[line:274] - INFO: epoch 011:    204 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.2, ups=0.9, wpb=112.3, bsz=40, num_updates=4320, lr=4.1161e-05, gnorm=0.364, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23994
2023-02-22 03:52:25 - progress_bar.py[line:274] - INFO: epoch 011:    214 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=4330, lr=4.11357e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24006
2023-02-22 03:52:36 - progress_bar.py[line:274] - INFO: epoch 011:    224 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.91, wpb=111.1, bsz=40, num_updates=4340, lr=4.11104e-05, gnorm=0.72, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24017
2023-02-22 03:52:47 - progress_bar.py[line:274] - INFO: epoch 011:    234 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111.6, bsz=40, num_updates=4350, lr=4.10852e-05, gnorm=0.546, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24028
2023-02-22 03:52:59 - progress_bar.py[line:274] - INFO: epoch 011:    244 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97, ups=0.87, wpb=111.7, bsz=40, num_updates=4360, lr=4.10599e-05, gnorm=0.465, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=24040
2023-02-22 03:53:10 - progress_bar.py[line:274] - INFO: epoch 011:    254 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.92, wpb=109.9, bsz=40, num_updates=4370, lr=4.10346e-05, gnorm=0.618, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24050
2023-02-22 03:53:21 - progress_bar.py[line:274] - INFO: epoch 011:    264 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=4380, lr=4.10093e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24062
2023-02-22 03:53:32 - progress_bar.py[line:274] - INFO: epoch 011:    274 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.8, bsz=40, num_updates=4390, lr=4.0984e-05, gnorm=0.489, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24073
2023-02-22 03:53:43 - progress_bar.py[line:274] - INFO: epoch 011:    284 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.91, wpb=111.2, bsz=40, num_updates=4400, lr=4.09587e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24084
2023-02-22 03:53:54 - progress_bar.py[line:274] - INFO: epoch 011:    294 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=4410, lr=4.09335e-05, gnorm=0.251, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24095
2023-02-22 03:54:05 - progress_bar.py[line:274] - INFO: epoch 011:    304 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.91, wpb=111.1, bsz=40, num_updates=4420, lr=4.09082e-05, gnorm=0.467, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24106
2023-02-22 03:54:16 - progress_bar.py[line:274] - INFO: epoch 011:    314 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=4430, lr=4.08829e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24117
2023-02-22 03:54:28 - progress_bar.py[line:274] - INFO: epoch 011:    324 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.7, ups=0.87, wpb=110.2, bsz=40, num_updates=4440, lr=4.08576e-05, gnorm=0.313, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24129
2023-02-22 03:54:39 - progress_bar.py[line:274] - INFO: epoch 011:    334 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.89, wpb=109.2, bsz=40, num_updates=4450, lr=4.08323e-05, gnorm=0.587, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24140
2023-02-22 03:54:50 - progress_bar.py[line:274] - INFO: epoch 011:    344 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.91, wpb=110.2, bsz=40, num_updates=4460, lr=4.0807e-05, gnorm=0.347, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24151
2023-02-22 03:55:01 - progress_bar.py[line:274] - INFO: epoch 011:    354 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.92, wpb=111.3, bsz=40, num_updates=4470, lr=4.07818e-05, gnorm=0.639, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24162
2023-02-22 03:55:12 - progress_bar.py[line:274] - INFO: epoch 011:    364 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=4480, lr=4.07565e-05, gnorm=0.337, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24173
2023-02-22 03:55:24 - progress_bar.py[line:274] - INFO: epoch 011:    374 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97, ups=0.87, wpb=110.9, bsz=40, num_updates=4490, lr=4.07312e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24185
2023-02-22 03:55:35 - progress_bar.py[line:274] - INFO: epoch 011:    384 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.91, wpb=110.8, bsz=40, num_updates=4500, lr=4.07059e-05, gnorm=0.469, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24196
2023-02-22 03:55:46 - progress_bar.py[line:274] - INFO: epoch 011:    394 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.92, wpb=110.6, bsz=40, num_updates=4510, lr=4.06806e-05, gnorm=0.287, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24207
2023-02-22 03:55:56 - progress_bar.py[line:274] - INFO: epoch 011:    404 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.4, ups=0.94, wpb=111.9, bsz=40, num_updates=4520, lr=4.06553e-05, gnorm=0.244, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24217
2023-02-22 03:56:05 - train.py[line:339] - INFO: end of epoch 11 (average epoch stats below)
2023-02-22 03:56:05 - progress_bar.py[line:282] - INFO: epoch 011 | loss 0.241 | loss_v1 0 | loss_v2 0 | nll_loss 0.055 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 99.2 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 4528 | lr 4.06351e-05 | gnorm 0.48 | clip 10.4 | loss_scale 512 | train_wall 458 | gb_free 10.4 | ema_decay 0.9999 | wall 24226
2023-02-22 03:56:05 - trainer.py[line:694] - INFO: loading train data for epoch 12
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 03:56:05 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 03:56:06 - trainer.py[line:758] - INFO: begin training epoch 12
2023-02-22 03:56:06 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 03:56:10 - progress_bar.py[line:274] - INFO: epoch 012:      2 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=81.8, ups=0.74, wpb=110.1, bsz=40, num_updates=4530, lr=4.06301e-05, gnorm=0.353, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=24231
2023-02-22 03:56:21 - progress_bar.py[line:274] - INFO: epoch 012:     12 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=4540, lr=4.06048e-05, gnorm=0.516, clip=30, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=24242
2023-02-22 03:56:32 - progress_bar.py[line:274] - INFO: epoch 012:     22 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.9, wpb=110.6, bsz=40, num_updates=4550, lr=4.05795e-05, gnorm=0.328, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24253
2023-02-22 03:56:44 - progress_bar.py[line:274] - INFO: epoch 012:     32 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.87, wpb=112.6, bsz=40, num_updates=4560, lr=4.05542e-05, gnorm=0.408, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24264
2023-02-22 03:56:55 - progress_bar.py[line:274] - INFO: epoch 012:     42 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.9, ups=0.87, wpb=110.4, bsz=40, num_updates=4570, lr=4.05289e-05, gnorm=0.377, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24276
2023-02-22 03:57:06 - progress_bar.py[line:274] - INFO: epoch 012:     52 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=4580, lr=4.05036e-05, gnorm=0.608, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24287
2023-02-22 03:57:18 - progress_bar.py[line:274] - INFO: epoch 012:     62 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.5, ups=0.87, wpb=110.8, bsz=40, num_updates=4590, lr=4.04784e-05, gnorm=0.405, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24299
2023-02-22 03:57:29 - progress_bar.py[line:274] - INFO: epoch 012:     72 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=4600, lr=4.04531e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24310
2023-02-22 03:57:40 - progress_bar.py[line:274] - INFO: epoch 012:     82 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.8, bsz=40, num_updates=4610, lr=4.04278e-05, gnorm=0.515, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24321
2023-02-22 03:57:51 - progress_bar.py[line:274] - INFO: epoch 012:     92 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=4620, lr=4.04025e-05, gnorm=0.424, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24332
2023-02-22 03:58:03 - progress_bar.py[line:274] - INFO: epoch 012:    102 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=4630, lr=4.03772e-05, gnorm=0.472, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24344
2023-02-22 03:58:14 - progress_bar.py[line:274] - INFO: epoch 012:    112 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.9, wpb=110.6, bsz=40, num_updates=4640, lr=4.03519e-05, gnorm=0.424, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24355
2023-02-22 03:58:25 - progress_bar.py[line:274] - INFO: epoch 012:    122 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=4650, lr=4.03267e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24366
2023-02-22 03:58:36 - progress_bar.py[line:274] - INFO: epoch 012:    132 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=4660, lr=4.03014e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24377
2023-02-22 03:58:48 - progress_bar.py[line:274] - INFO: epoch 012:    142 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.3, bsz=40, num_updates=4670, lr=4.02761e-05, gnorm=0.377, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24389
2023-02-22 03:58:59 - progress_bar.py[line:274] - INFO: epoch 012:    152 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97, ups=0.88, wpb=110.4, bsz=40, num_updates=4680, lr=4.02508e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24400
2023-02-22 03:59:10 - progress_bar.py[line:274] - INFO: epoch 012:    162 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.2, ups=0.89, wpb=110.4, bsz=40, num_updates=4690, lr=4.02255e-05, gnorm=0.473, clip=10, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=24411
2023-02-22 03:59:22 - progress_bar.py[line:274] - INFO: epoch 012:    172 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=4700, lr=4.02002e-05, gnorm=0.319, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24422
2023-02-22 03:59:33 - progress_bar.py[line:274] - INFO: epoch 012:    182 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.89, wpb=109.8, bsz=40, num_updates=4710, lr=4.0175e-05, gnorm=0.311, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24434
2023-02-22 03:59:44 - progress_bar.py[line:274] - INFO: epoch 012:    192 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.9, wpb=109.9, bsz=40, num_updates=4720, lr=4.01497e-05, gnorm=0.612, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24445
2023-02-22 03:59:55 - progress_bar.py[line:274] - INFO: epoch 012:    202 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=4730, lr=4.01244e-05, gnorm=0.401, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24456
2023-02-22 04:00:06 - progress_bar.py[line:274] - INFO: epoch 012:    212 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=4740, lr=4.00991e-05, gnorm=0.472, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24467
2023-02-22 04:00:17 - progress_bar.py[line:274] - INFO: epoch 012:    222 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=111.9, bsz=40, num_updates=4750, lr=4.00738e-05, gnorm=0.614, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24478
2023-02-22 04:00:28 - progress_bar.py[line:274] - INFO: epoch 012:    232 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.95, wpb=110.1, bsz=40, num_updates=4760, lr=4.00485e-05, gnorm=0.501, clip=20, loss_scale=1024, train_wall=10, gb_free=10.9, ema_decay=0.9999, wall=24489
2023-02-22 04:00:39 - progress_bar.py[line:274] - INFO: epoch 012:    242 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=4770, lr=4.00233e-05, gnorm=0.466, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24500
2023-02-22 04:00:50 - progress_bar.py[line:274] - INFO: epoch 012:    252 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.91, wpb=110, bsz=40, num_updates=4780, lr=3.9998e-05, gnorm=0.525, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24511
2023-02-22 04:01:01 - progress_bar.py[line:274] - INFO: epoch 012:    262 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.8, ups=0.92, wpb=112.2, bsz=40, num_updates=4790, lr=3.99727e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24522
2023-02-22 04:01:13 - progress_bar.py[line:274] - INFO: epoch 012:    272 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.88, wpb=111.7, bsz=40, num_updates=4800, lr=3.99474e-05, gnorm=0.504, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24533
2023-02-22 04:01:24 - progress_bar.py[line:274] - INFO: epoch 012:    282 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=4810, lr=3.99221e-05, gnorm=0.351, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=24545
2023-02-22 04:01:35 - progress_bar.py[line:274] - INFO: epoch 012:    292 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=4820, lr=3.98968e-05, gnorm=0.861, clip=30, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24556
2023-02-22 04:01:46 - progress_bar.py[line:274] - INFO: epoch 012:    302 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.7, bsz=40, num_updates=4830, lr=3.98716e-05, gnorm=0.629, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24567
2023-02-22 04:01:58 - progress_bar.py[line:274] - INFO: epoch 012:    312 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=4840, lr=3.98463e-05, gnorm=0.441, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24579
2023-02-22 04:02:09 - progress_bar.py[line:274] - INFO: epoch 012:    322 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.92, wpb=111.6, bsz=40, num_updates=4850, lr=3.9821e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24589
2023-02-22 04:02:20 - progress_bar.py[line:274] - INFO: epoch 012:    332 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.3, bsz=40, num_updates=4860, lr=3.97957e-05, gnorm=0.669, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24601
2023-02-22 04:02:31 - progress_bar.py[line:274] - INFO: epoch 012:    342 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.88, wpb=110.8, bsz=40, num_updates=4870, lr=3.97704e-05, gnorm=0.571, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24612
2023-02-22 04:02:42 - progress_bar.py[line:274] - INFO: epoch 012:    352 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=111.7, bsz=40, num_updates=4880, lr=3.97451e-05, gnorm=0.765, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=24623
2023-02-22 04:02:54 - progress_bar.py[line:274] - INFO: epoch 012:    362 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=4890, lr=3.97199e-05, gnorm=0.7, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24635
2023-02-22 04:03:05 - progress_bar.py[line:274] - INFO: epoch 012:    372 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.5, ups=0.87, wpb=111.6, bsz=40, num_updates=4900, lr=3.96946e-05, gnorm=0.317, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24646
2023-02-22 04:03:16 - progress_bar.py[line:274] - INFO: epoch 012:    382 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=4910, lr=3.96693e-05, gnorm=0.419, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24657
2023-02-22 04:03:27 - progress_bar.py[line:274] - INFO: epoch 012:    392 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.1, ups=0.92, wpb=110.7, bsz=40, num_updates=4920, lr=3.9644e-05, gnorm=0.509, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24668
2023-02-22 04:03:39 - progress_bar.py[line:274] - INFO: epoch 012:    402 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=4930, lr=3.96187e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24679
2023-02-22 04:03:50 - progress_bar.py[line:274] - INFO: epoch 012:    412 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=4940, lr=3.95934e-05, gnorm=0.513, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24691
2023-02-22 04:03:50 - train.py[line:339] - INFO: end of epoch 12 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 04:03:50 - progress_bar.py[line:282] - INFO: epoch 012 | loss 0.239 | loss_v1 0 | loss_v2 0 | nll_loss 0.053 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.6 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 4940 | lr 3.95934e-05 | gnorm 0.467 | clip 10.4 | loss_scale 1024 | train_wall 460 | gb_free 10.7 | ema_decay 0.9999 | wall 24691
2023-02-22 04:03:50 - trainer.py[line:694] - INFO: loading train data for epoch 13
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 04:03:50 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 04:03:50 - trainer.py[line:758] - INFO: begin training epoch 13
2023-02-22 04:03:50 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 04:04:03 - progress_bar.py[line:274] - INFO: epoch 013:     10 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=82.7, ups=0.74, wpb=111.1, bsz=40, num_updates=4950, lr=3.95682e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=24704
2023-02-22 04:04:14 - progress_bar.py[line:274] - INFO: epoch 013:     20 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.5, ups=0.93, wpb=111.6, bsz=40, num_updates=4960, lr=3.95429e-05, gnorm=0.258, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24715
2023-02-22 04:04:25 - progress_bar.py[line:274] - INFO: epoch 013:     30 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=4970, lr=3.95176e-05, gnorm=0.488, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24726
2023-02-22 04:04:37 - progress_bar.py[line:274] - INFO: epoch 013:     40 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.89, wpb=110, bsz=40, num_updates=4980, lr=3.94923e-05, gnorm=0.657, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24737
2023-02-22 04:04:48 - progress_bar.py[line:274] - INFO: epoch 013:     50 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.9, wpb=110.1, bsz=40, num_updates=4990, lr=3.9467e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24749
2023-02-22 04:04:58 - progress_bar.py[line:274] - INFO: epoch 013:     60 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.92, wpb=110.2, bsz=40, num_updates=5000, lr=3.94417e-05, gnorm=0.3, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24759
2023-02-22 04:04:58 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 04:05:00 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 04:05:00 - train.py[line:551] - INFO: load:0.80 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 04:07:02 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 04:07:02 - train.py[line:551] - INFO: load:0.82 valid_run:122.34 task_valid:118.05 collect_output:3.24
2023-02-22 04:09:02 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 04:09:02 - train.py[line:551] - INFO: load:0.85 valid_run:242.19 task_valid:232.82 collect_output:7.29
2023-02-22 04:11:04 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 04:11:04 - train.py[line:551] - INFO: load:0.87 valid_run:364.28 task_valid:348.35 collect_output:12.82
2023-02-22 04:13:06 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 04:13:06 - train.py[line:551] - INFO: load:0.90 valid_run:486.30 task_valid:461.18 collect_output:21.00
2023-02-22 04:15:07 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 04:15:07 - train.py[line:551] - INFO: load:0.93 valid_run:606.76 task_valid:577.48 collect_output:24.14
2023-02-22 04:17:09 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 04:17:09 - train.py[line:551] - INFO: load:0.95 valid_run:729.53 task_valid:695.22 collect_output:28.18
2023-02-22 04:19:13 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 04:19:13 - train.py[line:551] - INFO: load:0.98 valid_run:852.80 task_valid:812.43 collect_output:33.22
2023-02-22 04:21:15 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 04:21:15 - train.py[line:551] - INFO: load:1.00 valid_run:974.78 task_valid:927.95 collect_output:38.67
2023-02-22 04:23:19 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 04:23:19 - train.py[line:551] - INFO: load:1.03 valid_run:1098.98 task_valid:1044.23 collect_output:45.59
2023-02-22 04:25:21 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 04:25:21 - train.py[line:551] - INFO: load:1.06 valid_run:1221.08 task_valid:1156.08 collect_output:54.84
2023-02-22 04:27:22 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 04:27:22 - train.py[line:551] - INFO: load:1.08 valid_run:1341.35 task_valid:1270.99 collect_output:59.19
2023-02-22 04:29:23 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 04:29:23 - train.py[line:551] - INFO: load:1.11 valid_run:1462.98 task_valid:1386.92 collect_output:63.87
2023-02-22 04:31:22 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 04:31:22 - train.py[line:551] - INFO: load:1.13 valid_run:1582.07 task_valid:1499.72 collect_output:69.09
2023-02-22 04:33:23 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 04:33:23 - train.py[line:551] - INFO: load:1.16 valid_run:1702.82 task_valid:1616.57 collect_output:71.97
2023-02-22 04:35:24 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 04:35:24 - train.py[line:551] - INFO: load:1.18 valid_run:1823.95 task_valid:1731.63 collect_output:77.03
2023-02-22 04:37:26 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 04:37:26 - train.py[line:551] - INFO: load:1.21 valid_run:1945.34 task_valid:1844.57 collect_output:84.46
2023-02-22 04:39:27 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 04:39:27 - train.py[line:551] - INFO: load:1.24 valid_run:2066.55 task_valid:1959.73 collect_output:89.50
2023-02-22 04:41:28 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 04:41:28 - train.py[line:551] - INFO: load:1.26 valid_run:2186.92 task_valid:2076.67 collect_output:91.92
2023-02-22 04:43:29 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 04:43:29 - train.py[line:551] - INFO: load:1.29 valid_run:2308.01 task_valid:2192.55 collect_output:96.13
2023-02-22 04:45:29 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 04:45:29 - train.py[line:551] - INFO: load:1.31 valid_run:2428.25 task_valid:2308.15 collect_output:99.75
2023-02-22 04:47:31 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 04:47:31 - train.py[line:551] - INFO: load:1.34 valid_run:2549.96 task_valid:2423.70 collect_output:104.91
2023-02-22 04:49:33 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 04:49:33 - train.py[line:551] - INFO: load:1.37 valid_run:2671.75 task_valid:2541.58 collect_output:107.81
2023-02-22 04:51:33 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 04:51:33 - train.py[line:551] - INFO: load:1.39 valid_run:2792.43 task_valid:2654.90 collect_output:114.16
2023-02-22 04:53:33 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 04:53:33 - train.py[line:551] - INFO: load:1.42 valid_run:2912.32 task_valid:2770.12 collect_output:117.84
2023-02-22 04:55:35 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 04:55:35 - train.py[line:551] - INFO: load:1.45 valid_run:3034.29 task_valid:2885.45 collect_output:123.46
2023-02-22 04:57:39 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 04:57:39 - train.py[line:551] - INFO: load:1.47 valid_run:3157.46 task_valid:3000.57 collect_output:130.51
2023-02-22 04:59:38 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 04:59:38 - train.py[line:551] - INFO: load:1.50 valid_run:3277.14 task_valid:3113.79 collect_output:135.96
2023-02-22 05:01:40 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 05:01:40 - train.py[line:551] - INFO: load:1.53 valid_run:3398.69 task_valid:3232.30 collect_output:138.00
2023-02-22 05:03:42 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 05:03:42 - train.py[line:551] - INFO: load:1.55 valid_run:3520.67 task_valid:3346.82 collect_output:144.44
2023-02-22 05:05:44 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 05:05:44 - train.py[line:551] - INFO: load:1.58 valid_run:3642.37 task_valid:3464.41 collect_output:147.56
2023-02-22 05:07:44 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 05:07:44 - train.py[line:551] - INFO: load:1.60 valid_run:3762.96 task_valid:3581.90 collect_output:149.65

====================================================================================================
SGG eval:     R @ 50: 0.6530;     R @ 100: 0.6751;     R @ 500: 0.7006;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4424;    mR @ 100: 0.4645;    mR @ 500: 0.5059;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5000) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1875) (hanging from:0.3581) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9062) (playing:0.0000) (riding:0.9314) (says:0.0000) (sitting on:0.7069) (standing on:0.5813) (using:0.6000) (walking in:0.3333) (walking on:0.3649) (watching:0.5000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6530;     R @ 100: 0.6751;     R @ 500: 0.7006;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4424;    mR @ 100: 0.4645;    mR @ 500: 0.5059;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5000) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1875) (hanging from:0.3581) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9062) (playing:0.0000) (riding:0.9314) (says:0.0000) (sitting on:0.7069) (standing on:0.5813) (using:0.6000) (walking in:0.3333) (walking on:0.3649) (watching:0.5000) 
--------------------------------------------------------
====================================================================================================

2023-02-22 05:08:15 - train.py[line:487] - INFO: 0.6751263305322129
2023-02-22 05:08:15 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 05:08:15 - progress_bar.py[line:282] - INFO: epoch 013 | valid on 'valid' subset | loss 0.225 | loss_v1 0 | loss_v2 0 | nll_loss 0.054 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.675126 | ppl 1.04 | vqa_score 0.5462 | wps 118.2 | wpb 72 | bsz 24 | num_updates 5000 | best_R@100 0.68186
2023-02-22 05:08:15 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 13 @ 5000 updates
2023-02-22 05:08:15 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_13_5000.pt
2023-02-22 05:08:21 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_13_5000.pt
2023-02-22 05:08:23 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_13_5000.pt (epoch 13 @ 5000 updates, score 0.6751263305322129) (writing took 8.237258646637201 seconds)
2023-02-22 05:08:35 - progress_bar.py[line:274] - INFO: epoch 013:     70 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=110.3, bsz=40, num_updates=5010, lr=3.94165e-05, gnorm=0.447, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28576
2023-02-22 05:08:46 - progress_bar.py[line:274] - INFO: epoch 013:     80 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.9, ups=0.88, wpb=110.7, bsz=40, num_updates=5020, lr=3.93912e-05, gnorm=0.31, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28587
2023-02-22 05:08:57 - progress_bar.py[line:274] - INFO: epoch 013:     90 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=112.5, bsz=40, num_updates=5030, lr=3.93659e-05, gnorm=0.447, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28598
2023-02-22 05:09:08 - progress_bar.py[line:274] - INFO: epoch 013:    100 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=5040, lr=3.93406e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28609
2023-02-22 05:09:19 - progress_bar.py[line:274] - INFO: epoch 013:    110 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.9, wpb=111.1, bsz=40, num_updates=5050, lr=3.93153e-05, gnorm=0.439, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28620
2023-02-22 05:09:30 - progress_bar.py[line:274] - INFO: epoch 013:    120 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=5060, lr=3.929e-05, gnorm=0.356, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28631
2023-02-22 05:09:42 - progress_bar.py[line:274] - INFO: epoch 013:    130 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=5070, lr=3.92648e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28643
2023-02-22 05:09:53 - progress_bar.py[line:274] - INFO: epoch 013:    140 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.9, wpb=111.3, bsz=40, num_updates=5080, lr=3.92395e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28654
2023-02-22 05:10:04 - progress_bar.py[line:274] - INFO: epoch 013:    150 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.9, wpb=111, bsz=40, num_updates=5090, lr=3.92142e-05, gnorm=0.311, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28665
2023-02-22 05:10:15 - progress_bar.py[line:274] - INFO: epoch 013:    160 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=5100, lr=3.91889e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=28676
2023-02-22 05:10:26 - progress_bar.py[line:274] - INFO: epoch 013:    170 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.7, ups=0.89, wpb=110.8, bsz=40, num_updates=5110, lr=3.91636e-05, gnorm=0.234, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28687
2023-02-22 05:10:33 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 05:10:38 - progress_bar.py[line:274] - INFO: epoch 013:    181 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=92.6, ups=0.83, wpb=112, bsz=40, num_updates=5120, lr=3.91383e-05, gnorm=0.384, clip=20, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28699
2023-02-22 05:10:49 - progress_bar.py[line:274] - INFO: epoch 013:    191 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=5130, lr=3.91131e-05, gnorm=0.568, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28710
2023-02-22 05:11:01 - progress_bar.py[line:274] - INFO: epoch 013:    201 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.9, wpb=110.7, bsz=40, num_updates=5140, lr=3.90878e-05, gnorm=0.404, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28721
2023-02-22 05:11:12 - progress_bar.py[line:274] - INFO: epoch 013:    211 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.88, wpb=111.9, bsz=40, num_updates=5150, lr=3.90625e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28733
2023-02-22 05:11:23 - progress_bar.py[line:274] - INFO: epoch 013:    221 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.7, ups=0.87, wpb=111.2, bsz=40, num_updates=5160, lr=3.90372e-05, gnorm=0.662, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28744
2023-02-22 05:11:35 - progress_bar.py[line:274] - INFO: epoch 013:    231 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=5170, lr=3.90119e-05, gnorm=0.525, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=28756
2023-02-22 05:11:46 - progress_bar.py[line:274] - INFO: epoch 013:    241 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=5180, lr=3.89867e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28767
2023-02-22 05:11:57 - progress_bar.py[line:274] - INFO: epoch 013:    251 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.91, wpb=110.7, bsz=40, num_updates=5190, lr=3.89614e-05, gnorm=0.781, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28778
2023-02-22 05:12:08 - progress_bar.py[line:274] - INFO: epoch 013:    261 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=111.8, bsz=40, num_updates=5200, lr=3.89361e-05, gnorm=0.679, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28789
2023-02-22 05:12:19 - progress_bar.py[line:274] - INFO: epoch 013:    271 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.1, ups=0.92, wpb=111.2, bsz=40, num_updates=5210, lr=3.89108e-05, gnorm=0.669, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28800
2023-02-22 05:12:30 - progress_bar.py[line:274] - INFO: epoch 013:    281 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112, bsz=40, num_updates=5220, lr=3.88855e-05, gnorm=0.396, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28811
2023-02-22 05:12:41 - progress_bar.py[line:274] - INFO: epoch 013:    291 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.92, wpb=109.7, bsz=40, num_updates=5230, lr=3.88602e-05, gnorm=0.595, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28822
2023-02-22 05:12:52 - progress_bar.py[line:274] - INFO: epoch 013:    301 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.89, wpb=109.2, bsz=40, num_updates=5240, lr=3.8835e-05, gnorm=0.704, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28833
2023-02-22 05:13:03 - progress_bar.py[line:274] - INFO: epoch 013:    311 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.92, wpb=111.8, bsz=40, num_updates=5250, lr=3.88097e-05, gnorm=0.375, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28844
2023-02-22 05:13:14 - progress_bar.py[line:274] - INFO: epoch 013:    321 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.2, bsz=40, num_updates=5260, lr=3.87844e-05, gnorm=0.357, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=28855
2023-02-22 05:13:25 - progress_bar.py[line:274] - INFO: epoch 013:    331 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.9, wpb=111.1, bsz=40, num_updates=5270, lr=3.87591e-05, gnorm=0.299, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28866
2023-02-22 05:13:36 - progress_bar.py[line:274] - INFO: epoch 013:    341 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=5280, lr=3.87338e-05, gnorm=0.328, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28877
2023-02-22 05:13:47 - progress_bar.py[line:274] - INFO: epoch 013:    351 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=110.8, bsz=40, num_updates=5290, lr=3.87085e-05, gnorm=0.472, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28888
2023-02-22 05:13:59 - progress_bar.py[line:274] - INFO: epoch 013:    361 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=5300, lr=3.86833e-05, gnorm=0.299, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28900
2023-02-22 05:14:10 - progress_bar.py[line:274] - INFO: epoch 013:    371 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112, bsz=40, num_updates=5310, lr=3.8658e-05, gnorm=0.672, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28911
2023-02-22 05:14:21 - progress_bar.py[line:274] - INFO: epoch 013:    381 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=112.4, bsz=40, num_updates=5320, lr=3.86327e-05, gnorm=0.62, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28922
2023-02-22 05:14:32 - progress_bar.py[line:274] - INFO: epoch 013:    391 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.9, wpb=111.8, bsz=40, num_updates=5330, lr=3.86074e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28933
2023-02-22 05:14:43 - progress_bar.py[line:274] - INFO: epoch 013:    401 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.87, wpb=112.5, bsz=40, num_updates=5340, lr=3.85821e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28944
2023-02-22 05:14:54 - progress_bar.py[line:274] - INFO: epoch 013:    411 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=5350, lr=3.85568e-05, gnorm=0.422, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28955
2023-02-22 05:14:56 - train.py[line:339] - INFO: end of epoch 13 (average epoch stats below)
2023-02-22 05:14:56 - progress_bar.py[line:282] - INFO: epoch 013 | loss 0.238 | loss_v1 0 | loss_v2 0 | nll_loss 0.052 | ntokens 111.129 | nsentences 40 | sample_size 111.129 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 5351 | lr 3.85543e-05 | gnorm 0.419 | clip 10.2 | loss_scale 1024 | train_wall 457 | gb_free 10.7 | ema_decay 0.9999 | wall 28957
2023-02-22 05:14:56 - trainer.py[line:694] - INFO: loading train data for epoch 14
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 05:14:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 05:14:56 - trainer.py[line:758] - INFO: begin training epoch 14
2023-02-22 05:14:56 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 05:15:08 - progress_bar.py[line:274] - INFO: epoch 014:      9 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=83.1, ups=0.76, wpb=110, bsz=40, num_updates=5360, lr=3.85316e-05, gnorm=0.265, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=28969
2023-02-22 05:15:19 - progress_bar.py[line:274] - INFO: epoch 014:     19 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.2, ups=0.88, wpb=109.7, bsz=40, num_updates=5370, lr=3.85063e-05, gnorm=0.317, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28980
2023-02-22 05:15:30 - progress_bar.py[line:274] - INFO: epoch 014:     29 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=5380, lr=3.8481e-05, gnorm=0.336, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28991
2023-02-22 05:15:42 - progress_bar.py[line:274] - INFO: epoch 014:     39 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=111.7, bsz=40, num_updates=5390, lr=3.84557e-05, gnorm=0.611, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29003
2023-02-22 05:15:53 - progress_bar.py[line:274] - INFO: epoch 014:     49 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=5400, lr=3.84304e-05, gnorm=0.595, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29014
2023-02-22 05:16:04 - progress_bar.py[line:274] - INFO: epoch 014:     59 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=5410, lr=3.84051e-05, gnorm=0.711, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29025
2023-02-22 05:16:15 - progress_bar.py[line:274] - INFO: epoch 014:     69 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98, ups=0.89, wpb=110, bsz=40, num_updates=5420, lr=3.83799e-05, gnorm=0.271, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29036
2023-02-22 05:16:26 - progress_bar.py[line:274] - INFO: epoch 014:     79 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.92, wpb=109.2, bsz=40, num_updates=5430, lr=3.83546e-05, gnorm=0.401, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29047
2023-02-22 05:16:38 - progress_bar.py[line:274] - INFO: epoch 014:     89 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.89, wpb=112.5, bsz=40, num_updates=5440, lr=3.83293e-05, gnorm=0.272, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29059
2023-02-22 05:16:49 - progress_bar.py[line:274] - INFO: epoch 014:     99 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.5, bsz=40, num_updates=5450, lr=3.8304e-05, gnorm=0.478, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29070
2023-02-22 05:17:00 - progress_bar.py[line:274] - INFO: epoch 014:    109 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.1, bsz=40, num_updates=5460, lr=3.82787e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29081
2023-02-22 05:17:11 - progress_bar.py[line:274] - INFO: epoch 014:    119 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=110.4, bsz=40, num_updates=5470, lr=3.82534e-05, gnorm=0.303, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29092
2023-02-22 05:17:23 - progress_bar.py[line:274] - INFO: epoch 014:    129 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=5480, lr=3.82282e-05, gnorm=0.346, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29103
2023-02-22 05:17:34 - progress_bar.py[line:274] - INFO: epoch 014:    139 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=5490, lr=3.82029e-05, gnorm=0.355, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29115
2023-02-22 05:17:45 - progress_bar.py[line:274] - INFO: epoch 014:    149 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.3, ups=0.9, wpb=109.9, bsz=40, num_updates=5500, lr=3.81776e-05, gnorm=0.309, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=29126
2023-02-22 05:17:56 - progress_bar.py[line:274] - INFO: epoch 014:    159 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=112.1, bsz=40, num_updates=5510, lr=3.81523e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29137
2023-02-22 05:18:07 - progress_bar.py[line:274] - INFO: epoch 014:    169 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.92, wpb=112.9, bsz=40, num_updates=5520, lr=3.8127e-05, gnorm=0.333, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29148
2023-02-22 05:18:18 - progress_bar.py[line:274] - INFO: epoch 014:    179 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=5530, lr=3.81017e-05, gnorm=0.343, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29159
2023-02-22 05:18:29 - progress_bar.py[line:274] - INFO: epoch 014:    189 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=5540, lr=3.80765e-05, gnorm=0.383, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29170
2023-02-22 05:18:41 - progress_bar.py[line:274] - INFO: epoch 014:    199 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.89, wpb=111.7, bsz=40, num_updates=5550, lr=3.80512e-05, gnorm=0.187, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29182
2023-02-22 05:18:52 - progress_bar.py[line:274] - INFO: epoch 014:    209 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=5560, lr=3.80259e-05, gnorm=0.518, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29193
2023-02-22 05:19:03 - progress_bar.py[line:274] - INFO: epoch 014:    219 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.9, wpb=111, bsz=40, num_updates=5570, lr=3.80006e-05, gnorm=0.503, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29204
2023-02-22 05:19:14 - progress_bar.py[line:274] - INFO: epoch 014:    229 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.9, wpb=110.5, bsz=40, num_updates=5580, lr=3.79753e-05, gnorm=0.316, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29215
2023-02-22 05:19:25 - progress_bar.py[line:274] - INFO: epoch 014:    239 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=5590, lr=3.795e-05, gnorm=0.216, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29226
2023-02-22 05:19:36 - progress_bar.py[line:274] - INFO: epoch 014:    249 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=111, bsz=40, num_updates=5600, lr=3.79248e-05, gnorm=0.596, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=29237
2023-02-22 05:19:47 - progress_bar.py[line:274] - INFO: epoch 014:    259 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104, ups=0.92, wpb=112.5, bsz=40, num_updates=5610, lr=3.78995e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29248
2023-02-22 05:19:58 - progress_bar.py[line:274] - INFO: epoch 014:    269 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=5620, lr=3.78742e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29259
2023-02-22 05:20:09 - progress_bar.py[line:274] - INFO: epoch 014:    279 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.3, ups=0.93, wpb=113.4, bsz=40, num_updates=5630, lr=3.78489e-05, gnorm=0.392, clip=10, loss_scale=2048, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=29270
2023-02-22 05:20:20 - progress_bar.py[line:274] - INFO: epoch 014:    289 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96, ups=0.87, wpb=110.4, bsz=40, num_updates=5640, lr=3.78236e-05, gnorm=0.461, clip=10, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29281
2023-02-22 05:20:31 - progress_bar.py[line:274] - INFO: epoch 014:    299 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.2, ups=0.92, wpb=111.6, bsz=40, num_updates=5650, lr=3.77983e-05, gnorm=0.213, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29292
2023-02-22 05:20:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 05:20:43 - progress_bar.py[line:274] - INFO: epoch 014:    310 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=90.4, ups=0.83, wpb=109.3, bsz=40, num_updates=5660, lr=3.77731e-05, gnorm=0.4, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29304
2023-02-22 05:20:55 - progress_bar.py[line:274] - INFO: epoch 014:    320 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.87, wpb=111.5, bsz=40, num_updates=5670, lr=3.77478e-05, gnorm=0.416, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29316
2023-02-22 05:21:06 - progress_bar.py[line:274] - INFO: epoch 014:    330 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=5680, lr=3.77225e-05, gnorm=0.353, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29327
2023-02-22 05:21:17 - progress_bar.py[line:274] - INFO: epoch 014:    340 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.92, wpb=111.8, bsz=40, num_updates=5690, lr=3.76972e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29338
2023-02-22 05:21:28 - progress_bar.py[line:274] - INFO: epoch 014:    350 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=111.9, bsz=40, num_updates=5700, lr=3.76719e-05, gnorm=0.42, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29349
2023-02-22 05:21:40 - progress_bar.py[line:274] - INFO: epoch 014:    360 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=5710, lr=3.76466e-05, gnorm=0.492, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29361
2023-02-22 05:21:51 - progress_bar.py[line:274] - INFO: epoch 014:    370 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=5720, lr=3.76214e-05, gnorm=0.448, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29372
2023-02-22 05:22:01 - progress_bar.py[line:274] - INFO: epoch 014:    380 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104, ups=0.93, wpb=112, bsz=40, num_updates=5730, lr=3.75961e-05, gnorm=0.381, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29382
2023-02-22 05:22:12 - progress_bar.py[line:274] - INFO: epoch 014:    390 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.91, wpb=110, bsz=40, num_updates=5740, lr=3.75708e-05, gnorm=0.439, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29393
2023-02-22 05:22:24 - progress_bar.py[line:274] - INFO: epoch 014:    400 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.89, wpb=112.3, bsz=40, num_updates=5750, lr=3.75455e-05, gnorm=0.412, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29404
2023-02-22 05:22:35 - progress_bar.py[line:274] - INFO: epoch 014:    410 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.3, ups=0.87, wpb=112.1, bsz=40, num_updates=5760, lr=3.75202e-05, gnorm=0.208, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29416
2023-02-22 05:22:37 - train.py[line:339] - INFO: end of epoch 14 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 05:22:37 - progress_bar.py[line:282] - INFO: epoch 014 | loss 0.238 | loss_v1 0 | loss_v2 0 | nll_loss 0.052 | ntokens 111.119 | nsentences 40 | sample_size 111.119 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.9 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 5762 | lr 3.75152e-05 | gnorm 0.377 | clip 5.4 | loss_scale 1024 | train_wall 458 | gb_free 10.8 | ema_decay 0.9999 | wall 29418
2023-02-22 05:22:37 - trainer.py[line:694] - INFO: loading train data for epoch 15
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 05:22:37 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 05:22:38 - trainer.py[line:758] - INFO: begin training epoch 15
2023-02-22 05:22:38 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 05:22:48 - progress_bar.py[line:274] - INFO: epoch 015:      8 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=84.8, ups=0.76, wpb=111.7, bsz=40, num_updates=5770, lr=3.74949e-05, gnorm=0.313, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29429
2023-02-22 05:23:00 - progress_bar.py[line:274] - INFO: epoch 015:     18 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.88, wpb=112.7, bsz=40, num_updates=5780, lr=3.74697e-05, gnorm=0.382, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29441
2023-02-22 05:23:11 - progress_bar.py[line:274] - INFO: epoch 015:     28 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=5790, lr=3.74444e-05, gnorm=0.369, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29452
2023-02-22 05:23:22 - progress_bar.py[line:274] - INFO: epoch 015:     38 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.91, wpb=111.2, bsz=40, num_updates=5800, lr=3.74191e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29463
2023-02-22 05:23:33 - progress_bar.py[line:274] - INFO: epoch 015:     48 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98, ups=0.89, wpb=110.6, bsz=40, num_updates=5810, lr=3.73938e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29474
2023-02-22 05:23:44 - progress_bar.py[line:274] - INFO: epoch 015:     58 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.4, ups=0.9, wpb=110.2, bsz=40, num_updates=5820, lr=3.73685e-05, gnorm=0.144, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29485
2023-02-22 05:23:55 - progress_bar.py[line:274] - INFO: epoch 015:     68 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.92, wpb=111, bsz=40, num_updates=5830, lr=3.73432e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29496
2023-02-22 05:24:06 - progress_bar.py[line:274] - INFO: epoch 015:     78 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.5, bsz=40, num_updates=5840, lr=3.7318e-05, gnorm=0.373, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29507
2023-02-22 05:24:17 - progress_bar.py[line:274] - INFO: epoch 015:     88 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.4, bsz=40, num_updates=5850, lr=3.72927e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29518
2023-02-22 05:24:29 - progress_bar.py[line:274] - INFO: epoch 015:     98 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96, ups=0.87, wpb=110.5, bsz=40, num_updates=5860, lr=3.72674e-05, gnorm=0.351, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29530
2023-02-22 05:24:40 - progress_bar.py[line:274] - INFO: epoch 015:    108 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.89, wpb=111, bsz=40, num_updates=5870, lr=3.72421e-05, gnorm=0.457, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29541
2023-02-22 05:24:51 - progress_bar.py[line:274] - INFO: epoch 015:    118 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.9, wpb=112.1, bsz=40, num_updates=5880, lr=3.72168e-05, gnorm=0.499, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29552
2023-02-22 05:25:03 - progress_bar.py[line:274] - INFO: epoch 015:    128 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.9, wpb=110.7, bsz=40, num_updates=5890, lr=3.71915e-05, gnorm=0.431, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29563
2023-02-22 05:25:14 - progress_bar.py[line:274] - INFO: epoch 015:    138 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.9, wpb=109.5, bsz=40, num_updates=5900, lr=3.71663e-05, gnorm=0.132, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29575
2023-02-22 05:25:25 - progress_bar.py[line:274] - INFO: epoch 015:    148 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=110.3, bsz=40, num_updates=5910, lr=3.7141e-05, gnorm=0.517, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29586
2023-02-22 05:25:36 - progress_bar.py[line:274] - INFO: epoch 015:    158 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.89, wpb=109.8, bsz=40, num_updates=5920, lr=3.71157e-05, gnorm=0.517, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29597
2023-02-22 05:25:47 - progress_bar.py[line:274] - INFO: epoch 015:    168 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.9, wpb=111.4, bsz=40, num_updates=5930, lr=3.70904e-05, gnorm=0.29, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29608
2023-02-22 05:25:59 - progress_bar.py[line:274] - INFO: epoch 015:    178 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=5940, lr=3.70651e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29620
2023-02-22 05:26:10 - progress_bar.py[line:274] - INFO: epoch 015:    188 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=5950, lr=3.70398e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29631
2023-02-22 05:26:21 - progress_bar.py[line:274] - INFO: epoch 015:    198 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=5960, lr=3.70146e-05, gnorm=0.171, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29642
2023-02-22 05:26:31 - progress_bar.py[line:274] - INFO: epoch 015:    208 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.92, wpb=109.8, bsz=40, num_updates=5970, lr=3.69893e-05, gnorm=0.355, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29652
2023-02-22 05:26:43 - progress_bar.py[line:274] - INFO: epoch 015:    218 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=5980, lr=3.6964e-05, gnorm=0.611, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29664
2023-02-22 05:26:54 - progress_bar.py[line:274] - INFO: epoch 015:    228 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.88, wpb=110.9, bsz=40, num_updates=5990, lr=3.69387e-05, gnorm=0.681, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29675
2023-02-22 05:27:05 - progress_bar.py[line:274] - INFO: epoch 015:    238 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=6000, lr=3.69134e-05, gnorm=0.443, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29686
2023-02-22 05:27:05 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 05:27:06 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 05:27:06 - train.py[line:551] - INFO: load:0.83 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 05:29:08 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 05:29:08 - train.py[line:551] - INFO: load:0.85 valid_run:121.85 task_valid:117.95 collect_output:2.88
2023-02-22 05:31:08 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 05:31:08 - train.py[line:551] - INFO: load:0.87 valid_run:241.83 task_valid:232.78 collect_output:7.03
2023-02-22 05:33:10 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 05:33:10 - train.py[line:551] - INFO: load:0.90 valid_run:364.01 task_valid:348.54 collect_output:12.43
2023-02-22 05:35:12 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 05:35:12 - train.py[line:551] - INFO: load:0.92 valid_run:486.12 task_valid:461.29 collect_output:20.79
2023-02-22 05:37:13 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 05:37:13 - train.py[line:551] - INFO: load:0.94 valid_run:606.54 task_valid:577.69 collect_output:23.83
2023-02-22 05:39:16 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 05:39:16 - train.py[line:551] - INFO: load:0.97 valid_run:729.30 task_valid:695.67 collect_output:27.61
2023-02-22 05:41:19 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 05:41:19 - train.py[line:551] - INFO: load:0.99 valid_run:852.57 task_valid:813.09 collect_output:32.45
2023-02-22 05:43:21 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 05:43:21 - train.py[line:551] - INFO: load:1.02 valid_run:974.51 task_valid:928.86 collect_output:37.62
2023-02-22 05:45:25 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 05:45:25 - train.py[line:551] - INFO: load:1.04 valid_run:1098.52 task_valid:1045.23 collect_output:44.24
2023-02-22 05:47:27 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 05:47:27 - train.py[line:551] - INFO: load:1.06 valid_run:1220.55 task_valid:1157.04 collect_output:53.48
2023-02-22 05:49:27 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 05:49:27 - train.py[line:551] - INFO: load:1.09 valid_run:1340.78 task_valid:1271.73 collect_output:57.97
2023-02-22 05:51:29 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 05:51:29 - train.py[line:551] - INFO: load:1.11 valid_run:1462.39 task_valid:1387.75 collect_output:62.52
2023-02-22 05:53:28 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 05:53:28 - train.py[line:551] - INFO: load:1.14 valid_run:1581.31 task_valid:1500.75 collect_output:67.46
2023-02-22 05:55:29 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 05:55:29 - train.py[line:551] - INFO: load:1.16 valid_run:1702.10 task_valid:1617.70 collect_output:70.29
2023-02-22 05:57:30 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 05:57:30 - train.py[line:551] - INFO: load:1.18 valid_run:1823.18 task_valid:1732.97 collect_output:75.11
2023-02-22 05:59:31 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 05:59:31 - train.py[line:551] - INFO: load:1.21 valid_run:1944.51 task_valid:1846.26 collect_output:82.16
2023-02-22 06:01:33 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 06:01:33 - train.py[line:551] - INFO: load:1.23 valid_run:2065.82 task_valid:1961.51 collect_output:87.22
2023-02-22 06:03:33 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 06:03:33 - train.py[line:551] - INFO: load:1.26 valid_run:2186.26 task_valid:2078.70 collect_output:89.46
2023-02-22 06:05:35 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 06:05:35 - train.py[line:551] - INFO: load:1.28 valid_run:2307.39 task_valid:2194.96 collect_output:93.32
2023-02-22 06:07:35 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 06:07:35 - train.py[line:551] - INFO: load:1.31 valid_run:2427.65 task_valid:2310.89 collect_output:96.64
2023-02-22 06:09:37 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 06:09:37 - train.py[line:551] - INFO: load:1.33 valid_run:2549.49 task_valid:2426.72 collect_output:101.64
2023-02-22 06:11:39 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 06:11:39 - train.py[line:551] - INFO: load:1.35 valid_run:2671.45 task_valid:2544.95 collect_output:104.35
2023-02-22 06:13:39 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 06:13:39 - train.py[line:551] - INFO: load:1.38 valid_run:2791.88 task_valid:2658.49 collect_output:110.24
2023-02-22 06:15:39 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 06:15:39 - train.py[line:551] - INFO: load:1.40 valid_run:2911.83 task_valid:2773.81 collect_output:113.85
2023-02-22 06:17:41 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 06:17:41 - train.py[line:551] - INFO: load:1.43 valid_run:3033.65 task_valid:2889.17 collect_output:119.31
2023-02-22 06:19:45 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 06:19:45 - train.py[line:551] - INFO: load:1.45 valid_run:3157.02 task_valid:3004.35 collect_output:126.51
2023-02-22 06:21:44 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 06:21:44 - train.py[line:551] - INFO: load:1.48 valid_run:3276.68 task_valid:3117.80 collect_output:131.71
2023-02-22 06:23:46 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 06:23:46 - train.py[line:551] - INFO: load:1.50 valid_run:3398.18 task_valid:3236.39 collect_output:133.62
2023-02-22 06:25:48 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 06:25:48 - train.py[line:551] - INFO: load:1.52 valid_run:3520.34 task_valid:3351.30 collect_output:139.85
2023-02-22 06:27:50 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 06:27:50 - train.py[line:551] - INFO: load:1.55 valid_run:3642.12 task_valid:3468.94 collect_output:143.00
2023-02-22 06:29:51 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 06:29:51 - train.py[line:551] - INFO: load:1.57 valid_run:3762.97 task_valid:3586.60 collect_output:145.19

====================================================================================================
SGG eval:     R @ 50: 0.6435;     R @ 100: 0.6658;     R @ 500: 0.6936;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4403;    mR @ 100: 0.4640;    mR @ 500: 0.5015;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6250) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3516) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9062) (playing:0.0000) (riding:0.9167) (says:0.0000) (sitting on:0.6774) (standing on:0.5913) (using:0.6000) (walking in:0.3333) (walking on:0.3514) (watching:0.4583) 
--------------------------------------------------------
====================================================================================================

2023-02-22 06:30:21 - train.py[line:487] - INFO: 0.665775350140056

====================================================================================================
SGG eval:     R @ 50: 0.6435;     R @ 100: 0.6658;     R @ 500: 0.6936;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4403;    mR @ 100: 0.4640;    mR @ 500: 0.5015;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6250) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3516) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9062) (playing:0.0000) (riding:0.9167) (says:0.0000) (sitting on:0.6774) (standing on:0.5913) (using:0.6000) (walking in:0.3333) (walking on:0.3514) (watching:0.4583) 
--------------------------------------------------------
====================================================================================================

2023-02-22 06:30:22 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 06:30:22 - progress_bar.py[line:282] - INFO: epoch 015 | valid on 'valid' subset | loss 0.226 | loss_v1 0 | loss_v2 0 | nll_loss 0.061 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.665775 | ppl 1.04 | vqa_score 0.5158 | wps 118.2 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.68186
2023-02-22 06:30:22 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 15 @ 6000 updates
2023-02-22 06:30:22 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_15_6000.pt
2023-02-22 06:30:27 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_15_6000.pt
2023-02-22 06:30:30 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_15_6000.pt (epoch 15 @ 6000 updates, score 0.665775350140056) (writing took 8.219991397112608 seconds)
2023-02-22 06:30:41 - progress_bar.py[line:274] - INFO: epoch 015:    248 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=110.5, bsz=40, num_updates=6010, lr=3.68881e-05, gnorm=0.731, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33502
2023-02-22 06:30:52 - progress_bar.py[line:274] - INFO: epoch 015:    258 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=6020, lr=3.68629e-05, gnorm=0.21, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33513
2023-02-22 06:31:04 - progress_bar.py[line:274] - INFO: epoch 015:    268 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.87, wpb=111.9, bsz=40, num_updates=6030, lr=3.68376e-05, gnorm=0.43, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33525
2023-02-22 06:31:15 - progress_bar.py[line:274] - INFO: epoch 015:    278 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=6040, lr=3.68123e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33536
2023-02-22 06:31:26 - progress_bar.py[line:274] - INFO: epoch 015:    288 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.91, wpb=111.1, bsz=40, num_updates=6050, lr=3.6787e-05, gnorm=0.26, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33547
2023-02-22 06:31:37 - progress_bar.py[line:274] - INFO: epoch 015:    298 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.88, wpb=112.4, bsz=40, num_updates=6060, lr=3.67617e-05, gnorm=0.552, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33558
2023-02-22 06:31:48 - progress_bar.py[line:274] - INFO: epoch 015:    308 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=6070, lr=3.67364e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33569
2023-02-22 06:31:59 - progress_bar.py[line:274] - INFO: epoch 015:    318 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=6080, lr=3.67112e-05, gnorm=0.483, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33580
2023-02-22 06:32:10 - progress_bar.py[line:274] - INFO: epoch 015:    328 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.8, ups=0.93, wpb=111.1, bsz=40, num_updates=6090, lr=3.66859e-05, gnorm=0.343, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33591
2023-02-22 06:32:21 - progress_bar.py[line:274] - INFO: epoch 015:    338 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.89, wpb=111.6, bsz=40, num_updates=6100, lr=3.66606e-05, gnorm=0.47, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33602
2023-02-22 06:32:32 - progress_bar.py[line:274] - INFO: epoch 015:    348 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=111.5, bsz=40, num_updates=6110, lr=3.66353e-05, gnorm=0.451, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33613
2023-02-22 06:32:43 - progress_bar.py[line:274] - INFO: epoch 015:    358 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.92, wpb=111.8, bsz=40, num_updates=6120, lr=3.661e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=33624
2023-02-22 06:32:54 - progress_bar.py[line:274] - INFO: epoch 015:    368 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.9, wpb=110.9, bsz=40, num_updates=6130, lr=3.65847e-05, gnorm=0.33, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33635
2023-02-22 06:33:06 - progress_bar.py[line:274] - INFO: epoch 015:    378 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.04, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=6140, lr=3.65595e-05, gnorm=0.202, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33646
2023-02-22 06:33:17 - progress_bar.py[line:274] - INFO: epoch 015:    388 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.9, wpb=112.4, bsz=40, num_updates=6150, lr=3.65342e-05, gnorm=0.438, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33658
2023-02-22 06:33:28 - progress_bar.py[line:274] - INFO: epoch 015:    398 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=6160, lr=3.65089e-05, gnorm=0.311, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33669
2023-02-22 06:33:39 - progress_bar.py[line:274] - INFO: epoch 015:    408 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.92, wpb=110.9, bsz=40, num_updates=6170, lr=3.64836e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33680
2023-02-22 06:33:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 06:33:43 - train.py[line:339] - INFO: end of epoch 15 (average epoch stats below)
2023-02-22 06:33:43 - progress_bar.py[line:282] - INFO: epoch 015 | loss 0.237 | loss_v1 0 | loss_v2 0 | nll_loss 0.052 | ntokens 111.124 | nsentences 40 | sample_size 111.124 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 6173 | lr 3.6476e-05 | gnorm 0.377 | clip 7.1 | loss_scale 1024 | train_wall 457 | gb_free 10.5 | ema_decay 0.9999 | wall 33684
2023-02-22 06:33:43 - trainer.py[line:694] - INFO: loading train data for epoch 16
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 06:33:43 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 06:33:44 - trainer.py[line:758] - INFO: begin training epoch 16
2023-02-22 06:33:44 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 06:33:53 - progress_bar.py[line:274] - INFO: epoch 016:      7 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=77, ups=0.69, wpb=110.8, bsz=40, num_updates=6180, lr=3.64583e-05, gnorm=0.372, clip=10, loss_scale=1024, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=33694
2023-02-22 06:34:05 - progress_bar.py[line:274] - INFO: epoch 016:     17 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.1, ups=0.9, wpb=111.6, bsz=40, num_updates=6190, lr=3.64331e-05, gnorm=0.384, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=33706
2023-02-22 06:34:16 - progress_bar.py[line:274] - INFO: epoch 016:     27 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=110.8, bsz=40, num_updates=6200, lr=3.64078e-05, gnorm=0.411, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33717
2023-02-22 06:34:27 - progress_bar.py[line:274] - INFO: epoch 016:     37 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.89, wpb=111.9, bsz=40, num_updates=6210, lr=3.63825e-05, gnorm=0.34, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33728
2023-02-22 06:34:38 - progress_bar.py[line:274] - INFO: epoch 016:     47 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.039, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.91, wpb=110.6, bsz=40, num_updates=6220, lr=3.63572e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33739
2023-02-22 06:34:49 - progress_bar.py[line:274] - INFO: epoch 016:     57 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.89, wpb=112.6, bsz=40, num_updates=6230, lr=3.63319e-05, gnorm=0.449, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33750
2023-02-22 06:35:01 - progress_bar.py[line:274] - INFO: epoch 016:     67 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.6, ups=0.87, wpb=111, bsz=40, num_updates=6240, lr=3.63066e-05, gnorm=0.516, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33762
2023-02-22 06:35:12 - progress_bar.py[line:274] - INFO: epoch 016:     77 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.88, wpb=112.7, bsz=40, num_updates=6250, lr=3.62814e-05, gnorm=0.409, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33773
2023-02-22 06:35:23 - progress_bar.py[line:274] - INFO: epoch 016:     87 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=6260, lr=3.62561e-05, gnorm=0.35, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33784
2023-02-22 06:35:35 - progress_bar.py[line:274] - INFO: epoch 016:     97 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.88, wpb=111.1, bsz=40, num_updates=6270, lr=3.62308e-05, gnorm=0.496, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33796
2023-02-22 06:35:46 - progress_bar.py[line:274] - INFO: epoch 016:    107 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.5, ups=0.87, wpb=111.2, bsz=40, num_updates=6280, lr=3.62055e-05, gnorm=0.666, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33807
2023-02-22 06:35:57 - progress_bar.py[line:274] - INFO: epoch 016:    117 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=6290, lr=3.61802e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33818
2023-02-22 06:36:09 - progress_bar.py[line:274] - INFO: epoch 016:    127 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.8, ups=0.88, wpb=111.2, bsz=40, num_updates=6300, lr=3.61549e-05, gnorm=0.4, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33830
2023-02-22 06:36:20 - progress_bar.py[line:274] - INFO: epoch 016:    137 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.89, wpb=110.8, bsz=40, num_updates=6310, lr=3.61297e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33841
2023-02-22 06:36:31 - progress_bar.py[line:274] - INFO: epoch 016:    147 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=110.7, bsz=40, num_updates=6320, lr=3.61044e-05, gnorm=0.457, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33852
2023-02-22 06:36:42 - progress_bar.py[line:274] - INFO: epoch 016:    157 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=6330, lr=3.60791e-05, gnorm=0.449, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33863
2023-02-22 06:36:54 - progress_bar.py[line:274] - INFO: epoch 016:    167 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.4, ups=0.88, wpb=110.8, bsz=40, num_updates=6340, lr=3.60538e-05, gnorm=0.153, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33875
2023-02-22 06:37:05 - progress_bar.py[line:274] - INFO: epoch 016:    177 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=95.3, ups=0.87, wpb=109.6, bsz=40, num_updates=6350, lr=3.60285e-05, gnorm=0.141, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33886
2023-02-22 06:37:17 - progress_bar.py[line:274] - INFO: epoch 016:    187 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=6360, lr=3.60032e-05, gnorm=0.295, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=33897
2023-02-22 06:37:28 - progress_bar.py[line:274] - INFO: epoch 016:    197 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=6370, lr=3.5978e-05, gnorm=0.3, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33909
2023-02-22 06:37:39 - progress_bar.py[line:274] - INFO: epoch 016:    207 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=111.4, bsz=40, num_updates=6380, lr=3.59527e-05, gnorm=0.946, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33920
2023-02-22 06:37:50 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 06:37:51 - progress_bar.py[line:274] - INFO: epoch 016:    218 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=90.7, ups=0.82, wpb=111, bsz=40, num_updates=6390, lr=3.59274e-05, gnorm=0.348, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=33932
2023-02-22 06:38:02 - progress_bar.py[line:274] - INFO: epoch 016:    228 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=6400, lr=3.59021e-05, gnorm=0.409, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33943
2023-02-22 06:38:13 - progress_bar.py[line:274] - INFO: epoch 016:    238 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.91, wpb=109.9, bsz=40, num_updates=6410, lr=3.58768e-05, gnorm=0.372, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=33954
2023-02-22 06:38:24 - progress_bar.py[line:274] - INFO: epoch 016:    248 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=111.4, bsz=40, num_updates=6420, lr=3.58515e-05, gnorm=0.518, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=33965
2023-02-22 06:38:35 - progress_bar.py[line:274] - INFO: epoch 016:    258 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98, ups=0.89, wpb=110.3, bsz=40, num_updates=6430, lr=3.58263e-05, gnorm=0.331, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=33976
2023-02-22 06:38:47 - progress_bar.py[line:274] - INFO: epoch 016:    268 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=6440, lr=3.5801e-05, gnorm=0.704, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33988
2023-02-22 06:38:58 - progress_bar.py[line:274] - INFO: epoch 016:    278 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.91, wpb=109.9, bsz=40, num_updates=6450, lr=3.57757e-05, gnorm=0.426, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33999
2023-02-22 06:39:09 - progress_bar.py[line:274] - INFO: epoch 016:    288 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.88, wpb=112.3, bsz=40, num_updates=6460, lr=3.57504e-05, gnorm=0.503, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34010
2023-02-22 06:39:20 - progress_bar.py[line:274] - INFO: epoch 016:    298 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=6470, lr=3.57251e-05, gnorm=0.187, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34021
2023-02-22 06:39:31 - progress_bar.py[line:274] - INFO: epoch 016:    308 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.91, wpb=109.7, bsz=40, num_updates=6480, lr=3.56998e-05, gnorm=0.183, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34032
2023-02-22 06:39:43 - progress_bar.py[line:274] - INFO: epoch 016:    318 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.2, ups=0.87, wpb=110.9, bsz=40, num_updates=6490, lr=3.56746e-05, gnorm=0.473, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34044
2023-02-22 06:39:54 - progress_bar.py[line:274] - INFO: epoch 016:    328 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.2, ups=0.9, wpb=112.3, bsz=40, num_updates=6500, lr=3.56493e-05, gnorm=0.254, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=34055
2023-02-22 06:40:05 - progress_bar.py[line:274] - INFO: epoch 016:    338 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.7, ups=0.91, wpb=110, bsz=40, num_updates=6510, lr=3.5624e-05, gnorm=0.228, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34066
2023-02-22 06:40:17 - progress_bar.py[line:274] - INFO: epoch 016:    348 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96, ups=0.87, wpb=110, bsz=40, num_updates=6520, lr=3.55987e-05, gnorm=0.349, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34077
2023-02-22 06:40:28 - progress_bar.py[line:274] - INFO: epoch 016:    358 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.8, ups=0.88, wpb=111.2, bsz=40, num_updates=6530, lr=3.55734e-05, gnorm=0.209, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34089
2023-02-22 06:40:39 - progress_bar.py[line:274] - INFO: epoch 016:    368 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=6540, lr=3.55481e-05, gnorm=0.4, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34100
2023-02-22 06:40:50 - progress_bar.py[line:274] - INFO: epoch 016:    378 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.2, bsz=40, num_updates=6550, lr=3.55229e-05, gnorm=0.545, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34111
2023-02-22 06:41:02 - progress_bar.py[line:274] - INFO: epoch 016:    388 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=6560, lr=3.54976e-05, gnorm=0.48, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34123
2023-02-22 06:41:13 - progress_bar.py[line:274] - INFO: epoch 016:    398 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=6570, lr=3.54723e-05, gnorm=0.56, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34134
2023-02-22 06:41:24 - progress_bar.py[line:274] - INFO: epoch 016:    408 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.5, ups=0.88, wpb=112.1, bsz=40, num_updates=6580, lr=3.5447e-05, gnorm=0.314, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34145
2023-02-22 06:41:29 - train.py[line:339] - INFO: end of epoch 16 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 06:41:29 - progress_bar.py[line:282] - INFO: epoch 016 | loss 0.236 | loss_v1 0 | loss_v2 0 | nll_loss 0.05 | ntokens 111.131 | nsentences 40 | sample_size 111.131 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.1 | ups 0.88 | wpb 111.1 | bsz 40 | num_updates 6584 | lr 3.54369e-05 | gnorm 0.396 | clip 10 | loss_scale 512 | train_wall 461 | gb_free 10.5 | ema_decay 0.9999 | wall 34150
2023-02-22 06:41:29 - trainer.py[line:694] - INFO: loading train data for epoch 17
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 06:41:29 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 06:41:29 - trainer.py[line:758] - INFO: begin training epoch 17
2023-02-22 06:41:29 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 06:41:38 - progress_bar.py[line:274] - INFO: epoch 017:      6 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=83.8, ups=0.75, wpb=112.3, bsz=40, num_updates=6590, lr=3.54217e-05, gnorm=0.493, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=34159
2023-02-22 06:41:49 - progress_bar.py[line:274] - INFO: epoch 017:     16 / 412 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.6, ups=0.91, wpb=112.6, bsz=40, num_updates=6600, lr=3.53964e-05, gnorm=0.195, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34170
2023-02-22 06:42:00 - progress_bar.py[line:274] - INFO: epoch 017:     26 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.91, wpb=110.1, bsz=40, num_updates=6610, lr=3.53712e-05, gnorm=0.162, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34181
2023-02-22 06:42:11 - progress_bar.py[line:274] - INFO: epoch 017:     36 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.91, wpb=110.6, bsz=40, num_updates=6620, lr=3.53459e-05, gnorm=0.709, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=34192
2023-02-22 06:42:22 - progress_bar.py[line:274] - INFO: epoch 017:     46 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.4, ups=0.89, wpb=111.8, bsz=40, num_updates=6630, lr=3.53206e-05, gnorm=0.306, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34203
2023-02-22 06:42:34 - progress_bar.py[line:274] - INFO: epoch 017:     56 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=6640, lr=3.52953e-05, gnorm=0.354, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34214
2023-02-22 06:42:45 - progress_bar.py[line:274] - INFO: epoch 017:     66 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.3, ups=0.87, wpb=110.1, bsz=40, num_updates=6650, lr=3.527e-05, gnorm=0.483, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34226
2023-02-22 06:42:56 - progress_bar.py[line:274] - INFO: epoch 017:     76 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.9, wpb=111.6, bsz=40, num_updates=6660, lr=3.52447e-05, gnorm=0.634, clip=30, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=34237
2023-02-22 06:43:08 - progress_bar.py[line:274] - INFO: epoch 017:     86 / 412 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.04, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=6670, lr=3.52195e-05, gnorm=0.093, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34249
2023-02-22 06:43:18 - progress_bar.py[line:274] - INFO: epoch 017:     96 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.93, wpb=111.9, bsz=40, num_updates=6680, lr=3.51942e-05, gnorm=0.273, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34259
2023-02-22 06:43:29 - progress_bar.py[line:274] - INFO: epoch 017:    106 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.6, ups=0.93, wpb=111.8, bsz=40, num_updates=6690, lr=3.51689e-05, gnorm=0.465, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34270
2023-02-22 06:43:40 - progress_bar.py[line:274] - INFO: epoch 017:    116 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.93, wpb=112.3, bsz=40, num_updates=6700, lr=3.51436e-05, gnorm=0.583, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=34281
2023-02-22 06:43:51 - progress_bar.py[line:274] - INFO: epoch 017:    126 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=111.9, bsz=40, num_updates=6710, lr=3.51183e-05, gnorm=0.318, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34292
2023-02-22 06:44:02 - progress_bar.py[line:274] - INFO: epoch 017:    136 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=6720, lr=3.5093e-05, gnorm=0.545, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34303
2023-02-22 06:44:13 - progress_bar.py[line:274] - INFO: epoch 017:    146 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=6730, lr=3.50678e-05, gnorm=0.502, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=34314
2023-02-22 06:44:24 - progress_bar.py[line:274] - INFO: epoch 017:    156 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=6740, lr=3.50425e-05, gnorm=0.263, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34325
2023-02-22 06:44:36 - progress_bar.py[line:274] - INFO: epoch 017:    166 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111.2, bsz=40, num_updates=6750, lr=3.50172e-05, gnorm=0.272, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=34337
2023-02-22 06:44:47 - progress_bar.py[line:274] - INFO: epoch 017:    176 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=111.3, bsz=40, num_updates=6760, lr=3.49919e-05, gnorm=0.485, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34348
2023-02-22 06:44:58 - progress_bar.py[line:274] - INFO: epoch 017:    186 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=6770, lr=3.49666e-05, gnorm=0.4, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34359
2023-02-22 06:45:10 - progress_bar.py[line:274] - INFO: epoch 017:    196 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=6780, lr=3.49413e-05, gnorm=0.238, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34370
2023-02-22 06:45:21 - progress_bar.py[line:274] - INFO: epoch 017:    206 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=6790, lr=3.49161e-05, gnorm=0.523, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34382
2023-02-22 06:45:32 - progress_bar.py[line:274] - INFO: epoch 017:    216 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=6800, lr=3.48908e-05, gnorm=0.162, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34393
2023-02-22 06:45:43 - progress_bar.py[line:274] - INFO: epoch 017:    226 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.5, ups=0.93, wpb=111.4, bsz=40, num_updates=6810, lr=3.48655e-05, gnorm=0.321, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34404
2023-02-22 06:45:54 - progress_bar.py[line:274] - INFO: epoch 017:    236 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=6820, lr=3.48402e-05, gnorm=0.16, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34415
2023-02-22 06:46:05 - progress_bar.py[line:274] - INFO: epoch 017:    246 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=6830, lr=3.48149e-05, gnorm=0.105, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34426
2023-02-22 06:46:17 - progress_bar.py[line:274] - INFO: epoch 017:    256 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=96.6, ups=0.87, wpb=111, bsz=40, num_updates=6840, lr=3.47896e-05, gnorm=0.272, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34437
2023-02-22 06:46:28 - progress_bar.py[line:274] - INFO: epoch 017:    266 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=6850, lr=3.47644e-05, gnorm=0.495, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34449
2023-02-22 06:46:39 - progress_bar.py[line:274] - INFO: epoch 017:    276 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=6860, lr=3.47391e-05, gnorm=0.151, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34460
2023-02-22 06:46:50 - progress_bar.py[line:274] - INFO: epoch 017:    286 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=6870, lr=3.47138e-05, gnorm=0.501, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34471
2023-02-22 06:47:01 - progress_bar.py[line:274] - INFO: epoch 017:    296 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.89, wpb=109.6, bsz=40, num_updates=6880, lr=3.46885e-05, gnorm=0.327, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=34482
2023-02-22 06:47:13 - progress_bar.py[line:274] - INFO: epoch 017:    306 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=6890, lr=3.46632e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34494
2023-02-22 06:47:24 - progress_bar.py[line:274] - INFO: epoch 017:    316 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=6900, lr=3.46379e-05, gnorm=0.584, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34505
2023-02-22 06:47:35 - progress_bar.py[line:274] - INFO: epoch 017:    326 / 412 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.039, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=6910, lr=3.46127e-05, gnorm=0.127, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34516
2023-02-22 06:47:46 - progress_bar.py[line:274] - INFO: epoch 017:    336 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.4, bsz=40, num_updates=6920, lr=3.45874e-05, gnorm=0.346, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34527
2023-02-22 06:47:57 - progress_bar.py[line:274] - INFO: epoch 017:    346 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=6930, lr=3.45621e-05, gnorm=0.529, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34538
2023-02-22 06:48:08 - progress_bar.py[line:274] - INFO: epoch 017:    356 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.93, wpb=110.1, bsz=40, num_updates=6940, lr=3.45368e-05, gnorm=0.412, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34549
2023-02-22 06:48:19 - progress_bar.py[line:274] - INFO: epoch 017:    366 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=6950, lr=3.45115e-05, gnorm=0.42, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34560
2023-02-22 06:48:30 - progress_bar.py[line:274] - INFO: epoch 017:    376 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.3, ups=0.92, wpb=111.4, bsz=40, num_updates=6960, lr=3.44862e-05, gnorm=0.265, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34571
2023-02-22 06:48:41 - progress_bar.py[line:274] - INFO: epoch 017:    386 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.9, bsz=40, num_updates=6970, lr=3.4461e-05, gnorm=0.364, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34582
2023-02-22 06:48:53 - progress_bar.py[line:274] - INFO: epoch 017:    396 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.1, ups=0.87, wpb=111.3, bsz=40, num_updates=6980, lr=3.44357e-05, gnorm=0.299, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34594
2023-02-22 06:49:04 - progress_bar.py[line:274] - INFO: epoch 017:    406 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.88, wpb=110.8, bsz=40, num_updates=6990, lr=3.44104e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34605
2023-02-22 06:49:11 - train.py[line:339] - INFO: end of epoch 17 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 06:49:11 - progress_bar.py[line:282] - INFO: epoch 017 | loss 0.236 | loss_v1 0 | loss_v2 0 | nll_loss 0.05 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 99.1 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 6996 | lr 3.43952e-05 | gnorm 0.356 | clip 6.8 | loss_scale 1024 | train_wall 457 | gb_free 10.7 | ema_decay 0.9999 | wall 34612
2023-02-22 06:49:11 - trainer.py[line:694] - INFO: loading train data for epoch 18
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 06:49:11 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 06:49:12 - trainer.py[line:758] - INFO: begin training epoch 18
2023-02-22 06:49:12 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 06:49:18 - progress_bar.py[line:274] - INFO: epoch 018:      4 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=82.4, ups=0.75, wpb=110.6, bsz=40, num_updates=7000, lr=3.43851e-05, gnorm=0.317, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=34619
2023-02-22 06:49:18 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 06:49:19 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 06:49:19 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 06:51:21 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 06:51:21 - train.py[line:551] - INFO: load:1.07 valid_run:122.13 task_valid:118.59 collect_output:2.44
2023-02-22 06:53:21 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 06:53:21 - train.py[line:551] - INFO: load:1.09 valid_run:241.93 task_valid:233.21 collect_output:6.61
2023-02-22 06:55:23 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 06:55:23 - train.py[line:551] - INFO: load:1.11 valid_run:363.92 task_valid:348.59 collect_output:12.20
2023-02-22 06:57:25 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 06:57:25 - train.py[line:551] - INFO: load:1.14 valid_run:485.91 task_valid:461.19 collect_output:20.59
2023-02-22 06:59:26 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 06:59:26 - train.py[line:551] - INFO: load:1.16 valid_run:606.19 task_valid:577.56 collect_output:23.50
2023-02-22 07:01:28 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 07:01:28 - train.py[line:551] - INFO: load:1.19 valid_run:728.91 task_valid:695.35 collect_output:27.41
2023-02-22 07:03:32 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 07:03:32 - train.py[line:551] - INFO: load:1.21 valid_run:852.01 task_valid:812.50 collect_output:32.38
2023-02-22 07:05:34 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 07:05:34 - train.py[line:551] - INFO: load:1.23 valid_run:973.94 task_valid:927.90 collect_output:37.92
2023-02-22 07:07:38 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 07:07:38 - train.py[line:551] - INFO: load:1.26 valid_run:1098.00 task_valid:1044.36 collect_output:44.50
2023-02-22 07:09:40 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 07:09:40 - train.py[line:551] - INFO: load:1.28 valid_run:1219.83 task_valid:1156.05 collect_output:53.65
2023-02-22 07:11:40 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 07:11:40 - train.py[line:551] - INFO: load:1.31 valid_run:1340.17 task_valid:1270.59 collect_output:58.45
2023-02-22 07:13:42 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 07:13:42 - train.py[line:551] - INFO: load:1.33 valid_run:1461.99 task_valid:1386.58 collect_output:63.29
2023-02-22 07:15:41 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 07:15:41 - train.py[line:551] - INFO: load:1.35 valid_run:1580.86 task_valid:1499.43 collect_output:68.31
2023-02-22 07:17:42 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 07:17:42 - train.py[line:551] - INFO: load:1.38 valid_run:1701.73 task_valid:1616.42 collect_output:71.19
2023-02-22 07:19:43 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 07:19:43 - train.py[line:551] - INFO: load:1.40 valid_run:1822.84 task_valid:1731.54 collect_output:76.21
2023-02-22 07:21:44 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 07:21:44 - train.py[line:551] - INFO: load:1.43 valid_run:1944.16 task_valid:1844.32 collect_output:83.76
2023-02-22 07:23:46 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 07:23:46 - train.py[line:551] - INFO: load:1.45 valid_run:2065.41 task_valid:1959.53 collect_output:88.79
2023-02-22 07:25:46 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 07:25:46 - train.py[line:551] - INFO: load:1.48 valid_run:2185.68 task_valid:2076.40 collect_output:91.21
2023-02-22 07:27:47 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 07:27:47 - train.py[line:551] - INFO: load:1.50 valid_run:2306.73 task_valid:2192.30 collect_output:95.38
2023-02-22 07:29:47 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 07:29:47 - train.py[line:551] - INFO: load:1.53 valid_run:2427.06 task_valid:2307.89 collect_output:99.13
2023-02-22 07:31:49 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 07:31:49 - train.py[line:551] - INFO: load:1.55 valid_run:2548.82 task_valid:2423.46 collect_output:104.34
2023-02-22 07:33:51 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 07:33:51 - train.py[line:551] - INFO: load:1.58 valid_run:2670.54 task_valid:2541.35 collect_output:107.16
2023-02-22 07:35:51 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 07:35:51 - train.py[line:551] - INFO: load:1.60 valid_run:2790.77 task_valid:2654.52 collect_output:113.23
2023-02-22 07:37:51 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 07:37:51 - train.py[line:551] - INFO: load:1.63 valid_run:2910.49 task_valid:2769.68 collect_output:116.77
2023-02-22 07:39:53 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 07:39:53 - train.py[line:551] - INFO: load:1.65 valid_run:3032.21 task_valid:2884.92 collect_output:122.23
2023-02-22 07:41:56 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 07:41:56 - train.py[line:551] - INFO: load:1.68 valid_run:3155.43 task_valid:2999.93 collect_output:129.45
2023-02-22 07:43:56 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 07:43:56 - train.py[line:551] - INFO: load:1.70 valid_run:3274.98 task_valid:3113.02 collect_output:134.92
2023-02-22 07:45:57 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 07:45:57 - train.py[line:551] - INFO: load:1.73 valid_run:3396.36 task_valid:3231.54 collect_output:136.79
2023-02-22 07:47:59 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 07:47:59 - train.py[line:551] - INFO: load:1.75 valid_run:3518.30 task_valid:3346.08 collect_output:143.18
2023-02-22 07:50:01 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 07:50:01 - train.py[line:551] - INFO: load:1.78 valid_run:3640.01 task_valid:3463.59 collect_output:146.39
2023-02-22 07:52:01 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 07:52:01 - train.py[line:551] - INFO: load:1.80 valid_run:3760.56 task_valid:3581.10 collect_output:148.42

====================================================================================================
SGG eval:     R @ 50: 0.6401;     R @ 100: 0.6579;     R @ 500: 0.6807;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4392;    mR @ 100: 0.4620;    mR @ 500: 0.4929;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6250) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3774) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9062) (playing:0.0000) (riding:0.9167) (says:0.0000) (sitting on:0.6706) (standing on:0.5813) (using:0.6000) (walking in:0.3333) (walking on:0.2838) (watching:0.4167) 
--------------------------------------------------------
====================================================================================================

2023-02-22 07:52:32 - train.py[line:487] - INFO: 0.6578753501400559
2023-02-22 07:52:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6401;     R @ 100: 0.6579;     R @ 500: 0.6807;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4392;    mR @ 100: 0.4620;    mR @ 500: 0.4929;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6250) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3774) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9062) (playing:0.0000) (riding:0.9167) (says:0.0000) (sitting on:0.6706) (standing on:0.5813) (using:0.6000) (walking in:0.3333) (walking on:0.2838) (watching:0.4167) 
--------------------------------------------------------
====================================================================================================

2023-02-22 07:52:32 - progress_bar.py[line:282] - INFO: epoch 018 | valid on 'valid' subset | loss 0.229 | loss_v1 0 | loss_v2 0 | nll_loss 0.053 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.657875 | ppl 1.04 | vqa_score 0.5011 | wps 118.3 | wpb 72 | bsz 24 | num_updates 7000 | best_R@100 0.68186
2023-02-22 07:52:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 18 @ 7000 updates
2023-02-22 07:52:32 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_18_7000.pt
2023-02-22 07:52:38 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_18_7000.pt
2023-02-22 07:52:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_18_7000.pt (epoch 18 @ 7000 updates, score 0.6578753501400559) (writing took 8.223717862740159 seconds)
2023-02-22 07:52:52 - progress_bar.py[line:274] - INFO: epoch 018:     14 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=110.7, bsz=40, num_updates=7010, lr=3.43598e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38433
2023-02-22 07:53:03 - progress_bar.py[line:274] - INFO: epoch 018:     24 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=7020, lr=3.43345e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38444
2023-02-22 07:53:14 - progress_bar.py[line:274] - INFO: epoch 018:     34 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=7030, lr=3.43093e-05, gnorm=0.295, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38455
2023-02-22 07:53:25 - progress_bar.py[line:274] - INFO: epoch 018:     44 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.6, ups=0.88, wpb=111.5, bsz=40, num_updates=7040, lr=3.4284e-05, gnorm=0.419, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38466
2023-02-22 07:53:36 - progress_bar.py[line:274] - INFO: epoch 018:     54 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103, ups=0.93, wpb=111, bsz=40, num_updates=7050, lr=3.42587e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38477
2023-02-22 07:53:47 - progress_bar.py[line:274] - INFO: epoch 018:     64 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=7060, lr=3.42334e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38488
2023-02-22 07:53:59 - progress_bar.py[line:274] - INFO: epoch 018:     74 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=7070, lr=3.42081e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38500
2023-02-22 07:54:10 - progress_bar.py[line:274] - INFO: epoch 018:     84 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.91, wpb=110.7, bsz=40, num_updates=7080, lr=3.41828e-05, gnorm=0.32, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38511
2023-02-22 07:54:21 - progress_bar.py[line:274] - INFO: epoch 018:     94 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.91, wpb=111.5, bsz=40, num_updates=7090, lr=3.41576e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38522
2023-02-22 07:54:32 - progress_bar.py[line:274] - INFO: epoch 018:    104 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=7100, lr=3.41323e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38533
2023-02-22 07:54:43 - progress_bar.py[line:274] - INFO: epoch 018:    114 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=111.3, bsz=40, num_updates=7110, lr=3.4107e-05, gnorm=0.467, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38544
2023-02-22 07:54:55 - progress_bar.py[line:274] - INFO: epoch 018:    124 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.9, ups=0.88, wpb=111.5, bsz=40, num_updates=7120, lr=3.40817e-05, gnorm=0.19, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38556
2023-02-22 07:55:06 - progress_bar.py[line:274] - INFO: epoch 018:    134 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=110.6, bsz=40, num_updates=7130, lr=3.40564e-05, gnorm=0.518, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38567
2023-02-22 07:55:17 - progress_bar.py[line:274] - INFO: epoch 018:    144 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.89, wpb=112.2, bsz=40, num_updates=7140, lr=3.40311e-05, gnorm=0.362, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38578
2023-02-22 07:55:28 - progress_bar.py[line:274] - INFO: epoch 018:    154 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=7150, lr=3.40059e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38589
2023-02-22 07:55:40 - progress_bar.py[line:274] - INFO: epoch 018:    164 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=7160, lr=3.39806e-05, gnorm=0.36, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38601
2023-02-22 07:55:51 - progress_bar.py[line:274] - INFO: epoch 018:    174 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=7170, lr=3.39553e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38612
2023-02-22 07:56:02 - progress_bar.py[line:274] - INFO: epoch 018:    184 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=110.3, bsz=40, num_updates=7180, lr=3.393e-05, gnorm=0.736, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38623
2023-02-22 07:56:13 - progress_bar.py[line:274] - INFO: epoch 018:    194 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.91, wpb=110.7, bsz=40, num_updates=7190, lr=3.39047e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38634
2023-02-22 07:56:24 - progress_bar.py[line:274] - INFO: epoch 018:    204 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=7200, lr=3.38794e-05, gnorm=0.24, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38645
2023-02-22 07:56:36 - progress_bar.py[line:274] - INFO: epoch 018:    214 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.87, wpb=111.4, bsz=40, num_updates=7210, lr=3.38542e-05, gnorm=0.361, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38657
2023-02-22 07:56:47 - progress_bar.py[line:274] - INFO: epoch 018:    224 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.91, wpb=110.4, bsz=40, num_updates=7220, lr=3.38289e-05, gnorm=0.321, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38668
2023-02-22 07:56:58 - progress_bar.py[line:274] - INFO: epoch 018:    234 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=7230, lr=3.38036e-05, gnorm=0.383, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38679
2023-02-22 07:57:09 - progress_bar.py[line:274] - INFO: epoch 018:    244 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=7240, lr=3.37783e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38690
2023-02-22 07:57:21 - progress_bar.py[line:274] - INFO: epoch 018:    254 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.89, wpb=111.4, bsz=40, num_updates=7250, lr=3.3753e-05, gnorm=0.364, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38702
2023-02-22 07:57:32 - progress_bar.py[line:274] - INFO: epoch 018:    264 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.1, ups=0.88, wpb=111.4, bsz=40, num_updates=7260, lr=3.37278e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38713
2023-02-22 07:57:43 - progress_bar.py[line:274] - INFO: epoch 018:    274 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=7270, lr=3.37025e-05, gnorm=0.458, clip=10, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=38724
2023-02-22 07:57:55 - progress_bar.py[line:274] - INFO: epoch 018:    284 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=7280, lr=3.36772e-05, gnorm=0.394, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38736
2023-02-22 07:58:05 - progress_bar.py[line:274] - INFO: epoch 018:    294 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.7, ups=0.93, wpb=111.9, bsz=40, num_updates=7290, lr=3.36519e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38746
2023-02-22 07:58:17 - progress_bar.py[line:274] - INFO: epoch 018:    304 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.6, ups=0.89, wpb=111.4, bsz=40, num_updates=7300, lr=3.36266e-05, gnorm=0.336, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38758
2023-02-22 07:58:27 - progress_bar.py[line:274] - INFO: epoch 018:    314 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.5, ups=0.94, wpb=111, bsz=40, num_updates=7310, lr=3.36013e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38768
2023-02-22 07:58:39 - progress_bar.py[line:274] - INFO: epoch 018:    324 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=7320, lr=3.35761e-05, gnorm=0.309, clip=10, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=38779
2023-02-22 07:58:50 - progress_bar.py[line:274] - INFO: epoch 018:    334 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.8, ups=0.91, wpb=112, bsz=40, num_updates=7330, lr=3.35508e-05, gnorm=0.326, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38790
2023-02-22 07:59:01 - progress_bar.py[line:274] - INFO: epoch 018:    344 / 412 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.88, wpb=112.1, bsz=40, num_updates=7340, lr=3.35255e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38802
2023-02-22 07:59:12 - progress_bar.py[line:274] - INFO: epoch 018:    354 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.6, bsz=40, num_updates=7350, lr=3.35002e-05, gnorm=0.624, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38813
2023-02-22 07:59:23 - progress_bar.py[line:274] - INFO: epoch 018:    364 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.9, wpb=111.5, bsz=40, num_updates=7360, lr=3.34749e-05, gnorm=0.247, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38824
2023-02-22 07:59:34 - progress_bar.py[line:274] - INFO: epoch 018:    374 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=7370, lr=3.34496e-05, gnorm=0.315, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38835
2023-02-22 07:59:45 - progress_bar.py[line:274] - INFO: epoch 018:    384 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.92, wpb=111.7, bsz=40, num_updates=7380, lr=3.34244e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38846
2023-02-22 07:59:56 - progress_bar.py[line:274] - INFO: epoch 018:    394 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.9, wpb=111.6, bsz=40, num_updates=7390, lr=3.33991e-05, gnorm=0.365, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38857
2023-02-22 08:00:07 - progress_bar.py[line:274] - INFO: epoch 018:    404 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=7400, lr=3.33738e-05, gnorm=0.542, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38868
2023-02-22 08:00:17 - train.py[line:339] - INFO: end of epoch 18 (average epoch stats below)
2023-02-22 08:00:17 - progress_bar.py[line:282] - INFO: epoch 018 | loss 0.235 | loss_v1 0 | loss_v2 0 | nll_loss 0.049 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.03 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 7408 | lr 3.33536e-05 | gnorm 0.344 | clip 6.3 | loss_scale 1024 | train_wall 458 | gb_free 10.7 | ema_decay 0.9999 | wall 38878
2023-02-22 08:00:17 - trainer.py[line:694] - INFO: loading train data for epoch 19
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 08:00:17 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 08:00:17 - trainer.py[line:758] - INFO: begin training epoch 19
2023-02-22 08:00:17 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 08:00:21 - progress_bar.py[line:274] - INFO: epoch 019:      2 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=82.3, ups=0.74, wpb=111.5, bsz=40, num_updates=7410, lr=3.33485e-05, gnorm=0.412, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=38882
2023-02-22 08:00:32 - progress_bar.py[line:274] - INFO: epoch 019:     12 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.1, ups=0.91, wpb=110.2, bsz=40, num_updates=7420, lr=3.33232e-05, gnorm=0.206, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38893
2023-02-22 08:00:40 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 08:00:45 - progress_bar.py[line:274] - INFO: epoch 019:     23 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=89.3, ups=0.8, wpb=111.3, bsz=40, num_updates=7430, lr=3.32979e-05, gnorm=0.146, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=38905
2023-02-22 08:00:56 - progress_bar.py[line:274] - INFO: epoch 019:     33 / 412 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.8, ups=0.9, wpb=112.6, bsz=40, num_updates=7440, lr=3.32727e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38917
2023-02-22 08:01:06 - progress_bar.py[line:274] - INFO: epoch 019:     43 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.93, wpb=109.7, bsz=40, num_updates=7450, lr=3.32474e-05, gnorm=0.283, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38927
2023-02-22 08:01:18 - progress_bar.py[line:274] - INFO: epoch 019:     53 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.1, ups=0.89, wpb=109.2, bsz=40, num_updates=7460, lr=3.32221e-05, gnorm=0.617, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38939
2023-02-22 08:01:29 - progress_bar.py[line:274] - INFO: epoch 019:     63 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=7470, lr=3.31968e-05, gnorm=0.127, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38950
2023-02-22 08:01:40 - progress_bar.py[line:274] - INFO: epoch 019:     73 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.91, wpb=110.4, bsz=40, num_updates=7480, lr=3.31715e-05, gnorm=0.381, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38961
2023-02-22 08:01:51 - progress_bar.py[line:274] - INFO: epoch 019:     83 / 412 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.7, ups=0.93, wpb=112, bsz=40, num_updates=7490, lr=3.31462e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38972
2023-02-22 08:02:02 - progress_bar.py[line:274] - INFO: epoch 019:     93 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=7500, lr=3.3121e-05, gnorm=0.351, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=38983
2023-02-22 08:02:13 - progress_bar.py[line:274] - INFO: epoch 019:    103 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=7510, lr=3.30957e-05, gnorm=0.633, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38994
2023-02-22 08:02:24 - progress_bar.py[line:274] - INFO: epoch 019:    113 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.92, wpb=110.8, bsz=40, num_updates=7520, lr=3.30704e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39005
2023-02-22 08:02:36 - progress_bar.py[line:274] - INFO: epoch 019:    123 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=7530, lr=3.30451e-05, gnorm=0.638, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39017
2023-02-22 08:02:47 - progress_bar.py[line:274] - INFO: epoch 019:    133 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.9, wpb=111.5, bsz=40, num_updates=7540, lr=3.30198e-05, gnorm=0.588, clip=20, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=39028
2023-02-22 08:02:58 - progress_bar.py[line:274] - INFO: epoch 019:    143 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=7550, lr=3.29945e-05, gnorm=0.385, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39039
2023-02-22 08:03:09 - progress_bar.py[line:274] - INFO: epoch 019:    153 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=7560, lr=3.29693e-05, gnorm=0.382, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39050
2023-02-22 08:03:20 - progress_bar.py[line:274] - INFO: epoch 019:    163 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=7570, lr=3.2944e-05, gnorm=0.354, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39061
2023-02-22 08:03:31 - progress_bar.py[line:274] - INFO: epoch 019:    173 / 412 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=7580, lr=3.29187e-05, gnorm=0.317, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39072
2023-02-22 08:03:43 - progress_bar.py[line:274] - INFO: epoch 019:    183 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=95.9, ups=0.87, wpb=110.5, bsz=40, num_updates=7590, lr=3.28934e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39084
2023-02-22 08:03:54 - progress_bar.py[line:274] - INFO: epoch 019:    193 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.8, ups=0.91, wpb=111.4, bsz=40, num_updates=7600, lr=3.28681e-05, gnorm=0.28, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=39095
2023-02-22 08:04:05 - progress_bar.py[line:274] - INFO: epoch 019:    203 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=7610, lr=3.28428e-05, gnorm=0.647, clip=30, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39106
2023-02-22 08:04:16 - progress_bar.py[line:274] - INFO: epoch 019:    213 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.88, wpb=112.4, bsz=40, num_updates=7620, lr=3.28176e-05, gnorm=0.314, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39117
2023-02-22 08:04:27 - progress_bar.py[line:274] - INFO: epoch 019:    223 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.92, wpb=112, bsz=40, num_updates=7630, lr=3.27923e-05, gnorm=1.077, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39128
2023-02-22 08:04:38 - progress_bar.py[line:274] - INFO: epoch 019:    233 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.91, wpb=110.7, bsz=40, num_updates=7640, lr=3.2767e-05, gnorm=0.355, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39139
2023-02-22 08:04:50 - progress_bar.py[line:274] - INFO: epoch 019:    243 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=7650, lr=3.27417e-05, gnorm=0.397, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39151
2023-02-22 08:05:01 - progress_bar.py[line:274] - INFO: epoch 019:    253 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.91, wpb=110.9, bsz=40, num_updates=7660, lr=3.27164e-05, gnorm=0.176, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39162
2023-02-22 08:05:12 - progress_bar.py[line:274] - INFO: epoch 019:    263 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=7670, lr=3.26911e-05, gnorm=0.556, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39173
2023-02-22 08:05:24 - progress_bar.py[line:274] - INFO: epoch 019:    273 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=7680, lr=3.26659e-05, gnorm=0.239, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39185
2023-02-22 08:05:35 - progress_bar.py[line:274] - INFO: epoch 019:    283 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=7690, lr=3.26406e-05, gnorm=0.346, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39196
2023-02-22 08:05:46 - progress_bar.py[line:274] - INFO: epoch 019:    293 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=112, bsz=40, num_updates=7700, lr=3.26153e-05, gnorm=0.416, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39207
2023-02-22 08:05:57 - progress_bar.py[line:274] - INFO: epoch 019:    303 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.8, ups=0.89, wpb=109.8, bsz=40, num_updates=7710, lr=3.259e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39218
2023-02-22 08:06:09 - progress_bar.py[line:274] - INFO: epoch 019:    313 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.4, ups=0.88, wpb=111.5, bsz=40, num_updates=7720, lr=3.25647e-05, gnorm=0.286, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39230
2023-02-22 08:06:20 - progress_bar.py[line:274] - INFO: epoch 019:    323 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=7730, lr=3.25394e-05, gnorm=0.591, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39241
2023-02-22 08:06:31 - progress_bar.py[line:274] - INFO: epoch 019:    333 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.92, wpb=111.6, bsz=40, num_updates=7740, lr=3.25142e-05, gnorm=0.426, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39252
2023-02-22 08:06:42 - progress_bar.py[line:274] - INFO: epoch 019:    343 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.3, bsz=40, num_updates=7750, lr=3.24889e-05, gnorm=0.432, clip=20, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39263
2023-02-22 08:06:53 - progress_bar.py[line:274] - INFO: epoch 019:    353 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=7760, lr=3.24636e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39274
2023-02-22 08:07:04 - progress_bar.py[line:274] - INFO: epoch 019:    363 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=111.9, bsz=40, num_updates=7770, lr=3.24383e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39285
2023-02-22 08:07:15 - progress_bar.py[line:274] - INFO: epoch 019:    373 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=7780, lr=3.2413e-05, gnorm=0.477, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39296
2023-02-22 08:07:27 - progress_bar.py[line:274] - INFO: epoch 019:    383 / 412 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.7, ups=0.87, wpb=112.4, bsz=40, num_updates=7790, lr=3.23877e-05, gnorm=0.175, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39308
2023-02-22 08:07:38 - progress_bar.py[line:274] - INFO: epoch 019:    393 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=7800, lr=3.23625e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39319
2023-02-22 08:07:49 - progress_bar.py[line:274] - INFO: epoch 019:    403 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.92, wpb=111.9, bsz=40, num_updates=7810, lr=3.23372e-05, gnorm=0.283, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39330
2023-02-22 08:07:59 - train.py[line:339] - INFO: end of epoch 19 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 08:07:59 - progress_bar.py[line:282] - INFO: epoch 019 | loss 0.234 | loss_v1 0 | loss_v2 0 | nll_loss 0.048 | ntokens 111.124 | nsentences 40 | sample_size 111.124 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.03 | wps 98.9 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 7819 | lr 3.23144e-05 | gnorm 0.37 | clip 8.5 | loss_scale 1024 | train_wall 457 | gb_free 10.6 | ema_decay 0.9999 | wall 39340
2023-02-22 08:07:59 - trainer.py[line:694] - INFO: loading train data for epoch 20
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 08:07:59 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 08:07:59 - trainer.py[line:758] - INFO: begin training epoch 20
2023-02-22 08:07:59 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 08:08:02 - progress_bar.py[line:274] - INFO: epoch 020:      1 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=86.4, ups=0.78, wpb=110.6, bsz=40, num_updates=7820, lr=3.23119e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39343
2023-02-22 08:08:13 - progress_bar.py[line:274] - INFO: epoch 020:     11 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=7830, lr=3.22866e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39354
2023-02-22 08:08:24 - progress_bar.py[line:274] - INFO: epoch 020:     21 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=7840, lr=3.22613e-05, gnorm=0.349, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39365
2023-02-22 08:08:35 - progress_bar.py[line:274] - INFO: epoch 020:     31 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.2, ups=0.88, wpb=111.6, bsz=40, num_updates=7850, lr=3.2236e-05, gnorm=0.123, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39376
2023-02-22 08:08:46 - progress_bar.py[line:274] - INFO: epoch 020:     41 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.92, wpb=111, bsz=40, num_updates=7860, lr=3.22108e-05, gnorm=0.31, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39387
2023-02-22 08:08:58 - progress_bar.py[line:274] - INFO: epoch 020:     51 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=7870, lr=3.21855e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39399
2023-02-22 08:09:09 - progress_bar.py[line:274] - INFO: epoch 020:     61 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=7880, lr=3.21602e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39410
2023-02-22 08:09:20 - progress_bar.py[line:274] - INFO: epoch 020:     71 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.91, wpb=112.1, bsz=40, num_updates=7890, lr=3.21349e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39421
2023-02-22 08:09:31 - progress_bar.py[line:274] - INFO: epoch 020:     81 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=7900, lr=3.21096e-05, gnorm=0.14, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39432
2023-02-22 08:09:42 - progress_bar.py[line:274] - INFO: epoch 020:     91 / 412 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.04, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.9, wpb=112.8, bsz=40, num_updates=7910, lr=3.20843e-05, gnorm=0.158, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39443
2023-02-22 08:09:53 - progress_bar.py[line:274] - INFO: epoch 020:    101 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.9, wpb=110, bsz=40, num_updates=7920, lr=3.20591e-05, gnorm=0.561, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39454
2023-02-22 08:10:04 - progress_bar.py[line:274] - INFO: epoch 020:    111 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.9, ups=0.91, wpb=112.6, bsz=40, num_updates=7930, lr=3.20338e-05, gnorm=0.419, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39465
2023-02-22 08:10:16 - progress_bar.py[line:274] - INFO: epoch 020:    121 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.89, wpb=110, bsz=40, num_updates=7940, lr=3.20085e-05, gnorm=0.367, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39477
2023-02-22 08:10:27 - progress_bar.py[line:274] - INFO: epoch 020:    131 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=7950, lr=3.19832e-05, gnorm=0.185, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39488
2023-02-22 08:10:38 - progress_bar.py[line:274] - INFO: epoch 020:    141 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.91, wpb=111.1, bsz=40, num_updates=7960, lr=3.19579e-05, gnorm=0.482, clip=20, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39499
2023-02-22 08:10:49 - progress_bar.py[line:274] - INFO: epoch 020:    151 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=7970, lr=3.19326e-05, gnorm=0.12, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39510
2023-02-22 08:11:01 - progress_bar.py[line:274] - INFO: epoch 020:    161 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=96.6, ups=0.87, wpb=110.9, bsz=40, num_updates=7980, lr=3.19074e-05, gnorm=0.219, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39521
2023-02-22 08:11:12 - progress_bar.py[line:274] - INFO: epoch 020:    171 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=7990, lr=3.18821e-05, gnorm=0.111, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39533
2023-02-22 08:11:23 - progress_bar.py[line:274] - INFO: epoch 020:    181 / 412 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.041, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.91, wpb=111.3, bsz=40, num_updates=8000, lr=3.18568e-05, gnorm=0.236, clip=10, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39544
2023-02-22 08:11:23 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 08:11:24 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 08:11:24 - train.py[line:551] - INFO: load:1.03 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 08:13:27 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 08:13:27 - train.py[line:551] - INFO: load:1.06 valid_run:122.24 task_valid:118.44 collect_output:2.68
2023-02-22 08:15:27 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 08:15:27 - train.py[line:551] - INFO: load:1.09 valid_run:242.20 task_valid:233.48 collect_output:6.54
2023-02-22 08:17:29 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 08:17:29 - train.py[line:551] - INFO: load:1.11 valid_run:364.22 task_valid:349.00 collect_output:12.02
2023-02-22 08:19:31 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 08:19:31 - train.py[line:551] - INFO: load:1.13 valid_run:486.27 task_valid:461.79 collect_output:20.28
2023-02-22 08:21:31 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 08:21:31 - train.py[line:551] - INFO: load:1.16 valid_run:606.55 task_valid:578.30 collect_output:23.04
2023-02-22 08:23:34 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 08:23:34 - train.py[line:551] - INFO: load:1.18 valid_run:729.20 task_valid:696.17 collect_output:26.82
2023-02-22 08:25:37 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 08:25:37 - train.py[line:551] - INFO: load:1.21 valid_run:852.26 task_valid:813.46 collect_output:31.59
2023-02-22 08:27:39 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 08:27:39 - train.py[line:551] - INFO: load:1.23 valid_run:974.06 task_valid:929.05 collect_output:36.79
2023-02-22 08:29:43 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 08:29:43 - train.py[line:551] - INFO: load:1.26 valid_run:1097.95 task_valid:1045.26 collect_output:43.46
2023-02-22 08:31:45 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 08:31:45 - train.py[line:551] - INFO: load:1.28 valid_run:1220.01 task_valid:1156.99 collect_output:52.78
2023-02-22 08:33:45 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 08:33:45 - train.py[line:551] - INFO: load:1.31 valid_run:1340.35 task_valid:1271.61 collect_output:57.49
2023-02-22 08:35:47 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 08:35:47 - train.py[line:551] - INFO: load:1.33 valid_run:1461.88 task_valid:1387.45 collect_output:62.18
2023-02-22 08:37:46 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 08:37:46 - train.py[line:551] - INFO: load:1.36 valid_run:1580.62 task_valid:1500.15 collect_output:67.25
2023-02-22 08:39:46 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 08:39:46 - train.py[line:551] - INFO: load:1.38 valid_run:1701.31 task_valid:1617.01 collect_output:70.07
2023-02-22 08:41:47 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 08:41:47 - train.py[line:551] - INFO: load:1.41 valid_run:1822.08 task_valid:1732.20 collect_output:74.68
2023-02-22 08:43:48 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 08:43:48 - train.py[line:551] - INFO: load:1.43 valid_run:1943.24 task_valid:1845.05 collect_output:81.99
2023-02-22 08:45:50 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 08:45:50 - train.py[line:551] - INFO: load:1.46 valid_run:2064.44 task_valid:1960.10 collect_output:87.14
2023-02-22 08:47:50 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 08:47:50 - train.py[line:551] - INFO: load:1.48 valid_run:2184.72 task_valid:2077.04 collect_output:89.50
2023-02-22 08:49:51 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 08:49:51 - train.py[line:551] - INFO: load:1.51 valid_run:2305.70 task_valid:2193.08 collect_output:93.44
2023-02-22 08:51:51 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 08:51:51 - train.py[line:551] - INFO: load:1.53 valid_run:2426.03 task_valid:2308.88 collect_output:96.96
2023-02-22 08:53:53 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 08:53:53 - train.py[line:551] - INFO: load:1.56 valid_run:2547.79 task_valid:2424.49 collect_output:102.10
2023-02-22 08:55:55 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 08:55:55 - train.py[line:551] - INFO: load:1.58 valid_run:2669.58 task_valid:2542.29 collect_output:105.10
2023-02-22 08:57:55 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 08:57:55 - train.py[line:551] - INFO: load:1.61 valid_run:2789.84 task_valid:2655.52 collect_output:111.15
2023-02-22 08:59:55 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 08:59:55 - train.py[line:551] - INFO: load:1.63 valid_run:2909.45 task_valid:2770.69 collect_output:114.59
2023-02-22 09:01:57 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 09:01:57 - train.py[line:551] - INFO: load:1.66 valid_run:3031.15 task_valid:2885.96 collect_output:120.02
2023-02-22 09:04:00 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 09:04:00 - train.py[line:551] - INFO: load:1.68 valid_run:3154.26 task_valid:3000.96 collect_output:127.11
2023-02-22 09:06:00 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 09:06:00 - train.py[line:551] - INFO: load:1.71 valid_run:3273.81 task_valid:3114.08 collect_output:132.55
2023-02-22 09:08:01 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 09:08:01 - train.py[line:551] - INFO: load:1.73 valid_run:3395.10 task_valid:3232.45 collect_output:134.49
2023-02-22 09:10:03 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 09:10:03 - train.py[line:551] - INFO: load:1.76 valid_run:3516.98 task_valid:3347.16 collect_output:140.65
2023-02-22 09:12:05 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 09:12:05 - train.py[line:551] - INFO: load:1.78 valid_run:3638.70 task_valid:3464.77 collect_output:143.77
2023-02-22 09:14:05 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 09:14:05 - train.py[line:551] - INFO: load:1.81 valid_run:3759.28 task_valid:3582.27 collect_output:145.85

====================================================================================================
SGG eval:     R @ 50: 0.6383;     R @ 100: 0.6578;     R @ 500: 0.6793;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4352;    mR @ 100: 0.4574;    mR @ 500: 0.4852;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6250) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9062) (playing:0.0000) (riding:0.9069) (says:0.0000) (sitting on:0.6774) (standing on:0.6097) (using:0.6000) (walking in:0.3333) (walking on:0.2568) (watching:0.4167) 
--------------------------------------------------------
====================================================================================================

2023-02-22 09:14:36 - train.py[line:487] - INFO: 0.6578086834733894

====================================================================================================
SGG eval:     R @ 50: 0.6383;     R @ 100: 0.6578;     R @ 500: 0.6793;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4352;    mR @ 100: 0.4574;    mR @ 500: 0.4852;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6250) (covering:0.0000) (eating:0.7941) (flying in:1.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9062) (playing:0.0000) (riding:0.9069) (says:0.0000) (sitting on:0.6774) (standing on:0.6097) (using:0.6000) (walking in:0.3333) (walking on:0.2568) (watching:0.4167) 
--------------------------------------------------------
====================================================================================================

2023-02-22 09:14:36 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 09:14:36 - progress_bar.py[line:282] - INFO: epoch 020 | valid on 'valid' subset | loss 0.227 | loss_v1 0 | loss_v2 0 | nll_loss 0.058 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.657809 | ppl 1.04 | vqa_score 0.4854 | wps 118.3 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.68186
2023-02-22 09:14:36 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 8000 updates
2023-02-22 09:14:36 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_20_8000.pt
2023-02-22 09:14:42 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_20_8000.pt
2023-02-22 09:14:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new_pretrained_with_coco/1_B20_A1_E50_0.04_5e-5_480/checkpoint_20_8000.pt (epoch 20 @ 8000 updates, score 0.6578086834733894) (writing took 8.06208716519177 seconds)
2023-02-22 09:14:56 - progress_bar.py[line:274] - INFO: epoch 020:    191 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=0.3, ups=0, wpb=111.5, bsz=40, num_updates=8010, lr=3.18315e-05, gnorm=0.131, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43356
2023-02-22 09:15:07 - progress_bar.py[line:274] - INFO: epoch 020:    201 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.1, ups=0.9, wpb=109.6, bsz=40, num_updates=8020, lr=3.18062e-05, gnorm=0.199, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43368
2023-02-22 09:15:18 - progress_bar.py[line:274] - INFO: epoch 020:    211 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.1, ups=0.88, wpb=111.2, bsz=40, num_updates=8030, lr=3.17809e-05, gnorm=0.241, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43379
2023-02-22 09:15:29 - progress_bar.py[line:274] - INFO: epoch 020:    221 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.88, wpb=112.3, bsz=40, num_updates=8040, lr=3.17557e-05, gnorm=0.47, clip=10, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43390
2023-02-22 09:15:41 - progress_bar.py[line:274] - INFO: epoch 020:    231 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=95.8, ups=0.87, wpb=110.3, bsz=40, num_updates=8050, lr=3.17304e-05, gnorm=0.293, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43402
2023-02-22 09:15:52 - progress_bar.py[line:274] - INFO: epoch 020:    241 / 412 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=8060, lr=3.17051e-05, gnorm=0.246, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43413
2023-02-22 09:16:03 - progress_bar.py[line:274] - INFO: epoch 020:    251 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=8070, lr=3.16798e-05, gnorm=0.459, clip=20, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43424
2023-02-22 09:16:14 - progress_bar.py[line:274] - INFO: epoch 020:    261 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.6, ups=0.94, wpb=111.2, bsz=40, num_updates=8080, lr=3.16545e-05, gnorm=0.181, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43435
2023-02-22 09:16:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 09:16:26 - progress_bar.py[line:274] - INFO: epoch 020:    272 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=93.1, ups=0.84, wpb=111, bsz=40, num_updates=8090, lr=3.16292e-05, gnorm=0.431, clip=10, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=43447
2023-02-22 09:16:37 - progress_bar.py[line:274] - INFO: epoch 020:    282 / 412 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=8100, lr=3.1604e-05, gnorm=0.327, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43458
2023-02-22 09:16:48 - progress_bar.py[line:274] - INFO: epoch 020:    292 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=8110, lr=3.15787e-05, gnorm=0.176, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43469
2023-02-22 09:16:59 - progress_bar.py[line:274] - INFO: epoch 020:    302 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.2, ups=0.88, wpb=111.6, bsz=40, num_updates=8120, lr=3.15534e-05, gnorm=0.327, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43480
2023-02-22 09:17:10 - progress_bar.py[line:274] - INFO: epoch 020:    312 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=8130, lr=3.15281e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43491
2023-02-22 09:17:21 - progress_bar.py[line:274] - INFO: epoch 020:    322 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.2, ups=0.91, wpb=113, bsz=40, num_updates=8140, lr=3.15028e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43502
2023-02-22 09:17:33 - progress_bar.py[line:274] - INFO: epoch 020:    332 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.4, ups=0.9, wpb=110.2, bsz=40, num_updates=8150, lr=3.14775e-05, gnorm=0.763, clip=10, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=43513
2023-02-22 09:17:44 - progress_bar.py[line:274] - INFO: epoch 020:    342 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=8160, lr=3.14523e-05, gnorm=0.139, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43525
2023-02-22 09:17:55 - progress_bar.py[line:274] - INFO: epoch 020:    352 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.8, ups=0.87, wpb=110.2, bsz=40, num_updates=8170, lr=3.1427e-05, gnorm=0.517, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43536
2023-02-22 09:18:07 - progress_bar.py[line:274] - INFO: epoch 020:    362 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.88, wpb=112.4, bsz=40, num_updates=8180, lr=3.14017e-05, gnorm=0.398, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43548
2023-02-22 09:18:18 - progress_bar.py[line:274] - INFO: epoch 020:    372 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.7, ups=0.87, wpb=110.1, bsz=40, num_updates=8190, lr=3.13764e-05, gnorm=0.397, clip=20, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=43559
2023-02-22 09:18:30 - progress_bar.py[line:274] - INFO: epoch 020:    382 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=8200, lr=3.13511e-05, gnorm=0.27, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43571
2023-02-22 09:18:41 - progress_bar.py[line:274] - INFO: epoch 020:    392 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=8210, lr=3.13258e-05, gnorm=0.462, clip=20, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=43582
2023-02-22 09:18:52 - progress_bar.py[line:274] - INFO: epoch 020:    402 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.9, ups=0.88, wpb=111.2, bsz=40, num_updates=8220, lr=3.13006e-05, gnorm=0.32, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43593
2023-02-22 09:19:04 - progress_bar.py[line:274] - INFO: epoch 020:    412 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.7, ups=0.88, wpb=111.8, bsz=40, num_updates=8230, lr=3.12753e-05, gnorm=0.464, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43605
2023-02-22 09:19:04 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 09:19:05 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 09:19:05 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 09:21:07 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 09:21:07 - train.py[line:551] - INFO: load:1.06 valid_run:122.11 task_valid:118.21 collect_output:2.87
2023-02-22 09:23:07 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 09:23:07 - train.py[line:551] - INFO: load:1.09 valid_run:242.03 task_valid:232.81 collect_output:7.18
2023-02-22 09:25:09 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 09:25:09 - train.py[line:551] - INFO: load:1.12 valid_run:364.08 task_valid:348.26 collect_output:12.78
2023-02-22 09:27:11 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 09:27:11 - train.py[line:551] - INFO: load:1.14 valid_run:486.20 task_valid:460.91 collect_output:21.22
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3931893
Killing subprocess 3931894
Main process received SIGINT, exiting
