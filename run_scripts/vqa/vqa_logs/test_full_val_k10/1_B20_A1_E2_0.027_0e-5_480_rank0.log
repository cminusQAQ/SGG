2023-02-20 14:16:07 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-20 14:16:07 - utils.py[line:261] - INFO: Start init
2023-02-20 14:16:07 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-02-20 14:16:07 - utils.py[line:261] - INFO: Start init
2023-02-20 14:16:07 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-20 14:16:07 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-20 14:16:07 - utils.py[line:261] - INFO: Start init
2023-02-20 14:16:07 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-20 14:16:07 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-20 14:16:07 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-20 14:16:07 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-02-20 14:16:07 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-20 14:16:12 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_full_val_k10', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 3, 'distributed_num_procs': 3, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 3, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 3}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=3, distributed_port=-1, distributed_rank=0, distributed_world_size=3, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[0.0], lr_scheduler='polynomial_decay', max_epoch=2, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=3, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_full_val_k10', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [0.0]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-20 14:16:12 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-20 14:16:12 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-20 14:16:17 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-20 14:16:17 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-20 14:16:17 - train.py[line:119] - INFO: model: OFAModel
2023-02-20 14:16:17 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-20 14:16:17 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-20 14:16:17 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 1 row count 140300 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 0 row count 140300 total row count 420900
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 2 row count 140300 total row count 420900
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-20 14:16:18 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-20 14:16:18 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-20 14:16:19 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 3 workers***********************
2023-02-20 14:16:19 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-20 14:16:19 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-20 14:16:19 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-20 14:16:19 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 3 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-20 14:16:20 - train.py[line:161] - INFO: training on 3 devices (GPUs/TPUs)
2023-02-20 14:16:20 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-20 14:16:20 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_best.pt
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-20 14:16:44 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-20 14:16:44 - trainer.py[line:656] - INFO: Loading EMA from checkpoint
2023-02-20 14:16:44 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-20 14:16:44 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-20 14:16:45 - trainer.py[line:663] - INFO: Loading EMA fp32 params from checkpoint
2023-02-20 14:16:45 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_best.pt (epoch 1 @ 0 updates)
2023-02-20 14:16:45 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 1 row count 946815 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 2 row count 946814 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 0 row count 946815 total row count 2840444
2023-02-20 14:16:47 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 94682, warmup steps 2556, warmup_factor 0.0003912363067292645
2023-02-20 14:16:47 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-20 14:16:47 - train.py[line:312] - INFO: Start iterating over samples
Total steps 94682, warmup steps 2556, warmup_factor 0.0003912363067292645
Total steps 94682, warmup steps 2556, warmup_factor 0.0003912363067292645
2023-02-20 14:17:05 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 47341 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=167.3, nsentences=60, sample_size=167.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=139.2, ups=0.83, wpb=167.3, bsz=60, num_updates=10, lr=0, gnorm=0.214, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=46
2023-02-20 14:17:17 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 47341 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=168.8, nsentences=60, sample_size=168.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=140.2, ups=0.83, wpb=168.8, bsz=60, num_updates=20, lr=0, gnorm=0.188, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=58
2023-02-20 14:17:29 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 47341 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=167.1, nsentences=60, sample_size=167.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=141.4, ups=0.85, wpb=167.1, bsz=60, num_updates=30, lr=0, gnorm=0.185, clip=0, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=70
2023-02-20 14:17:40 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 47341 loss=0.153, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=165.9, nsentences=60, sample_size=165.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=142.3, ups=0.86, wpb=165.9, bsz=60, num_updates=40, lr=0, gnorm=0.161, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=82
2023-02-20 14:17:52 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 47341 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=168.2, nsentences=60, sample_size=168.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=149, ups=0.89, wpb=168.2, bsz=60, num_updates=50, lr=0, gnorm=0.192, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=93
2023-02-20 14:18:03 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 47341 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=168.8, nsentences=60, sample_size=168.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=144.8, ups=0.86, wpb=168.8, bsz=60, num_updates=60, lr=0, gnorm=0.223, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=105
2023-02-20 14:18:14 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 47341 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=168.1, nsentences=60, sample_size=168.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=148.8, ups=0.89, wpb=168.1, bsz=60, num_updates=70, lr=0, gnorm=0.216, clip=0, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=116
2023-02-20 14:18:26 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 47341 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=168, nsentences=60, sample_size=168, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=144.8, ups=0.86, wpb=168, bsz=60, num_updates=80, lr=0, gnorm=0.23, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=128
2023-02-20 14:18:38 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 47341 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=168.1, nsentences=60, sample_size=168.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=145.3, ups=0.86, wpb=168.1, bsz=60, num_updates=90, lr=0, gnorm=0.181, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=139
2023-02-20 14:18:49 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 47341 loss=0.151, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=168.2, nsentences=60, sample_size=168.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147.1, ups=0.87, wpb=168.2, bsz=60, num_updates=100, lr=0, gnorm=0.188, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=151
2023-02-20 14:19:00 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 47341 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=168.5, nsentences=60, sample_size=168.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=149.5, ups=0.89, wpb=168.5, bsz=60, num_updates=110, lr=0, gnorm=0.16, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=162
2023-02-20 14:19:12 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 47341 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=166.2, nsentences=60, sample_size=166.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=144.9, ups=0.87, wpb=166.2, bsz=60, num_updates=120, lr=0, gnorm=0.181, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=173
2023-02-20 14:19:23 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 47341 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=168.5, nsentences=60, sample_size=168.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=150.1, ups=0.89, wpb=168.5, bsz=60, num_updates=130, lr=0, gnorm=0.192, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=185
2023-02-20 14:19:35 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 47341 loss=0.152, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=166.9, nsentences=60, sample_size=166.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=145.2, ups=0.87, wpb=166.9, bsz=60, num_updates=140, lr=0, gnorm=0.152, clip=0, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=196
2023-02-20 14:19:46 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 47341 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=167, nsentences=60, sample_size=167, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147.2, ups=0.88, wpb=167, bsz=60, num_updates=150, lr=0, gnorm=0.24, clip=0, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=207
2023-02-20 14:19:57 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 47341 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=166.3, nsentences=60, sample_size=166.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=146, ups=0.88, wpb=166.3, bsz=60, num_updates=160, lr=0, gnorm=0.187, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=219
2023-02-20 14:20:09 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 47341 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=166.2, nsentences=60, sample_size=166.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=146.4, ups=0.88, wpb=166.2, bsz=60, num_updates=170, lr=0, gnorm=0.257, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=230
2023-02-20 14:20:20 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 47341 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=166.8, nsentences=60, sample_size=166.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=145.1, ups=0.87, wpb=166.8, bsz=60, num_updates=180, lr=0, gnorm=0.176, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=242
2023-02-20 14:20:32 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 47341 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=166.9, nsentences=60, sample_size=166.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=146.8, ups=0.88, wpb=166.9, bsz=60, num_updates=190, lr=0, gnorm=0.154, clip=0, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=253
2023-02-20 14:20:43 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 47341 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=167.9, nsentences=60, sample_size=167.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=150.2, ups=0.89, wpb=167.9, bsz=60, num_updates=200, lr=0, gnorm=0.2, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=264
2023-02-20 14:20:43 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 14:20:43 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-20 14:20:44 - train.py[line:549] - INFO: 0 / 11692
2023-02-20 14:20:44 - train.py[line:551] - INFO: load:1.12 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 14:22:51 - train.py[line:549] - INFO: 200 / 11692
2023-02-20 14:22:51 - train.py[line:551] - INFO: load:1.14 valid_run:126.77 task_valid:121.69 collect_output:3.94
2023-02-20 14:24:53 - train.py[line:549] - INFO: 400 / 11692
2023-02-20 14:24:53 - train.py[line:551] - INFO: load:1.17 valid_run:249.02 task_valid:237.67 collect_output:9.10
2023-02-20 14:26:57 - train.py[line:549] - INFO: 600 / 11692
2023-02-20 14:26:57 - train.py[line:551] - INFO: load:1.19 valid_run:372.66 task_valid:354.41 collect_output:14.89
2023-02-20 14:28:58 - train.py[line:549] - INFO: 800 / 11692
2023-02-20 14:28:58 - train.py[line:551] - INFO: load:1.22 valid_run:493.66 task_valid:468.44 collect_output:20.79
2023-02-20 14:31:01 - train.py[line:549] - INFO: 1000 / 11692
2023-02-20 14:31:01 - train.py[line:551] - INFO: load:1.24 valid_run:616.49 task_valid:585.76 collect_output:25.23
2023-02-20 14:33:04 - train.py[line:549] - INFO: 1200 / 11692
2023-02-20 14:33:04 - train.py[line:551] - INFO: load:1.27 valid_run:739.15 task_valid:704.34 collect_output:28.25
2023-02-20 14:35:06 - train.py[line:549] - INFO: 1400 / 11692
2023-02-20 14:35:06 - train.py[line:551] - INFO: load:1.29 valid_run:861.07 task_valid:822.46 collect_output:30.99
2023-02-20 14:37:09 - train.py[line:549] - INFO: 1600 / 11692
2023-02-20 14:37:09 - train.py[line:551] - INFO: load:1.32 valid_run:983.76 task_valid:938.77 collect_output:36.32
2023-02-20 14:39:10 - train.py[line:549] - INFO: 1800 / 11692
2023-02-20 14:39:10 - train.py[line:551] - INFO: load:1.34 valid_run:1104.88 task_valid:1055.68 collect_output:39.49
2023-02-20 14:41:11 - train.py[line:549] - INFO: 2000 / 11692
2023-02-20 14:41:11 - train.py[line:551] - INFO: load:1.36 valid_run:1225.89 task_valid:1168.12 collect_output:47.02
2023-02-20 14:43:12 - train.py[line:549] - INFO: 2200 / 11692
2023-02-20 14:43:12 - train.py[line:551] - INFO: load:1.39 valid_run:1347.27 task_valid:1283.53 collect_output:51.92
2023-02-20 14:45:15 - train.py[line:549] - INFO: 2400 / 11692
2023-02-20 14:45:15 - train.py[line:551] - INFO: load:1.41 valid_run:1469.48 task_valid:1400.39 collect_output:56.22
2023-02-20 14:47:18 - train.py[line:549] - INFO: 2600 / 11692
2023-02-20 14:47:18 - train.py[line:551] - INFO: load:1.44 valid_run:1592.93 task_valid:1514.35 collect_output:64.65
2023-02-20 14:49:21 - train.py[line:549] - INFO: 2800 / 11692
2023-02-20 14:49:21 - train.py[line:551] - INFO: load:1.46 valid_run:1716.31 task_valid:1631.94 collect_output:69.37
2023-02-20 14:51:24 - train.py[line:549] - INFO: 3000 / 11692
2023-02-20 14:51:24 - train.py[line:551] - INFO: load:1.49 valid_run:1838.68 task_valid:1748.03 collect_output:74.60
2023-02-20 14:53:26 - train.py[line:549] - INFO: 3200 / 11692
2023-02-20 14:53:26 - train.py[line:551] - INFO: load:1.51 valid_run:1961.03 task_valid:1861.77 collect_output:82.15
2023-02-20 14:55:29 - train.py[line:549] - INFO: 3400 / 11692
2023-02-20 14:55:29 - train.py[line:551] - INFO: load:1.54 valid_run:2084.01 task_valid:1977.82 collect_output:87.98
2023-02-20 14:57:32 - train.py[line:549] - INFO: 3600 / 11692
2023-02-20 14:57:32 - train.py[line:551] - INFO: load:1.56 valid_run:2206.13 task_valid:2095.99 collect_output:90.86
2023-02-20 14:59:33 - train.py[line:549] - INFO: 3800 / 11692
2023-02-20 14:59:33 - train.py[line:551] - INFO: load:1.59 valid_run:2328.03 task_valid:2213.15 collect_output:94.51
2023-02-20 15:01:35 - train.py[line:549] - INFO: 4000 / 11692
2023-02-20 15:01:35 - train.py[line:551] - INFO: load:1.61 valid_run:2449.16 task_valid:2329.81 collect_output:97.90
2023-02-20 15:03:37 - train.py[line:549] - INFO: 4200 / 11692
2023-02-20 15:03:37 - train.py[line:551] - INFO: load:1.64 valid_run:2571.25 task_valid:2446.13 collect_output:102.63
2023-02-20 15:05:39 - train.py[line:549] - INFO: 4400 / 11692
2023-02-20 15:05:39 - train.py[line:551] - INFO: load:1.66 valid_run:2693.50 task_valid:2564.96 collect_output:104.98
2023-02-20 15:07:42 - train.py[line:549] - INFO: 4600 / 11692
2023-02-20 15:07:42 - train.py[line:551] - INFO: load:1.69 valid_run:2815.95 task_valid:2679.26 collect_output:112.08
2023-02-20 15:09:44 - train.py[line:549] - INFO: 4800 / 11692
2023-02-20 15:09:44 - train.py[line:551] - INFO: load:1.71 valid_run:2937.93 task_valid:2795.86 collect_output:116.37
2023-02-20 15:11:45 - train.py[line:549] - INFO: 5000 / 11692
2023-02-20 15:11:45 - train.py[line:551] - INFO: load:1.74 valid_run:3059.54 task_valid:2912.88 collect_output:119.85
2023-02-20 15:13:47 - train.py[line:549] - INFO: 5200 / 11692
2023-02-20 15:13:47 - train.py[line:551] - INFO: load:1.77 valid_run:3181.09 task_valid:3029.02 collect_output:124.20
2023-02-20 15:15:47 - train.py[line:549] - INFO: 5400 / 11692
2023-02-20 15:15:47 - train.py[line:551] - INFO: load:1.79 valid_run:3301.29 task_valid:3143.12 collect_output:129.26
2023-02-20 15:17:50 - train.py[line:549] - INFO: 5600 / 11692
2023-02-20 15:17:50 - train.py[line:551] - INFO: load:1.82 valid_run:3424.45 task_valid:3262.59 collect_output:131.89
2023-02-20 15:19:53 - train.py[line:549] - INFO: 5800 / 11692
2023-02-20 15:19:53 - train.py[line:551] - INFO: load:1.84 valid_run:3546.79 task_valid:3378.09 collect_output:137.64
2023-02-20 15:21:56 - train.py[line:549] - INFO: 6000 / 11692
2023-02-20 15:21:56 - train.py[line:551] - INFO: load:1.87 valid_run:3670.07 task_valid:3496.47 collect_output:141.51
2023-02-20 15:24:00 - train.py[line:549] - INFO: 6200 / 11692
2023-02-20 15:24:00 - train.py[line:551] - INFO: load:1.89 valid_run:3793.37 task_valid:3615.26 collect_output:144.92
2023-02-20 15:26:02 - train.py[line:549] - INFO: 6400 / 11692
2023-02-20 15:26:02 - train.py[line:551] - INFO: load:1.92 valid_run:3915.76 task_valid:3731.77 collect_output:149.70
2023-02-20 15:28:02 - train.py[line:549] - INFO: 6600 / 11692
2023-02-20 15:28:02 - train.py[line:551] - INFO: load:1.94 valid_run:4035.99 task_valid:3847.38 collect_output:153.26
2023-02-20 15:30:04 - train.py[line:549] - INFO: 6800 / 11692
2023-02-20 15:30:04 - train.py[line:551] - INFO: load:1.97 valid_run:4157.56 task_valid:3964.94 collect_output:156.18
2023-02-20 15:32:09 - train.py[line:549] - INFO: 7000 / 11692
2023-02-20 15:32:09 - train.py[line:551] - INFO: load:1.99 valid_run:4282.45 task_valid:4085.73 collect_output:159.22
2023-02-20 15:34:11 - train.py[line:549] - INFO: 7200 / 11692
2023-02-20 15:34:11 - train.py[line:551] - INFO: load:2.02 valid_run:4404.17 task_valid:4201.14 collect_output:164.46
2023-02-20 15:36:13 - train.py[line:549] - INFO: 7400 / 11692
2023-02-20 15:36:13 - train.py[line:551] - INFO: load:2.05 valid_run:4526.46 task_valid:4318.09 collect_output:168.75
2023-02-20 15:38:17 - train.py[line:549] - INFO: 7600 / 11692
2023-02-20 15:38:17 - train.py[line:551] - INFO: load:2.07 valid_run:4650.43 task_valid:4437.77 collect_output:171.97
2023-02-20 15:40:20 - train.py[line:549] - INFO: 7800 / 11692
2023-02-20 15:40:20 - train.py[line:551] - INFO: load:2.10 valid_run:4773.19 task_valid:4557.10 collect_output:174.33
2023-02-20 15:42:24 - train.py[line:549] - INFO: 8000 / 11692
2023-02-20 15:42:24 - train.py[line:551] - INFO: load:2.13 valid_run:4897.46 task_valid:4678.90 collect_output:175.76
2023-02-20 15:44:27 - train.py[line:549] - INFO: 8200 / 11692
2023-02-20 15:44:27 - train.py[line:551] - INFO: load:2.15 valid_run:5020.43 task_valid:4799.20 collect_output:177.39
2023-02-20 15:46:30 - train.py[line:549] - INFO: 8400 / 11692
2023-02-20 15:46:30 - train.py[line:551] - INFO: load:2.18 valid_run:5143.15 task_valid:4915.75 collect_output:182.48
2023-02-20 15:48:32 - train.py[line:549] - INFO: 8600 / 11692
2023-02-20 15:48:32 - train.py[line:551] - INFO: load:2.21 valid_run:5265.37 task_valid:5034.16 collect_output:185.23
2023-02-20 15:50:36 - train.py[line:549] - INFO: 8800 / 11692
2023-02-20 15:50:36 - train.py[line:551] - INFO: load:2.23 valid_run:5388.97 task_valid:5149.64 collect_output:192.30
2023-02-20 15:52:38 - train.py[line:549] - INFO: 9000 / 11692
2023-02-20 15:52:38 - train.py[line:551] - INFO: load:2.26 valid_run:5510.65 task_valid:5266.41 collect_output:196.12
2023-02-20 15:54:40 - train.py[line:549] - INFO: 9200 / 11692
2023-02-20 15:54:40 - train.py[line:551] - INFO: load:2.28 valid_run:5633.35 task_valid:5383.12 collect_output:201.04
2023-02-20 15:56:43 - train.py[line:549] - INFO: 9400 / 11692
2023-02-20 15:56:43 - train.py[line:551] - INFO: load:2.31 valid_run:5756.00 task_valid:5501.62 collect_output:204.11
2023-02-20 15:58:46 - train.py[line:549] - INFO: 9600 / 11692
2023-02-20 15:58:46 - train.py[line:551] - INFO: load:2.34 valid_run:5878.65 task_valid:5618.39 collect_output:208.87
2023-02-20 16:00:49 - train.py[line:549] - INFO: 9800 / 11692
2023-02-20 16:00:49 - train.py[line:551] - INFO: load:2.37 valid_run:6001.92 task_valid:5732.87 collect_output:216.59
2023-02-20 16:02:53 - train.py[line:549] - INFO: 10000 / 11692
2023-02-20 16:02:53 - train.py[line:551] - INFO: load:2.40 valid_run:6125.17 task_valid:5848.45 collect_output:223.19
2023-02-20 16:04:54 - train.py[line:549] - INFO: 10200 / 11692
2023-02-20 16:04:54 - train.py[line:551] - INFO: load:2.42 valid_run:6246.91 task_valid:5964.82 collect_output:227.49
2023-02-20 16:06:57 - train.py[line:549] - INFO: 10400 / 11692
2023-02-20 16:06:57 - train.py[line:551] - INFO: load:2.45 valid_run:6369.24 task_valid:6082.16 collect_output:231.42
2023-02-20 16:09:02 - train.py[line:549] - INFO: 10600 / 11692
2023-02-20 16:09:02 - train.py[line:551] - INFO: load:2.47 valid_run:6494.52 task_valid:6200.63 collect_output:237.17
2023-02-20 16:11:04 - train.py[line:549] - INFO: 10800 / 11692
2023-02-20 16:11:04 - train.py[line:551] - INFO: load:2.50 valid_run:6616.50 task_valid:6316.03 collect_output:242.68
2023-02-20 16:13:07 - train.py[line:549] - INFO: 11000 / 11692
2023-02-20 16:13:07 - train.py[line:551] - INFO: load:2.52 valid_run:6739.17 task_valid:6432.93 collect_output:247.37
2023-02-20 16:15:10 - train.py[line:549] - INFO: 11200 / 11692
2023-02-20 16:15:10 - train.py[line:551] - INFO: load:2.55 valid_run:6862.16 task_valid:6550.26 collect_output:251.94
2023-02-20 16:17:13 - train.py[line:549] - INFO: 11400 / 11692
2023-02-20 16:17:13 - train.py[line:551] - INFO: load:2.58 valid_run:6985.35 task_valid:6671.09 collect_output:253.25
2023-02-20 16:19:16 - train.py[line:549] - INFO: 11600 / 11692
2023-02-20 16:19:16 - train.py[line:551] - INFO: load:2.60 valid_run:7108.20 task_valid:6787.65 collect_output:258.43

====================================================================================================
SGG eval:     R @ 50: 0.6525;     R @ 100: 0.6812;     R @ 500: 0.6989;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4677;    mR @ 100: 0.4972;    mR @ 500: 0.5495;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8481) (covered in:0.6029) (covering:0.4385) (eating:0.8585) (flying in:0.5000) (growing on:0.2400) (hanging from:0.6062) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9833) (says:0.0000) (sitting on:0.7227) (standing on:0.3907) (using:0.6607) (walking in:0.0769) (walking on:0.6780) (watching:0.3722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6525;     R @ 100: 0.6812;     R @ 500: 0.6989;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4677;    mR @ 100: 0.4972;    mR @ 500: 0.5495;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8481) (covered in:0.6029) (covering:0.4385) (eating:0.8585) (flying in:0.5000) (growing on:0.2400) (hanging from:0.6062) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9833) (says:0.0000) (sitting on:0.7227) (standing on:0.3907) (using:0.6607) (walking in:0.0769) (walking on:0.6780) (watching:0.3722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6525;     R @ 100: 0.6812;     R @ 500: 0.6989;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4677;    mR @ 100: 0.4972;    mR @ 500: 0.5495;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8481) (covered in:0.6029) (covering:0.4385) (eating:0.8585) (flying in:0.5000) (growing on:0.2400) (hanging from:0.6062) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9833) (says:0.0000) (sitting on:0.7227) (standing on:0.3907) (using:0.6607) (walking in:0.0769) (walking on:0.6780) (watching:0.3722) 
--------------------------------------------------------
====================================================================================================

2023-02-20 16:20:41 - train.py[line:487] - INFO: 0.6812338345299807
2023-02-20 16:20:41 - train.py[line:575] - INFO: logits:torch.Size([420900, 21]) sample_ids:torch.Size([420900])
2023-02-20 16:20:42 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.25 | loss_v1 0 | loss_v2 0 | nll_loss 0.082 | ntokens 107.915 | nsentences 35.999 | sample_size 107.915 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.681234 | ppl 1.06 | vqa_score 0.5384 | wps 175.4 | wpb 107.9 | bsz 36 | num_updates 200
2023-02-20 16:20:42 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 200 updates
2023-02-20 16:20:42 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-20 16:20:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-20 16:20:56 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt (epoch 1 @ 200 updates, score 0.6812338345299807) (writing took 14.177007138729095 seconds)
2023-02-20 16:21:07 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 47341 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=166.9, nsentences=60, sample_size=166.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=0.2, ups=0, wpb=166.9, bsz=60, num_updates=210, lr=0, gnorm=0.192, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7489
2023-02-20 16:21:18 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 47341 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=166.5, nsentences=60, sample_size=166.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=149.3, ups=0.9, wpb=166.5, bsz=60, num_updates=220, lr=0, gnorm=0.206, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7500
2023-02-20 16:21:30 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 47341 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=167.4, nsentences=60, sample_size=167.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=148.7, ups=0.89, wpb=167.4, bsz=60, num_updates=230, lr=0, gnorm=0.199, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7511
2023-02-20 16:21:41 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 47341 loss=0.146, loss_v1=0, loss_v2=0, nll_loss=0.039, ntokens=167, nsentences=60, sample_size=167, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147.1, ups=0.88, wpb=167, bsz=60, num_updates=240, lr=0, gnorm=0.136, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7522
2023-02-20 16:21:52 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 47341 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=166.8, nsentences=60, sample_size=166.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147.1, ups=0.88, wpb=166.8, bsz=60, num_updates=250, lr=0, gnorm=0.155, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7534
2023-02-20 16:22:04 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 47341 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=167, nsentences=60, sample_size=167, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=145.6, ups=0.87, wpb=167, bsz=60, num_updates=260, lr=0, gnorm=0.261, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7545
2023-02-20 16:22:15 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 47341 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.041, ntokens=167.6, nsentences=60, sample_size=167.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=149.8, ups=0.89, wpb=167.6, bsz=60, num_updates=270, lr=0, gnorm=0.113, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7556
2023-02-20 16:22:26 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 47341 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=167.4, nsentences=60, sample_size=167.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=149.7, ups=0.89, wpb=167.4, bsz=60, num_updates=280, lr=0, gnorm=0.156, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7568
2023-02-20 16:22:38 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 47341 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.04, ntokens=168.8, nsentences=60, sample_size=168.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147, ups=0.87, wpb=168.8, bsz=60, num_updates=290, lr=0, gnorm=0.151, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7579
2023-02-20 16:22:49 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 47341 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=167.3, nsentences=60, sample_size=167.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=153.9, ups=0.92, wpb=167.3, bsz=60, num_updates=300, lr=0, gnorm=0.191, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7590
2023-02-20 16:23:00 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 47341 loss=0.152, loss_v1=0, loss_v2=0, nll_loss=0.041, ntokens=168.5, nsentences=60, sample_size=168.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=149.1, ups=0.89, wpb=168.5, bsz=60, num_updates=310, lr=0, gnorm=0.13, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7601
2023-02-20 16:23:11 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 47341 loss=0.146, loss_v1=0, loss_v2=0, nll_loss=0.04, ntokens=168, nsentences=60, sample_size=168, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=146.6, ups=0.87, wpb=168, bsz=60, num_updates=320, lr=0, gnorm=0.178, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7613
2023-02-20 16:23:23 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 47341 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=166.8, nsentences=60, sample_size=166.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=145.4, ups=0.87, wpb=166.8, bsz=60, num_updates=330, lr=0, gnorm=0.181, clip=0, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7624
2023-02-20 16:23:34 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 47341 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=167.1, nsentences=60, sample_size=167.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=145.1, ups=0.87, wpb=167.1, bsz=60, num_updates=340, lr=0, gnorm=0.18, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7636
2023-02-20 16:23:46 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 47341 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=167.9, nsentences=60, sample_size=167.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=149.7, ups=0.89, wpb=167.9, bsz=60, num_updates=350, lr=0, gnorm=0.131, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7647
2023-02-20 16:23:57 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 47341 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=167.8, nsentences=60, sample_size=167.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=152.2, ups=0.91, wpb=167.8, bsz=60, num_updates=360, lr=0, gnorm=0.201, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7658
2023-02-20 16:24:08 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 47341 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=167.1, nsentences=60, sample_size=167.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=145.4, ups=0.87, wpb=167.1, bsz=60, num_updates=370, lr=0, gnorm=0.19, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7670
2023-02-20 16:24:20 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 47341 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=167.2, nsentences=60, sample_size=167.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147.3, ups=0.88, wpb=167.2, bsz=60, num_updates=380, lr=0, gnorm=0.142, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7681
2023-02-20 16:24:31 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 47341 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=166.9, nsentences=60, sample_size=166.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147, ups=0.88, wpb=166.9, bsz=60, num_updates=390, lr=0, gnorm=0.187, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7692
2023-02-20 16:24:42 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 47341 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=168.9, nsentences=60, sample_size=168.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=147.1, ups=0.87, wpb=168.9, bsz=60, num_updates=400, lr=0, gnorm=0.223, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7704
2023-02-20 16:24:42 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 16:24:44 - train.py[line:549] - INFO: 0 / 11692
2023-02-20 16:24:44 - train.py[line:551] - INFO: load:1.06 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 16:26:50 - train.py[line:549] - INFO: 200 / 11692
2023-02-20 16:26:50 - train.py[line:551] - INFO: load:1.08 valid_run:125.91 task_valid:120.08 collect_output:4.74
2023-02-20 16:28:52 - train.py[line:549] - INFO: 400 / 11692
2023-02-20 16:28:52 - train.py[line:551] - INFO: load:1.10 valid_run:248.48 task_valid:235.86 collect_output:10.41
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3577481
Killing subprocess 3577482
Killing subprocess 3577483
Main process received SIGINT, exiting
2023-02-20 16:33:49 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-20 16:33:49 - utils.py[line:261] - INFO: Start init
2023-02-20 16:33:49 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-20 16:33:49 - utils.py[line:261] - INFO: Start init
2023-02-20 16:33:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-20 16:33:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-20 16:33:50 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-20 16:33:50 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-20 16:33:54 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_full_val_k10', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[0.0], lr_scheduler='polynomial_decay', max_epoch=2, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_full_val_k10', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [0.0]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-20 16:33:54 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-20 16:33:54 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-20 16:33:59 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-20 16:33:59 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-20 16:33:59 - train.py[line:119] - INFO: model: OFAModel
2023-02-20 16:33:59 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-20 16:33:59 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-20 16:33:59 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 0 row count 210450 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 1 row count 210450 total row count 420900
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-20 16:34:00 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-20 16:34:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-20 16:34:01 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-20 16:34:01 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-20 16:34:01 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-20 16:34:01 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-20 16:34:02 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-20 16:34:02 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-20 16:34:02 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_best.pt
2023-02-20 16:34:29 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-20 16:34:29 - trainer.py[line:656] - INFO: Loading EMA from checkpoint
2023-02-20 16:34:29 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-20 16:34:29 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-20 16:34:30 - trainer.py[line:663] - INFO: Loading EMA fp32 params from checkpoint
2023-02-20 16:34:30 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_best.pt (epoch 1 @ 0 updates)
2023-02-20 16:34:30 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 1 row count 1420222 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 0 row count 1420222 total row count 2840444
2023-02-20 16:34:32 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 142024, warmup steps 3834, warmup_factor 0.0002608242044861763
Total steps 142024, warmup steps 3834, warmup_factor 0.0002608242044861763
2023-02-20 16:34:33 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-20 16:34:33 - train.py[line:312] - INFO: Start iterating over samples
2023-02-20 16:34:50 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 71012 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.2, ups=0.87, wpb=112.4, bsz=40, num_updates=10, lr=0, gnorm=0.305, clip=0, loss_scale=128, train_wall=15, gb_free=10.5, ema_decay=0.9999, wall=49
2023-02-20 16:35:01 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 71012 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.9, ups=0.86, wpb=112.1, bsz=40, num_updates=20, lr=0, gnorm=0.314, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=60
2023-02-20 16:35:12 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 71012 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.9, ups=0.89, wpb=110.5, bsz=40, num_updates=30, lr=0, gnorm=0.245, clip=0, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=71
2023-02-20 16:35:24 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 71012 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=40, lr=0, gnorm=0.479, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=83
2023-02-20 16:35:35 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 71012 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.89, wpb=111.2, bsz=40, num_updates=50, lr=0, gnorm=0.341, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=94
2023-02-20 16:35:47 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 71012 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.87, wpb=113.7, bsz=40, num_updates=60, lr=0, gnorm=0.422, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=106
2023-02-20 16:35:58 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 71012 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=70, lr=0, gnorm=0.35, clip=10, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=117
2023-02-20 16:36:09 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 71012 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.6, bsz=40, num_updates=80, lr=0, gnorm=0.314, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=128
2023-02-20 16:36:20 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 71012 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=111.8, bsz=40, num_updates=90, lr=0, gnorm=0.417, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=139
2023-02-20 16:36:31 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 71012 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.92, wpb=111.2, bsz=40, num_updates=100, lr=0, gnorm=0.373, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=150
2023-02-20 16:36:42 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 71012 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=112.1, bsz=40, num_updates=110, lr=0, gnorm=0.362, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=161
2023-02-20 16:36:53 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 71012 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=111.3, bsz=40, num_updates=120, lr=0, gnorm=0.386, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=172
2023-02-20 16:37:05 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 71012 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=113.4, bsz=40, num_updates=130, lr=0, gnorm=0.434, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=184
2023-02-20 16:37:16 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 71012 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=111.7, bsz=40, num_updates=140, lr=0, gnorm=0.422, clip=10, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=195
2023-02-20 16:37:27 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 71012 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.3, ups=0.87, wpb=110.5, bsz=40, num_updates=150, lr=0, gnorm=0.301, clip=0, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=206
2023-02-20 16:37:38 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 71012 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=160, lr=0, gnorm=0.347, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=217
2023-02-20 16:37:50 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 71012 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=170, lr=0, gnorm=0.35, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=229
2023-02-20 16:38:01 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 71012 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.91, wpb=110.2, bsz=40, num_updates=180, lr=0, gnorm=0.434, clip=10, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=240
2023-02-20 16:38:12 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 71012 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.88, wpb=111.5, bsz=40, num_updates=190, lr=0, gnorm=0.313, clip=0, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=251
2023-02-20 16:38:23 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 71012 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.92, wpb=112.2, bsz=40, num_updates=200, lr=0, gnorm=0.353, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=262
2023-02-20 16:38:23 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 16:38:23 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-20 16:38:24 - train.py[line:549] - INFO: 0 / 17538
2023-02-20 16:38:24 - train.py[line:551] - INFO: load:1.25 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 16:40:33 - train.py[line:549] - INFO: 200 / 17538
2023-02-20 16:40:33 - train.py[line:551] - INFO: load:1.28 valid_run:128.19 task_valid:122.81 collect_output:4.12
2023-02-20 16:42:33 - train.py[line:549] - INFO: 400 / 17538
2023-02-20 16:42:33 - train.py[line:551] - INFO: load:1.31 valid_run:248.85 task_valid:240.41 collect_output:6.02
2023-02-20 16:44:35 - train.py[line:549] - INFO: 600 / 17538
2023-02-20 16:44:35 - train.py[line:551] - INFO: load:1.33 valid_run:369.98 task_valid:357.30 collect_output:9.17
2023-02-20 16:46:34 - train.py[line:549] - INFO: 800 / 17538
2023-02-20 16:46:34 - train.py[line:551] - INFO: load:1.36 valid_run:488.94 task_valid:471.60 collect_output:12.70
2023-02-20 16:48:35 - train.py[line:549] - INFO: 1000 / 17538
2023-02-20 16:48:35 - train.py[line:551] - INFO: load:1.38 valid_run:610.44 task_valid:589.62 collect_output:15.07
2023-02-20 16:50:39 - train.py[line:549] - INFO: 1200 / 17538
2023-02-20 16:50:39 - train.py[line:551] - INFO: load:1.41 valid_run:734.01 task_valid:709.47 collect_output:17.63
2023-02-20 16:52:41 - train.py[line:549] - INFO: 1400 / 17538
2023-02-20 16:52:41 - train.py[line:551] - INFO: load:1.44 valid_run:855.80 task_valid:828.37 collect_output:19.39
2023-02-20 16:54:41 - train.py[line:549] - INFO: 1600 / 17538
2023-02-20 16:54:41 - train.py[line:551] - INFO: load:1.47 valid_run:976.43 task_valid:945.22 collect_output:22.05
2023-02-20 16:56:44 - train.py[line:549] - INFO: 1800 / 17538
2023-02-20 16:56:44 - train.py[line:551] - INFO: load:1.49 valid_run:1098.91 task_valid:1063.16 collect_output:25.44
2023-02-20 16:58:44 - train.py[line:549] - INFO: 2000 / 17538
2023-02-20 16:58:44 - train.py[line:551] - INFO: load:1.52 valid_run:1218.80 task_valid:1176.35 collect_output:31.01
2023-02-20 17:00:44 - train.py[line:549] - INFO: 2200 / 17538
2023-02-20 17:00:44 - train.py[line:551] - INFO: load:1.55 valid_run:1338.71 task_valid:1292.52 collect_output:33.64
2023-02-20 17:02:45 - train.py[line:549] - INFO: 2400 / 17538
2023-02-20 17:02:45 - train.py[line:551] - INFO: load:1.57 valid_run:1459.73 task_valid:1409.87 collect_output:36.22
2023-02-20 17:04:46 - train.py[line:549] - INFO: 2600 / 17538
2023-02-20 17:04:46 - train.py[line:551] - INFO: load:1.60 valid_run:1580.20 task_valid:1524.29 collect_output:41.15
2023-02-20 17:06:47 - train.py[line:549] - INFO: 2800 / 17538
2023-02-20 17:06:47 - train.py[line:551] - INFO: load:1.63 valid_run:1701.86 task_valid:1642.64 collect_output:43.33
2023-02-20 17:08:49 - train.py[line:549] - INFO: 3000 / 17538
2023-02-20 17:08:49 - train.py[line:551] - INFO: load:1.65 valid_run:1823.64 task_valid:1759.45 collect_output:47.20
2023-02-20 17:10:49 - train.py[line:549] - INFO: 3200 / 17538
2023-02-20 17:10:49 - train.py[line:551] - INFO: load:1.68 valid_run:1943.59 task_valid:1874.38 collect_output:51.08
2023-02-20 17:12:51 - train.py[line:549] - INFO: 3400 / 17538
2023-02-20 17:12:51 - train.py[line:551] - INFO: load:1.71 valid_run:2065.83 task_valid:1991.41 collect_output:55.17
2023-02-20 17:14:54 - train.py[line:549] - INFO: 3600 / 17538
2023-02-20 17:14:54 - train.py[line:551] - INFO: load:1.73 valid_run:2188.57 task_valid:2110.29 collect_output:57.88
2023-02-20 17:16:57 - train.py[line:549] - INFO: 3800 / 17538
2023-02-20 17:16:57 - train.py[line:551] - INFO: load:1.76 valid_run:2311.21 task_valid:2228.42 collect_output:61.27
2023-02-20 17:19:00 - train.py[line:549] - INFO: 4000 / 17538
2023-02-20 17:19:00 - train.py[line:551] - INFO: load:1.79 valid_run:2434.15 task_valid:2346.06 collect_output:65.44
2023-02-20 17:21:03 - train.py[line:549] - INFO: 4200 / 17538
2023-02-20 17:21:03 - train.py[line:551] - INFO: load:1.81 valid_run:2557.40 task_valid:2463.40 collect_output:70.23
2023-02-20 17:23:05 - train.py[line:549] - INFO: 4400 / 17538
2023-02-20 17:23:05 - train.py[line:551] - INFO: load:1.84 valid_run:2679.46 task_valid:2583.03 collect_output:71.54
2023-02-20 17:25:05 - train.py[line:549] - INFO: 4600 / 17538
2023-02-20 17:25:05 - train.py[line:551] - INFO: load:1.87 valid_run:2799.16 task_valid:2697.89 collect_output:75.28
2023-02-20 17:27:06 - train.py[line:549] - INFO: 4800 / 17538
2023-02-20 17:27:06 - train.py[line:551] - INFO: load:1.90 valid_run:2920.01 task_valid:2814.80 collect_output:78.09
2023-02-20 17:29:07 - train.py[line:549] - INFO: 5000 / 17538
2023-02-20 17:29:07 - train.py[line:551] - INFO: load:1.92 valid_run:3040.87 task_valid:2932.08 collect_output:80.53
2023-02-20 17:31:08 - train.py[line:549] - INFO: 5200 / 17538
2023-02-20 17:31:08 - train.py[line:551] - INFO: load:1.95 valid_run:3161.97 task_valid:3049.02 collect_output:83.55
2023-02-20 17:33:09 - train.py[line:549] - INFO: 5400 / 17538
2023-02-20 17:33:09 - train.py[line:551] - INFO: load:1.98 valid_run:3282.94 task_valid:3164.02 collect_output:88.42
2023-02-20 17:35:12 - train.py[line:549] - INFO: 5600 / 17538
2023-02-20 17:35:12 - train.py[line:551] - INFO: load:2.00 valid_run:3405.48 task_valid:3284.01 collect_output:89.88
2023-02-20 17:37:14 - train.py[line:549] - INFO: 5800 / 17538
2023-02-20 17:37:14 - train.py[line:551] - INFO: load:2.03 valid_run:3527.49 task_valid:3400.33 collect_output:94.41
2023-02-20 17:39:16 - train.py[line:549] - INFO: 6000 / 17538
2023-02-20 17:39:16 - train.py[line:551] - INFO: load:2.06 valid_run:3649.42 task_valid:3519.28 collect_output:96.31
2023-02-20 17:41:17 - train.py[line:549] - INFO: 6200 / 17538
2023-02-20 17:41:17 - train.py[line:551] - INFO: load:2.08 valid_run:3771.01 task_valid:3638.41 collect_output:97.67
2023-02-20 17:43:19 - train.py[line:549] - INFO: 6400 / 17538
2023-02-20 17:43:19 - train.py[line:551] - INFO: load:2.11 valid_run:3892.77 task_valid:3755.13 collect_output:101.63
2023-02-20 17:45:20 - train.py[line:549] - INFO: 6600 / 17538
2023-02-20 17:45:20 - train.py[line:551] - INFO: load:2.14 valid_run:4013.00 task_valid:3871.15 collect_output:104.76
2023-02-20 17:47:22 - train.py[line:549] - INFO: 6800 / 17538
2023-02-20 17:47:22 - train.py[line:551] - INFO: load:2.16 valid_run:4135.68 task_valid:3989.25 collect_output:108.23
2023-02-20 17:49:26 - train.py[line:549] - INFO: 7000 / 17538
2023-02-20 17:49:26 - train.py[line:551] - INFO: load:2.19 valid_run:4259.39 task_valid:4109.87 collect_output:110.23
2023-02-20 17:51:26 - train.py[line:549] - INFO: 7200 / 17538
2023-02-20 17:51:26 - train.py[line:551] - INFO: load:2.22 valid_run:4378.99 task_valid:4225.79 collect_output:112.84
2023-02-20 17:53:29 - train.py[line:549] - INFO: 7400 / 17538
2023-02-20 17:53:29 - train.py[line:551] - INFO: load:2.24 valid_run:4501.96 task_valid:4342.98 collect_output:117.56
2023-02-20 17:55:31 - train.py[line:549] - INFO: 7600 / 17538
2023-02-20 17:55:31 - train.py[line:551] - INFO: load:2.27 valid_run:4624.31 task_valid:4462.89 collect_output:118.92
2023-02-20 17:57:33 - train.py[line:549] - INFO: 7800 / 17538
2023-02-20 17:57:33 - train.py[line:551] - INFO: load:2.29 valid_run:4746.54 task_valid:4582.04 collect_output:120.94
2023-02-20 17:59:37 - train.py[line:549] - INFO: 8000 / 17538
2023-02-20 17:59:37 - train.py[line:551] - INFO: load:2.32 valid_run:4870.47 task_valid:4704.22 collect_output:121.61
2023-02-20 18:01:41 - train.py[line:549] - INFO: 8200 / 17538
2023-02-20 18:01:41 - train.py[line:551] - INFO: load:2.35 valid_run:4994.02 task_valid:4825.29 collect_output:123.00
2023-02-20 18:03:42 - train.py[line:549] - INFO: 8400 / 17538
2023-02-20 18:03:42 - train.py[line:551] - INFO: load:2.37 valid_run:5114.95 task_valid:4942.07 collect_output:126.06
2023-02-20 18:05:46 - train.py[line:549] - INFO: 8600 / 17538
2023-02-20 18:05:46 - train.py[line:551] - INFO: load:2.40 valid_run:5238.55 task_valid:5061.57 collect_output:129.06
2023-02-20 18:07:48 - train.py[line:549] - INFO: 8800 / 17538
2023-02-20 18:07:48 - train.py[line:551] - INFO: load:2.42 valid_run:5361.11 task_valid:5178.89 collect_output:133.14
2023-02-20 18:09:52 - train.py[line:549] - INFO: 9000 / 17538
2023-02-20 18:09:52 - train.py[line:551] - INFO: load:2.45 valid_run:5484.55 task_valid:5297.04 collect_output:137.26
2023-02-20 18:11:55 - train.py[line:549] - INFO: 9200 / 17538
2023-02-20 18:11:55 - train.py[line:551] - INFO: load:2.48 valid_run:5608.04 task_valid:5415.02 collect_output:141.64
2023-02-20 18:13:58 - train.py[line:549] - INFO: 9400 / 17538
2023-02-20 18:13:58 - train.py[line:551] - INFO: load:2.50 valid_run:5730.56 task_valid:5534.16 collect_output:143.89
2023-02-20 18:15:59 - train.py[line:549] - INFO: 9600 / 17538
2023-02-20 18:15:59 - train.py[line:551] - INFO: load:2.53 valid_run:5851.94 task_valid:5651.93 collect_output:146.38
2023-02-20 18:18:00 - train.py[line:549] - INFO: 9800 / 17538
2023-02-20 18:18:00 - train.py[line:551] - INFO: load:2.56 valid_run:5972.27 task_valid:5768.24 collect_output:149.22
2023-02-20 18:20:00 - train.py[line:549] - INFO: 10000 / 17538
2023-02-20 18:20:00 - train.py[line:551] - INFO: load:2.58 valid_run:6092.62 task_valid:5885.21 collect_output:151.46
2023-02-20 18:22:02 - train.py[line:549] - INFO: 10200 / 17538
2023-02-20 18:22:02 - train.py[line:551] - INFO: load:2.61 valid_run:6214.32 task_valid:6002.86 collect_output:154.38
2023-02-20 18:24:05 - train.py[line:549] - INFO: 10400 / 17538
2023-02-20 18:24:05 - train.py[line:551] - INFO: load:2.64 valid_run:6337.24 task_valid:6121.62 collect_output:157.41
2023-02-20 18:26:08 - train.py[line:549] - INFO: 10600 / 17538
2023-02-20 18:26:08 - train.py[line:551] - INFO: load:2.67 valid_run:6460.42 task_valid:6241.61 collect_output:159.48
2023-02-20 18:28:09 - train.py[line:549] - INFO: 10800 / 17538
2023-02-20 18:28:09 - train.py[line:551] - INFO: load:2.70 valid_run:6581.41 task_valid:6358.35 collect_output:162.58
2023-02-20 18:30:11 - train.py[line:549] - INFO: 11000 / 17538
2023-02-20 18:30:11 - train.py[line:551] - INFO: load:2.73 valid_run:6703.28 task_valid:6476.96 collect_output:164.71
2023-02-20 18:32:13 - train.py[line:549] - INFO: 11200 / 17538
2023-02-20 18:32:13 - train.py[line:551] - INFO: load:2.75 valid_run:6824.94 task_valid:6595.55 collect_output:166.64
2023-02-20 18:34:17 - train.py[line:549] - INFO: 11400 / 17538
2023-02-20 18:34:17 - train.py[line:551] - INFO: load:2.78 valid_run:6949.24 task_valid:6717.92 collect_output:167.47
2023-02-20 18:36:21 - train.py[line:549] - INFO: 11600 / 17538
2023-02-20 18:36:21 - train.py[line:551] - INFO: load:2.81 valid_run:7072.68 task_valid:6835.67 collect_output:172.03
2023-02-20 18:38:23 - train.py[line:549] - INFO: 11800 / 17538
2023-02-20 18:38:23 - train.py[line:551] - INFO: load:2.83 valid_run:7195.09 task_valid:6951.72 collect_output:177.23
2023-02-20 18:40:27 - train.py[line:549] - INFO: 12000 / 17538
2023-02-20 18:40:27 - train.py[line:551] - INFO: load:2.86 valid_run:7318.94 task_valid:7071.22 collect_output:180.46
2023-02-20 18:42:30 - train.py[line:549] - INFO: 12200 / 17538
2023-02-20 18:42:30 - train.py[line:551] - INFO: load:2.89 valid_run:7441.57 task_valid:7188.17 collect_output:185.00
2023-02-20 18:44:30 - train.py[line:549] - INFO: 12400 / 17538
2023-02-20 18:44:30 - train.py[line:551] - INFO: load:2.91 valid_run:7561.65 task_valid:7303.56 collect_output:188.53
2023-02-20 18:46:33 - train.py[line:549] - INFO: 12600 / 17538
2023-02-20 18:46:33 - train.py[line:551] - INFO: load:2.94 valid_run:7684.45 task_valid:7423.25 collect_output:190.45
2023-02-20 18:48:36 - train.py[line:549] - INFO: 12800 / 17538
2023-02-20 18:48:36 - train.py[line:551] - INFO: load:2.97 valid_run:7807.17 task_valid:7541.53 collect_output:193.70
2023-02-20 18:50:37 - train.py[line:549] - INFO: 13000 / 17538
2023-02-20 18:50:37 - train.py[line:551] - INFO: load:2.99 valid_run:7928.58 task_valid:7657.50 collect_output:198.04
2023-02-20 18:52:39 - train.py[line:549] - INFO: 13200 / 17538
2023-02-20 18:52:39 - train.py[line:551] - INFO: load:3.02 valid_run:8050.71 task_valid:7773.52 collect_output:203.05
2023-02-20 18:54:41 - train.py[line:549] - INFO: 13400 / 17538
2023-02-20 18:54:41 - train.py[line:551] - INFO: load:3.05 valid_run:8172.30 task_valid:7890.99 collect_output:206.09
2023-02-20 18:56:43 - train.py[line:549] - INFO: 13600 / 17538
2023-02-20 18:56:43 - train.py[line:551] - INFO: load:3.07 valid_run:8294.00 task_valid:8008.85 collect_output:208.81
2023-02-20 18:58:45 - train.py[line:549] - INFO: 13800 / 17538
2023-02-20 18:58:45 - train.py[line:551] - INFO: load:3.10 valid_run:8416.22 task_valid:8127.00 collect_output:211.78
2023-02-20 19:00:46 - train.py[line:549] - INFO: 14000 / 17538
2023-02-20 19:00:46 - train.py[line:551] - INFO: load:3.13 valid_run:8536.99 task_valid:8242.53 collect_output:215.91
2023-02-20 19:02:49 - train.py[line:549] - INFO: 14200 / 17538
2023-02-20 19:02:49 - train.py[line:551] - INFO: load:3.15 valid_run:8660.07 task_valid:8361.79 collect_output:218.65
2023-02-20 19:04:53 - train.py[line:549] - INFO: 14400 / 17538
2023-02-20 19:04:53 - train.py[line:551] - INFO: load:3.18 valid_run:8783.94 task_valid:8481.76 collect_output:221.45
2023-02-20 19:06:54 - train.py[line:549] - INFO: 14600 / 17538
2023-02-20 19:06:54 - train.py[line:551] - INFO: load:3.21 valid_run:8905.05 task_valid:8597.15 collect_output:226.06
2023-02-20 19:08:55 - train.py[line:549] - INFO: 14800 / 17538
2023-02-20 19:08:55 - train.py[line:551] - INFO: load:3.23 valid_run:9026.37 task_valid:8714.32 collect_output:229.12
2023-02-20 19:10:57 - train.py[line:549] - INFO: 15000 / 17538
2023-02-20 19:10:57 - train.py[line:551] - INFO: load:3.26 valid_run:9148.01 task_valid:8832.37 collect_output:231.58
2023-02-20 19:12:59 - train.py[line:549] - INFO: 15200 / 17538
2023-02-20 19:12:59 - train.py[line:551] - INFO: load:3.29 valid_run:9269.91 task_valid:8949.08 collect_output:235.67
2023-02-20 19:15:01 - train.py[line:549] - INFO: 15400 / 17538
2023-02-20 19:15:01 - train.py[line:551] - INFO: load:3.31 valid_run:9391.28 task_valid:9066.00 collect_output:239.03
2023-02-20 19:17:03 - train.py[line:549] - INFO: 15600 / 17538
2023-02-20 19:17:03 - train.py[line:551] - INFO: load:3.34 valid_run:9513.36 task_valid:9182.72 collect_output:243.26
2023-02-20 19:19:04 - train.py[line:549] - INFO: 15800 / 17538
2023-02-20 19:19:04 - train.py[line:551] - INFO: load:3.37 valid_run:9634.58 task_valid:9301.23 collect_output:244.87
2023-02-20 19:21:04 - train.py[line:549] - INFO: 16000 / 17538
2023-02-20 19:21:04 - train.py[line:551] - INFO: load:3.39 valid_run:9754.74 task_valid:9417.99 collect_output:247.14
2023-02-20 19:23:06 - train.py[line:549] - INFO: 16200 / 17538
2023-02-20 19:23:06 - train.py[line:551] - INFO: load:3.42 valid_run:9876.40 task_valid:9535.08 collect_output:250.55
2023-02-20 19:25:09 - train.py[line:549] - INFO: 16400 / 17538
2023-02-20 19:25:09 - train.py[line:551] - INFO: load:3.45 valid_run:9999.59 task_valid:9651.73 collect_output:255.98
2023-02-20 19:27:12 - train.py[line:549] - INFO: 16600 / 17538
2023-02-20 19:27:12 - train.py[line:551] - INFO: load:3.47 valid_run:10121.97 task_valid:9770.63 collect_output:258.34
2023-02-20 19:29:14 - train.py[line:549] - INFO: 16800 / 17538
2023-02-20 19:29:14 - train.py[line:551] - INFO: load:3.50 valid_run:10244.29 task_valid:9887.59 collect_output:262.61
2023-02-20 19:31:16 - train.py[line:549] - INFO: 17000 / 17538
2023-02-20 19:31:16 - train.py[line:551] - INFO: load:3.53 valid_run:10365.91 task_valid:10004.39 collect_output:266.33
2023-02-20 19:33:15 - train.py[line:549] - INFO: 17200 / 17538
2023-02-20 19:33:15 - train.py[line:551] - INFO: load:3.55 valid_run:10485.26 task_valid:10120.47 collect_output:268.50
2023-02-20 19:35:19 - train.py[line:549] - INFO: 17400 / 17538
2023-02-20 19:35:19 - train.py[line:551] - INFO: load:3.58 valid_run:10609.33 task_valid:10242.21 collect_output:269.73

====================================================================================================
SGG eval:     R @ 50: 0.6476;     R @ 100: 0.6801;     R @ 500: 0.7013;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4764;    mR @ 100: 0.5236;    mR @ 500: 0.5669;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5588) (covering:0.5538) (eating:0.8491) (flying in:0.7727) (growing on:0.2400) (hanging from:0.5959) (lying on:0.3810) (mounted on:0.0061) (painted on:0.2000) (parked on:1.0000) (playing:0.5714) (riding:0.9734) (says:0.0000) (sitting on:0.7195) (standing on:0.3595) (using:0.6429) (walking in:0.1282) (walking on:0.6894) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6476;     R @ 100: 0.6801;     R @ 500: 0.7013;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4764;    mR @ 100: 0.5236;    mR @ 500: 0.5669;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5588) (covering:0.5538) (eating:0.8491) (flying in:0.7727) (growing on:0.2400) (hanging from:0.5959) (lying on:0.3810) (mounted on:0.0061) (painted on:0.2000) (parked on:1.0000) (playing:0.5714) (riding:0.9734) (says:0.0000) (sitting on:0.7195) (standing on:0.3595) (using:0.6429) (walking in:0.1282) (walking on:0.6894) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-02-20 19:37:11 - train.py[line:487] - INFO: 0.6800921494802495
2023-02-20 19:37:12 - train.py[line:575] - INFO: logits:torch.Size([420900, 21]) sample_ids:torch.Size([420900])
2023-02-20 19:37:12 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.263 | loss_v1 0 | loss_v2 0 | nll_loss 0.1 | ntokens 71.943 | nsentences 23.999 | sample_size 71.943 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.680092 | ppl 1.07 | vqa_score 0.53 | wps 117.6 | wpb 71.9 | bsz 24 | num_updates 200
2023-02-20 19:37:12 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 200 updates
2023-02-20 19:37:12 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-20 19:37:18 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-20 19:37:27 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt (epoch 1 @ 200 updates, score 0.6800921494802495) (writing took 14.990850871428847 seconds)
2023-02-20 19:37:38 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 71012 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.1, ups=0, wpb=111, bsz=40, num_updates=210, lr=0, gnorm=0.356, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11017
2023-02-20 19:37:49 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 71012 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.91, wpb=111.8, bsz=40, num_updates=220, lr=0, gnorm=0.372, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11028
2023-02-20 19:38:01 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 71012 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=111.2, bsz=40, num_updates=230, lr=0, gnorm=0.314, clip=10, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11040
2023-02-20 19:38:12 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 71012 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=113.1, bsz=40, num_updates=240, lr=0, gnorm=0.427, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11051
2023-02-20 19:38:23 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 71012 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.87, wpb=112.7, bsz=40, num_updates=250, lr=0, gnorm=0.352, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11062
2023-02-20 19:38:34 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 71012 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=260, lr=0, gnorm=0.394, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=11073
2023-02-20 19:38:45 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 71012 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=270, lr=0, gnorm=0.416, clip=0, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=11084
2023-02-20 19:38:57 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 71012 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=280, lr=0, gnorm=0.341, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11096
2023-02-20 19:39:08 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 71012 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.7, bsz=40, num_updates=290, lr=0, gnorm=0.359, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11107
2023-02-20 19:39:19 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 71012 loss=0.147, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.91, wpb=112, bsz=40, num_updates=300, lr=0, gnorm=0.201, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11118
2023-02-20 19:39:30 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 71012 loss=0.166, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=310, lr=0, gnorm=0.4, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11129
2023-02-20 19:39:41 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 71012 loss=0.151, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=320, lr=0, gnorm=0.255, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11140
2023-02-20 19:39:52 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 71012 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=330, lr=0, gnorm=0.216, clip=0, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=11151
2023-02-20 19:40:03 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 71012 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95, ups=0.85, wpb=111.2, bsz=40, num_updates=340, lr=0, gnorm=0.348, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=11162
2023-02-20 19:40:15 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 71012 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=350, lr=0, gnorm=0.359, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11174
2023-02-20 19:40:26 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 71012 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.9, wpb=110.9, bsz=40, num_updates=360, lr=0, gnorm=0.291, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11185
2023-02-20 19:40:37 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 71012 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=370, lr=0, gnorm=0.336, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11196
2023-02-20 19:40:48 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 71012 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=380, lr=0, gnorm=0.405, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11207
2023-02-20 19:40:59 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 71012 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.9, wpb=110.7, bsz=40, num_updates=390, lr=0, gnorm=0.369, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11218
2023-02-20 19:41:11 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 71012 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.1, bsz=40, num_updates=400, lr=0, gnorm=0.336, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11230
2023-02-20 19:41:11 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 19:41:12 - train.py[line:549] - INFO: 0 / 17538
2023-02-20 19:41:12 - train.py[line:551] - INFO: load:1.29 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 19:43:16 - train.py[line:549] - INFO: 200 / 17538
2023-02-20 19:43:16 - train.py[line:551] - INFO: load:1.31 valid_run:123.57 task_valid:119.76 collect_output:2.70
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3596623
Killing subprocess 3596624
Main process received SIGINT, exiting
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 6): env://
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 5): env://
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 7): env://
2023-02-21 15:10:37 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - utils.py[line:261] - INFO: Start init
2023-02-21 15:10:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-21 15:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-21 15:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-21 15:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-21 15:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-21 15:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-21 15:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-21 15:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 6
single-machine distributed training is initialized.
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 5
single-machine distributed training is initialized.
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 4
single-machine distributed training is initialized.
2023-02-21 15:10:38 - utils.py[line:274] - INFO: initialized host node4 as rank 7
single-machine distributed training is initialized.
2023-02-21 15:10:49 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_full_val_k10', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k100alpha1.0_/1_B20_A1_E1_0.027_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[0.0], lr_scheduler='polynomial_decay', max_epoch=2, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=8, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k100alpha1.0_/1_B20_A1_E1_0.027_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_full_val_k10', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [0.0]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-21 15:10:49 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-21 15:10:49 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-21 15:10:53 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-21 15:10:53 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-21 15:10:53 - train.py[line:119] - INFO: model: OFAModel
2023-02-21 15:10:53 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-21 15:10:53 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-21 15:10:53 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 3 row count 52613 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 1 row count 52613 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 5 row count 52612 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 0 row count 52613 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 6 row count 52612 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 7 row count 52612 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 2 row count 52613 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 4 row count 52612 total row count 420900
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-21 15:10:54 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-21 15:10:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-21 15:11:00 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   5: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   6: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:765] - INFO: rank   7: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 15:11:00 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 8 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-21 15:11:01 - train.py[line:161] - INFO: training on 8 devices (GPUs/TPUs)
2023-02-21 15:11:01 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-21 15:11:01 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k100alpha1.0_/1_B20_A1_E1_0.027_5e-5_480/checkpoint_best.pt
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-21 15:11:28 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-21 15:11:28 - trainer.py[line:656] - INFO: Loading EMA from checkpoint
2023-02-21 15:11:29 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-21 15:11:29 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-21 15:11:30 - trainer.py[line:663] - INFO: Loading EMA fp32 params from checkpoint
2023-02-21 15:11:30 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k100alpha1.0_/1_B20_A1_E1_0.027_5e-5_480/checkpoint_best.pt (epoch 1 @ 0 updates)
2023-02-21 15:11:30 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 4 row count 355055 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 3 row count 355056 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 2 row count 355056 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 5 row count 355055 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 6 row count 355055 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 7 row count 355055 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 1 row count 355056 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 0 row count 355056 total row count 2840444
2023-02-21 15:11:32 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
2023-02-21 15:11:33 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-21 15:11:33 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 15:12:02 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=444.6, nsentences=160, sample_size=444.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=270, ups=0.61, wpb=444.6, bsz=160, num_updates=10, lr=0, gnorm=0.067, clip=0, loss_scale=128, train_wall=26, gb_free=10.5, ema_decay=0.9999, wall=62
2023-02-21 15:12:14 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=445.7, nsentences=160, sample_size=445.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=360.6, ups=0.81, wpb=445.7, bsz=160, num_updates=20, lr=0, gnorm=0.083, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=74
2023-02-21 15:12:27 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=447.3, nsentences=160, sample_size=447.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=334.5, ups=0.75, wpb=447.3, bsz=160, num_updates=30, lr=0, gnorm=0.068, clip=0, loss_scale=128, train_wall=13, gb_free=10.2, ema_decay=0.9999, wall=88
2023-02-21 15:12:39 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=445.8, nsentences=160, sample_size=445.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=370.3, ups=0.83, wpb=445.8, bsz=160, num_updates=40, lr=0, gnorm=0.068, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=100
2023-02-21 15:12:52 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=445.5, nsentences=160, sample_size=445.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=364.7, ups=0.82, wpb=445.5, bsz=160, num_updates=50, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=112
2023-02-21 15:13:04 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 17753 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=448, nsentences=160, sample_size=448, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=366.6, ups=0.82, wpb=448, bsz=160, num_updates=60, lr=0, gnorm=0.07, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=124
2023-02-21 15:13:15 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=445.2, nsentences=160, sample_size=445.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=384.6, ups=0.86, wpb=445.2, bsz=160, num_updates=70, lr=0, gnorm=0.071, clip=0, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=136
2023-02-21 15:13:27 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=446.6, nsentences=160, sample_size=446.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=379.2, ups=0.85, wpb=446.6, bsz=160, num_updates=80, lr=0, gnorm=0.069, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=148
2023-02-21 15:13:40 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 17753 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=448, nsentences=160, sample_size=448, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=350.6, ups=0.78, wpb=448, bsz=160, num_updates=90, lr=0, gnorm=0.073, clip=0, loss_scale=128, train_wall=13, gb_free=10.7, ema_decay=0.9999, wall=160
2023-02-21 15:13:52 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 17753 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=446.5, nsentences=160, sample_size=446.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=385.9, ups=0.86, wpb=446.5, bsz=160, num_updates=100, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=172
2023-02-21 15:14:04 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 17753 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=446.4, nsentences=160, sample_size=446.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=369.2, ups=0.83, wpb=446.4, bsz=160, num_updates=110, lr=0, gnorm=0.08, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=184
2023-02-21 15:14:15 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=446, nsentences=160, sample_size=446, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.6, ups=0.86, wpb=446, bsz=160, num_updates=120, lr=0, gnorm=0.068, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=196
2023-02-21 15:14:27 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 17753 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=449.4, nsentences=160, sample_size=449.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=384.1, ups=0.85, wpb=449.4, bsz=160, num_updates=130, lr=0, gnorm=0.072, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=207
2023-02-21 15:14:39 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 17753 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=446.5, nsentences=160, sample_size=446.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=371.2, ups=0.83, wpb=446.5, bsz=160, num_updates=140, lr=0, gnorm=0.088, clip=0, loss_scale=128, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=219
2023-02-21 15:14:51 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=442.2, nsentences=160, sample_size=442.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=372.1, ups=0.84, wpb=442.2, bsz=160, num_updates=150, lr=0, gnorm=0.081, clip=0, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=231
2023-02-21 15:15:03 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=444.3, nsentences=160, sample_size=444.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=376.4, ups=0.85, wpb=444.3, bsz=160, num_updates=160, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=243
2023-02-21 15:15:14 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=442.2, nsentences=160, sample_size=442.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.6, ups=0.86, wpb=442.2, bsz=160, num_updates=170, lr=0, gnorm=0.068, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=255
2023-02-21 15:15:26 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 17753 loss=0.166, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=447.7, nsentences=160, sample_size=447.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=386.9, ups=0.86, wpb=447.7, bsz=160, num_updates=180, lr=0, gnorm=0.07, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=266
2023-02-21 15:15:37 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=448.6, nsentences=160, sample_size=448.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=387.2, ups=0.86, wpb=448.6, bsz=160, num_updates=190, lr=0, gnorm=0.066, clip=0, loss_scale=128, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=278
2023-02-21 15:15:49 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 17753 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=448.5, nsentences=160, sample_size=448.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386.5, ups=0.86, wpb=448.5, bsz=160, num_updates=200, lr=0, gnorm=0.072, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=290
2023-02-21 15:15:49 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 15:15:49 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 15:15:51 - train.py[line:549] - INFO: 0 / 4385
2023-02-21 15:15:51 - train.py[line:551] - INFO: load:1.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 15:18:06 - train.py[line:549] - INFO: 200 / 4385
2023-02-21 15:18:06 - train.py[line:551] - INFO: load:1.91 valid_run:134.92 task_valid:123.00 collect_output:10.87
2023-02-21 15:20:12 - train.py[line:549] - INFO: 400 / 4385
2023-02-21 15:20:12 - train.py[line:551] - INFO: load:1.93 valid_run:261.05 task_valid:239.25 collect_output:19.73
2023-02-21 15:22:19 - train.py[line:549] - INFO: 600 / 4385
2023-02-21 15:22:19 - train.py[line:551] - INFO: load:1.95 valid_run:387.18 task_valid:356.21 collect_output:27.89
2023-02-21 15:24:24 - train.py[line:549] - INFO: 800 / 4385
2023-02-21 15:24:24 - train.py[line:551] - INFO: load:1.98 valid_run:512.38 task_valid:470.48 collect_output:37.76
2023-02-21 15:26:30 - train.py[line:549] - INFO: 1000 / 4385
2023-02-21 15:26:30 - train.py[line:551] - INFO: load:2.00 valid_run:638.40 task_valid:588.12 collect_output:45.14
2023-02-21 15:28:37 - train.py[line:549] - INFO: 1200 / 4385
2023-02-21 15:28:37 - train.py[line:551] - INFO: load:2.02 valid_run:765.85 task_valid:707.04 collect_output:52.66
2023-02-21 15:30:43 - train.py[line:549] - INFO: 1400 / 4385
2023-02-21 15:30:43 - train.py[line:551] - INFO: load:2.05 valid_run:891.01 task_valid:825.14 collect_output:58.71
2023-02-21 15:32:49 - train.py[line:549] - INFO: 1600 / 4385
2023-02-21 15:32:49 - train.py[line:551] - INFO: load:2.07 valid_run:1017.33 task_valid:942.01 collect_output:67.14
2023-02-21 15:34:55 - train.py[line:549] - INFO: 1800 / 4385
2023-02-21 15:34:55 - train.py[line:551] - INFO: load:2.09 valid_run:1143.07 task_valid:1059.60 collect_output:74.27
2023-02-21 15:37:01 - train.py[line:549] - INFO: 2000 / 4385
2023-02-21 15:37:01 - train.py[line:551] - INFO: load:2.12 valid_run:1269.00 task_valid:1172.62 collect_output:86.17
2023-02-21 15:39:05 - train.py[line:549] - INFO: 2200 / 4385
2023-02-21 15:39:05 - train.py[line:551] - INFO: load:2.14 valid_run:1393.36 task_valid:1288.52 collect_output:93.61
2023-02-21 15:41:11 - train.py[line:549] - INFO: 2400 / 4385
2023-02-21 15:41:11 - train.py[line:551] - INFO: load:2.16 valid_run:1518.70 task_valid:1405.71 collect_output:100.74
2023-02-21 15:43:18 - train.py[line:549] - INFO: 2600 / 4385
2023-02-21 15:43:18 - train.py[line:551] - INFO: load:2.19 valid_run:1645.71 task_valid:1519.81 collect_output:112.65
2023-02-21 15:45:24 - train.py[line:549] - INFO: 2800 / 4385
2023-02-21 15:45:24 - train.py[line:551] - INFO: load:2.21 valid_run:1771.51 task_valid:1637.84 collect_output:119.43
2023-02-21 15:47:30 - train.py[line:549] - INFO: 3000 / 4385
2023-02-21 15:47:30 - train.py[line:551] - INFO: load:2.24 valid_run:1897.76 task_valid:1754.33 collect_output:128.16
2023-02-21 15:49:37 - train.py[line:549] - INFO: 3200 / 4385
2023-02-21 15:49:37 - train.py[line:551] - INFO: load:2.28 valid_run:2024.57 task_valid:1868.63 collect_output:139.64
2023-02-21 15:51:44 - train.py[line:549] - INFO: 3400 / 4385
2023-02-21 15:51:44 - train.py[line:551] - INFO: load:2.30 valid_run:2151.41 task_valid:1984.80 collect_output:149.30
2023-02-21 15:53:49 - train.py[line:549] - INFO: 3600 / 4385
2023-02-21 15:53:49 - train.py[line:551] - INFO: load:2.33 valid_run:2277.19 task_valid:2102.73 collect_output:156.14
2023-02-21 15:55:55 - train.py[line:549] - INFO: 3800 / 4385
2023-02-21 15:55:55 - train.py[line:551] - INFO: load:2.35 valid_run:2403.00 task_valid:2219.79 collect_output:163.89
2023-02-21 15:58:01 - train.py[line:549] - INFO: 4000 / 4385
2023-02-21 15:58:01 - train.py[line:551] - INFO: load:2.37 valid_run:2528.53 task_valid:2336.47 collect_output:171.75
2023-02-21 16:00:07 - train.py[line:549] - INFO: 4200 / 4385
2023-02-21 16:00:07 - train.py[line:551] - INFO: load:2.40 valid_run:2654.66 task_valid:2453.05 collect_output:180.30

====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================

2023-02-21 16:02:31 - train.py[line:487] - INFO: 0.6798023743341526

====================================================================================================
SGG eval:     R @ 50: 0.6485;     R @ 100: 0.6798;     R @ 500: 0.6976;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4426;    mR @ 100: 0.5213;    mR @ 500: 0.5803;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8111) (covered in:0.5196) (covering:0.5154) (eating:0.8868) (flying in:0.5000) (growing on:0.4300) (hanging from:0.5619) (lying on:0.2476) (mounted on:0.0000) (painted on:0.1000) (parked on:0.9649) (playing:0.5714) (riding:0.9745) (says:0.3333) (sitting on:0.7303) (standing on:0.3616) (using:0.6607) (walking in:0.0769) (walking on:0.7083) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================

2023-02-21 16:02:32 - train.py[line:575] - INFO: logits:torch.Size([420900, 21]) sample_ids:torch.Size([420900])
2023-02-21 16:02:32 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.22 | loss_v1 0 | loss_v2 0 | nll_loss 0.049 | ntokens 287.741 | nsentences 95.986 | sample_size 287.741 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.679802 | ppl 1.03 | vqa_score 0.3231 | wps 451.2 | wpb 287.7 | bsz 96 | num_updates 200
2023-02-21 16:02:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 200 updates
2023-02-21 16:02:32 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-21 16:02:37 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-21 16:02:42 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt (epoch 1 @ 200 updates, score 0.6798023743341526) (writing took 10.24764420837164 seconds)
2023-02-21 16:02:54 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=446.1, nsentences=160, sample_size=446.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=1.6, ups=0, wpb=446.1, bsz=160, num_updates=210, lr=0, gnorm=0.078, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3114
2023-02-21 16:03:05 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 17753 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=447.9, nsentences=160, sample_size=447.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.1, ups=0.85, wpb=447.9, bsz=160, num_updates=220, lr=0, gnorm=0.068, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3126
2023-02-21 16:03:17 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=445.6, nsentences=160, sample_size=445.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=377.9, ups=0.85, wpb=445.6, bsz=160, num_updates=230, lr=0, gnorm=0.053, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3138
2023-02-21 16:03:29 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 17753 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=448, nsentences=160, sample_size=448, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=390.9, ups=0.87, wpb=448, bsz=160, num_updates=240, lr=0, gnorm=0.072, clip=0, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3149
2023-02-21 16:03:40 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 17753 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=445.2, nsentences=160, sample_size=445.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=384.6, ups=0.86, wpb=445.2, bsz=160, num_updates=250, lr=0, gnorm=0.082, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3161
2023-02-21 16:03:52 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 17753 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=447.8, nsentences=160, sample_size=447.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386.5, ups=0.86, wpb=447.8, bsz=160, num_updates=260, lr=0, gnorm=0.07, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=3172
2023-02-21 16:04:03 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 17753 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=445.1, nsentences=160, sample_size=445.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=383.1, ups=0.86, wpb=445.1, bsz=160, num_updates=270, lr=0, gnorm=0.079, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=3184
2023-02-21 16:04:15 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=446, nsentences=160, sample_size=446, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.9, ups=0.87, wpb=446, bsz=160, num_updates=280, lr=0, gnorm=0.06, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3195
2023-02-21 16:04:27 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 17753 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=448.6, nsentences=160, sample_size=448.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=387.7, ups=0.86, wpb=448.6, bsz=160, num_updates=290, lr=0, gnorm=0.062, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3207
2023-02-21 16:04:38 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=445.6, nsentences=160, sample_size=445.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.5, ups=0.86, wpb=445.6, bsz=160, num_updates=300, lr=0, gnorm=0.078, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3219
2023-02-21 16:04:50 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=447.1, nsentences=160, sample_size=447.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=387, ups=0.87, wpb=447.1, bsz=160, num_updates=310, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3230
2023-02-21 16:05:02 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=449, nsentences=160, sample_size=449, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=379.7, ups=0.85, wpb=449, bsz=160, num_updates=320, lr=0, gnorm=0.066, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3242
2023-02-21 16:05:13 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=446, nsentences=160, sample_size=446, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.1, ups=0.86, wpb=446, bsz=160, num_updates=330, lr=0, gnorm=0.072, clip=0, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3254
2023-02-21 16:05:25 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=447, nsentences=160, sample_size=447, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=380.8, ups=0.85, wpb=447, bsz=160, num_updates=340, lr=0, gnorm=0.073, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3265
2023-02-21 16:05:36 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=445.3, nsentences=160, sample_size=445.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=388.6, ups=0.87, wpb=445.3, bsz=160, num_updates=350, lr=0, gnorm=0.072, clip=0, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3277
2023-02-21 16:05:48 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=444.5, nsentences=160, sample_size=444.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=383.7, ups=0.86, wpb=444.5, bsz=160, num_updates=360, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3288
2023-02-21 16:06:00 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=446.6, nsentences=160, sample_size=446.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=385.1, ups=0.86, wpb=446.6, bsz=160, num_updates=370, lr=0, gnorm=0.08, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3300
2023-02-21 16:06:11 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=444.4, nsentences=160, sample_size=444.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.1, ups=0.86, wpb=444.4, bsz=160, num_updates=380, lr=0, gnorm=0.07, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3312
2023-02-21 16:06:23 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=442, nsentences=160, sample_size=442, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=382.1, ups=0.86, wpb=442, bsz=160, num_updates=390, lr=0, gnorm=0.066, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3323
2023-02-21 16:06:34 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=449.3, nsentences=160, sample_size=449.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=388.1, ups=0.86, wpb=449.3, bsz=160, num_updates=400, lr=0, gnorm=0.067, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3335
2023-02-21 16:06:34 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 16:06:36 - train.py[line:549] - INFO: 0 / 4385
2023-02-21 16:06:36 - train.py[line:551] - INFO: load:1.62 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 16:08:47 - train.py[line:549] - INFO: 200 / 4385
2023-02-21 16:08:47 - train.py[line:551] - INFO: load:1.65 valid_run:131.01 task_valid:119.34 collect_output:10.67
2023-02-21 16:10:53 - train.py[line:549] - INFO: 400 / 4385
2023-02-21 16:10:53 - train.py[line:551] - INFO: load:1.67 valid_run:257.10 task_valid:235.39 collect_output:19.71
2023-02-21 16:12:59 - train.py[line:549] - INFO: 600 / 4385
2023-02-21 16:12:59 - train.py[line:551] - INFO: load:1.69 valid_run:382.97 task_valid:351.95 collect_output:28.02
2023-02-21 16:15:04 - train.py[line:549] - INFO: 800 / 4385
2023-02-21 16:15:04 - train.py[line:551] - INFO: load:1.71 valid_run:508.00 task_valid:465.76 collect_output:38.26
2023-02-21 16:17:10 - train.py[line:549] - INFO: 1000 / 4385
2023-02-21 16:17:10 - train.py[line:551] - INFO: load:1.74 valid_run:633.45 task_valid:583.11 collect_output:45.37
2023-02-21 16:19:17 - train.py[line:549] - INFO: 1200 / 4385
2023-02-21 16:19:17 - train.py[line:551] - INFO: load:1.76 valid_run:760.24 task_valid:702.21 collect_output:52.01
2023-02-21 16:21:22 - train.py[line:549] - INFO: 1400 / 4385
2023-02-21 16:21:22 - train.py[line:551] - INFO: load:1.78 valid_run:885.28 task_valid:820.17 collect_output:58.09
2023-02-21 16:23:28 - train.py[line:549] - INFO: 1600 / 4385
2023-02-21 16:23:28 - train.py[line:551] - INFO: load:1.81 valid_run:1011.73 task_valid:936.97 collect_output:66.75
2023-02-21 16:25:34 - train.py[line:549] - INFO: 1800 / 4385
2023-02-21 16:25:34 - train.py[line:551] - INFO: load:1.83 valid_run:1137.39 task_valid:1054.42 collect_output:73.93
2023-02-21 16:27:40 - train.py[line:549] - INFO: 2000 / 4385
2023-02-21 16:27:40 - train.py[line:551] - INFO: load:1.85 valid_run:1263.01 task_valid:1167.28 collect_output:85.67
2023-02-21 16:29:44 - train.py[line:549] - INFO: 2200 / 4385
2023-02-21 16:29:44 - train.py[line:551] - INFO: load:1.88 valid_run:1387.38 task_valid:1283.17 collect_output:93.17
2023-02-21 16:31:50 - train.py[line:549] - INFO: 2400 / 4385
2023-02-21 16:31:50 - train.py[line:551] - INFO: load:1.90 valid_run:1512.94 task_valid:1400.62 collect_output:100.23
2023-02-21 16:33:57 - train.py[line:549] - INFO: 2600 / 4385
2023-02-21 16:33:57 - train.py[line:551] - INFO: load:1.92 valid_run:1639.97 task_valid:1514.76 collect_output:112.13
2023-02-21 16:36:03 - train.py[line:549] - INFO: 2800 / 4385
2023-02-21 16:36:03 - train.py[line:551] - INFO: load:1.95 valid_run:1765.56 task_valid:1632.68 collect_output:118.78
2023-02-21 16:38:09 - train.py[line:549] - INFO: 3000 / 4385
2023-02-21 16:38:09 - train.py[line:551] - INFO: load:1.97 valid_run:1891.61 task_valid:1749.24 collect_output:127.23
Traceback (most recent call last):
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
  File "../../train.py", line 632, in <module>
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3877829
Killing subprocess 3877830
Killing subprocess 3877831
Killing subprocess 3877832
Killing subprocess 3877833
Killing subprocess 3877834
Killing subprocess 3877836
Killing subprocess 3877839
Main process received SIGINT, exiting
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 6): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 5): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 7): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 7
2023-02-21 16:39:36 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-02-21 16:39:36 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2023-02-21 16:39:36 - utils.py[line:261] - INFO: Start init
2023-02-21 16:39:36 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2023-02-21 16:39:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 6
2023-02-21 16:39:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-02-21 16:39:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-21 16:39:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 5
2023-02-21 16:39:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 6
single-machine distributed training is initialized.
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 5
single-machine distributed training is initialized.
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 4
single-machine distributed training is initialized.
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 7
single-machine distributed training is initialized.
2023-02-21 16:39:37 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-02-21 16:39:48 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_full_val_k10', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_/1_B20_A1_E2_0.027_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[0.0], lr_scheduler='polynomial_decay', max_epoch=2, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=8, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_/1_B20_A1_E2_0.027_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_full_val_k10', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [0.0]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-21 16:39:48 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-21 16:39:48 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-21 16:39:53 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-21 16:39:53 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-21 16:39:53 - train.py[line:119] - INFO: model: OFAModel
2023-02-21 16:39:53 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-21 16:39:53 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-21 16:39:53 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 7 row count 52612 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 2 row count 52613 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 6 row count 52612 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 4 row count 52612 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 1 row count 52613 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 3 row count 52613 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 5 row count 52612 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_1566.tsv slice_id 0 row count 52613 total row count 420900
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-21 16:39:54 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-21 16:39:54 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-21 16:39:59 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 8 workers***********************
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   5: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   6: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:765] - INFO: rank   7: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 16:39:59 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 8 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-21 16:40:01 - train.py[line:161] - INFO: training on 8 devices (GPUs/TPUs)
2023-02-21 16:40:01 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-21 16:40:01 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_/1_B20_A1_E2_0.027_5e-5_480/checkpoint_best.pt
Done 0.95 cuda cpu, cpu
2023-02-21 16:40:27 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-21 16:40:27 - trainer.py[line:656] - INFO: Loading EMA from checkpoint
2023-02-21 16:40:28 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-21 16:40:28 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-21 16:40:28 - trainer.py[line:663] - INFO: Loading EMA fp32 params from checkpoint
2023-02-21 16:40:29 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_/1_B20_A1_E2_0.027_5e-5_480/checkpoint_best.pt (epoch 1 @ 0 updates)
2023-02-21 16:40:29 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 5 row count 355055 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 2 row count 355056 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 7 row count 355055 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 0 row count 355056 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 4 row count 355055 total row count 2840444file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 3 row count 355056 total row count 2840444

file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 6 row count 355055 total row count 2840444
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k50alpha1.0_train_NA1_E0.tsv slice_id 1 row count 355056 total row count 2840444
2023-02-21 16:40:33 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
2023-02-21 16:40:36 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-21 16:40:36 - train.py[line:312] - INFO: Start iterating over samples
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
Total steps 35506, warmup steps 958, warmup_factor 0.0010438413361169101
2023-02-21 16:41:13 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=444.6, nsentences=160, sample_size=444.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=247, ups=0.56, wpb=444.6, bsz=160, num_updates=10, lr=0, gnorm=0.078, clip=0, loss_scale=128, train_wall=26, gb_free=10.5, ema_decay=0.9999, wall=74
2023-02-21 16:41:28 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=445.7, nsentences=160, sample_size=445.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=302.9, ups=0.68, wpb=445.7, bsz=160, num_updates=20, lr=0, gnorm=0.078, clip=0, loss_scale=128, train_wall=15, gb_free=10.7, ema_decay=0.9999, wall=89
2023-02-21 16:41:42 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 17753 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=447.3, nsentences=160, sample_size=447.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=325.4, ups=0.73, wpb=447.3, bsz=160, num_updates=30, lr=0, gnorm=0.066, clip=0, loss_scale=128, train_wall=14, gb_free=10.2, ema_decay=0.9999, wall=103
2023-02-21 16:41:54 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=445.8, nsentences=160, sample_size=445.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=365.7, ups=0.82, wpb=445.8, bsz=160, num_updates=40, lr=0, gnorm=0.069, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=115
2023-02-21 16:42:07 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=445.5, nsentences=160, sample_size=445.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=345.5, ups=0.78, wpb=445.5, bsz=160, num_updates=50, lr=0, gnorm=0.075, clip=0, loss_scale=128, train_wall=13, gb_free=10.7, ema_decay=0.9999, wall=128
2023-02-21 16:42:20 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 17753 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=448, nsentences=160, sample_size=448, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=357.1, ups=0.8, wpb=448, bsz=160, num_updates=60, lr=0, gnorm=0.087, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=140
2023-02-21 16:42:31 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=445.2, nsentences=160, sample_size=445.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=380, ups=0.85, wpb=445.2, bsz=160, num_updates=70, lr=0, gnorm=0.065, clip=0, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=152
2023-02-21 16:42:43 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 17753 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=446.6, nsentences=160, sample_size=446.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=368.7, ups=0.83, wpb=446.6, bsz=160, num_updates=80, lr=0, gnorm=0.064, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=164
2023-02-21 16:42:56 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=448, nsentences=160, sample_size=448, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=355.7, ups=0.79, wpb=448, bsz=160, num_updates=90, lr=0, gnorm=0.096, clip=0, loss_scale=128, train_wall=13, gb_free=10.7, ema_decay=0.9999, wall=177
2023-02-21 16:43:08 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=446.5, nsentences=160, sample_size=446.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.1, ups=0.86, wpb=446.5, bsz=160, num_updates=100, lr=0, gnorm=0.08, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=188
2023-02-21 16:43:20 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=446.4, nsentences=160, sample_size=446.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=360.5, ups=0.81, wpb=446.4, bsz=160, num_updates=110, lr=0, gnorm=0.083, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=201
2023-02-21 16:43:32 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=446, nsentences=160, sample_size=446, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.7, ups=0.86, wpb=446, bsz=160, num_updates=120, lr=0, gnorm=0.067, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=212
2023-02-21 16:43:44 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=449.4, nsentences=160, sample_size=449.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.9, ups=0.85, wpb=449.4, bsz=160, num_updates=130, lr=0, gnorm=0.071, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=224
2023-02-21 16:43:56 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=446.5, nsentences=160, sample_size=446.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=369.4, ups=0.83, wpb=446.5, bsz=160, num_updates=140, lr=0, gnorm=0.072, clip=0, loss_scale=128, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=236
2023-02-21 16:44:07 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 17753 loss=0.153, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=442.2, nsentences=160, sample_size=442.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=372.4, ups=0.84, wpb=442.2, bsz=160, num_updates=150, lr=0, gnorm=0.065, clip=0, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=248
2023-02-21 16:44:19 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 17753 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=444.3, nsentences=160, sample_size=444.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=376.5, ups=0.85, wpb=444.3, bsz=160, num_updates=160, lr=0, gnorm=0.06, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=260
2023-02-21 16:44:31 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=442.2, nsentences=160, sample_size=442.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=380.1, ups=0.86, wpb=442.2, bsz=160, num_updates=170, lr=0, gnorm=0.073, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=272
2023-02-21 16:44:43 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=447.7, nsentences=160, sample_size=447.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386.1, ups=0.86, wpb=447.7, bsz=160, num_updates=180, lr=0, gnorm=0.059, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=283
2023-02-21 16:44:54 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 17753 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=448.6, nsentences=160, sample_size=448.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.3, ups=0.86, wpb=448.6, bsz=160, num_updates=190, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=295
2023-02-21 16:45:06 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=448.5, nsentences=160, sample_size=448.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386, ups=0.86, wpb=448.5, bsz=160, num_updates=200, lr=0, gnorm=0.066, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=306
2023-02-21 16:45:06 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 16:45:06 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 16:45:09 - train.py[line:549] - INFO: 0 / 4385
2023-02-21 16:45:09 - train.py[line:551] - INFO: load:2.45 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 16:47:23 - train.py[line:549] - INFO: 200 / 4385
2023-02-21 16:47:23 - train.py[line:551] - INFO: load:2.48 valid_run:134.22 task_valid:123.23 collect_output:9.91
2023-02-21 16:49:29 - train.py[line:549] - INFO: 400 / 4385
2023-02-21 16:49:29 - train.py[line:551] - INFO: load:2.50 valid_run:260.13 task_valid:239.70 collect_output:18.28
2023-02-21 16:51:35 - train.py[line:549] - INFO: 600 / 4385
2023-02-21 16:51:35 - train.py[line:551] - INFO: load:2.53 valid_run:386.11 task_valid:356.62 collect_output:26.31
2023-02-21 16:53:40 - train.py[line:549] - INFO: 800 / 4385
2023-02-21 16:53:40 - train.py[line:551] - INFO: load:2.55 valid_run:511.37 task_valid:470.81 collect_output:36.35
2023-02-21 16:55:46 - train.py[line:549] - INFO: 1000 / 4385
2023-02-21 16:55:46 - train.py[line:551] - INFO: load:2.57 valid_run:636.76 task_valid:588.57 collect_output:42.97
2023-02-21 16:57:53 - train.py[line:549] - INFO: 1200 / 4385
2023-02-21 16:57:53 - train.py[line:551] - INFO: load:2.60 valid_run:764.16 task_valid:707.71 collect_output:50.17
2023-02-21 16:59:59 - train.py[line:549] - INFO: 1400 / 4385
2023-02-21 16:59:59 - train.py[line:551] - INFO: load:2.62 valid_run:889.73 task_valid:826.10 collect_output:56.31
2023-02-21 17:02:06 - train.py[line:549] - INFO: 1600 / 4385
2023-02-21 17:02:06 - train.py[line:551] - INFO: load:2.64 valid_run:1017.16 task_valid:943.24 collect_output:65.55
2023-02-21 17:04:13 - train.py[line:549] - INFO: 1800 / 4385
2023-02-21 17:04:13 - train.py[line:551] - INFO: load:2.67 valid_run:1143.24 task_valid:1060.85 collect_output:72.97
2023-02-21 17:06:19 - train.py[line:549] - INFO: 2000 / 4385
2023-02-21 17:06:19 - train.py[line:551] - INFO: load:2.69 valid_run:1269.27 task_valid:1173.86 collect_output:84.95
2023-02-21 17:08:23 - train.py[line:549] - INFO: 2200 / 4385
2023-02-21 17:08:23 - train.py[line:551] - INFO: load:2.72 valid_run:1394.04 task_valid:1290.29 collect_output:92.20
2023-02-21 17:10:29 - train.py[line:549] - INFO: 2400 / 4385
2023-02-21 17:10:29 - train.py[line:551] - INFO: load:2.74 valid_run:1519.93 task_valid:1407.97 collect_output:99.38
2023-02-21 17:12:37 - train.py[line:549] - INFO: 2600 / 4385
2023-02-21 17:12:37 - train.py[line:551] - INFO: load:2.76 valid_run:1647.78 task_valid:1522.27 collect_output:111.86
2023-02-21 17:14:43 - train.py[line:549] - INFO: 2800 / 4385
2023-02-21 17:14:43 - train.py[line:551] - INFO: load:2.79 valid_run:1773.50 task_valid:1640.42 collect_output:118.41
2023-02-21 17:16:49 - train.py[line:549] - INFO: 3000 / 4385
2023-02-21 17:16:49 - train.py[line:551] - INFO: load:2.81 valid_run:1899.65 task_valid:1757.00 collect_output:126.93
2023-02-21 17:18:57 - train.py[line:549] - INFO: 3200 / 4385
2023-02-21 17:18:57 - train.py[line:551] - INFO: load:2.83 valid_run:2026.88 task_valid:1871.50 collect_output:138.62
2023-02-21 17:21:03 - train.py[line:549] - INFO: 3400 / 4385
2023-02-21 17:21:03 - train.py[line:551] - INFO: load:2.86 valid_run:2153.70 task_valid:1988.13 collect_output:147.74
2023-02-21 17:23:09 - train.py[line:549] - INFO: 3600 / 4385
2023-02-21 17:23:09 - train.py[line:551] - INFO: load:2.88 valid_run:2279.62 task_valid:2106.26 collect_output:154.51
2023-02-21 17:25:16 - train.py[line:549] - INFO: 3800 / 4385
2023-02-21 17:25:16 - train.py[line:551] - INFO: load:2.91 valid_run:2405.68 task_valid:2223.70 collect_output:162.09
2023-02-21 17:27:21 - train.py[line:549] - INFO: 4000 / 4385
2023-02-21 17:27:21 - train.py[line:551] - INFO: load:2.93 valid_run:2531.18 task_valid:2340.88 collect_output:169.33
2023-02-21 17:29:27 - train.py[line:549] - INFO: 4200 / 4385
2023-02-21 17:29:27 - train.py[line:551] - INFO: load:2.95 valid_run:2657.25 task_valid:2457.67 collect_output:177.61

====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================

2023-02-21 17:31:51 - train.py[line:487] - INFO: 0.6840328979612279

====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================

2023-02-21 17:31:52 - train.py[line:575] - INFO: logits:torch.Size([420900, 21]) sample_ids:torch.Size([420900])
2023-02-21 17:31:52 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.236 | loss_v1 0 | loss_v2 0 | nll_loss 0.07 | ntokens 287.741 | nsentences 95.986 | sample_size 287.741 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.684033 | ppl 1.05 | vqa_score 0.4721 | wps 450.7 | wpb 287.7 | bsz 96 | num_updates 200

====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6840;     R @ 500: 0.7058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4699;    mR @ 100: 0.5012;    mR @ 500: 0.5628;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8333) (covered in:0.5294) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3143) (hanging from:0.5577) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9732) (says:0.0000) (sitting on:0.7277) (standing on:0.3890) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================

2023-02-21 17:31:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 200 updates
2023-02-21 17:31:52 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-21 17:31:58 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt
2023-02-21 17:32:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_200.pt (epoch 1 @ 200 updates, score 0.6840328979612279) (writing took 10.446916600689292 seconds)
2023-02-21 17:32:14 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=446.1, nsentences=160, sample_size=446.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=1.6, ups=0, wpb=446.1, bsz=160, num_updates=210, lr=0, gnorm=0.069, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3135
2023-02-21 17:32:26 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=447.9, nsentences=160, sample_size=447.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386.1, ups=0.86, wpb=447.9, bsz=160, num_updates=220, lr=0, gnorm=0.079, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3146
2023-02-21 17:32:38 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 17753 loss=0.154, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=445.6, nsentences=160, sample_size=445.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=368.5, ups=0.83, wpb=445.6, bsz=160, num_updates=230, lr=0, gnorm=0.068, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3159
2023-02-21 17:32:50 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=448, nsentences=160, sample_size=448, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.5, ups=0.85, wpb=448, bsz=160, num_updates=240, lr=0, gnorm=0.088, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3170
2023-02-21 17:33:01 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 17753 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=445.2, nsentences=160, sample_size=445.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.1, ups=0.86, wpb=445.2, bsz=160, num_updates=250, lr=0, gnorm=0.075, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3182
2023-02-21 17:33:13 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=447.8, nsentences=160, sample_size=447.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=383.9, ups=0.86, wpb=447.8, bsz=160, num_updates=260, lr=0, gnorm=0.061, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=3194
2023-02-21 17:33:25 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=445.1, nsentences=160, sample_size=445.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.4, ups=0.86, wpb=445.1, bsz=160, num_updates=270, lr=0, gnorm=0.071, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=3205
2023-02-21 17:33:36 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=446, nsentences=160, sample_size=446, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.5, ups=0.86, wpb=446, bsz=160, num_updates=280, lr=0, gnorm=0.079, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3217
2023-02-21 17:33:48 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=448.6, nsentences=160, sample_size=448.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386, ups=0.86, wpb=448.6, bsz=160, num_updates=290, lr=0, gnorm=0.078, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3228
2023-02-21 17:34:00 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 17753 loss=0.15, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=445.6, nsentences=160, sample_size=445.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.6, ups=0.86, wpb=445.6, bsz=160, num_updates=300, lr=0, gnorm=0.071, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3240
2023-02-21 17:34:11 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=447.1, nsentences=160, sample_size=447.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.5, ups=0.86, wpb=447.1, bsz=160, num_updates=310, lr=0, gnorm=0.069, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3252
2023-02-21 17:34:23 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=449, nsentences=160, sample_size=449, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=383.6, ups=0.85, wpb=449, bsz=160, num_updates=320, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3263
2023-02-21 17:34:34 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=446, nsentences=160, sample_size=446, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385, ups=0.86, wpb=446, bsz=160, num_updates=330, lr=0, gnorm=0.075, clip=0, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3275
2023-02-21 17:34:46 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=447, nsentences=160, sample_size=447, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.4, ups=0.85, wpb=447, bsz=160, num_updates=340, lr=0, gnorm=0.067, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3287
2023-02-21 17:34:58 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=445.3, nsentences=160, sample_size=445.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.2, ups=0.86, wpb=445.3, bsz=160, num_updates=350, lr=0, gnorm=0.068, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3298
2023-02-21 17:35:09 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=444.5, nsentences=160, sample_size=444.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.8, ups=0.86, wpb=444.5, bsz=160, num_updates=360, lr=0, gnorm=0.073, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3310
2023-02-21 17:35:21 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=446.6, nsentences=160, sample_size=446.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.2, ups=0.86, wpb=446.6, bsz=160, num_updates=370, lr=0, gnorm=0.064, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3322
2023-02-21 17:35:33 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 17753 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=444.4, nsentences=160, sample_size=444.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.9, ups=0.86, wpb=444.4, bsz=160, num_updates=380, lr=0, gnorm=0.059, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3333
2023-02-21 17:35:44 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 17753 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=442, nsentences=160, sample_size=442, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.2, ups=0.86, wpb=442, bsz=160, num_updates=390, lr=0, gnorm=0.071, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3345
2023-02-21 17:35:56 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=449.3, nsentences=160, sample_size=449.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=387.1, ups=0.86, wpb=449.3, bsz=160, num_updates=400, lr=0, gnorm=0.064, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3356
2023-02-21 17:35:56 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 17:35:58 - train.py[line:549] - INFO: 0 / 4385
2023-02-21 17:35:58 - train.py[line:551] - INFO: load:1.59 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 17:38:08 - train.py[line:549] - INFO: 200 / 4385
2023-02-21 17:38:08 - train.py[line:551] - INFO: load:1.62 valid_run:130.28 task_valid:119.66 collect_output:9.58
2023-02-21 17:40:14 - train.py[line:549] - INFO: 400 / 4385
2023-02-21 17:40:14 - train.py[line:551] - INFO: load:1.64 valid_run:256.39 task_valid:235.83 collect_output:18.48
2023-02-21 17:42:20 - train.py[line:549] - INFO: 600 / 4385
2023-02-21 17:42:20 - train.py[line:551] - INFO: load:1.67 valid_run:382.48 task_valid:353.05 collect_output:26.27
2023-02-21 17:44:26 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.14 GiB (GPU 7; 39.59 GiB total capacity; 8.10 GiB already allocated; 2.24 GiB free; 32.13 GiB reserved in total by PyTorch)
2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8299 MB |   13378 MB |  526949 GB |  526941 GB |
|       from large pool |    8154 MB |   13233 MB |  526825 GB |  526817 GB |
|       from small pool |     144 MB |     145 MB |     124 GB |     123 GB |
|---------------------------------------------------------------------------|
| Active memory         |    8299 MB |   13378 MB |  526949 GB |  526941 GB |
|       from large pool |    8154 MB |   13233 MB |  526825 GB |  526817 GB |
|       from small pool |     144 MB |     145 MB |     124 GB |     123 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   32900 MB |   32900 MB |  198952 MB |  166052 MB |
|       from large pool |   32754 MB |   32754 MB |  198668 MB |  165914 MB |
|       from small pool |     146 MB |     154 MB |     284 MB |     138 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24600 MB |   24697 MB |  588457 GB |  588433 GB |
|       from large pool |   24599 MB |   24695 MB |  588327 GB |  588303 GB |
|       from small pool |       1 MB |       1 MB |     129 GB |     129 GB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |   27496 K  |   27493 K  |
|       from large pool |     563    |     575    |    9252 K  |    9251 K  |
|       from small pool |    3106    |    3116    |   18244 K  |   18241 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |   27496 K  |   27493 K  |
|       from large pool |     563    |     575    |    9252 K  |    9251 K  |
|       from small pool |    3106    |    3116    |   18244 K  |   18241 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     187    |     201    |     530    |     343    |
|       from large pool |     114    |     124    |     388    |     274    |
|       from small pool |      73    |      77    |     142    |      69    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     148    |   18800 K  |   18800 K  |
|       from large pool |      91    |      97    |    2800 K  |    2800 K  |
|       from small pool |      37    |      54    |   16000 K  |   16000 K  |
|===========================================================================|

2023-02-21 17:44:26 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-02-21 17:44:27 - train.py[line:549] - INFO: 800 / 4385
2023-02-21 17:44:27 - train.py[line:551] - INFO: load:1.69 valid_run:509.23 task_valid:467.31 collect_output:37.71
2023-02-21 17:46:34 - train.py[line:549] - INFO: 1000 / 4385
2023-02-21 17:46:34 - train.py[line:551] - INFO: load:1.71 valid_run:635.55 task_valid:585.12 collect_output:45.17
2023-02-21 17:48:41 - train.py[line:549] - INFO: 1200 / 4385
2023-02-21 17:48:41 - train.py[line:551] - INFO: load:1.74 valid_run:763.31 task_valid:704.37 collect_output:52.63
2023-02-21 17:50:47 - train.py[line:549] - INFO: 1400 / 4385
2023-02-21 17:50:47 - train.py[line:551] - INFO: load:1.76 valid_run:889.10 task_valid:822.97 collect_output:58.72
2023-02-21 17:52:54 - train.py[line:549] - INFO: 1600 / 4385
2023-02-21 17:52:54 - train.py[line:551] - INFO: load:1.78 valid_run:1015.63 task_valid:940.16 collect_output:67.01
2023-02-21 17:55:00 - train.py[line:549] - INFO: 1800 / 4385
2023-02-21 17:55:00 - train.py[line:551] - INFO: load:1.81 valid_run:1141.44 task_valid:1057.79 collect_output:74.12
2023-02-21 17:57:06 - train.py[line:549] - INFO: 2000 / 4385
2023-02-21 17:57:06 - train.py[line:551] - INFO: load:1.83 valid_run:1267.68 task_valid:1171.01 collect_output:86.07
2023-02-21 17:59:11 - train.py[line:549] - INFO: 2200 / 4385
2023-02-21 17:59:11 - train.py[line:551] - INFO: load:1.86 valid_run:1392.80 task_valid:1287.47 collect_output:93.68
2023-02-21 18:01:17 - train.py[line:549] - INFO: 2400 / 4385
2023-02-21 18:01:17 - train.py[line:551] - INFO: load:1.88 valid_run:1519.01 task_valid:1405.33 collect_output:100.94
2023-02-21 18:03:25 - train.py[line:549] - INFO: 2600 / 4385
2023-02-21 18:03:25 - train.py[line:551] - INFO: load:1.90 valid_run:1646.25 task_valid:1519.70 collect_output:112.77
2023-02-21 18:05:31 - train.py[line:549] - INFO: 2800 / 4385
2023-02-21 18:05:31 - train.py[line:551] - INFO: load:1.93 valid_run:1772.01 task_valid:1638.05 collect_output:119.08
2023-02-21 18:07:37 - train.py[line:549] - INFO: 3000 / 4385
2023-02-21 18:07:37 - train.py[line:551] - INFO: load:1.95 valid_run:1898.44 task_valid:1754.90 collect_output:127.60
2023-02-21 18:09:45 - train.py[line:549] - INFO: 3200 / 4385
2023-02-21 18:09:45 - train.py[line:551] - INFO: load:1.98 valid_run:2025.95 task_valid:1869.55 collect_output:139.36
2023-02-21 18:11:52 - train.py[line:549] - INFO: 3400 / 4385
2023-02-21 18:11:52 - train.py[line:551] - INFO: load:2.00 valid_run:2153.50 task_valid:1986.16 collect_output:149.26
2023-02-21 18:13:59 - train.py[line:549] - INFO: 3600 / 4385
2023-02-21 18:13:59 - train.py[line:551] - INFO: load:2.03 valid_run:2279.90 task_valid:2104.76 collect_output:155.97
2023-02-21 18:16:05 - train.py[line:549] - INFO: 3800 / 4385
2023-02-21 18:16:05 - train.py[line:551] - INFO: load:2.05 valid_run:2406.35 task_valid:2222.36 collect_output:163.76
2023-02-21 18:18:11 - train.py[line:549] - INFO: 4000 / 4385
2023-02-21 18:18:11 - train.py[line:551] - INFO: load:2.08 valid_run:2532.22 task_valid:2339.33 collect_output:171.64
2023-02-21 18:20:18 - train.py[line:549] - INFO: 4200 / 4385
2023-02-21 18:20:18 - train.py[line:551] - INFO: load:2.10 valid_run:2658.80 task_valid:2456.67 collect_output:179.80

====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================

2023-02-21 18:22:42 - train.py[line:487] - INFO: 0.6864541410433904

====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================

2023-02-21 18:22:43 - train.py[line:575] - INFO: logits:torch.Size([420900, 21]) sample_ids:torch.Size([420900])

====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================

2023-02-21 18:22:43 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.236 | loss_v1 0 | loss_v2 0 | nll_loss 0.07 | ntokens 287.741 | nsentences 95.986 | sample_size 287.741 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.686454 | ppl 1.05 | vqa_score 0.4768 | wps 450.1 | wpb 287.7 | bsz 96 | num_updates 400 | best_R@100 0.686454
2023-02-21 18:22:43 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 400 updates
2023-02-21 18:22:43 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_400.pt

====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6592;     R @ 100: 0.6865;     R @ 500: 0.7070;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4700;    mR @ 100: 0.5034;    mR @ 500: 0.5642;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8407) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5680) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:1.0000) (playing:0.5714) (riding:0.9748) (says:0.0000) (sitting on:0.7259) (standing on:0.3957) (using:0.6964) (walking in:0.0769) (walking on:0.7121) (watching:0.4556) 
--------------------------------------------------------
====================================================================================================

2023-02-21 18:22:48 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_400.pt
2023-02-21 18:22:53 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_400.pt (epoch 1 @ 400 updates, score 0.6864541410433904) (writing took 10.414708444848657 seconds)
2023-02-21 18:23:05 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=444.9, nsentences=160, sample_size=444.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=1.6, ups=0, wpb=444.9, bsz=160, num_updates=410, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6185
2023-02-21 18:23:17 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 17753 loss=0.152, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=444.8, nsentences=160, sample_size=444.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.9, ups=0.86, wpb=444.8, bsz=160, num_updates=420, lr=0, gnorm=0.061, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=6197
2023-02-21 18:23:28 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 17753 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=450.2, nsentences=160, sample_size=450.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386, ups=0.86, wpb=450.2, bsz=160, num_updates=430, lr=0, gnorm=0.07, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6209
2023-02-21 18:23:40 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=447.3, nsentences=160, sample_size=447.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.3, ups=0.86, wpb=447.3, bsz=160, num_updates=440, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6220
2023-02-21 18:23:52 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 17753 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=446.9, nsentences=160, sample_size=446.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386.7, ups=0.87, wpb=446.9, bsz=160, num_updates=450, lr=0, gnorm=0.066, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6232
2023-02-21 18:24:03 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 17753 loss=0.161, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=445.3, nsentences=160, sample_size=445.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.6, ups=0.86, wpb=445.3, bsz=160, num_updates=460, lr=0, gnorm=0.082, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6244
2023-02-21 18:24:15 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=446.6, nsentences=160, sample_size=446.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.7, ups=0.86, wpb=446.6, bsz=160, num_updates=470, lr=0, gnorm=0.071, clip=0, loss_scale=128, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=6255
2023-02-21 18:24:27 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=448.5, nsentences=160, sample_size=448.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.1, ups=0.86, wpb=448.5, bsz=160, num_updates=480, lr=0, gnorm=0.082, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6267
2023-02-21 18:24:38 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=442, nsentences=160, sample_size=442, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=379.6, ups=0.86, wpb=442, bsz=160, num_updates=490, lr=0, gnorm=0.076, clip=0, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6279
2023-02-21 18:24:50 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=449.5, nsentences=160, sample_size=449.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=387.6, ups=0.86, wpb=449.5, bsz=160, num_updates=500, lr=0, gnorm=0.074, clip=0, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6290
2023-02-21 18:25:01 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=448.6, nsentences=160, sample_size=448.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386.8, ups=0.86, wpb=448.6, bsz=160, num_updates=510, lr=0, gnorm=0.073, clip=0, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=6302
2023-02-21 18:25:13 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 17753 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=447.5, nsentences=160, sample_size=447.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=383.8, ups=0.86, wpb=447.5, bsz=160, num_updates=520, lr=0, gnorm=0.067, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6314
2023-02-21 18:25:25 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=448.3, nsentences=160, sample_size=448.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=381.7, ups=0.85, wpb=448.3, bsz=160, num_updates=530, lr=0, gnorm=0.076, clip=0, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6325
2023-02-21 18:25:36 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=447.9, nsentences=160, sample_size=447.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=387.1, ups=0.86, wpb=447.9, bsz=160, num_updates=540, lr=0, gnorm=0.071, clip=0, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=6337
2023-02-21 18:25:48 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=447.6, nsentences=160, sample_size=447.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.7, ups=0.86, wpb=447.6, bsz=160, num_updates=550, lr=0, gnorm=0.065, clip=0, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6349
2023-02-21 18:26:00 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=448.1, nsentences=160, sample_size=448.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=380.7, ups=0.85, wpb=448.1, bsz=160, num_updates=560, lr=0, gnorm=0.072, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6360
2023-02-21 18:26:11 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 17753 loss=0.154, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=444.1, nsentences=160, sample_size=444.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.7, ups=0.86, wpb=444.1, bsz=160, num_updates=570, lr=0, gnorm=0.069, clip=0, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6372
2023-02-21 18:26:23 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 17753 loss=0.15, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=445.9, nsentences=160, sample_size=445.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=383.8, ups=0.86, wpb=445.9, bsz=160, num_updates=580, lr=0, gnorm=0.07, clip=0, loss_scale=256, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=6384
2023-02-21 18:26:35 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=448.2, nsentences=160, sample_size=448.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.8, ups=0.86, wpb=448.2, bsz=160, num_updates=590, lr=0, gnorm=0.076, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6395
2023-02-21 18:26:46 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 17753 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=445.9, nsentences=160, sample_size=445.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.5, ups=0.86, wpb=445.9, bsz=160, num_updates=600, lr=0, gnorm=0.067, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6407
2023-02-21 18:26:46 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 18:26:48 - train.py[line:549] - INFO: 0 / 4385
2023-02-21 18:26:48 - train.py[line:551] - INFO: load:1.50 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 18:28:58 - train.py[line:549] - INFO: 200 / 4385
2023-02-21 18:28:58 - train.py[line:551] - INFO: load:1.52 valid_run:130.33 task_valid:119.53 collect_output:9.75
2023-02-21 18:31:05 - train.py[line:549] - INFO: 400 / 4385
2023-02-21 18:31:05 - train.py[line:551] - INFO: load:1.54 valid_run:256.39 task_valid:235.86 collect_output:18.42
2023-02-21 18:33:10 - train.py[line:549] - INFO: 600 / 4385
2023-02-21 18:33:10 - train.py[line:551] - INFO: load:1.57 valid_run:382.23 task_valid:352.86 collect_output:26.21
2023-02-21 18:35:15 - train.py[line:549] - INFO: 800 / 4385
2023-02-21 18:35:15 - train.py[line:551] - INFO: load:1.59 valid_run:506.97 task_valid:467.07 collect_output:35.67
2023-02-21 18:37:21 - train.py[line:549] - INFO: 1000 / 4385
2023-02-21 18:37:21 - train.py[line:551] - INFO: load:1.61 valid_run:632.50 task_valid:584.60 collect_output:42.66
2023-02-21 18:39:28 - train.py[line:549] - INFO: 1200 / 4385
2023-02-21 18:39:28 - train.py[line:551] - INFO: load:1.64 valid_run:759.57 task_valid:703.45 collect_output:49.89
2023-02-21 18:41:34 - train.py[line:549] - INFO: 1400 / 4385
2023-02-21 18:41:34 - train.py[line:551] - INFO: load:1.66 valid_run:885.09 task_valid:821.86 collect_output:55.94
2023-02-21 18:43:39 - train.py[line:549] - INFO: 1600 / 4385
2023-02-21 18:43:39 - train.py[line:551] - INFO: load:1.68 valid_run:1010.79 task_valid:938.74 collect_output:63.71
2023-02-21 18:45:45 - train.py[line:549] - INFO: 1800 / 4385
2023-02-21 18:45:45 - train.py[line:551] - INFO: load:1.71 valid_run:1136.31 task_valid:1056.35 collect_output:70.57
2023-02-21 18:47:51 - train.py[line:549] - INFO: 2000 / 4385
2023-02-21 18:47:51 - train.py[line:551] - INFO: load:1.73 valid_run:1261.97 task_valid:1169.19 collect_output:82.39
2023-02-21 18:49:55 - train.py[line:549] - INFO: 2200 / 4385
2023-02-21 18:49:55 - train.py[line:551] - INFO: load:1.75 valid_run:1386.38 task_valid:1285.03 collect_output:89.95
2023-02-21 18:52:01 - train.py[line:549] - INFO: 2400 / 4385
2023-02-21 18:52:01 - train.py[line:551] - INFO: load:1.78 valid_run:1511.89 task_valid:1402.35 collect_output:97.07
2023-02-21 18:54:07 - train.py[line:549] - INFO: 2600 / 4385
2023-02-21 18:54:07 - train.py[line:551] - INFO: load:1.80 valid_run:1638.22 task_valid:1516.63 collect_output:108.05
2023-02-21 18:56:13 - train.py[line:549] - INFO: 2800 / 4385
2023-02-21 18:56:13 - train.py[line:551] - INFO: load:1.83 valid_run:1763.92 task_valid:1634.72 collect_output:114.59
2023-02-21 18:58:19 - train.py[line:549] - INFO: 3000 / 4385
2023-02-21 18:58:19 - train.py[line:551] - INFO: load:1.85 valid_run:1889.77 task_valid:1751.29 collect_output:122.80
2023-02-21 18:59:53 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.47 GiB (GPU 7; 39.59 GiB total capacity; 8.28 GiB already allocated; 5.03 GiB free; 29.34 GiB reserved in total by PyTorch)
2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8474 MB |   13714 MB |    1130 TB |    1130 TB |
|       from large pool |    8329 MB |   13569 MB |    1129 TB |    1129 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8474 MB |   13714 MB |    1130 TB |    1130 TB |
|       from large pool |    8329 MB |   13569 MB |    1129 TB |    1129 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   30040 MB |   30928 MB |  240182 MB |  210142 MB |
|       from large pool |   29894 MB |   30782 MB |  239872 MB |  209978 MB |
|       from small pool |     146 MB |     146 MB |     310 MB |     164 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   21565 MB |   25520 MB |    1214 TB |    1214 TB |
|       from large pool |   21564 MB |   25518 MB |    1214 TB |    1214 TB |
|       from small pool |       1 MB |       1 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |   61514 K  |   61510 K  |
|       from large pool |     563    |     575    |   20408 K  |   20407 K  |
|       from small pool |    3106    |    3116    |   41105 K  |   41102 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |   61514 K  |   61510 K  |
|       from large pool |     563    |     575    |   20408 K  |   20407 K  |
|       from small pool |    3106    |    3116    |   41105 K  |   41102 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     131    |     132    |     559    |     428    |
|       from large pool |      58    |      59    |     404    |     346    |
|       from small pool |      73    |      73    |     155    |      82    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      82    |      90    |   45039 K  |   45039 K  |
|       from large pool |      45    |      48    |    8406 K  |    8406 K  |
|       from small pool |      37    |      48    |   36632 K  |   36632 K  |
|===========================================================================|

2023-02-21 18:59:53 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-02-21 19:00:26 - train.py[line:549] - INFO: 3200 / 4385
2023-02-21 19:00:26 - train.py[line:551] - INFO: load:1.87 valid_run:2017.06 task_valid:1865.65 collect_output:134.68
2023-02-21 19:02:33 - train.py[line:549] - INFO: 3400 / 4385
2023-02-21 19:02:33 - train.py[line:551] - INFO: load:1.90 valid_run:2143.91 task_valid:1982.18 collect_output:143.92
2023-02-21 19:04:39 - train.py[line:549] - INFO: 3600 / 4385
2023-02-21 19:04:39 - train.py[line:551] - INFO: load:1.92 valid_run:2269.78 task_valid:2100.51 collect_output:150.42
2023-02-21 19:06:45 - train.py[line:549] - INFO: 3800 / 4385
2023-02-21 19:06:45 - train.py[line:551] - INFO: load:1.95 valid_run:2395.82 task_valid:2217.75 collect_output:158.21
2023-02-21 19:08:50 - train.py[line:549] - INFO: 4000 / 4385
2023-02-21 19:08:50 - train.py[line:551] - INFO: load:1.97 valid_run:2521.20 task_valid:2334.55 collect_output:165.78
2023-02-21 19:10:57 - train.py[line:549] - INFO: 4200 / 4385
2023-02-21 19:10:57 - train.py[line:551] - INFO: load:2.00 valid_run:2647.29 task_valid:2451.26 collect_output:174.15

====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================

2023-02-21 19:13:21 - train.py[line:487] - INFO: 0.6876674232911555

====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================

2023-02-21 19:13:21 - train.py[line:575] - INFO: logits:torch.Size([420900, 21]) sample_ids:torch.Size([420900])

====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================

2023-02-21 19:13:21 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.236 | loss_v1 0 | loss_v2 0 | nll_loss 0.07 | ntokens 287.741 | nsentences 95.986 | sample_size 287.741 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.687667 | ppl 1.05 | vqa_score 0.4801 | wps 452 | wpb 287.7 | bsz 96 | num_updates 600 | best_R@100 0.687667
2023-02-21 19:13:21 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 600 updates
2023-02-21 19:13:21 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_600.pt

====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6601;     R @ 100: 0.6877;     R @ 500: 0.7066;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4730;    mR @ 100: 0.5038;    mR @ 500: 0.5627;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8370) (covered in:0.5441) (covering:0.4256) (eating:0.8679) (flying in:0.5000) (growing on:0.3200) (hanging from:0.5722) (lying on:0.3429) (mounted on:0.0000) (painted on:0.0500) (parked on:0.9825) (playing:0.5714) (riding:0.9765) (says:0.0000) (sitting on:0.7237) (standing on:0.4025) (using:0.6964) (walking in:0.0769) (walking on:0.7235) (watching:0.4639) 
--------------------------------------------------------
====================================================================================================

2023-02-21 19:13:27 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_600.pt
2023-02-21 19:13:32 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_full_val_k10/1_B20_A1_E2_0.027_0e-5_480/checkpoint_1_600.pt (epoch 1 @ 600 updates, score 0.6876674232911555) (writing took 10.47231007926166 seconds)
2023-02-21 19:13:43 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=445.6, nsentences=160, sample_size=445.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=1.6, ups=0, wpb=445.6, bsz=160, num_updates=610, lr=0, gnorm=0.07, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9224
2023-02-21 19:13:55 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=446.6, nsentences=160, sample_size=446.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=383.4, ups=0.86, wpb=446.6, bsz=160, num_updates=620, lr=0, gnorm=0.089, clip=0, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=9235
2023-02-21 19:14:07 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 17753 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=446.6, nsentences=160, sample_size=446.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=380.1, ups=0.85, wpb=446.6, bsz=160, num_updates=630, lr=0, gnorm=0.071, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9247
2023-02-21 19:14:18 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 17753 loss=0.154, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=444, nsentences=160, sample_size=444, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=387.8, ups=0.87, wpb=444, bsz=160, num_updates=640, lr=0, gnorm=0.067, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=9259
2023-02-21 19:14:30 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=448.5, nsentences=160, sample_size=448.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=383.3, ups=0.85, wpb=448.5, bsz=160, num_updates=650, lr=0, gnorm=0.071, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9270
2023-02-21 19:14:41 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 17753 loss=0.154, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=446.5, nsentences=160, sample_size=446.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.1, ups=0.86, wpb=446.5, bsz=160, num_updates=660, lr=0, gnorm=0.062, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=9282
2023-02-21 19:14:53 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=446.4, nsentences=160, sample_size=446.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.2, ups=0.86, wpb=446.4, bsz=160, num_updates=670, lr=0, gnorm=0.09, clip=0, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=9294
2023-02-21 19:15:05 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 17753 loss=0.158, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=446.5, nsentences=160, sample_size=446.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.6, ups=0.86, wpb=446.5, bsz=160, num_updates=680, lr=0, gnorm=0.062, clip=0, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=9305
2023-02-21 19:15:16 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 17753 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=447.9, nsentences=160, sample_size=447.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=386.2, ups=0.86, wpb=447.9, bsz=160, num_updates=690, lr=0, gnorm=0.08, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9317
2023-02-21 19:15:28 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 17753 loss=0.153, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=446.5, nsentences=160, sample_size=446.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.8, ups=0.86, wpb=446.5, bsz=160, num_updates=700, lr=0, gnorm=0.062, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=9328
2023-02-21 19:15:40 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=445.9, nsentences=160, sample_size=445.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.4, ups=0.86, wpb=445.9, bsz=160, num_updates=710, lr=0, gnorm=0.066, clip=0, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=9340
2023-02-21 19:15:51 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 17753 loss=0.16, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=449, nsentences=160, sample_size=449, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=386.6, ups=0.86, wpb=449, bsz=160, num_updates=720, lr=0, gnorm=0.072, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9352
2023-02-21 19:16:03 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 17753 loss=0.154, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=445.8, nsentences=160, sample_size=445.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=383.5, ups=0.86, wpb=445.8, bsz=160, num_updates=730, lr=0, gnorm=0.064, clip=0, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=9363
2023-02-21 19:16:14 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 17753 loss=0.155, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=446.3, nsentences=160, sample_size=446.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.9, ups=0.86, wpb=446.3, bsz=160, num_updates=740, lr=0, gnorm=0.068, clip=0, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=9375
2023-02-21 19:16:26 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 17753 loss=0.157, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=444.8, nsentences=160, sample_size=444.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=378.7, ups=0.85, wpb=444.8, bsz=160, num_updates=750, lr=0, gnorm=0.079, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9387
2023-02-21 19:16:38 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 17753 loss=0.163, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=447.4, nsentences=160, sample_size=447.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=385.7, ups=0.86, wpb=447.4, bsz=160, num_updates=760, lr=0, gnorm=0.076, clip=0, loss_scale=256, train_wall=12, gb_free=11.3, ema_decay=0.9999, wall=9398
2023-02-21 19:16:49 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 17753 loss=0.156, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=443.2, nsentences=160, sample_size=443.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=384.2, ups=0.87, wpb=443.2, bsz=160, num_updates=770, lr=0, gnorm=0.067, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9410
2023-02-21 19:17:01 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 17753 loss=0.153, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=444.4, nsentences=160, sample_size=444.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=382.9, ups=0.86, wpb=444.4, bsz=160, num_updates=780, lr=0, gnorm=0.074, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9422
2023-02-21 19:17:13 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 17753 loss=0.159, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=448.4, nsentences=160, sample_size=448.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=387, ups=0.86, wpb=448.4, bsz=160, num_updates=790, lr=0, gnorm=0.074, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9433
2023-02-21 19:17:24 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 17753 loss=0.154, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=443.8, nsentences=160, sample_size=443.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=378.4, ups=0.85, wpb=443.8, bsz=160, num_updates=800, lr=0, gnorm=0.076, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9445
2023-02-21 19:17:24 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 19:17:26 - train.py[line:549] - INFO: 0 / 4385
2023-02-21 19:17:26 - train.py[line:551] - INFO: load:1.72 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 19:18:21 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.03 GiB (GPU 7; 39.59 GiB total capacity; 8.01 GiB already allocated; 1.73 GiB free; 32.64 GiB reserved in total by PyTorch)
2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 5                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 6                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 7                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8203 MB |   13256 MB |    1278 TB |    1278 TB |
|       from large pool |    8058 MB |   13111 MB |    1278 TB |    1278 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8203 MB |   13256 MB |    1278 TB |    1278 TB |
|       from large pool |    8058 MB |   13111 MB |    1278 TB |    1278 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   33428 MB |   33428 MB |  257184 MB |  223756 MB |
|       from large pool |   33282 MB |   33282 MB |  256850 MB |  223568 MB |
|       from small pool |     146 MB |     146 MB |     334 MB |     188 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25224 MB |   28915 MB |    1426 TB |    1426 TB |
|       from large pool |   25223 MB |   28913 MB |    1426 TB |    1426 TB |
|       from small pool |       1 MB |       1 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |   69036 K  |   69032 K  |
|       from large pool |     563    |     575    |   23030 K  |   23029 K  |
|       from small pool |    3106    |    3116    |   46006 K  |   46003 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |   69036 K  |   69032 K  |
|       from large pool |     563    |     575    |   23030 K  |   23029 K  |
|       from small pool |    3106    |    3116    |   46006 K  |   46003 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     125    |     125    |     574    |     449    |
|       from large pool |      52    |      52    |     407    |     355    |
|       from small pool |      73    |      73    |     167    |      94    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      72    |      87    |   50783 K  |   50783 K  |
|       from large pool |      38    |      43    |    9895 K  |    9895 K  |
|       from small pool |      34    |      50    |   40888 K  |   40888 K  |
|===========================================================================|

2023-02-21 19:18:21 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-02-21 19:19:35 - train.py[line:549] - INFO: 200 / 4385
2023-02-21 19:19:35 - train.py[line:551] - INFO: load:1.75 valid_run:128.75 task_valid:119.42 collect_output:8.31
2023-02-21 19:21:41 - train.py[line:549] - INFO: 400 / 4385
2023-02-21 19:21:41 - train.py[line:551] - INFO: load:1.77 valid_run:254.47 task_valid:235.69 collect_output:16.72
2023-02-21 19:23:46 - train.py[line:549] - INFO: 600 / 4385
2023-02-21 19:23:46 - train.py[line:551] - INFO: load:1.79 valid_run:379.53 task_valid:352.70 collect_output:23.75
2023-02-21 19:25:51 - train.py[line:549] - INFO: 800 / 4385
2023-02-21 19:25:51 - train.py[line:551] - INFO: load:1.82 valid_run:504.35 task_valid:467.12 collect_output:33.10
2023-02-21 19:27:57 - train.py[line:549] - INFO: 1000 / 4385
2023-02-21 19:27:57 - train.py[line:551] - INFO: load:1.84 valid_run:629.99 task_valid:584.97 collect_output:39.87
2023-02-21 19:30:03 - train.py[line:549] - INFO: 1200 / 4385
2023-02-21 19:30:03 - train.py[line:551] - INFO: load:1.86 valid_run:756.75 task_valid:704.23 collect_output:46.33
2023-02-21 19:32:09 - train.py[line:549] - INFO: 1400 / 4385
2023-02-21 19:32:09 - train.py[line:551] - INFO: load:1.89 valid_run:882.23 task_valid:822.53 collect_output:52.48
2023-02-21 19:34:15 - train.py[line:549] - INFO: 1600 / 4385
2023-02-21 19:34:15 - train.py[line:551] - INFO: load:1.91 valid_run:1008.02 task_valid:939.42 collect_output:60.33
2023-02-21 19:36:21 - train.py[line:549] - INFO: 1800 / 4385
2023-02-21 19:36:21 - train.py[line:551] - INFO: load:1.93 valid_run:1133.80 task_valid:1057.14 collect_output:67.35
2023-02-21 19:38:26 - train.py[line:549] - INFO: 2000 / 4385
2023-02-21 19:38:26 - train.py[line:551] - INFO: load:1.96 valid_run:1259.50 task_valid:1170.24 collect_output:78.87
2023-02-21 19:40:31 - train.py[line:549] - INFO: 2200 / 4385
2023-02-21 19:40:31 - train.py[line:551] - INFO: load:1.98 valid_run:1384.13 task_valid:1286.47 collect_output:86.23
2023-02-21 19:42:37 - train.py[line:549] - INFO: 2400 / 4385
2023-02-21 19:42:37 - train.py[line:551] - INFO: load:2.01 valid_run:1510.03 task_valid:1403.77 collect_output:93.79
2023-02-21 19:44:44 - train.py[line:549] - INFO: 2600 / 4385
2023-02-21 19:44:44 - train.py[line:551] - INFO: load:2.03 valid_run:1636.52 task_valid:1518.14 collect_output:104.86
2023-02-21 19:46:50 - train.py[line:549] - INFO: 2800 / 4385
2023-02-21 19:46:50 - train.py[line:551] - INFO: load:2.05 valid_run:1762.38 task_valid:1636.24 collect_output:111.57
2023-02-21 19:48:56 - train.py[line:549] - INFO: 3000 / 4385
2023-02-21 19:48:56 - train.py[line:551] - INFO: load:2.08 valid_run:1888.48 task_valid:1752.83 collect_output:120.03
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3892815
Killing subprocess 3892816
Killing subprocess 3892817
Killing subprocess 3892818
Killing subprocess 3892819
Killing subprocess 3892820
Killing subprocess 3892821
Killing subprocess 3892822
Main process received SIGINT, exiting
