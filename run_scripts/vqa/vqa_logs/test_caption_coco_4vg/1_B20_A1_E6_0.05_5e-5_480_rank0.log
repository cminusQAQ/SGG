*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 60308
Killing subprocess 60309
Main process received SIGINT, exiting
2023-02-22 11:29:34 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-22 11:29:34 - utils.py[line:261] - INFO: Start init
2023-02-22 11:29:34 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-22 11:29:34 - utils.py[line:261] - INFO: Start init
2023-02-22 11:29:35 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-22 11:29:35 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-22 11:29:35 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-22 11:29:35 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-22 11:29:39 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_caption_coco_4vg', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 6, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=6, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_caption_coco_4vg', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-22 11:29:39 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-22 11:29:39 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-22 11:29:43 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-22 11:29:43 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-22 11:29:43 - train.py[line:119] - INFO: model: OFAModel
2023-02-22 11:29:43 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-22 11:29:43 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-22 11:29:43 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-22 11:29:43 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-22 11:29:44 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-22 11:29:44 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:29:44 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:29:44 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-22 11:29:45 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-22 11:29:45 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-22 11:29:45 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-02-22 11:29:54 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-22 11:29:54 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:29:55 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:29:55 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-22 11:29:55 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-22 11:29:55 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-22 11:29:55 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv slice_id 1 row count 68217 total row count 136434
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv slice_id 0 row count 68217 total row count 136434
2023-02-22 11:29:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 11:29:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
Total steps 20466, warmup steps 1023, warmup_factor 0.0009775171065493646
Total steps 20466, warmup steps 1023, warmup_factor 0.0009775171065493646
2023-02-22 11:29:57 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-22 11:29:57 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 11:30:19 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 3411 loss=1.146, loss_v1=0, loss_v2=0, nll_loss=0.993, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.99, wps=88.2, ups=0.79, wpb=110.9, bsz=40, num_updates=10, lr=4.88759e-07, gnorm=14.692, clip=100, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35
2023-02-22 11:30:30 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 3411 loss=1.127, loss_v1=0, loss_v2=0, nll_loss=0.976, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.97, wps=96.6, ups=0.86, wpb=111.8, bsz=40, num_updates=20, lr=9.77517e-07, gnorm=13.747, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=46
2023-02-22 11:30:42 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 3411 loss=0.963, loss_v1=0, loss_v2=0, nll_loss=0.817, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=96.8, ups=0.86, wpb=112.2, bsz=40, num_updates=30, lr=1.46628e-06, gnorm=10.893, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=58
@@@@ ERROR IN DATA @@@@ lie on
2023-02-22 11:30:53 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 3411 loss=0.865, loss_v1=0, loss_v2=0, nll_loss=0.73, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=98.2, ups=0.88, wpb=111.2, bsz=40, num_updates=40, lr=1.95503e-06, gnorm=8.527, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69
2023-02-22 11:31:04 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 3411 loss=0.749, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=50, lr=2.44379e-06, gnorm=6.784, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=80
2023-02-22 11:31:16 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 3411 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.547, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=98.7, ups=0.88, wpb=112.8, bsz=40, num_updates=60, lr=2.93255e-06, gnorm=5.444, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=92
2023-02-22 11:31:27 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 3411 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=70, lr=3.42131e-06, gnorm=4.389, clip=100, loss_scale=128, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=103
2023-02-22 11:31:38 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 3411 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.1, ups=0.89, wpb=111.8, bsz=40, num_updates=80, lr=3.91007e-06, gnorm=3.844, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=114
2023-02-22 11:31:49 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 3411 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=90, lr=4.39883e-06, gnorm=3.866, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=126
@@@@ ERROR IN DATA @@@@ lie on
2023-02-22 11:32:01 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 3411 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.1, ups=0.87, wpb=111.5, bsz=40, num_updates=100, lr=4.88759e-06, gnorm=3.349, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=137
2023-02-22 11:32:12 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 3411 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=110, lr=5.37634e-06, gnorm=3.171, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=148
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 60733
Killing subprocess 60735
Main process received SIGINT, exiting
2023-02-22 11:32:59 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-22 11:32:59 - utils.py[line:261] - INFO: Start init
2023-02-22 11:32:59 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-22 11:32:59 - utils.py[line:261] - INFO: Start init
2023-02-22 11:33:00 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-22 11:33:00 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-22 11:33:00 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-22 11:33:00 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-22 11:33:05 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_caption_coco_4vg', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 6, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=6, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_caption_coco_4vg', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-22 11:33:05 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-22 11:33:05 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-22 11:33:09 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-22 11:33:09 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-22 11:33:09 - train.py[line:119] - INFO: model: OFAModel
2023-02-22 11:33:09 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-22 11:33:09 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-22 11:33:09 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-22 11:33:09 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-22 11:33:10 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-22 11:33:10 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:33:10 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:33:10 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-22 11:33:11 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-22 11:33:11 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-22 11:33:11 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-02-22 11:33:19 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-22 11:33:19 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:33:20 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:33:20 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-22 11:33:20 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-22 11:33:20 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-22 11:33:20 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv slice_id 0 row count 68217 total row count 136434
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E0.tsv slice_id 1 row count 68217 total row count 136434
2023-02-22 11:33:21 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 11:33:21 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
Total steps 20466, warmup steps 1023, warmup_factor 0.0009775171065493646
2023-02-22 11:33:22 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-22 11:33:22 - train.py[line:312] - INFO: Start iterating over samples
Total steps 20466, warmup steps 1023, warmup_factor 0.0009775171065493646
2023-02-22 11:33:39 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 3411 loss=1.146, loss_v1=0, loss_v2=0, nll_loss=0.993, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.99, wps=82.7, ups=0.74, wpb=110.9, bsz=40, num_updates=10, lr=4.88759e-07, gnorm=14.703, clip=100, loss_scale=128, train_wall=15, gb_free=10.4, ema_decay=0.9999, wall=30
2023-02-22 11:33:51 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 3411 loss=1.127, loss_v1=0, loss_v2=0, nll_loss=0.976, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.97, wps=95.9, ups=0.86, wpb=111.8, bsz=40, num_updates=20, lr=9.77517e-07, gnorm=13.72, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41
2023-02-22 11:34:03 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 3411 loss=0.963, loss_v1=0, loss_v2=0, nll_loss=0.817, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=98, ups=0.87, wpb=112.2, bsz=40, num_updates=30, lr=1.46628e-06, gnorm=10.924, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53
2023-02-22 11:34:14 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 3411 loss=0.865, loss_v1=0, loss_v2=0, nll_loss=0.73, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=97, ups=0.87, wpb=111.2, bsz=40, num_updates=40, lr=1.95503e-06, gnorm=8.538, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64
2023-02-22 11:34:25 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 3411 loss=0.749, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=102.8, ups=0.91, wpb=113.3, bsz=40, num_updates=50, lr=2.44379e-06, gnorm=6.821, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=75
2023-02-22 11:34:37 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 3411 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.547, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=98.9, ups=0.88, wpb=112.8, bsz=40, num_updates=60, lr=2.93255e-06, gnorm=5.409, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=87
2023-02-22 11:34:48 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 3411 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.515, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.8, ups=0.9, wpb=111, bsz=40, num_updates=70, lr=3.42131e-06, gnorm=4.399, clip=100, loss_scale=128, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=98
2023-02-22 11:34:59 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 3411 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=98.9, ups=0.89, wpb=111.8, bsz=40, num_updates=80, lr=3.91007e-06, gnorm=3.826, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=109
2023-02-22 11:35:10 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 3411 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=99, ups=0.89, wpb=111.6, bsz=40, num_updates=90, lr=4.39883e-06, gnorm=3.822, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=120
2023-02-22 11:35:22 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 3411 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.6, ups=0.87, wpb=111.5, bsz=40, num_updates=100, lr=4.88759e-06, gnorm=3.299, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=132
2023-02-22 11:35:33 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 3411 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=110, lr=5.37634e-06, gnorm=3.171, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=143
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 11:35:44 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 3411 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.89, wpb=113, bsz=40, num_updates=120, lr=5.8651e-06, gnorm=3.3, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=154
2023-02-22 11:35:55 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 3411 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.8, ups=0.91, wpb=111.9, bsz=40, num_updates=130, lr=6.35386e-06, gnorm=3.086, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=165
2023-02-22 11:36:06 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 3411 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.1, ups=0.91, wpb=109.3, bsz=40, num_updates=140, lr=6.84262e-06, gnorm=3.36, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=177
2023-02-22 11:36:17 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 3411 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=103.4, ups=0.92, wpb=112.2, bsz=40, num_updates=150, lr=7.33138e-06, gnorm=3.395, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=187
2023-02-22 11:36:28 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 3411 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=104.1, ups=0.93, wpb=112.3, bsz=40, num_updates=160, lr=7.82014e-06, gnorm=3.5, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=198
2023-02-22 11:36:39 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 3411 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.2, ups=0.89, wpb=111.7, bsz=40, num_updates=170, lr=8.3089e-06, gnorm=3.081, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=209
2023-02-22 11:36:51 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 3411 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.87, wpb=112.4, bsz=40, num_updates=180, lr=8.79765e-06, gnorm=2.827, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=221
2023-02-22 11:37:02 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 3411 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.88, wpb=111.5, bsz=40, num_updates=190, lr=9.28641e-06, gnorm=2.764, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=232
2023-02-22 11:37:13 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 3411 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.9, wpb=111.8, bsz=40, num_updates=200, lr=9.77517e-06, gnorm=2.889, clip=100, loss_scale=128, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=243
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 11:37:25 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 3411 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.8, ups=0.89, wpb=111.2, bsz=40, num_updates=210, lr=1.02639e-05, gnorm=2.433, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=255
2023-02-22 11:37:36 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 3411 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.5, ups=0.92, wpb=110.8, bsz=40, num_updates=220, lr=1.07527e-05, gnorm=3.091, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=266
2023-02-22 11:37:47 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 3411 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.9, ups=0.9, wpb=112.9, bsz=40, num_updates=230, lr=1.12414e-05, gnorm=2.753, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=277
2023-02-22 11:37:58 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 3411 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98, ups=0.88, wpb=111.7, bsz=40, num_updates=240, lr=1.17302e-05, gnorm=2.342, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=288
2023-02-22 11:38:09 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 3411 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=250, lr=1.2219e-05, gnorm=2.731, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=299
2023-02-22 11:38:21 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 3411 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100, ups=0.89, wpb=112.2, bsz=40, num_updates=260, lr=1.27077e-05, gnorm=2.488, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=311
2023-02-22 11:38:32 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 3411 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.5, ups=0.88, wpb=111.9, bsz=40, num_updates=270, lr=1.31965e-05, gnorm=2.663, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=322
2023-02-22 11:38:43 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 3411 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=280, lr=1.36852e-05, gnorm=2.527, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=333
2023-02-22 11:38:54 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 3411 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.9, wpb=110.6, bsz=40, num_updates=290, lr=1.4174e-05, gnorm=2.178, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=344
2023-02-22 11:39:06 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 3411 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.5, ups=0.88, wpb=112.6, bsz=40, num_updates=300, lr=1.46628e-05, gnorm=1.856, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=356
2023-02-22 11:39:17 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 3411 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.8, ups=0.9, wpb=112, bsz=40, num_updates=310, lr=1.51515e-05, gnorm=2.145, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=367
2023-02-22 11:39:28 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 3411 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=104, ups=0.91, wpb=114.3, bsz=40, num_updates=320, lr=1.56403e-05, gnorm=2.325, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=378
2023-02-22 11:39:38 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 3411 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=104.6, ups=0.93, wpb=113, bsz=40, num_updates=330, lr=1.6129e-05, gnorm=2.229, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=389
2023-02-22 11:39:50 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 3411 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.9, ups=0.88, wpb=111.9, bsz=40, num_updates=340, lr=1.66178e-05, gnorm=2.207, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=400
2023-02-22 11:40:01 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 3411 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=350, lr=1.71065e-05, gnorm=2.226, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=411
2023-02-22 11:40:12 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 3411 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=360, lr=1.75953e-05, gnorm=2.274, clip=100, loss_scale=128, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=423
2023-02-22 11:40:23 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 3411 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.8, ups=0.91, wpb=111.7, bsz=40, num_updates=370, lr=1.80841e-05, gnorm=2.273, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=434
2023-02-22 11:40:34 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 3411 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=380, lr=1.85728e-05, gnorm=2.134, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=445
2023-02-22 11:40:46 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 3411 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.5, ups=0.9, wpb=111.5, bsz=40, num_updates=390, lr=1.90616e-05, gnorm=1.918, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=456
2023-02-22 11:40:57 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 3411 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.7, ups=0.89, wpb=111.2, bsz=40, num_updates=400, lr=1.95503e-05, gnorm=1.914, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=467
2023-02-22 11:41:08 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 3411 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=410, lr=2.00391e-05, gnorm=1.872, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=478
2023-02-22 11:41:19 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 3411 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=104.1, ups=0.94, wpb=110.8, bsz=40, num_updates=420, lr=2.05279e-05, gnorm=1.915, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=489
2023-02-22 11:41:29 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 3411 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=104.2, ups=0.93, wpb=112.4, bsz=40, num_updates=430, lr=2.10166e-05, gnorm=1.649, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=499
2023-02-22 11:41:41 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 3411 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.9, ups=0.89, wpb=112.4, bsz=40, num_updates=440, lr=2.15054e-05, gnorm=2.052, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=511
2023-02-22 11:41:52 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 3411 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=450, lr=2.19941e-05, gnorm=1.917, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=522
2023-02-22 11:42:03 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 3411 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.6, ups=0.91, wpb=111.1, bsz=40, num_updates=460, lr=2.24829e-05, gnorm=1.941, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=533
2023-02-22 11:42:14 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 3411 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.3, ups=0.87, wpb=112.6, bsz=40, num_updates=470, lr=2.29717e-05, gnorm=1.806, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=544
2023-02-22 11:42:26 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 3411 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=480, lr=2.34604e-05, gnorm=1.76, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=556
2023-02-22 11:42:37 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 3411 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101, ups=0.9, wpb=112.1, bsz=40, num_updates=490, lr=2.39492e-05, gnorm=1.756, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=567
2023-02-22 11:42:47 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 3411 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103.1, ups=0.93, wpb=111.2, bsz=40, num_updates=500, lr=2.44379e-05, gnorm=1.97, clip=90, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=578
2023-02-22 11:42:58 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 3411 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.6, ups=0.91, wpb=112.5, bsz=40, num_updates=510, lr=2.49267e-05, gnorm=1.792, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=589
2023-02-22 11:43:10 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 3411 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.6, ups=0.9, wpb=112.3, bsz=40, num_updates=520, lr=2.54154e-05, gnorm=1.852, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=600
2023-02-22 11:43:20 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 3411 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.7, ups=0.92, wpb=113.2, bsz=40, num_updates=530, lr=2.59042e-05, gnorm=1.998, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=611
2023-02-22 11:43:31 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 3411 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.6, ups=0.91, wpb=112.7, bsz=40, num_updates=540, lr=2.6393e-05, gnorm=1.794, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=622
2023-02-22 11:43:43 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 3411 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.8, ups=0.89, wpb=112.7, bsz=40, num_updates=550, lr=2.68817e-05, gnorm=1.695, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=633
2023-02-22 11:43:54 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 3411 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=104.5, ups=0.93, wpb=112.9, bsz=40, num_updates=560, lr=2.73705e-05, gnorm=1.724, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=644
2023-02-22 11:44:04 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 3411 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=103.5, ups=0.92, wpb=112.1, bsz=40, num_updates=570, lr=2.78592e-05, gnorm=1.884, clip=90, loss_scale=256, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=655
2023-02-22 11:44:16 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 3411 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=580, lr=2.8348e-05, gnorm=1.769, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=666
2023-02-22 11:44:26 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 3411 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=105.6, ups=0.94, wpb=112.3, bsz=40, num_updates=590, lr=2.88368e-05, gnorm=1.76, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=676
2023-02-22 11:44:37 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 3411 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.3, ups=0.89, wpb=112.7, bsz=40, num_updates=600, lr=2.93255e-05, gnorm=1.518, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=688
2023-02-22 11:44:49 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 3411 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.3, ups=0.89, wpb=111.8, bsz=40, num_updates=610, lr=2.98143e-05, gnorm=1.729, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=699
2023-02-22 11:45:00 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 3411 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.1, ups=0.9, wpb=112, bsz=40, num_updates=620, lr=3.0303e-05, gnorm=1.623, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=710
2023-02-22 11:45:11 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 3411 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99, ups=0.88, wpb=112.4, bsz=40, num_updates=630, lr=3.07918e-05, gnorm=1.509, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=721
2023-02-22 11:45:22 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 3411 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=640, lr=3.12805e-05, gnorm=1.783, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=732
2023-02-22 11:45:33 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 3411 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.5, ups=0.91, wpb=113.3, bsz=40, num_updates=650, lr=3.17693e-05, gnorm=1.549, clip=90, loss_scale=256, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=743
2023-02-22 11:45:44 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 3411 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.9, ups=0.9, wpb=112, bsz=40, num_updates=660, lr=3.22581e-05, gnorm=1.667, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=755
2023-02-22 11:45:56 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 3411 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.5, ups=0.88, wpb=111.5, bsz=40, num_updates=670, lr=3.27468e-05, gnorm=1.437, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=766
2023-02-22 11:46:07 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 3411 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.8, ups=0.9, wpb=112.6, bsz=40, num_updates=680, lr=3.32356e-05, gnorm=1.61, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=777
2023-02-22 11:46:18 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 3411 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=690, lr=3.37243e-05, gnorm=1.434, clip=80, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=788
2023-02-22 11:46:29 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 3411 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.8, ups=0.89, wpb=113.4, bsz=40, num_updates=700, lr=3.42131e-05, gnorm=1.791, clip=100, loss_scale=256, train_wall=11, gb_free=10, ema_decay=0.9999, wall=799
2023-02-22 11:46:40 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 3411 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.2, ups=0.92, wpb=112.8, bsz=40, num_updates=710, lr=3.47019e-05, gnorm=1.572, clip=100, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=810
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 11:46:51 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 3411 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.3, ups=0.91, wpb=111.9, bsz=40, num_updates=720, lr=3.51906e-05, gnorm=1.504, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=821
2023-02-22 11:47:02 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 3411 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.3, ups=0.9, wpb=112.2, bsz=40, num_updates=730, lr=3.56794e-05, gnorm=1.693, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=832
2023-02-22 11:47:13 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 3411 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=105.5, ups=0.94, wpb=112.1, bsz=40, num_updates=740, lr=3.61681e-05, gnorm=1.841, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=843
2023-02-22 11:47:24 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 3411 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=750, lr=3.66569e-05, gnorm=1.827, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=854
2023-02-22 11:47:35 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 3411 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.9, ups=0.9, wpb=111.8, bsz=40, num_updates=760, lr=3.71457e-05, gnorm=1.864, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=865
2023-02-22 11:47:46 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 3411 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=770, lr=3.76344e-05, gnorm=1.54, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=877
2023-02-22 11:47:58 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 3411 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.8, ups=0.89, wpb=111.1, bsz=40, num_updates=780, lr=3.81232e-05, gnorm=1.67, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=888
2023-02-22 11:48:09 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 3411 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.1, ups=0.89, wpb=111.3, bsz=40, num_updates=790, lr=3.86119e-05, gnorm=1.804, clip=90, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=899
2023-02-22 11:48:20 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 3411 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.2, ups=0.93, wpb=111.5, bsz=40, num_updates=800, lr=3.91007e-05, gnorm=1.647, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=910
2023-02-22 11:48:31 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 3411 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.9, ups=0.91, wpb=112.5, bsz=40, num_updates=810, lr=3.95894e-05, gnorm=1.441, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=921
2023-02-22 11:48:42 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 3411 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.9, ups=0.91, wpb=111.8, bsz=40, num_updates=820, lr=4.00782e-05, gnorm=1.67, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=932
2023-02-22 11:48:52 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 3411 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.8, ups=0.93, wpb=112, bsz=40, num_updates=830, lr=4.0567e-05, gnorm=1.664, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=943
2023-02-22 11:49:04 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 3411 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=95.7, ups=0.87, wpb=110.3, bsz=40, num_updates=840, lr=4.10557e-05, gnorm=1.618, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=954
2023-02-22 11:49:15 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 3411 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.3, ups=0.88, wpb=110.8, bsz=40, num_updates=850, lr=4.15445e-05, gnorm=1.279, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=966
2023-02-22 11:49:27 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 3411 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=860, lr=4.20332e-05, gnorm=1.372, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=977
2023-02-22 11:49:38 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 3411 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=870, lr=4.2522e-05, gnorm=1.671, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=988
2023-02-22 11:49:49 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 3411 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=880, lr=4.30108e-05, gnorm=1.408, clip=80, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=999
2023-02-22 11:50:00 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 3411 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=890, lr=4.34995e-05, gnorm=1.656, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1010
2023-02-22 11:50:12 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.3, ups=0.88, wpb=112, bsz=40, num_updates=900, lr=4.39883e-05, gnorm=1.377, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1022
2023-02-22 11:50:23 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.89, wpb=111.6, bsz=40, num_updates=910, lr=4.4477e-05, gnorm=1.558, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1033
2023-02-22 11:50:34 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 3411 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=920, lr=4.49658e-05, gnorm=1.554, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1044
2023-02-22 11:50:45 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 3411 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.1, ups=0.88, wpb=112.1, bsz=40, num_updates=930, lr=4.54545e-05, gnorm=1.707, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1056
2023-02-22 11:50:57 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 3411 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.8, ups=0.88, wpb=111.1, bsz=40, num_updates=940, lr=4.59433e-05, gnorm=1.705, clip=90, loss_scale=256, train_wall=11, gb_free=10, ema_decay=0.9999, wall=1067
@@@@ ERROR IN DATA @@@@ play
2023-02-22 11:51:08 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 3411 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.7, ups=0.9, wpb=112.8, bsz=40, num_updates=950, lr=4.64321e-05, gnorm=1.388, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1078
2023-02-22 11:51:19 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 3411 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.3, ups=0.88, wpb=111.6, bsz=40, num_updates=960, lr=4.69208e-05, gnorm=1.553, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1089
2023-02-22 11:51:31 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 3411 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.2, ups=0.88, wpb=110.9, bsz=40, num_updates=970, lr=4.74096e-05, gnorm=1.914, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1101
2023-02-22 11:51:42 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 3411 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=980, lr=4.78983e-05, gnorm=1.455, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1112
2023-02-22 11:51:53 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 3411 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.2, ups=0.89, wpb=112.4, bsz=40, num_updates=990, lr=4.83871e-05, gnorm=1.358, clip=70, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=1123
2023-02-22 11:52:04 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 3411 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.5, ups=0.88, wpb=111.9, bsz=40, num_updates=1000, lr=4.88759e-05, gnorm=1.495, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1135
2023-02-22 11:52:15 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 3411 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.7, ups=0.91, wpb=112.4, bsz=40, num_updates=1010, lr=4.93646e-05, gnorm=1.511, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1146
2023-02-22 11:52:27 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 3411 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.89, wpb=112.2, bsz=40, num_updates=1020, lr=4.98534e-05, gnorm=1.292, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1157
2023-02-22 11:52:38 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 3411 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=1030, lr=4.9982e-05, gnorm=1.55, clip=80, loss_scale=512, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=1168
2023-02-22 11:52:49 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 3411 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=1040, lr=4.99563e-05, gnorm=1.482, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1179
2023-02-22 11:53:00 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101, ups=0.9, wpb=112, bsz=40, num_updates=1050, lr=4.99306e-05, gnorm=1.377, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1190
2023-02-22 11:53:11 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 3411 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102, ups=0.92, wpb=111.4, bsz=40, num_updates=1060, lr=4.99049e-05, gnorm=1.351, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1201
2023-02-22 11:53:23 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 3411 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.9, ups=0.87, wpb=112.9, bsz=40, num_updates=1070, lr=4.98791e-05, gnorm=2.293, clip=100, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1213
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 11:53:34 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 3411 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.1, ups=0.89, wpb=110.4, bsz=40, num_updates=1080, lr=4.98534e-05, gnorm=1.418, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1224
2023-02-22 11:53:45 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 3411 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.7, ups=0.88, wpb=111.6, bsz=40, num_updates=1090, lr=4.98277e-05, gnorm=1.524, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1235
2023-02-22 11:53:57 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 3411 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99, ups=0.88, wpb=112.7, bsz=40, num_updates=1100, lr=4.9802e-05, gnorm=1.162, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1247
2023-02-22 11:54:08 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 3411 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.9, ups=0.9, wpb=113.8, bsz=40, num_updates=1110, lr=4.97763e-05, gnorm=1.617, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1258
2023-02-22 11:54:19 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 3411 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.6, ups=0.9, wpb=112.2, bsz=40, num_updates=1120, lr=4.97506e-05, gnorm=1.059, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1269
2023-02-22 11:54:30 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 3411 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=1130, lr=4.97248e-05, gnorm=1.382, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1280
2023-02-22 11:54:41 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 3411 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=1140, lr=4.96991e-05, gnorm=1.498, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1291
2023-02-22 11:54:52 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 3411 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.9, wpb=111, bsz=40, num_updates=1150, lr=4.96734e-05, gnorm=1.168, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1302
2023-02-22 11:55:03 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.9, wpb=111.8, bsz=40, num_updates=1160, lr=4.96477e-05, gnorm=1.366, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1314
2023-02-22 11:55:14 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 3411 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=103.2, ups=0.93, wpb=111.3, bsz=40, num_updates=1170, lr=4.9622e-05, gnorm=1.9, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1324
2023-02-22 11:55:25 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 3411 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102, ups=0.91, wpb=112.1, bsz=40, num_updates=1180, lr=4.95963e-05, gnorm=1.632, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1335
2023-02-22 11:55:37 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 3411 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.8, ups=0.88, wpb=111.8, bsz=40, num_updates=1190, lr=4.95705e-05, gnorm=1.739, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1347
2023-02-22 11:55:48 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 3411 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.4, ups=0.87, wpb=111.4, bsz=40, num_updates=1200, lr=4.95448e-05, gnorm=1.422, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1358
2023-02-22 11:55:59 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 3411 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.9, ups=0.89, wpb=112.1, bsz=40, num_updates=1210, lr=4.95191e-05, gnorm=1.705, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1370
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 11:56:10 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 3411 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=1220, lr=4.94934e-05, gnorm=1.599, clip=90, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1381
2023-02-22 11:56:22 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 3411 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=1230, lr=4.94677e-05, gnorm=1.59, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1392
2023-02-22 11:56:33 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 3411 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99, ups=0.89, wpb=111.2, bsz=40, num_updates=1240, lr=4.9442e-05, gnorm=1.592, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1403
2023-02-22 11:56:44 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 3411 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.4, ups=0.87, wpb=112.2, bsz=40, num_updates=1250, lr=4.94162e-05, gnorm=1.352, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1415
2023-02-22 11:56:56 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 3411 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99, ups=0.89, wpb=111.2, bsz=40, num_updates=1260, lr=4.93905e-05, gnorm=1.494, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1426
2023-02-22 11:57:07 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 3411 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100, ups=0.89, wpb=112.3, bsz=40, num_updates=1270, lr=4.93648e-05, gnorm=1.533, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1437
2023-02-22 11:57:18 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 3411 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.4, ups=0.9, wpb=113.3, bsz=40, num_updates=1280, lr=4.93391e-05, gnorm=1.366, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1448
2023-02-22 11:57:29 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 3411 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102.1, ups=0.9, wpb=113.1, bsz=40, num_updates=1290, lr=4.93134e-05, gnorm=1.191, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1459
2023-02-22 11:57:40 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 3411 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.9, ups=0.9, wpb=113.2, bsz=40, num_updates=1300, lr=4.92877e-05, gnorm=1.153, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1470
2023-02-22 11:57:52 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 3411 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.7, ups=0.87, wpb=110.8, bsz=40, num_updates=1310, lr=4.92619e-05, gnorm=1.397, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1482
2023-02-22 11:58:03 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 3411 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.9, ups=0.9, wpb=111.9, bsz=40, num_updates=1320, lr=4.92362e-05, gnorm=1.33, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1493
2023-02-22 11:58:14 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 3411 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.5, ups=0.88, wpb=112, bsz=40, num_updates=1330, lr=4.92105e-05, gnorm=1.476, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1504
2023-02-22 11:58:25 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 3411 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.7, ups=0.9, wpb=111, bsz=40, num_updates=1340, lr=4.91848e-05, gnorm=1.321, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1515
2023-02-22 11:58:37 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 3411 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.2, ups=0.88, wpb=111.9, bsz=40, num_updates=1350, lr=4.91591e-05, gnorm=1.547, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1527
2023-02-22 11:58:48 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 3411 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.6, ups=0.87, wpb=111.6, bsz=40, num_updates=1360, lr=4.91334e-05, gnorm=1.217, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1538
2023-02-22 11:58:59 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 3411 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.3, ups=0.9, wpb=111.7, bsz=40, num_updates=1370, lr=4.91076e-05, gnorm=1.418, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1550
2023-02-22 11:59:10 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 3411 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.4, ups=0.91, wpb=112.1, bsz=40, num_updates=1380, lr=4.90819e-05, gnorm=1.128, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1561
2023-02-22 11:59:22 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 3411 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=1390, lr=4.90562e-05, gnorm=1.16, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1572
2023-02-22 11:59:33 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 3411 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.9, ups=0.87, wpb=111.5, bsz=40, num_updates=1400, lr=4.90305e-05, gnorm=1.423, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1583
2023-02-22 11:59:44 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 3411 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.89, wpb=111, bsz=40, num_updates=1410, lr=4.90048e-05, gnorm=1.167, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1595
2023-02-22 11:59:56 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 3411 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=1420, lr=4.89791e-05, gnorm=1.914, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1606
2023-02-22 12:00:07 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 3411 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=1430, lr=4.89534e-05, gnorm=1.302, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1617
2023-02-22 12:00:18 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 3411 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.9, ups=0.92, wpb=111.3, bsz=40, num_updates=1440, lr=4.89276e-05, gnorm=1.283, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1628
2023-02-22 12:00:29 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 3411 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.8, ups=0.9, wpb=112.5, bsz=40, num_updates=1450, lr=4.89019e-05, gnorm=1.222, clip=80, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=1639
2023-02-22 12:00:40 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 3411 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.9, ups=0.88, wpb=110.6, bsz=40, num_updates=1460, lr=4.88762e-05, gnorm=1.523, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1650
2023-02-22 12:00:51 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 3411 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.6, ups=0.9, wpb=111.5, bsz=40, num_updates=1470, lr=4.88505e-05, gnorm=1.106, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1661
2023-02-22 12:01:02 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 3411 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.4, ups=0.9, wpb=111.3, bsz=40, num_updates=1480, lr=4.88248e-05, gnorm=1.166, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1673
2023-02-22 12:01:14 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 3411 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=1490, lr=4.87991e-05, gnorm=1.674, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1684
2023-02-22 12:01:25 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 3411 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=1500, lr=4.87733e-05, gnorm=1.183, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1695
2023-02-22 12:01:36 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 3411 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=1510, lr=4.87476e-05, gnorm=1.323, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1706
2023-02-22 12:01:47 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 3411 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102.1, ups=0.92, wpb=111.1, bsz=40, num_updates=1520, lr=4.87219e-05, gnorm=1.537, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1717
2023-02-22 12:01:58 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 3411 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.7, ups=0.88, wpb=111.3, bsz=40, num_updates=1530, lr=4.86962e-05, gnorm=1.255, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1728
2023-02-22 12:02:10 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=1540, lr=4.86705e-05, gnorm=1.113, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1740
2023-02-22 12:02:21 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 3411 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.4, ups=0.87, wpb=112, bsz=40, num_updates=1550, lr=4.86448e-05, gnorm=1.098, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1751
2023-02-22 12:02:32 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 3411 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=1560, lr=4.8619e-05, gnorm=1.046, clip=50, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1762
2023-02-22 12:02:43 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 3411 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=1570, lr=4.85933e-05, gnorm=1.268, clip=80, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1774
2023-02-22 12:02:55 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 3411 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=1580, lr=4.85676e-05, gnorm=1.086, clip=40, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1785
2023-02-22 12:03:06 - progress_bar.py[line:274] - INFO: epoch 001:   1590 / 3411 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.9, ups=0.88, wpb=112.3, bsz=40, num_updates=1590, lr=4.85419e-05, gnorm=1.306, clip=60, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1796
2023-02-22 12:03:17 - progress_bar.py[line:274] - INFO: epoch 001:   1600 / 3411 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=1600, lr=4.85162e-05, gnorm=1.221, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1807
2023-02-22 12:03:28 - progress_bar.py[line:274] - INFO: epoch 001:   1610 / 3411 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.6, ups=0.89, wpb=112.7, bsz=40, num_updates=1610, lr=4.84905e-05, gnorm=1.274, clip=90, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1818
2023-02-22 12:03:40 - progress_bar.py[line:274] - INFO: epoch 001:   1620 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=94.4, ups=0.85, wpb=110.7, bsz=40, num_updates=1620, lr=4.84647e-05, gnorm=1.448, clip=90, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1830
2023-02-22 12:03:51 - progress_bar.py[line:274] - INFO: epoch 001:   1630 / 3411 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.88, wpb=112.3, bsz=40, num_updates=1630, lr=4.8439e-05, gnorm=1.089, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1842
2023-02-22 12:04:03 - progress_bar.py[line:274] - INFO: epoch 001:   1640 / 3411 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.88, wpb=112.4, bsz=40, num_updates=1640, lr=4.84133e-05, gnorm=1.33, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1853
2023-02-22 12:04:14 - progress_bar.py[line:274] - INFO: epoch 001:   1650 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.3, ups=0.88, wpb=112.9, bsz=40, num_updates=1650, lr=4.83876e-05, gnorm=1.608, clip=90, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1864
@@@@ ERROR IN DATA @@@@ play
2023-02-22 12:04:25 - progress_bar.py[line:274] - INFO: epoch 001:   1660 / 3411 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100, ups=0.89, wpb=112.3, bsz=40, num_updates=1660, lr=4.83619e-05, gnorm=1.121, clip=60, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1876
2023-02-22 12:04:36 - progress_bar.py[line:274] - INFO: epoch 001:   1670 / 3411 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.5, ups=0.91, wpb=111.4, bsz=40, num_updates=1670, lr=4.83362e-05, gnorm=1.144, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1887
2023-02-22 12:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   1680 / 3411 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.9, wpb=110.6, bsz=40, num_updates=1680, lr=4.83104e-05, gnorm=1.341, clip=80, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1898
2023-02-22 12:04:59 - progress_bar.py[line:274] - INFO: epoch 001:   1690 / 3411 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=1690, lr=4.82847e-05, gnorm=1.252, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1909
2023-02-22 12:05:10 - progress_bar.py[line:274] - INFO: epoch 001:   1700 / 3411 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=1700, lr=4.8259e-05, gnorm=1.341, clip=90, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=1920
2023-02-22 12:05:21 - progress_bar.py[line:274] - INFO: epoch 001:   1710 / 3411 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=1710, lr=4.82333e-05, gnorm=1.35, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1931
2023-02-22 12:05:32 - progress_bar.py[line:274] - INFO: epoch 001:   1720 / 3411 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=1720, lr=4.82076e-05, gnorm=1.236, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1942
2023-02-22 12:05:43 - progress_bar.py[line:274] - INFO: epoch 001:   1730 / 3411 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=1730, lr=4.81819e-05, gnorm=1.402, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1953
2023-02-22 12:05:54 - progress_bar.py[line:274] - INFO: epoch 001:   1740 / 3411 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.9, ups=0.91, wpb=111.8, bsz=40, num_updates=1740, lr=4.81561e-05, gnorm=1.336, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1964
2023-02-22 12:06:05 - progress_bar.py[line:274] - INFO: epoch 001:   1750 / 3411 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.1, ups=0.89, wpb=111.4, bsz=40, num_updates=1750, lr=4.81304e-05, gnorm=1.209, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1976
2023-02-22 12:06:17 - progress_bar.py[line:274] - INFO: epoch 001:   1760 / 3411 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.6, ups=0.9, wpb=114.1, bsz=40, num_updates=1760, lr=4.81047e-05, gnorm=1.366, clip=70, loss_scale=1024, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=1987
2023-02-22 12:06:28 - progress_bar.py[line:274] - INFO: epoch 001:   1770 / 3411 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.6, ups=0.89, wpb=112.7, bsz=40, num_updates=1770, lr=4.8079e-05, gnorm=1.403, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1998
2023-02-22 12:06:39 - progress_bar.py[line:274] - INFO: epoch 001:   1780 / 3411 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.2, ups=0.88, wpb=112.7, bsz=40, num_updates=1780, lr=4.80533e-05, gnorm=1.656, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2009
2023-02-22 12:06:50 - progress_bar.py[line:274] - INFO: epoch 001:   1790 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.6, ups=0.92, wpb=112.7, bsz=40, num_updates=1790, lr=4.80276e-05, gnorm=1.201, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2020
2023-02-22 12:07:01 - progress_bar.py[line:274] - INFO: epoch 001:   1800 / 3411 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.8, ups=0.9, wpb=112.2, bsz=40, num_updates=1800, lr=4.80019e-05, gnorm=1.184, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2031
2023-02-22 12:07:12 - progress_bar.py[line:274] - INFO: epoch 001:   1810 / 3411 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.6, ups=0.9, wpb=110.5, bsz=40, num_updates=1810, lr=4.79761e-05, gnorm=1.288, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2043
2023-02-22 12:07:23 - progress_bar.py[line:274] - INFO: epoch 001:   1820 / 3411 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=1820, lr=4.79504e-05, gnorm=1.123, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2054
2023-02-22 12:07:35 - progress_bar.py[line:274] - INFO: epoch 001:   1830 / 3411 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.5, ups=0.89, wpb=112.4, bsz=40, num_updates=1830, lr=4.79247e-05, gnorm=1.163, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2065
2023-02-22 12:07:46 - progress_bar.py[line:274] - INFO: epoch 001:   1840 / 3411 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=1840, lr=4.7899e-05, gnorm=1.164, clip=80, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=2076
2023-02-22 12:07:57 - progress_bar.py[line:274] - INFO: epoch 001:   1850 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.1, ups=0.92, wpb=112.2, bsz=40, num_updates=1850, lr=4.78733e-05, gnorm=1.112, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2087
2023-02-22 12:08:08 - progress_bar.py[line:274] - INFO: epoch 001:   1860 / 3411 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.7, ups=0.88, wpb=112.2, bsz=40, num_updates=1860, lr=4.78476e-05, gnorm=1.154, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2098
2023-02-22 12:08:19 - progress_bar.py[line:274] - INFO: epoch 001:   1870 / 3411 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.8, ups=0.93, wpb=111.8, bsz=40, num_updates=1870, lr=4.78218e-05, gnorm=1.1, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2109
2023-02-22 12:08:30 - progress_bar.py[line:274] - INFO: epoch 001:   1880 / 3411 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.7, ups=0.88, wpb=111.1, bsz=40, num_updates=1880, lr=4.77961e-05, gnorm=1.514, clip=90, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=2121
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 12:08:42 - progress_bar.py[line:274] - INFO: epoch 001:   1890 / 3411 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.4, ups=0.89, wpb=112.8, bsz=40, num_updates=1890, lr=4.77704e-05, gnorm=1.171, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2132
2023-02-22 12:08:53 - progress_bar.py[line:274] - INFO: epoch 001:   1900 / 3411 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.3, ups=0.9, wpb=112.4, bsz=40, num_updates=1900, lr=4.77447e-05, gnorm=1.065, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2143
2023-02-22 12:09:04 - progress_bar.py[line:274] - INFO: epoch 001:   1910 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.2, ups=0.88, wpb=111.6, bsz=40, num_updates=1910, lr=4.7719e-05, gnorm=1.091, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2154
2023-02-22 12:09:15 - progress_bar.py[line:274] - INFO: epoch 001:   1920 / 3411 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.3, ups=0.89, wpb=111.4, bsz=40, num_updates=1920, lr=4.76933e-05, gnorm=1.059, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2165
2023-02-22 12:09:26 - progress_bar.py[line:274] - INFO: epoch 001:   1930 / 3411 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.4, ups=0.9, wpb=111.4, bsz=40, num_updates=1930, lr=4.76675e-05, gnorm=1.475, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2177
2023-02-22 12:09:37 - progress_bar.py[line:274] - INFO: epoch 001:   1940 / 3411 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.4, ups=0.9, wpb=113.5, bsz=40, num_updates=1940, lr=4.76418e-05, gnorm=1.055, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2188
2023-02-22 12:09:49 - progress_bar.py[line:274] - INFO: epoch 001:   1950 / 3411 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=1950, lr=4.76161e-05, gnorm=1.005, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2199
2023-02-22 12:10:00 - progress_bar.py[line:274] - INFO: epoch 001:   1960 / 3411 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=1960, lr=4.75904e-05, gnorm=1.17, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2210
2023-02-22 12:10:11 - progress_bar.py[line:274] - INFO: epoch 001:   1970 / 3411 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.9, ups=0.89, wpb=111.8, bsz=40, num_updates=1970, lr=4.75647e-05, gnorm=1.202, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2221
2023-02-22 12:10:23 - progress_bar.py[line:274] - INFO: epoch 001:   1980 / 3411 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.88, wpb=111.9, bsz=40, num_updates=1980, lr=4.7539e-05, gnorm=1.152, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2233
2023-02-22 12:10:34 - progress_bar.py[line:274] - INFO: epoch 001:   1990 / 3411 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.88, wpb=113.2, bsz=40, num_updates=1990, lr=4.75132e-05, gnorm=1.212, clip=80, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2244
2023-02-22 12:10:45 - progress_bar.py[line:274] - INFO: epoch 001:   2000 / 3411 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.4, ups=0.91, wpb=110.3, bsz=40, num_updates=2000, lr=4.74875e-05, gnorm=1.35, clip=80, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2255
2023-02-22 12:10:45 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 12:10:45 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 12:10:46 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 12:10:46 - train.py[line:551] - INFO: load:0.99 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 12:12:51 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 12:12:51 - train.py[line:551] - INFO: load:1.02 valid_run:124.95 task_valid:121.43 collect_output:2.45
2023-02-22 12:14:52 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 12:14:52 - train.py[line:551] - INFO: load:1.04 valid_run:245.31 task_valid:237.56 collect_output:5.65
2023-02-22 12:16:55 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 12:16:55 - train.py[line:551] - INFO: load:1.07 valid_run:368.31 task_valid:354.39 collect_output:10.78
2023-02-22 12:18:57 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 12:18:57 - train.py[line:551] - INFO: load:1.09 valid_run:490.50 task_valid:468.46 collect_output:17.89
2023-02-22 12:20:58 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 12:20:58 - train.py[line:551] - INFO: load:1.11 valid_run:611.29 task_valid:586.12 collect_output:20.01
2023-02-22 12:23:01 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 12:23:01 - train.py[line:551] - INFO: load:1.14 valid_run:734.53 task_valid:705.15 collect_output:23.22
2023-02-22 12:25:05 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 12:25:05 - train.py[line:551] - INFO: load:1.16 valid_run:857.98 task_valid:823.58 collect_output:27.22
2023-02-22 12:27:07 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 12:27:07 - train.py[line:551] - INFO: load:1.19 valid_run:980.32 task_valid:940.40 collect_output:31.74
2023-02-22 12:29:11 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 12:29:11 - train.py[line:551] - INFO: load:1.21 valid_run:1104.60 task_valid:1057.99 collect_output:37.43
2023-02-22 12:31:14 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 12:31:14 - train.py[line:551] - INFO: load:1.24 valid_run:1226.62 task_valid:1171.00 collect_output:45.43
2023-02-22 12:33:14 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 12:33:14 - train.py[line:551] - INFO: load:1.26 valid_run:1347.25 task_valid:1287.07 collect_output:48.96
2023-02-22 12:35:16 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 12:35:16 - train.py[line:551] - INFO: load:1.29 valid_run:1469.40 task_valid:1404.45 collect_output:52.71
2023-02-22 12:37:16 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 12:37:16 - train.py[line:551] - INFO: load:1.32 valid_run:1588.94 task_valid:1518.65 collect_output:57.00
2023-02-22 12:39:18 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 12:39:18 - train.py[line:551] - INFO: load:1.34 valid_run:1710.45 task_valid:1636.82 collect_output:59.34
2023-02-22 12:41:19 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 12:41:19 - train.py[line:551] - INFO: load:1.37 valid_run:1831.73 task_valid:1753.25 collect_output:63.18
2023-02-22 12:43:20 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 12:43:20 - train.py[line:551] - INFO: load:1.39 valid_run:1953.24 task_valid:1867.54 collect_output:69.39
2023-02-22 12:45:22 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 12:45:22 - train.py[line:551] - INFO: load:1.42 valid_run:2074.98 task_valid:1983.97 collect_output:73.69
2023-02-22 12:47:23 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 12:47:23 - train.py[line:551] - INFO: load:1.44 valid_run:2196.05 task_valid:2102.23 collect_output:75.49
2023-02-22 12:49:25 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 12:49:25 - train.py[line:551] - INFO: load:1.47 valid_run:2317.71 task_valid:2219.56 collect_output:78.81
2023-02-22 12:51:26 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 12:51:26 - train.py[line:551] - INFO: load:1.50 valid_run:2438.48 task_valid:2336.52 collect_output:81.60
2023-02-22 12:53:28 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 12:53:28 - train.py[line:551] - INFO: load:1.52 valid_run:2560.58 task_valid:2453.44 collect_output:85.75
2023-02-22 12:55:31 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 12:55:31 - train.py[line:551] - INFO: load:1.55 valid_run:2683.06 task_valid:2572.89 collect_output:87.74
2023-02-22 12:57:31 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 12:57:31 - train.py[line:551] - INFO: load:1.57 valid_run:2803.72 task_valid:2687.55 collect_output:92.69
2023-02-22 12:59:32 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 12:59:32 - train.py[line:551] - INFO: load:1.60 valid_run:2923.89 task_valid:2804.02 collect_output:95.37
2023-02-22 13:01:34 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 13:01:34 - train.py[line:551] - INFO: load:1.62 valid_run:3045.98 task_valid:2920.64 collect_output:99.80
2023-02-22 13:03:37 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 13:03:37 - train.py[line:551] - INFO: load:1.65 valid_run:3169.15 task_valid:3036.87 collect_output:105.72
2023-02-22 13:05:37 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 13:05:37 - train.py[line:551] - INFO: load:1.68 valid_run:3289.12 task_valid:3151.43 collect_output:110.09
2023-02-22 13:07:39 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 13:07:39 - train.py[line:551] - INFO: load:1.70 valid_run:3411.42 task_valid:3271.26 collect_output:111.55
2023-02-22 13:09:42 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 13:09:42 - train.py[line:551] - INFO: load:1.73 valid_run:3533.51 task_valid:3387.25 collect_output:116.63
2023-02-22 13:11:44 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 13:11:44 - train.py[line:551] - INFO: load:1.75 valid_run:3655.79 task_valid:3506.06 collect_output:119.07
2023-02-22 13:13:45 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 13:13:45 - train.py[line:551] - INFO: load:1.78 valid_run:3777.29 task_valid:3624.87 collect_output:120.72

====================================================================================================
SGG eval:     R @ 50: 0.4280;     R @ 100: 0.4751;     R @ 500: 0.5420;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2159;    mR @ 100: 0.2629;    mR @ 500: 0.3347;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1951) (covered in:0.0625) (covering:0.1429) (eating:0.5882) (flying in:0.6364) (growing on:0.1250) (hanging from:0.5226) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.6520) (says:0.0000) (sitting on:0.5235) (standing on:0.6267) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0417) 
--------------------------------------------------------
====================================================================================================

2023-02-22 13:14:16 - train.py[line:487] - INFO: 0.4750787878787879

====================================================================================================
SGG eval:     R @ 50: 0.4280;     R @ 100: 0.4751;     R @ 500: 0.5420;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2159;    mR @ 100: 0.2629;    mR @ 500: 0.3347;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1951) (covered in:0.0625) (covering:0.1429) (eating:0.5882) (flying in:0.6364) (growing on:0.1250) (hanging from:0.5226) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.6520) (says:0.0000) (sitting on:0.5235) (standing on:0.6267) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0417) 
--------------------------------------------------------
====================================================================================================

2023-02-22 13:14:16 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 13:14:16 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.252 | loss_v1 0 | loss_v2 0 | nll_loss 0.092 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.475079 | ppl 1.07 | vqa_score 0.2005 | wps 117.8 | wpb 72 | bsz 24 | num_updates 2000
2023-02-22 13:14:16 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-02-22 13:14:16 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_1_2000.pt
2023-02-22 13:14:22 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_1_2000.pt
2023-02-22 13:14:27 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.4750787878787879) (writing took 10.773245358839631 seconds)
2023-02-22 13:14:39 - progress_bar.py[line:274] - INFO: epoch 001:   2010 / 3411 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=0.3, ups=0, wpb=112.6, bsz=40, num_updates=2010, lr=4.74618e-05, gnorm=1.432, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6089
2023-02-22 13:14:50 - progress_bar.py[line:274] - INFO: epoch 001:   2020 / 3411 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.7, ups=0.89, wpb=110.9, bsz=40, num_updates=2020, lr=4.74361e-05, gnorm=1.601, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6100
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:15:01 - progress_bar.py[line:274] - INFO: epoch 001:   2030 / 3411 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.91, wpb=109.1, bsz=40, num_updates=2030, lr=4.74104e-05, gnorm=1.026, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6111
2023-02-22 13:15:15 - progress_bar.py[line:274] - INFO: epoch 001:   2040 / 3411 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.6, ups=0.9, wpb=110.8, bsz=40, num_updates=2040, lr=4.73847e-05, gnorm=1.338, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6122
2023-02-22 13:15:27 - progress_bar.py[line:274] - INFO: epoch 001:   2050 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.88, wpb=111.3, bsz=40, num_updates=2050, lr=4.73589e-05, gnorm=1.183, clip=60, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6137
2023-02-22 13:15:38 - progress_bar.py[line:274] - INFO: epoch 001:   2060 / 3411 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=2060, lr=4.73332e-05, gnorm=1.132, clip=60, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6148
2023-02-22 13:15:41 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 13:15:50 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 3411 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=93.2, ups=0.84, wpb=111.2, bsz=40, num_updates=2070, lr=4.73075e-05, gnorm=1.184, clip=70, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6160
2023-02-22 13:16:01 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 3411 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.3, ups=0.91, wpb=111.7, bsz=40, num_updates=2080, lr=4.72818e-05, gnorm=1.141, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6171
2023-02-22 13:16:12 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=2090, lr=4.72561e-05, gnorm=0.893, clip=40, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6182
2023-02-22 13:16:21 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 13:16:24 - progress_bar.py[line:274] - INFO: epoch 001:   2102 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=89.5, ups=0.81, wpb=111, bsz=40, num_updates=2100, lr=4.72304e-05, gnorm=1.136, clip=70, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6194
2023-02-22 13:16:36 - progress_bar.py[line:274] - INFO: epoch 001:   2112 / 3411 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100, ups=0.88, wpb=113.2, bsz=40, num_updates=2110, lr=4.72046e-05, gnorm=1.343, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6206
2023-02-22 13:16:47 - progress_bar.py[line:274] - INFO: epoch 001:   2122 / 3411 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.2, ups=0.87, wpb=111.1, bsz=40, num_updates=2120, lr=4.71789e-05, gnorm=1.273, clip=70, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=6217
2023-02-22 13:16:59 - progress_bar.py[line:274] - INFO: epoch 001:   2132 / 3411 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.87, wpb=112, bsz=40, num_updates=2130, lr=4.71532e-05, gnorm=1.129, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6229
2023-02-22 13:17:10 - progress_bar.py[line:274] - INFO: epoch 001:   2142 / 3411 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.7, ups=0.88, wpb=112.3, bsz=40, num_updates=2140, lr=4.71275e-05, gnorm=1.256, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6240
2023-02-22 13:17:21 - progress_bar.py[line:274] - INFO: epoch 001:   2152 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=2150, lr=4.71018e-05, gnorm=1.241, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6251
2023-02-22 13:17:32 - progress_bar.py[line:274] - INFO: epoch 001:   2162 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.2, ups=0.91, wpb=112.6, bsz=40, num_updates=2160, lr=4.70761e-05, gnorm=1.409, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6262
2023-02-22 13:17:44 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 3411 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.9, ups=0.89, wpb=112, bsz=40, num_updates=2170, lr=4.70504e-05, gnorm=1.574, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6274
2023-02-22 13:17:54 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102.7, ups=0.91, wpb=112.3, bsz=40, num_updates=2180, lr=4.70246e-05, gnorm=1.159, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6285
2023-02-22 13:18:06 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 3411 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.9, wpb=113.3, bsz=40, num_updates=2190, lr=4.69989e-05, gnorm=1.052, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6296
2023-02-22 13:18:17 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.9, ups=0.91, wpb=112.8, bsz=40, num_updates=2200, lr=4.69732e-05, gnorm=1.228, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6307
2023-02-22 13:18:27 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 3411 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=106.8, ups=0.95, wpb=113, bsz=40, num_updates=2210, lr=4.69475e-05, gnorm=1.294, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6317
2023-02-22 13:18:38 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 3411 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=2220, lr=4.69218e-05, gnorm=1.33, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6328
2023-02-22 13:18:50 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 3411 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=2230, lr=4.68961e-05, gnorm=1.046, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6340
2023-02-22 13:19:01 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 3411 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.9, ups=0.88, wpb=112.7, bsz=40, num_updates=2240, lr=4.68703e-05, gnorm=1.103, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6351
2023-02-22 13:19:12 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 3411 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=2250, lr=4.68446e-05, gnorm=2.317, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6363
2023-02-22 13:19:24 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 3411 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.9, ups=0.88, wpb=112.9, bsz=40, num_updates=2260, lr=4.68189e-05, gnorm=1.554, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6374
2023-02-22 13:19:35 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 3411 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.3, ups=0.88, wpb=113.8, bsz=40, num_updates=2270, lr=4.67932e-05, gnorm=1.016, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6385
2023-02-22 13:19:46 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 3411 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.2, ups=0.93, wpb=111.1, bsz=40, num_updates=2280, lr=4.67675e-05, gnorm=1.083, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6396
2023-02-22 13:19:57 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=112.2, bsz=40, num_updates=2290, lr=4.67418e-05, gnorm=0.891, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6407
2023-02-22 13:20:08 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 3411 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=2300, lr=4.6716e-05, gnorm=1.563, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6418
2023-02-22 13:20:19 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=2310, lr=4.66903e-05, gnorm=1.187, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6430
2023-02-22 13:20:31 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.1, ups=0.88, wpb=111.5, bsz=40, num_updates=2320, lr=4.66646e-05, gnorm=1.273, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6441
2023-02-22 13:20:42 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.8, ups=0.93, wpb=112, bsz=40, num_updates=2330, lr=4.66389e-05, gnorm=1.219, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6452
2023-02-22 13:20:53 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 3411 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=2340, lr=4.66132e-05, gnorm=1.324, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6463
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:21:04 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=2350, lr=4.65875e-05, gnorm=1.016, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6474
2023-02-22 13:21:15 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 3411 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.88, wpb=111.9, bsz=40, num_updates=2360, lr=4.65617e-05, gnorm=1.169, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6485
2023-02-22 13:21:26 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.3, ups=0.93, wpb=111.9, bsz=40, num_updates=2370, lr=4.6536e-05, gnorm=0.909, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6496
2023-02-22 13:21:37 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 3411 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=2380, lr=4.65103e-05, gnorm=1.171, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6507
2023-02-22 13:21:48 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 3411 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.88, wpb=112.6, bsz=40, num_updates=2390, lr=4.64846e-05, gnorm=1.189, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6519
2023-02-22 13:21:59 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 3411 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101, ups=0.92, wpb=110, bsz=40, num_updates=2400, lr=4.64589e-05, gnorm=1.365, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6529
2023-02-22 13:22:10 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 3411 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.2, ups=0.9, wpb=113.3, bsz=40, num_updates=2410, lr=4.64332e-05, gnorm=1.299, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6541
2023-02-22 13:22:22 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 3411 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.7, ups=0.87, wpb=112.2, bsz=40, num_updates=2420, lr=4.64074e-05, gnorm=1.31, clip=80, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=6552
2023-02-22 13:22:33 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.88, wpb=112.4, bsz=40, num_updates=2430, lr=4.63817e-05, gnorm=0.989, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6563
2023-02-22 13:22:44 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=2440, lr=4.6356e-05, gnorm=1.231, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6575
2023-02-22 13:22:56 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 3411 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=2450, lr=4.63303e-05, gnorm=1.281, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6586
2023-02-22 13:23:07 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.3, ups=0.91, wpb=110.8, bsz=40, num_updates=2460, lr=4.63046e-05, gnorm=1.014, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6597
2023-02-22 13:23:18 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 3411 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.8, ups=0.91, wpb=112.9, bsz=40, num_updates=2470, lr=4.62789e-05, gnorm=1.002, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6608
2023-02-22 13:23:28 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=105.1, ups=0.93, wpb=112.6, bsz=40, num_updates=2480, lr=4.62532e-05, gnorm=1.056, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6619
2023-02-22 13:23:39 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 3411 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=2490, lr=4.62274e-05, gnorm=1.25, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6629
2023-02-22 13:23:50 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=104.3, ups=0.92, wpb=113.9, bsz=40, num_updates=2500, lr=4.62017e-05, gnorm=1.069, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6640
2023-02-22 13:24:02 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 3411 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.4, ups=0.87, wpb=111, bsz=40, num_updates=2510, lr=4.6176e-05, gnorm=1.265, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6652
2023-02-22 13:24:13 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 3411 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.87, wpb=111.9, bsz=40, num_updates=2520, lr=4.61503e-05, gnorm=1.294, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6663
2023-02-22 13:24:24 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 3411 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.5, ups=0.9, wpb=112.2, bsz=40, num_updates=2530, lr=4.61246e-05, gnorm=0.92, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6674
2023-02-22 13:24:36 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=2540, lr=4.60989e-05, gnorm=1.077, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6686
2023-02-22 13:24:47 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 3411 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=2550, lr=4.60731e-05, gnorm=1.073, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6697
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 13:24:58 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.88, wpb=112.1, bsz=40, num_updates=2560, lr=4.60474e-05, gnorm=0.978, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6708
2023-02-22 13:25:09 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 3411 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.5, ups=0.93, wpb=111.4, bsz=40, num_updates=2570, lr=4.60217e-05, gnorm=0.976, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6719
2023-02-22 13:25:20 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 3411 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.91, wpb=110.9, bsz=40, num_updates=2580, lr=4.5996e-05, gnorm=1.659, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6730
2023-02-22 13:25:31 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.7, ups=0.92, wpb=111.3, bsz=40, num_updates=2590, lr=4.59703e-05, gnorm=0.945, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6741
2023-02-22 13:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.9, wpb=112.9, bsz=40, num_updates=2600, lr=4.59446e-05, gnorm=0.766, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6752
2023-02-22 13:25:53 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 3411 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.2, ups=0.87, wpb=110.8, bsz=40, num_updates=2610, lr=4.59188e-05, gnorm=1.249, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6764
2023-02-22 13:26:04 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 3411 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.1, ups=0.91, wpb=111.7, bsz=40, num_updates=2620, lr=4.58931e-05, gnorm=1.265, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6774
2023-02-22 13:26:16 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 3411 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.5, ups=0.87, wpb=111.2, bsz=40, num_updates=2630, lr=4.58674e-05, gnorm=1.225, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6786
2023-02-22 13:26:27 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=2640, lr=4.58417e-05, gnorm=1.115, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6797
2023-02-22 13:26:38 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 3411 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.5, ups=0.91, wpb=112.1, bsz=40, num_updates=2650, lr=4.5816e-05, gnorm=1.167, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6808
2023-02-22 13:26:49 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 3411 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=2660, lr=4.57903e-05, gnorm=1.264, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6819
2023-02-22 13:27:00 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 3411 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=2670, lr=4.57645e-05, gnorm=1.138, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6830
2023-02-22 13:27:12 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 3411 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.87, wpb=112.9, bsz=40, num_updates=2680, lr=4.57388e-05, gnorm=1.104, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6842
2023-02-22 13:27:23 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 3411 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.7, ups=0.91, wpb=112.2, bsz=40, num_updates=2690, lr=4.57131e-05, gnorm=1.203, clip=80, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=6853
2023-02-22 13:27:34 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=2700, lr=4.56874e-05, gnorm=1.231, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6864
2023-02-22 13:27:45 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 3411 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=2710, lr=4.56617e-05, gnorm=1.079, clip=70, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6875
2023-02-22 13:27:57 - progress_bar.py[line:274] - INFO: epoch 001:   2722 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.88, wpb=111.6, bsz=40, num_updates=2720, lr=4.5636e-05, gnorm=1.099, clip=50, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=6887
2023-02-22 13:28:08 - progress_bar.py[line:274] - INFO: epoch 001:   2732 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=111.8, bsz=40, num_updates=2730, lr=4.56102e-05, gnorm=1.181, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6898
2023-02-22 13:28:19 - progress_bar.py[line:274] - INFO: epoch 001:   2742 / 3411 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.6, ups=0.92, wpb=112, bsz=40, num_updates=2740, lr=4.55845e-05, gnorm=1.094, clip=60, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=6909
2023-02-22 13:28:30 - progress_bar.py[line:274] - INFO: epoch 001:   2752 / 3411 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.9, wpb=112.8, bsz=40, num_updates=2750, lr=4.55588e-05, gnorm=0.961, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6920
2023-02-22 13:28:41 - progress_bar.py[line:274] - INFO: epoch 001:   2762 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.92, wpb=111, bsz=40, num_updates=2760, lr=4.55331e-05, gnorm=1.159, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6931
2023-02-22 13:28:52 - progress_bar.py[line:274] - INFO: epoch 001:   2772 / 3411 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.8, ups=0.92, wpb=113.2, bsz=40, num_updates=2770, lr=4.55074e-05, gnorm=1.07, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6942
2023-02-22 13:29:03 - progress_bar.py[line:274] - INFO: epoch 001:   2782 / 3411 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=2780, lr=4.54817e-05, gnorm=1.249, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6953
2023-02-22 13:29:13 - progress_bar.py[line:274] - INFO: epoch 001:   2792 / 3411 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.8, ups=0.93, wpb=111.4, bsz=40, num_updates=2790, lr=4.54559e-05, gnorm=1.61, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6964
2023-02-22 13:29:25 - progress_bar.py[line:274] - INFO: epoch 001:   2802 / 3411 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=2800, lr=4.54302e-05, gnorm=1.296, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6975
2023-02-22 13:29:36 - progress_bar.py[line:274] - INFO: epoch 001:   2812 / 3411 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=2810, lr=4.54045e-05, gnorm=1.161, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6986
2023-02-22 13:29:47 - progress_bar.py[line:274] - INFO: epoch 001:   2822 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=2820, lr=4.53788e-05, gnorm=1.021, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6997
2023-02-22 13:29:58 - progress_bar.py[line:274] - INFO: epoch 001:   2832 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.7, ups=0.91, wpb=112.2, bsz=40, num_updates=2830, lr=4.53531e-05, gnorm=1.135, clip=50, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=7008
2023-02-22 13:30:09 - progress_bar.py[line:274] - INFO: epoch 001:   2842 / 3411 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=2840, lr=4.53274e-05, gnorm=1.087, clip=60, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7019
2023-02-22 13:30:20 - progress_bar.py[line:274] - INFO: epoch 001:   2852 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.92, wpb=110.8, bsz=40, num_updates=2850, lr=4.53017e-05, gnorm=1.186, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7030
2023-02-22 13:30:31 - progress_bar.py[line:274] - INFO: epoch 001:   2862 / 3411 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.88, wpb=111.4, bsz=40, num_updates=2860, lr=4.52759e-05, gnorm=1.111, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7042
2023-02-22 13:30:43 - progress_bar.py[line:274] - INFO: epoch 001:   2872 / 3411 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=2870, lr=4.52502e-05, gnorm=1.099, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7053
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:30:54 - progress_bar.py[line:274] - INFO: epoch 001:   2882 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.9, wpb=111.9, bsz=40, num_updates=2880, lr=4.52245e-05, gnorm=1.085, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7064
2023-02-22 13:31:05 - progress_bar.py[line:274] - INFO: epoch 001:   2892 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=2890, lr=4.51988e-05, gnorm=1.257, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7075
2023-02-22 13:31:16 - progress_bar.py[line:274] - INFO: epoch 001:   2902 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=110.7, bsz=40, num_updates=2900, lr=4.51731e-05, gnorm=0.894, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7086
2023-02-22 13:31:27 - progress_bar.py[line:274] - INFO: epoch 001:   2912 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.9, wpb=112.7, bsz=40, num_updates=2910, lr=4.51474e-05, gnorm=1.111, clip=50, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7097
2023-02-22 13:31:38 - progress_bar.py[line:274] - INFO: epoch 001:   2922 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.88, wpb=110.7, bsz=40, num_updates=2920, lr=4.51216e-05, gnorm=0.972, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7108
2023-02-22 13:31:50 - progress_bar.py[line:274] - INFO: epoch 001:   2932 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=2930, lr=4.50959e-05, gnorm=1.127, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7120
2023-02-22 13:32:01 - progress_bar.py[line:274] - INFO: epoch 001:   2942 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.89, wpb=113.6, bsz=40, num_updates=2940, lr=4.50702e-05, gnorm=1.262, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7131
2023-02-22 13:32:12 - progress_bar.py[line:274] - INFO: epoch 001:   2952 / 3411 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.2, ups=0.91, wpb=113.2, bsz=40, num_updates=2950, lr=4.50445e-05, gnorm=0.992, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7142
2023-02-22 13:32:14 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 13:32:24 - progress_bar.py[line:274] - INFO: epoch 001:   2963 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=90, ups=0.81, wpb=111.7, bsz=40, num_updates=2960, lr=4.50188e-05, gnorm=0.982, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7154
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 13:32:35 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 3411 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=2970, lr=4.49931e-05, gnorm=1.257, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7165
2023-02-22 13:32:46 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=111.5, bsz=40, num_updates=2980, lr=4.49673e-05, gnorm=0.985, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7177
2023-02-22 13:32:58 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.88, wpb=112.7, bsz=40, num_updates=2990, lr=4.49416e-05, gnorm=1.026, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7188
2023-02-22 13:33:09 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=111.5, bsz=40, num_updates=3000, lr=4.49159e-05, gnorm=1.085, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7199
2023-02-22 13:33:20 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 3411 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.89, wpb=110.8, bsz=40, num_updates=3010, lr=4.48902e-05, gnorm=0.882, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7210
2023-02-22 13:33:31 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.7, ups=0.92, wpb=113.1, bsz=40, num_updates=3020, lr=4.48645e-05, gnorm=0.919, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7221
2023-02-22 13:33:42 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.1, ups=0.91, wpb=112.8, bsz=40, num_updates=3030, lr=4.48388e-05, gnorm=0.967, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7232
2023-02-22 13:33:53 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 3411 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=3040, lr=4.4813e-05, gnorm=1.004, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7243
2023-02-22 13:34:04 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 3411 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=3050, lr=4.47873e-05, gnorm=1.157, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7255
2023-02-22 13:34:15 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 3411 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.9, wpb=112.4, bsz=40, num_updates=3060, lr=4.47616e-05, gnorm=1.003, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7266
2023-02-22 13:34:27 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 3411 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=3070, lr=4.47359e-05, gnorm=1.098, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7277
2023-02-22 13:34:38 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 3411 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.87, wpb=112.3, bsz=40, num_updates=3080, lr=4.47102e-05, gnorm=1.198, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7288
2023-02-22 13:34:49 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 3411 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.89, wpb=112.3, bsz=40, num_updates=3090, lr=4.46845e-05, gnorm=0.998, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7300
2023-02-22 13:35:00 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.91, wpb=111, bsz=40, num_updates=3100, lr=4.46587e-05, gnorm=1.079, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7311
2023-02-22 13:35:12 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 3411 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.3, ups=0.9, wpb=112.6, bsz=40, num_updates=3110, lr=4.4633e-05, gnorm=1.379, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7322
2023-02-22 13:35:23 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 3411 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=3120, lr=4.46073e-05, gnorm=1.065, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7333
2023-02-22 13:35:34 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=3130, lr=4.45816e-05, gnorm=1.609, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7344
2023-02-22 13:35:45 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=112.5, bsz=40, num_updates=3140, lr=4.45559e-05, gnorm=1.124, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7355
2023-02-22 13:35:57 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 3411 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.88, wpb=111.5, bsz=40, num_updates=3150, lr=4.45302e-05, gnorm=0.938, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7367
2023-02-22 13:36:08 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.2, ups=0.86, wpb=111.8, bsz=40, num_updates=3160, lr=4.45044e-05, gnorm=1.207, clip=60, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7379
2023-02-22 13:36:19 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.1, ups=0.9, wpb=112, bsz=40, num_updates=3170, lr=4.44787e-05, gnorm=1.002, clip=40, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=7390
2023-02-22 13:36:31 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.89, wpb=113.6, bsz=40, num_updates=3180, lr=4.4453e-05, gnorm=0.987, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7401
2023-02-22 13:36:42 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=3190, lr=4.44273e-05, gnorm=0.991, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7412
2023-02-22 13:36:53 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 3411 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.8, ups=0.89, wpb=113.2, bsz=40, num_updates=3200, lr=4.44016e-05, gnorm=1.544, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7423
2023-02-22 13:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.9, wpb=111.2, bsz=40, num_updates=3210, lr=4.43759e-05, gnorm=1.265, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7434
2023-02-22 13:37:15 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.9, wpb=111.2, bsz=40, num_updates=3220, lr=4.43502e-05, gnorm=1.106, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7446
2023-02-22 13:37:26 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=112.2, bsz=40, num_updates=3230, lr=4.43244e-05, gnorm=0.862, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7457
2023-02-22 13:37:37 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.4, ups=0.93, wpb=111.6, bsz=40, num_updates=3240, lr=4.42987e-05, gnorm=1.106, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7467
2023-02-22 13:37:49 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 3411 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.5, ups=0.89, wpb=111.9, bsz=40, num_updates=3250, lr=4.4273e-05, gnorm=0.969, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7479
2023-02-22 13:38:00 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 3411 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.5, ups=0.89, wpb=111.8, bsz=40, num_updates=3260, lr=4.42473e-05, gnorm=1.142, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7490
2023-02-22 13:38:11 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.4, ups=0.91, wpb=112.2, bsz=40, num_updates=3270, lr=4.42216e-05, gnorm=1.141, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7501
2023-02-22 13:38:22 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.4, ups=0.93, wpb=112.5, bsz=40, num_updates=3280, lr=4.41959e-05, gnorm=0.915, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7512
2023-02-22 13:38:33 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 3411 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=3290, lr=4.41701e-05, gnorm=1.518, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7523
2023-02-22 13:38:44 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.87, wpb=112, bsz=40, num_updates=3300, lr=4.41444e-05, gnorm=0.998, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7534
2023-02-22 13:38:55 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=112.3, bsz=40, num_updates=3310, lr=4.41187e-05, gnorm=0.9, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7546
2023-02-22 13:39:07 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.87, wpb=111.9, bsz=40, num_updates=3320, lr=4.4093e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7557
2023-02-22 13:39:18 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.9, wpb=110.9, bsz=40, num_updates=3330, lr=4.40673e-05, gnorm=0.731, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7568
2023-02-22 13:39:29 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 3411 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=104.1, ups=0.93, wpb=112.3, bsz=40, num_updates=3340, lr=4.40416e-05, gnorm=1.241, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7579
2023-02-22 13:39:40 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 3411 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=3350, lr=4.40158e-05, gnorm=1.103, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7590
2023-02-22 13:39:51 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.89, wpb=113.2, bsz=40, num_updates=3360, lr=4.39901e-05, gnorm=0.897, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7601
2023-02-22 13:40:02 - progress_bar.py[line:274] - INFO: epoch 001:   3373 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=3370, lr=4.39644e-05, gnorm=1.042, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7613
2023-02-22 13:40:14 - progress_bar.py[line:274] - INFO: epoch 001:   3383 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.89, wpb=112.4, bsz=40, num_updates=3380, lr=4.39387e-05, gnorm=1.091, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7624
2023-02-22 13:40:25 - progress_bar.py[line:274] - INFO: epoch 001:   3393 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=3390, lr=4.3913e-05, gnorm=0.904, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7635
2023-02-22 13:40:36 - progress_bar.py[line:274] - INFO: epoch 001:   3403 / 3411 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.3, ups=0.93, wpb=110.5, bsz=40, num_updates=3400, lr=4.38873e-05, gnorm=1.448, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7646
2023-02-22 13:40:45 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-02-22 13:40:45 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.338 | loss_v1 0 | loss_v2 0 | nll_loss 0.166 | ntokens 111.863 | nsentences 39.998 | sample_size 111.863 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.12 | wps 49.9 | ups 0.45 | wpb 111.9 | bsz 40 | num_updates 3408 | lr 4.38667e-05 | gnorm 1.612 | clip 73.5 | loss_scale 512 | train_wall 3795 | gb_free 14.3 | ema_decay 0.9999 | wall 7655
2023-02-22 13:40:45 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E1.tsv slice_id 1 row count 68217 total row count 136434
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E1.tsv slice_id 0 row count 68217 total row count 136434
2023-02-22 13:40:45 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 13:40:45 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 13:40:46 - trainer.py[line:758] - INFO: begin training epoch 2
2023-02-22 13:40:46 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 13:40:50 - progress_bar.py[line:274] - INFO: epoch 002:      2 / 3411 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.4, nsentences=39.4, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=79.6, ups=0.72, wpb=110.4, bsz=39.4, num_updates=3410, lr=4.38615e-05, gnorm=0.889, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7660
2023-02-22 13:41:01 - progress_bar.py[line:274] - INFO: epoch 002:     12 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103, ups=0.91, wpb=113.4, bsz=40, num_updates=3420, lr=4.38358e-05, gnorm=1.007, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7671
2023-02-22 13:41:12 - progress_bar.py[line:274] - INFO: epoch 002:     22 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=112.5, bsz=40, num_updates=3430, lr=4.38101e-05, gnorm=1.027, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7682
2023-02-22 13:41:23 - progress_bar.py[line:274] - INFO: epoch 002:     32 / 3411 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98, ups=0.88, wpb=111.8, bsz=40, num_updates=3440, lr=4.37844e-05, gnorm=1.252, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7694
2023-02-22 13:41:35 - progress_bar.py[line:274] - INFO: epoch 002:     42 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.87, wpb=112.2, bsz=40, num_updates=3450, lr=4.37587e-05, gnorm=0.775, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7705
2023-02-22 13:41:46 - progress_bar.py[line:274] - INFO: epoch 002:     52 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=3460, lr=4.3733e-05, gnorm=1.434, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7716
2023-02-22 13:41:58 - progress_bar.py[line:274] - INFO: epoch 002:     62 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.88, wpb=112.5, bsz=40, num_updates=3470, lr=4.37072e-05, gnorm=1.235, clip=60, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7728
2023-02-22 13:42:09 - progress_bar.py[line:274] - INFO: epoch 002:     72 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.91, wpb=112.9, bsz=40, num_updates=3480, lr=4.36815e-05, gnorm=0.904, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7739
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 13:42:20 - progress_bar.py[line:274] - INFO: epoch 002:     82 / 3411 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.5, ups=0.91, wpb=111.2, bsz=40, num_updates=3490, lr=4.36558e-05, gnorm=1.332, clip=60, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=7750
2023-02-22 13:42:31 - progress_bar.py[line:274] - INFO: epoch 002:     92 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.88, wpb=111.8, bsz=40, num_updates=3500, lr=4.36301e-05, gnorm=1.112, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7761
2023-02-22 13:42:42 - progress_bar.py[line:274] - INFO: epoch 002:    102 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.91, wpb=113.3, bsz=40, num_updates=3510, lr=4.36044e-05, gnorm=0.963, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7772
2023-02-22 13:42:53 - progress_bar.py[line:274] - INFO: epoch 002:    112 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.91, wpb=111.8, bsz=40, num_updates=3520, lr=4.35787e-05, gnorm=1.051, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7783
2023-02-22 13:43:04 - progress_bar.py[line:274] - INFO: epoch 002:    122 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.89, wpb=113.2, bsz=40, num_updates=3530, lr=4.35529e-05, gnorm=0.935, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7794
2023-02-22 13:43:16 - progress_bar.py[line:274] - INFO: epoch 002:    132 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=111.5, bsz=40, num_updates=3540, lr=4.35272e-05, gnorm=1.156, clip=60, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=7806
2023-02-22 13:43:27 - progress_bar.py[line:274] - INFO: epoch 002:    142 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=111.4, bsz=40, num_updates=3550, lr=4.35015e-05, gnorm=1.249, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7817
2023-02-22 13:43:38 - progress_bar.py[line:274] - INFO: epoch 002:    152 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=3560, lr=4.34758e-05, gnorm=0.928, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7828
2023-02-22 13:43:50 - progress_bar.py[line:274] - INFO: epoch 002:    162 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.87, wpb=112.1, bsz=40, num_updates=3570, lr=4.34501e-05, gnorm=0.912, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7840
2023-02-22 13:44:01 - progress_bar.py[line:274] - INFO: epoch 002:    172 / 3411 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=3580, lr=4.34244e-05, gnorm=1.547, clip=80, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=7851
2023-02-22 13:44:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 13:44:13 - progress_bar.py[line:274] - INFO: epoch 002:    183 / 3411 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=92.6, ups=0.83, wpb=112.1, bsz=40, num_updates=3590, lr=4.33987e-05, gnorm=1.276, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7863
2023-02-22 13:44:24 - progress_bar.py[line:274] - INFO: epoch 002:    193 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=111.8, bsz=40, num_updates=3600, lr=4.33729e-05, gnorm=0.948, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7874
2023-02-22 13:44:35 - progress_bar.py[line:274] - INFO: epoch 002:    203 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=3610, lr=4.33472e-05, gnorm=0.955, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7885
2023-02-22 13:44:46 - progress_bar.py[line:274] - INFO: epoch 002:    213 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=3620, lr=4.33215e-05, gnorm=0.939, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7896
2023-02-22 13:44:57 - progress_bar.py[line:274] - INFO: epoch 002:    223 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.5, ups=0.92, wpb=111.8, bsz=40, num_updates=3630, lr=4.32958e-05, gnorm=0.99, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7907
2023-02-22 13:45:08 - progress_bar.py[line:274] - INFO: epoch 002:    233 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.2, ups=0.91, wpb=113.9, bsz=40, num_updates=3640, lr=4.32701e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7918
2023-02-22 13:45:19 - progress_bar.py[line:274] - INFO: epoch 002:    243 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=3650, lr=4.32444e-05, gnorm=0.925, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7929
2023-02-22 13:45:30 - progress_bar.py[line:274] - INFO: epoch 002:    253 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=112, bsz=40, num_updates=3660, lr=4.32186e-05, gnorm=1.097, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7940
2023-02-22 13:45:41 - progress_bar.py[line:274] - INFO: epoch 002:    263 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=3670, lr=4.31929e-05, gnorm=0.934, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7952
2023-02-22 13:45:52 - progress_bar.py[line:274] - INFO: epoch 002:    273 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=3680, lr=4.31672e-05, gnorm=1.397, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7963
2023-02-22 13:46:03 - progress_bar.py[line:274] - INFO: epoch 002:    283 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.91, wpb=112, bsz=40, num_updates=3690, lr=4.31415e-05, gnorm=0.911, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7974
2023-02-22 13:46:15 - progress_bar.py[line:274] - INFO: epoch 002:    293 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=3700, lr=4.31158e-05, gnorm=1.013, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7985
2023-02-22 13:46:26 - progress_bar.py[line:274] - INFO: epoch 002:    303 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.91, wpb=111.1, bsz=40, num_updates=3710, lr=4.30901e-05, gnorm=0.979, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7996
2023-02-22 13:46:37 - progress_bar.py[line:274] - INFO: epoch 002:    313 / 3411 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.9, wpb=112, bsz=40, num_updates=3720, lr=4.30643e-05, gnorm=1.114, clip=50, loss_scale=512, train_wall=11, gb_free=9.4, ema_decay=0.9999, wall=8007
2023-02-22 13:46:48 - progress_bar.py[line:274] - INFO: epoch 002:    323 / 3411 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.92, wpb=110.7, bsz=40, num_updates=3730, lr=4.30386e-05, gnorm=1.003, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8018
2023-02-22 13:46:59 - progress_bar.py[line:274] - INFO: epoch 002:    333 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.91, wpb=113.2, bsz=40, num_updates=3740, lr=4.30129e-05, gnorm=0.918, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8029
2023-02-22 13:47:10 - progress_bar.py[line:274] - INFO: epoch 002:    343 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102, ups=0.91, wpb=112.5, bsz=40, num_updates=3750, lr=4.29872e-05, gnorm=1.135, clip=50, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=8040
2023-02-22 13:47:21 - progress_bar.py[line:274] - INFO: epoch 002:    353 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.91, wpb=113.4, bsz=40, num_updates=3760, lr=4.29615e-05, gnorm=0.99, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8051
2023-02-22 13:47:32 - progress_bar.py[line:274] - INFO: epoch 002:    363 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.89, wpb=113.1, bsz=40, num_updates=3770, lr=4.29358e-05, gnorm=0.898, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8062
2023-02-22 13:47:43 - progress_bar.py[line:274] - INFO: epoch 002:    373 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.9, wpb=112.5, bsz=40, num_updates=3780, lr=4.291e-05, gnorm=1.119, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8073
2023-02-22 13:47:54 - progress_bar.py[line:274] - INFO: epoch 002:    383 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.1, ups=0.87, wpb=110.7, bsz=40, num_updates=3790, lr=4.28843e-05, gnorm=1.192, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8085
2023-02-22 13:48:06 - progress_bar.py[line:274] - INFO: epoch 002:    393 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=112.5, bsz=40, num_updates=3800, lr=4.28586e-05, gnorm=1, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8096
2023-02-22 13:48:17 - progress_bar.py[line:274] - INFO: epoch 002:    403 / 3411 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.9, wpb=111.8, bsz=40, num_updates=3810, lr=4.28329e-05, gnorm=1.327, clip=80, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=8107
2023-02-22 13:48:28 - progress_bar.py[line:274] - INFO: epoch 002:    413 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.9, wpb=110.9, bsz=40, num_updates=3820, lr=4.28072e-05, gnorm=1.175, clip=60, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=8118
2023-02-22 13:48:39 - progress_bar.py[line:274] - INFO: epoch 002:    423 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=111.4, bsz=40, num_updates=3830, lr=4.27815e-05, gnorm=1.047, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8129
2023-02-22 13:48:50 - progress_bar.py[line:274] - INFO: epoch 002:    433 / 3411 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=3840, lr=4.27557e-05, gnorm=1.092, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8141
2023-02-22 13:49:02 - progress_bar.py[line:274] - INFO: epoch 002:    443 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.9, wpb=112.8, bsz=40, num_updates=3850, lr=4.273e-05, gnorm=1.143, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8152
2023-02-22 13:49:13 - progress_bar.py[line:274] - INFO: epoch 002:    453 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=3860, lr=4.27043e-05, gnorm=1.09, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8163
2023-02-22 13:49:24 - progress_bar.py[line:274] - INFO: epoch 002:    463 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103, ups=0.93, wpb=111.1, bsz=40, num_updates=3870, lr=4.26786e-05, gnorm=0.899, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8174
2023-02-22 13:49:35 - progress_bar.py[line:274] - INFO: epoch 002:    473 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.87, wpb=111, bsz=40, num_updates=3880, lr=4.26529e-05, gnorm=1.013, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8185
2023-02-22 13:49:46 - progress_bar.py[line:274] - INFO: epoch 002:    483 / 3411 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.5, ups=0.89, wpb=111.5, bsz=40, num_updates=3890, lr=4.26272e-05, gnorm=1.187, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8196
2023-02-22 13:49:57 - progress_bar.py[line:274] - INFO: epoch 002:    493 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=111.7, bsz=40, num_updates=3900, lr=4.26015e-05, gnorm=1.069, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8208
2023-02-22 13:50:09 - progress_bar.py[line:274] - INFO: epoch 002:    503 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=3910, lr=4.25757e-05, gnorm=1.189, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8219
2023-02-22 13:50:20 - progress_bar.py[line:274] - INFO: epoch 002:    513 / 3411 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=112.8, bsz=40, num_updates=3920, lr=4.255e-05, gnorm=1.302, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8230
2023-02-22 13:50:31 - progress_bar.py[line:274] - INFO: epoch 002:    523 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.93, wpb=111.5, bsz=40, num_updates=3930, lr=4.25243e-05, gnorm=0.956, clip=50, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=8241
2023-02-22 13:50:42 - progress_bar.py[line:274] - INFO: epoch 002:    533 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.87, wpb=111.1, bsz=40, num_updates=3940, lr=4.24986e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8252
2023-02-22 13:50:53 - progress_bar.py[line:274] - INFO: epoch 002:    543 / 3411 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.88, wpb=111.9, bsz=40, num_updates=3950, lr=4.24729e-05, gnorm=1.134, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8264
2023-02-22 13:51:05 - progress_bar.py[line:274] - INFO: epoch 002:    553 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112.6, bsz=40, num_updates=3960, lr=4.24472e-05, gnorm=0.924, clip=30, loss_scale=512, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=8275
2023-02-22 13:51:16 - progress_bar.py[line:274] - INFO: epoch 002:    563 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.8, bsz=40, num_updates=3970, lr=4.24214e-05, gnorm=0.899, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=8287
@@@@ ERROR IN DATA @@@@ play
2023-02-22 13:51:28 - progress_bar.py[line:274] - INFO: epoch 002:    573 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=3980, lr=4.23957e-05, gnorm=0.705, clip=20, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=8298
2023-02-22 13:51:39 - progress_bar.py[line:274] - INFO: epoch 002:    583 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112, bsz=40, num_updates=3990, lr=4.237e-05, gnorm=0.89, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8309
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:51:50 - progress_bar.py[line:274] - INFO: epoch 002:    593 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=4000, lr=4.23443e-05, gnorm=1.23, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8320
2023-02-22 13:51:50 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 13:51:52 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 13:51:52 - train.py[line:551] - INFO: load:1.03 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 13:53:55 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 13:53:55 - train.py[line:551] - INFO: load:1.06 valid_run:123.59 task_valid:120.51 collect_output:2.04
2023-02-22 13:55:56 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 13:55:56 - train.py[line:551] - INFO: load:1.08 valid_run:243.97 task_valid:236.78 collect_output:5.12
2023-02-22 13:57:59 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 13:57:59 - train.py[line:551] - INFO: load:1.10 valid_run:367.07 task_valid:353.65 collect_output:10.31
2023-02-22 14:00:01 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 14:00:01 - train.py[line:551] - INFO: load:1.13 valid_run:489.20 task_valid:467.80 collect_output:17.28
2023-02-22 14:02:02 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 14:02:02 - train.py[line:551] - INFO: load:1.15 valid_run:609.95 task_valid:585.51 collect_output:19.29
2023-02-22 14:04:05 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 14:04:05 - train.py[line:551] - INFO: load:1.17 valid_run:733.28 task_valid:704.58 collect_output:22.53
2023-02-22 14:06:09 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 14:06:09 - train.py[line:551] - INFO: load:1.20 valid_run:856.65 task_valid:823.18 collect_output:26.27
2023-02-22 14:08:11 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 14:08:11 - train.py[line:551] - INFO: load:1.22 valid_run:978.87 task_valid:940.25 collect_output:30.37
2023-02-22 14:10:15 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 14:10:15 - train.py[line:551] - INFO: load:1.25 valid_run:1102.89 task_valid:1057.92 collect_output:35.67
2023-02-22 14:12:17 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 14:12:17 - train.py[line:551] - INFO: load:1.27 valid_run:1224.77 task_valid:1170.99 collect_output:43.47
2023-02-22 14:14:17 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 14:14:17 - train.py[line:551] - INFO: load:1.30 valid_run:1345.23 task_valid:1287.01 collect_output:46.88
2023-02-22 14:16:20 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 14:16:20 - train.py[line:551] - INFO: load:1.32 valid_run:1467.35 task_valid:1404.40 collect_output:50.56
2023-02-22 14:18:19 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 14:18:19 - train.py[line:551] - INFO: load:1.34 valid_run:1586.68 task_valid:1518.63 collect_output:54.63
2023-02-22 14:20:21 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 14:20:21 - train.py[line:551] - INFO: load:1.37 valid_run:1708.15 task_valid:1636.94 collect_output:56.76
2023-02-22 14:22:22 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 14:22:22 - train.py[line:551] - INFO: load:1.39 valid_run:1829.46 task_valid:1753.58 collect_output:60.41
2023-02-22 14:24:24 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 14:24:24 - train.py[line:551] - INFO: load:1.42 valid_run:1951.07 task_valid:1868.02 collect_output:66.57
2023-02-22 14:26:25 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 14:26:25 - train.py[line:551] - INFO: load:1.44 valid_run:2072.78 task_valid:1984.53 collect_output:70.76
2023-02-22 14:28:26 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 14:28:26 - train.py[line:551] - INFO: load:1.47 valid_run:2193.86 task_valid:2102.79 collect_output:72.56
2023-02-22 14:30:28 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 14:30:28 - train.py[line:551] - INFO: load:1.49 valid_run:2315.48 task_valid:2220.21 collect_output:75.74
2023-02-22 14:32:29 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 14:32:29 - train.py[line:551] - INFO: load:1.52 valid_run:2436.10 task_valid:2337.19 collect_output:78.37
2023-02-22 14:34:31 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 14:34:31 - train.py[line:551] - INFO: load:1.54 valid_run:2558.24 task_valid:2454.18 collect_output:82.50
2023-02-22 14:36:33 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 14:36:33 - train.py[line:551] - INFO: load:1.57 valid_run:2680.52 task_valid:2573.51 collect_output:84.45
2023-02-22 14:38:34 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 14:38:34 - train.py[line:551] - INFO: load:1.59 valid_run:2800.88 task_valid:2688.12 collect_output:89.16
2023-02-22 14:40:34 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 14:40:34 - train.py[line:551] - INFO: load:1.61 valid_run:2920.87 task_valid:2804.77 collect_output:91.47
2023-02-22 14:42:36 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 14:42:36 - train.py[line:551] - INFO: load:1.64 valid_run:3042.79 task_valid:2921.45 collect_output:95.67
2023-02-22 14:44:39 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 14:44:39 - train.py[line:551] - INFO: load:1.66 valid_run:3165.87 task_valid:3037.90 collect_output:101.29
2023-02-22 14:46:39 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 14:46:39 - train.py[line:551] - INFO: load:1.69 valid_run:3285.66 task_valid:3152.39 collect_output:105.58
2023-02-22 14:48:41 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 14:48:41 - train.py[line:551] - INFO: load:1.71 valid_run:3407.93 task_valid:3272.26 collect_output:106.97
2023-02-22 14:50:43 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 14:50:43 - train.py[line:551] - INFO: load:1.74 valid_run:3529.94 task_valid:3388.31 collect_output:111.92
2023-02-22 14:52:46 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 14:52:46 - train.py[line:551] - INFO: load:1.76 valid_run:3652.21 task_valid:3507.21 collect_output:114.27
2023-02-22 14:54:47 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 14:54:47 - train.py[line:551] - INFO: load:1.79 valid_run:3773.81 task_valid:3626.07 collect_output:115.95

====================================================================================================
SGG eval:     R @ 50: 0.6116;     R @ 100: 0.6618;     R @ 500: 0.6996;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3996;    mR @ 100: 0.4499;    mR @ 500: 0.5023;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6829) (covered in:0.4375) (covering:0.2857) (eating:0.7059) (flying in:0.8182) (growing on:0.2500) (hanging from:0.5323) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6434) (standing on:0.5727) (using:0.3500) (walking in:0.0000) (walking on:0.5676) (watching:0.4167) 
--------------------------------------------------------
====================================================================================================

2023-02-22 14:55:18 - train.py[line:487] - INFO: 0.6617780494015788
2023-02-22 14:55:18 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6116;     R @ 100: 0.6618;     R @ 500: 0.6996;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3996;    mR @ 100: 0.4499;    mR @ 500: 0.5023;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6829) (covered in:0.4375) (covering:0.2857) (eating:0.7059) (flying in:0.8182) (growing on:0.2500) (hanging from:0.5323) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6434) (standing on:0.5727) (using:0.3500) (walking in:0.0000) (walking on:0.5676) (watching:0.4167) 
--------------------------------------------------------
====================================================================================================

2023-02-22 14:55:18 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.254 | loss_v1 0 | loss_v2 0 | nll_loss 0.09 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.661778 | ppl 1.06 | vqa_score 0.4595 | wps 117.9 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.661778
2023-02-22 14:55:18 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 4000 updates
2023-02-22 14:55:18 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_2_4000.pt
2023-02-22 14:55:24 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_2_4000.pt
2023-02-22 14:55:28 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_2_4000.pt (epoch 2 @ 4000 updates, score 0.6617780494015788) (writing took 10.37537232413888 seconds)
2023-02-22 14:55:40 - progress_bar.py[line:274] - INFO: epoch 002:    603 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.3, ups=0, wpb=113.2, bsz=40, num_updates=4010, lr=4.23186e-05, gnorm=0.998, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12150
@@@@ ERROR IN DATA @@@@ play
2023-02-22 14:55:51 - progress_bar.py[line:274] - INFO: epoch 002:    613 / 3411 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=4020, lr=4.22929e-05, gnorm=1.54, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12161
2023-02-22 14:56:02 - progress_bar.py[line:274] - INFO: epoch 002:    623 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=111.5, bsz=40, num_updates=4030, lr=4.22671e-05, gnorm=1.609, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12172
2023-02-22 14:56:13 - progress_bar.py[line:274] - INFO: epoch 002:    633 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.89, wpb=113.2, bsz=40, num_updates=4040, lr=4.22414e-05, gnorm=0.982, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=12184
2023-02-22 14:56:25 - progress_bar.py[line:274] - INFO: epoch 002:    643 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=111.9, bsz=40, num_updates=4050, lr=4.22157e-05, gnorm=0.974, clip=60, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=12195
2023-02-22 14:56:36 - progress_bar.py[line:274] - INFO: epoch 002:    653 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.1, ups=0.92, wpb=112.2, bsz=40, num_updates=4060, lr=4.219e-05, gnorm=1.328, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12206
2023-02-22 14:56:47 - progress_bar.py[line:274] - INFO: epoch 002:    663 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112, bsz=40, num_updates=4070, lr=4.21643e-05, gnorm=0.843, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12217
2023-02-22 14:56:58 - progress_bar.py[line:274] - INFO: epoch 002:    673 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=4080, lr=4.21386e-05, gnorm=1.122, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12228
2023-02-22 14:57:09 - progress_bar.py[line:274] - INFO: epoch 002:    683 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=4090, lr=4.21128e-05, gnorm=1.457, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12239
2023-02-22 14:57:20 - progress_bar.py[line:274] - INFO: epoch 002:    693 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.7, ups=0.91, wpb=113.4, bsz=40, num_updates=4100, lr=4.20871e-05, gnorm=1.125, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12250
2023-02-22 14:57:31 - progress_bar.py[line:274] - INFO: epoch 002:    703 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=112.3, bsz=40, num_updates=4110, lr=4.20614e-05, gnorm=0.834, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12261
2023-02-22 14:57:42 - progress_bar.py[line:274] - INFO: epoch 002:    713 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.89, wpb=112.5, bsz=40, num_updates=4120, lr=4.20357e-05, gnorm=1.01, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12273
2023-02-22 14:57:54 - progress_bar.py[line:274] - INFO: epoch 002:    723 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.89, wpb=112.4, bsz=40, num_updates=4130, lr=4.201e-05, gnorm=1.169, clip=40, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=12284
2023-02-22 14:58:05 - progress_bar.py[line:274] - INFO: epoch 002:    733 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=4140, lr=4.19843e-05, gnorm=0.804, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12295
2023-02-22 14:58:16 - progress_bar.py[line:274] - INFO: epoch 002:    743 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.93, wpb=110.2, bsz=40, num_updates=4150, lr=4.19585e-05, gnorm=1.183, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12306
2023-02-22 14:58:27 - progress_bar.py[line:274] - INFO: epoch 002:    753 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.9, wpb=112.2, bsz=40, num_updates=4160, lr=4.19328e-05, gnorm=1.456, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12317
2023-02-22 14:58:38 - progress_bar.py[line:274] - INFO: epoch 002:    763 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=4170, lr=4.19071e-05, gnorm=1.102, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12328
2023-02-22 14:58:49 - progress_bar.py[line:274] - INFO: epoch 002:    773 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=4180, lr=4.18814e-05, gnorm=0.868, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12339
2023-02-22 14:59:00 - progress_bar.py[line:274] - INFO: epoch 002:    783 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.6, ups=0.92, wpb=113.1, bsz=40, num_updates=4190, lr=4.18557e-05, gnorm=0.999, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12350
2023-02-22 14:59:11 - progress_bar.py[line:274] - INFO: epoch 002:    793 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=110.8, bsz=40, num_updates=4200, lr=4.183e-05, gnorm=1.312, clip=70, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12362
2023-02-22 14:59:22 - progress_bar.py[line:274] - INFO: epoch 002:    803 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103, ups=0.91, wpb=112.8, bsz=40, num_updates=4210, lr=4.18042e-05, gnorm=1.106, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12373
2023-02-22 14:59:34 - progress_bar.py[line:274] - INFO: epoch 002:    813 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.88, wpb=112, bsz=40, num_updates=4220, lr=4.17785e-05, gnorm=0.93, clip=50, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12384
2023-02-22 14:59:45 - progress_bar.py[line:274] - INFO: epoch 002:    823 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.3, ups=0.93, wpb=113.5, bsz=40, num_updates=4230, lr=4.17528e-05, gnorm=1.315, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12395
2023-02-22 14:59:56 - progress_bar.py[line:274] - INFO: epoch 002:    833 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=111.3, bsz=40, num_updates=4240, lr=4.17271e-05, gnorm=0.891, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12406
2023-02-22 15:00:07 - progress_bar.py[line:274] - INFO: epoch 002:    843 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.89, wpb=112.8, bsz=40, num_updates=4250, lr=4.17014e-05, gnorm=0.857, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12417
2023-02-22 15:00:18 - progress_bar.py[line:274] - INFO: epoch 002:    853 / 3411 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.9, wpb=110.2, bsz=40, num_updates=4260, lr=4.16757e-05, gnorm=1.279, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12428
2023-02-22 15:00:29 - progress_bar.py[line:274] - INFO: epoch 002:    863 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=112.4, bsz=40, num_updates=4270, lr=4.165e-05, gnorm=1.045, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12439
2023-02-22 15:00:40 - progress_bar.py[line:274] - INFO: epoch 002:    873 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.9, ups=0.93, wpb=112, bsz=40, num_updates=4280, lr=4.16242e-05, gnorm=0.99, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12450
2023-02-22 15:00:51 - progress_bar.py[line:274] - INFO: epoch 002:    883 / 3411 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.9, wpb=112.3, bsz=40, num_updates=4290, lr=4.15985e-05, gnorm=1.091, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12461
2023-02-22 15:01:02 - progress_bar.py[line:274] - INFO: epoch 002:    893 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.1, bsz=40, num_updates=4300, lr=4.15728e-05, gnorm=0.829, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12472
2023-02-22 15:01:13 - progress_bar.py[line:274] - INFO: epoch 002:    903 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=4310, lr=4.15471e-05, gnorm=0.896, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12484
2023-02-22 15:01:25 - progress_bar.py[line:274] - INFO: epoch 002:    913 / 3411 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=4320, lr=4.15214e-05, gnorm=0.919, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12495
2023-02-22 15:01:36 - progress_bar.py[line:274] - INFO: epoch 002:    923 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=4330, lr=4.14957e-05, gnorm=0.821, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12506
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 15:01:47 - progress_bar.py[line:274] - INFO: epoch 002:    933 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=4340, lr=4.14699e-05, gnorm=1.144, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12517
2023-02-22 15:01:58 - progress_bar.py[line:274] - INFO: epoch 002:    943 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=4350, lr=4.14442e-05, gnorm=0.922, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12528
2023-02-22 15:02:10 - progress_bar.py[line:274] - INFO: epoch 002:    953 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.2, ups=0.87, wpb=111.8, bsz=40, num_updates=4360, lr=4.14185e-05, gnorm=1.102, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12540
2023-02-22 15:02:21 - progress_bar.py[line:274] - INFO: epoch 002:    963 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.9, wpb=113.7, bsz=40, num_updates=4370, lr=4.13928e-05, gnorm=0.676, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12551
2023-02-22 15:02:32 - progress_bar.py[line:274] - INFO: epoch 002:    973 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.89, wpb=110.9, bsz=40, num_updates=4380, lr=4.13671e-05, gnorm=1.12, clip=60, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=12562
2023-02-22 15:02:43 - progress_bar.py[line:274] - INFO: epoch 002:    983 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=4390, lr=4.13414e-05, gnorm=1.15, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12573
2023-02-22 15:02:54 - progress_bar.py[line:274] - INFO: epoch 002:    993 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.9, wpb=113.3, bsz=40, num_updates=4400, lr=4.13156e-05, gnorm=0.89, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12584
2023-02-22 15:03:06 - progress_bar.py[line:274] - INFO: epoch 002:   1003 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.9, wpb=110.6, bsz=40, num_updates=4410, lr=4.12899e-05, gnorm=1.029, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12596
2023-02-22 15:03:17 - progress_bar.py[line:274] - INFO: epoch 002:   1013 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=112.2, bsz=40, num_updates=4420, lr=4.12642e-05, gnorm=0.747, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12607
2023-02-22 15:03:28 - progress_bar.py[line:274] - INFO: epoch 002:   1023 / 3411 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.89, wpb=112.6, bsz=40, num_updates=4430, lr=4.12385e-05, gnorm=1.089, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12618
2023-02-22 15:03:39 - progress_bar.py[line:274] - INFO: epoch 002:   1033 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=112.1, bsz=40, num_updates=4440, lr=4.12128e-05, gnorm=1.222, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12629
2023-02-22 15:03:50 - progress_bar.py[line:274] - INFO: epoch 002:   1043 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=4450, lr=4.11871e-05, gnorm=1.077, clip=40, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=12640
2023-02-22 15:04:02 - progress_bar.py[line:274] - INFO: epoch 002:   1053 / 3411 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=4460, lr=4.11613e-05, gnorm=1.066, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12652
2023-02-22 15:04:13 - progress_bar.py[line:274] - INFO: epoch 002:   1063 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.91, wpb=111.5, bsz=40, num_updates=4470, lr=4.11356e-05, gnorm=1.148, clip=50, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=12663
2023-02-22 15:04:24 - progress_bar.py[line:274] - INFO: epoch 002:   1073 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.89, wpb=110.9, bsz=40, num_updates=4480, lr=4.11099e-05, gnorm=0.636, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12674
2023-02-22 15:04:35 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 15:04:36 - progress_bar.py[line:274] - INFO: epoch 002:   1084 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=91.7, ups=0.82, wpb=111.3, bsz=40, num_updates=4490, lr=4.10842e-05, gnorm=1.039, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=12686
2023-02-22 15:04:47 - progress_bar.py[line:274] - INFO: epoch 002:   1094 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.91, wpb=110.8, bsz=40, num_updates=4500, lr=4.10585e-05, gnorm=1.012, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12697
2023-02-22 15:04:58 - progress_bar.py[line:274] - INFO: epoch 002:   1104 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=4510, lr=4.10328e-05, gnorm=1.12, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12708
2023-02-22 15:05:10 - progress_bar.py[line:274] - INFO: epoch 002:   1114 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=112.2, bsz=40, num_updates=4520, lr=4.1007e-05, gnorm=1.112, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12720
2023-02-22 15:05:21 - progress_bar.py[line:274] - INFO: epoch 002:   1124 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.87, wpb=111.9, bsz=40, num_updates=4530, lr=4.09813e-05, gnorm=0.877, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12731
2023-02-22 15:05:32 - progress_bar.py[line:274] - INFO: epoch 002:   1134 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.89, wpb=112.2, bsz=40, num_updates=4540, lr=4.09556e-05, gnorm=1.317, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12743
2023-02-22 15:05:43 - progress_bar.py[line:274] - INFO: epoch 002:   1144 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.91, wpb=110.9, bsz=40, num_updates=4550, lr=4.09299e-05, gnorm=1.358, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12754
2023-02-22 15:05:54 - progress_bar.py[line:274] - INFO: epoch 002:   1154 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=4560, lr=4.09042e-05, gnorm=1.19, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12765
2023-02-22 15:06:06 - progress_bar.py[line:274] - INFO: epoch 002:   1164 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.2, bsz=40, num_updates=4570, lr=4.08785e-05, gnorm=0.826, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12776
2023-02-22 15:06:17 - progress_bar.py[line:274] - INFO: epoch 002:   1174 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=111.4, bsz=40, num_updates=4580, lr=4.08527e-05, gnorm=0.926, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12787
2023-02-22 15:06:28 - progress_bar.py[line:274] - INFO: epoch 002:   1184 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=4590, lr=4.0827e-05, gnorm=1.093, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12798
2023-02-22 15:06:39 - progress_bar.py[line:274] - INFO: epoch 002:   1194 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=111.8, bsz=40, num_updates=4600, lr=4.08013e-05, gnorm=1.26, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12809
2023-02-22 15:06:50 - progress_bar.py[line:274] - INFO: epoch 002:   1204 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.87, wpb=112.3, bsz=40, num_updates=4610, lr=4.07756e-05, gnorm=1.472, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12820
2023-02-22 15:07:02 - progress_bar.py[line:274] - INFO: epoch 002:   1214 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=111.8, bsz=40, num_updates=4620, lr=4.07499e-05, gnorm=1.141, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12832
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:07:13 - progress_bar.py[line:274] - INFO: epoch 002:   1224 / 3411 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=4630, lr=4.07242e-05, gnorm=0.96, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12843
2023-02-22 15:07:23 - progress_bar.py[line:274] - INFO: epoch 002:   1234 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.7, ups=0.94, wpb=112.4, bsz=40, num_updates=4640, lr=4.06985e-05, gnorm=0.857, clip=30, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=12853
2023-02-22 15:07:34 - progress_bar.py[line:274] - INFO: epoch 002:   1244 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.4, ups=0.91, wpb=113.3, bsz=40, num_updates=4650, lr=4.06727e-05, gnorm=0.909, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=12864
2023-02-22 15:07:46 - progress_bar.py[line:274] - INFO: epoch 002:   1254 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.88, wpb=112.4, bsz=40, num_updates=4660, lr=4.0647e-05, gnorm=1.232, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12876
2023-02-22 15:07:57 - progress_bar.py[line:274] - INFO: epoch 002:   1264 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.91, wpb=112.3, bsz=40, num_updates=4670, lr=4.06213e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12887
2023-02-22 15:08:08 - progress_bar.py[line:274] - INFO: epoch 002:   1274 / 3411 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.89, wpb=110.3, bsz=40, num_updates=4680, lr=4.05956e-05, gnorm=0.959, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12898
2023-02-22 15:08:19 - progress_bar.py[line:274] - INFO: epoch 002:   1284 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.92, wpb=112.6, bsz=40, num_updates=4690, lr=4.05699e-05, gnorm=1.04, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12909
2023-02-22 15:08:30 - progress_bar.py[line:274] - INFO: epoch 002:   1294 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.92, wpb=112.1, bsz=40, num_updates=4700, lr=4.05442e-05, gnorm=0.993, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12920
2023-02-22 15:08:41 - progress_bar.py[line:274] - INFO: epoch 002:   1304 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=4710, lr=4.05184e-05, gnorm=1.031, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12931
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:08:52 - progress_bar.py[line:274] - INFO: epoch 002:   1314 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.91, wpb=111.8, bsz=40, num_updates=4720, lr=4.04927e-05, gnorm=0.956, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12942
2023-02-22 15:09:03 - progress_bar.py[line:274] - INFO: epoch 002:   1324 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.93, wpb=111.5, bsz=40, num_updates=4730, lr=4.0467e-05, gnorm=0.856, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12953
2023-02-22 15:09:14 - progress_bar.py[line:274] - INFO: epoch 002:   1334 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.87, wpb=111.6, bsz=40, num_updates=4740, lr=4.04413e-05, gnorm=0.998, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12964
2023-02-22 15:09:25 - progress_bar.py[line:274] - INFO: epoch 002:   1344 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=4750, lr=4.04156e-05, gnorm=1.105, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12975
2023-02-22 15:09:36 - progress_bar.py[line:274] - INFO: epoch 002:   1354 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.94, wpb=110.5, bsz=40, num_updates=4760, lr=4.03899e-05, gnorm=1.48, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12986
2023-02-22 15:09:47 - progress_bar.py[line:274] - INFO: epoch 002:   1364 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.91, wpb=110.4, bsz=40, num_updates=4770, lr=4.03641e-05, gnorm=1.063, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12997
2023-02-22 15:09:58 - progress_bar.py[line:274] - INFO: epoch 002:   1374 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.9, wpb=112.3, bsz=40, num_updates=4780, lr=4.03384e-05, gnorm=1.027, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13008
2023-02-22 15:10:09 - progress_bar.py[line:274] - INFO: epoch 002:   1384 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.9, wpb=112, bsz=40, num_updates=4790, lr=4.03127e-05, gnorm=0.842, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13019
2023-02-22 15:10:20 - progress_bar.py[line:274] - INFO: epoch 002:   1394 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=4800, lr=4.0287e-05, gnorm=0.814, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13030
2023-02-22 15:10:31 - progress_bar.py[line:274] - INFO: epoch 002:   1404 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.88, wpb=112.5, bsz=40, num_updates=4810, lr=4.02613e-05, gnorm=1.024, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13042
2023-02-22 15:10:43 - progress_bar.py[line:274] - INFO: epoch 002:   1414 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=4820, lr=4.02356e-05, gnorm=0.839, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13053
2023-02-22 15:10:54 - progress_bar.py[line:274] - INFO: epoch 002:   1424 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.88, wpb=111.5, bsz=40, num_updates=4830, lr=4.02098e-05, gnorm=0.998, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13064
2023-02-22 15:11:05 - progress_bar.py[line:274] - INFO: epoch 002:   1434 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112.2, bsz=40, num_updates=4840, lr=4.01841e-05, gnorm=0.999, clip=30, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=13075
2023-02-22 15:11:17 - progress_bar.py[line:274] - INFO: epoch 002:   1444 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=4850, lr=4.01584e-05, gnorm=1.145, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13087
2023-02-22 15:11:28 - progress_bar.py[line:274] - INFO: epoch 002:   1454 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=4860, lr=4.01327e-05, gnorm=0.712, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13098
2023-02-22 15:11:38 - progress_bar.py[line:274] - INFO: epoch 002:   1464 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.7, ups=0.94, wpb=112.9, bsz=40, num_updates=4870, lr=4.0107e-05, gnorm=1.097, clip=50, loss_scale=512, train_wall=11, gb_free=9.4, ema_decay=0.9999, wall=13109
2023-02-22 15:11:50 - progress_bar.py[line:274] - INFO: epoch 002:   1474 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=111.1, bsz=40, num_updates=4880, lr=4.00813e-05, gnorm=1.019, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13120
2023-02-22 15:12:01 - progress_bar.py[line:274] - INFO: epoch 002:   1484 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.88, wpb=112.7, bsz=40, num_updates=4890, lr=4.00555e-05, gnorm=0.75, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13131
2023-02-22 15:12:12 - progress_bar.py[line:274] - INFO: epoch 002:   1494 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=112, bsz=40, num_updates=4900, lr=4.00298e-05, gnorm=0.709, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13142
2023-02-22 15:12:23 - progress_bar.py[line:274] - INFO: epoch 002:   1504 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=4910, lr=4.00041e-05, gnorm=1.094, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13154
2023-02-22 15:12:35 - progress_bar.py[line:274] - INFO: epoch 002:   1514 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.88, wpb=111.2, bsz=40, num_updates=4920, lr=3.99784e-05, gnorm=0.884, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13165
2023-02-22 15:12:46 - progress_bar.py[line:274] - INFO: epoch 002:   1524 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=4930, lr=3.99527e-05, gnorm=0.927, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13176
2023-02-22 15:12:57 - progress_bar.py[line:274] - INFO: epoch 002:   1534 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.6, ups=0.91, wpb=112.2, bsz=40, num_updates=4940, lr=3.9927e-05, gnorm=0.996, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13187
2023-02-22 15:13:08 - progress_bar.py[line:274] - INFO: epoch 002:   1544 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=4950, lr=3.99012e-05, gnorm=1.026, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13198
2023-02-22 15:13:19 - progress_bar.py[line:274] - INFO: epoch 002:   1554 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=111.3, bsz=40, num_updates=4960, lr=3.98755e-05, gnorm=1.034, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13210
2023-02-22 15:13:31 - progress_bar.py[line:274] - INFO: epoch 002:   1564 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=4970, lr=3.98498e-05, gnorm=0.885, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13221
2023-02-22 15:13:42 - progress_bar.py[line:274] - INFO: epoch 002:   1574 / 3411 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.87, wpb=111.9, bsz=40, num_updates=4980, lr=3.98241e-05, gnorm=1.105, clip=40, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=13232
2023-02-22 15:13:54 - progress_bar.py[line:274] - INFO: epoch 002:   1584 / 3411 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.89, wpb=111.5, bsz=40, num_updates=4990, lr=3.97984e-05, gnorm=1.34, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13244
2023-02-22 15:14:05 - progress_bar.py[line:274] - INFO: epoch 002:   1594 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.89, wpb=113.6, bsz=40, num_updates=5000, lr=3.97727e-05, gnorm=0.733, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13255
2023-02-22 15:14:16 - progress_bar.py[line:274] - INFO: epoch 002:   1604 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.88, wpb=112.4, bsz=40, num_updates=5010, lr=3.9747e-05, gnorm=0.921, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13266
2023-02-22 15:14:27 - progress_bar.py[line:274] - INFO: epoch 002:   1614 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=112.9, bsz=40, num_updates=5020, lr=3.97212e-05, gnorm=0.908, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13277
2023-02-22 15:14:38 - progress_bar.py[line:274] - INFO: epoch 002:   1624 / 3411 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.1, ups=0.92, wpb=112.7, bsz=40, num_updates=5030, lr=3.96955e-05, gnorm=1.177, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13288
2023-02-22 15:14:49 - progress_bar.py[line:274] - INFO: epoch 002:   1634 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.89, wpb=112.7, bsz=40, num_updates=5040, lr=3.96698e-05, gnorm=0.901, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13300
2023-02-22 15:15:00 - progress_bar.py[line:274] - INFO: epoch 002:   1644 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=5050, lr=3.96441e-05, gnorm=0.876, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13311
2023-02-22 15:15:12 - progress_bar.py[line:274] - INFO: epoch 002:   1654 / 3411 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=5060, lr=3.96184e-05, gnorm=0.942, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13322
2023-02-22 15:15:23 - progress_bar.py[line:274] - INFO: epoch 002:   1664 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.92, wpb=110.5, bsz=40, num_updates=5070, lr=3.95927e-05, gnorm=0.902, clip=30, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=13333
2023-02-22 15:15:34 - progress_bar.py[line:274] - INFO: epoch 002:   1674 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=112, bsz=40, num_updates=5080, lr=3.95669e-05, gnorm=0.951, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13344
2023-02-22 15:15:45 - progress_bar.py[line:274] - INFO: epoch 002:   1684 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.6, ups=0.87, wpb=110.2, bsz=40, num_updates=5090, lr=3.95412e-05, gnorm=1.216, clip=50, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13356
2023-02-22 15:15:57 - progress_bar.py[line:274] - INFO: epoch 002:   1694 / 3411 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.9, wpb=111.2, bsz=40, num_updates=5100, lr=3.95155e-05, gnorm=1.22, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13367
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 15:16:08 - progress_bar.py[line:274] - INFO: epoch 002:   1704 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=5110, lr=3.94898e-05, gnorm=0.936, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13378
2023-02-22 15:16:19 - progress_bar.py[line:274] - INFO: epoch 002:   1714 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=5120, lr=3.94641e-05, gnorm=1.082, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13389
2023-02-22 15:16:30 - progress_bar.py[line:274] - INFO: epoch 002:   1724 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.5, bsz=40, num_updates=5130, lr=3.94384e-05, gnorm=0.95, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13400
2023-02-22 15:16:41 - progress_bar.py[line:274] - INFO: epoch 002:   1734 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=5140, lr=3.94126e-05, gnorm=1.037, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13411
2023-02-22 15:16:53 - progress_bar.py[line:274] - INFO: epoch 002:   1744 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=5150, lr=3.93869e-05, gnorm=0.993, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13423
2023-02-22 15:17:04 - progress_bar.py[line:274] - INFO: epoch 002:   1754 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=5160, lr=3.93612e-05, gnorm=0.932, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13434
2023-02-22 15:17:15 - progress_bar.py[line:274] - INFO: epoch 002:   1764 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.9, ups=0.93, wpb=111.8, bsz=40, num_updates=5170, lr=3.93355e-05, gnorm=1.253, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13445
2023-02-22 15:17:26 - progress_bar.py[line:274] - INFO: epoch 002:   1774 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.92, wpb=111.2, bsz=40, num_updates=5180, lr=3.93098e-05, gnorm=0.819, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13456
2023-02-22 15:17:37 - progress_bar.py[line:274] - INFO: epoch 002:   1784 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.9, wpb=109.8, bsz=40, num_updates=5190, lr=3.92841e-05, gnorm=0.801, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13467
2023-02-22 15:17:48 - progress_bar.py[line:274] - INFO: epoch 002:   1794 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.7, ups=0.92, wpb=112.4, bsz=40, num_updates=5200, lr=3.92583e-05, gnorm=0.782, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13478
2023-02-22 15:17:59 - progress_bar.py[line:274] - INFO: epoch 002:   1804 / 3411 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=5210, lr=3.92326e-05, gnorm=0.946, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13489
2023-02-22 15:18:10 - progress_bar.py[line:274] - INFO: epoch 002:   1814 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.92, wpb=112.9, bsz=40, num_updates=5220, lr=3.92069e-05, gnorm=0.88, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13500
2023-02-22 15:18:20 - progress_bar.py[line:274] - INFO: epoch 002:   1824 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.93, wpb=111.5, bsz=40, num_updates=5230, lr=3.91812e-05, gnorm=0.934, clip=30, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13511
2023-02-22 15:18:32 - progress_bar.py[line:274] - INFO: epoch 002:   1834 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=5240, lr=3.91555e-05, gnorm=0.946, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13522
2023-02-22 15:18:35 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 15:18:44 - progress_bar.py[line:274] - INFO: epoch 002:   1845 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=94.5, ups=0.84, wpb=111.8, bsz=40, num_updates=5250, lr=3.91298e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13534
2023-02-22 15:18:55 - progress_bar.py[line:274] - INFO: epoch 002:   1855 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.88, wpb=111.5, bsz=40, num_updates=5260, lr=3.9104e-05, gnorm=1.048, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13545
2023-02-22 15:19:06 - progress_bar.py[line:274] - INFO: epoch 002:   1865 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.92, wpb=113.3, bsz=40, num_updates=5270, lr=3.90783e-05, gnorm=0.909, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13556
2023-02-22 15:19:17 - progress_bar.py[line:274] - INFO: epoch 002:   1875 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.7, bsz=40, num_updates=5280, lr=3.90526e-05, gnorm=0.763, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13567
2023-02-22 15:19:28 - progress_bar.py[line:274] - INFO: epoch 002:   1885 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=5290, lr=3.90269e-05, gnorm=0.906, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13578
2023-02-22 15:19:40 - progress_bar.py[line:274] - INFO: epoch 002:   1895 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=5300, lr=3.90012e-05, gnorm=1.04, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13590
2023-02-22 15:19:51 - progress_bar.py[line:274] - INFO: epoch 002:   1905 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=5310, lr=3.89755e-05, gnorm=0.898, clip=30, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=13601
2023-02-22 15:20:02 - progress_bar.py[line:274] - INFO: epoch 002:   1915 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.91, wpb=111.4, bsz=40, num_updates=5320, lr=3.89498e-05, gnorm=1.273, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13612
2023-02-22 15:20:13 - progress_bar.py[line:274] - INFO: epoch 002:   1925 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112.1, bsz=40, num_updates=5330, lr=3.8924e-05, gnorm=1.175, clip=70, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13623
2023-02-22 15:20:24 - progress_bar.py[line:274] - INFO: epoch 002:   1935 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=5340, lr=3.88983e-05, gnorm=1.01, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13634
2023-02-22 15:20:35 - progress_bar.py[line:274] - INFO: epoch 002:   1945 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112.3, bsz=40, num_updates=5350, lr=3.88726e-05, gnorm=0.971, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13646
2023-02-22 15:20:46 - progress_bar.py[line:274] - INFO: epoch 002:   1955 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.93, wpb=111.5, bsz=40, num_updates=5360, lr=3.88469e-05, gnorm=0.92, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13656
2023-02-22 15:20:57 - progress_bar.py[line:274] - INFO: epoch 002:   1965 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.91, wpb=109.7, bsz=40, num_updates=5370, lr=3.88212e-05, gnorm=1.377, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13667
2023-02-22 15:21:08 - progress_bar.py[line:274] - INFO: epoch 002:   1975 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.9, ups=0.93, wpb=113.1, bsz=40, num_updates=5380, lr=3.87955e-05, gnorm=1.208, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13678
2023-02-22 15:21:19 - progress_bar.py[line:274] - INFO: epoch 002:   1985 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=5390, lr=3.87697e-05, gnorm=0.868, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13689
2023-02-22 15:21:30 - progress_bar.py[line:274] - INFO: epoch 002:   1995 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.9, wpb=112.5, bsz=40, num_updates=5400, lr=3.8744e-05, gnorm=0.916, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13701
2023-02-22 15:21:42 - progress_bar.py[line:274] - INFO: epoch 002:   2005 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.2, bsz=40, num_updates=5410, lr=3.87183e-05, gnorm=0.593, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13712
2023-02-22 15:21:53 - progress_bar.py[line:274] - INFO: epoch 002:   2015 / 3411 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=112.1, bsz=40, num_updates=5420, lr=3.86926e-05, gnorm=1.062, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13723
2023-02-22 15:22:04 - progress_bar.py[line:274] - INFO: epoch 002:   2025 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=112.3, bsz=40, num_updates=5430, lr=3.86669e-05, gnorm=0.825, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13734
2023-02-22 15:22:15 - progress_bar.py[line:274] - INFO: epoch 002:   2035 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.92, wpb=112.2, bsz=40, num_updates=5440, lr=3.86412e-05, gnorm=1.038, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13745
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:22:26 - progress_bar.py[line:274] - INFO: epoch 002:   2045 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=111.2, bsz=40, num_updates=5450, lr=3.86154e-05, gnorm=1.152, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13756
2023-02-22 15:22:37 - progress_bar.py[line:274] - INFO: epoch 002:   2055 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.9, wpb=113.8, bsz=40, num_updates=5460, lr=3.85897e-05, gnorm=0.804, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13767
2023-02-22 15:22:48 - progress_bar.py[line:274] - INFO: epoch 002:   2065 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=5470, lr=3.8564e-05, gnorm=0.87, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13779
2023-02-22 15:23:00 - progress_bar.py[line:274] - INFO: epoch 002:   2075 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=112, bsz=40, num_updates=5480, lr=3.85383e-05, gnorm=0.831, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13790
2023-02-22 15:23:11 - progress_bar.py[line:274] - INFO: epoch 002:   2085 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112.5, bsz=40, num_updates=5490, lr=3.85126e-05, gnorm=1.032, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13801
2023-02-22 15:23:23 - progress_bar.py[line:274] - INFO: epoch 002:   2095 / 3411 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.88, wpb=110.3, bsz=40, num_updates=5500, lr=3.84869e-05, gnorm=0.863, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13813
2023-02-22 15:23:34 - progress_bar.py[line:274] - INFO: epoch 002:   2105 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.88, wpb=111, bsz=40, num_updates=5510, lr=3.84611e-05, gnorm=0.863, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13824
2023-02-22 15:23:45 - progress_bar.py[line:274] - INFO: epoch 002:   2115 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=112, bsz=40, num_updates=5520, lr=3.84354e-05, gnorm=0.952, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13835
2023-02-22 15:23:56 - progress_bar.py[line:274] - INFO: epoch 002:   2125 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.9, wpb=113.1, bsz=40, num_updates=5530, lr=3.84097e-05, gnorm=1.138, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13846
2023-02-22 15:24:08 - progress_bar.py[line:274] - INFO: epoch 002:   2135 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.88, wpb=110.2, bsz=40, num_updates=5540, lr=3.8384e-05, gnorm=1.229, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13858
2023-02-22 15:24:19 - progress_bar.py[line:274] - INFO: epoch 002:   2145 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=5550, lr=3.83583e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13869
2023-02-22 15:24:30 - progress_bar.py[line:274] - INFO: epoch 002:   2155 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.5, bsz=40, num_updates=5560, lr=3.83326e-05, gnorm=0.785, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13880
2023-02-22 15:24:41 - progress_bar.py[line:274] - INFO: epoch 002:   2165 / 3411 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.7, ups=0.92, wpb=112.2, bsz=40, num_updates=5570, lr=3.83068e-05, gnorm=1.332, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13891
2023-02-22 15:24:53 - progress_bar.py[line:274] - INFO: epoch 002:   2175 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=112.1, bsz=40, num_updates=5580, lr=3.82811e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13903
2023-02-22 15:25:04 - progress_bar.py[line:274] - INFO: epoch 002:   2185 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.9, wpb=113.3, bsz=40, num_updates=5590, lr=3.82554e-05, gnorm=0.886, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=13914
2023-02-22 15:25:15 - progress_bar.py[line:274] - INFO: epoch 002:   2195 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=5600, lr=3.82297e-05, gnorm=0.895, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13925
2023-02-22 15:25:26 - progress_bar.py[line:274] - INFO: epoch 002:   2205 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.6, ups=0.87, wpb=111.2, bsz=40, num_updates=5610, lr=3.8204e-05, gnorm=1.33, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13937
2023-02-22 15:25:38 - progress_bar.py[line:274] - INFO: epoch 002:   2215 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.88, wpb=112.1, bsz=40, num_updates=5620, lr=3.81783e-05, gnorm=1.27, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13948
2023-02-22 15:25:49 - progress_bar.py[line:274] - INFO: epoch 002:   2225 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.1, bsz=40, num_updates=5630, lr=3.81525e-05, gnorm=1.109, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13959
2023-02-22 15:26:00 - progress_bar.py[line:274] - INFO: epoch 002:   2235 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=111, bsz=40, num_updates=5640, lr=3.81268e-05, gnorm=1.027, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13970
2023-02-22 15:26:11 - progress_bar.py[line:274] - INFO: epoch 002:   2245 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.91, wpb=111.4, bsz=40, num_updates=5650, lr=3.81011e-05, gnorm=0.914, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=13981
2023-02-22 15:26:22 - progress_bar.py[line:274] - INFO: epoch 002:   2255 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=111.5, bsz=40, num_updates=5660, lr=3.80754e-05, gnorm=1.162, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13992
2023-02-22 15:26:33 - progress_bar.py[line:274] - INFO: epoch 002:   2265 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=5670, lr=3.80497e-05, gnorm=1.258, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14004
2023-02-22 15:26:45 - progress_bar.py[line:274] - INFO: epoch 002:   2275 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=5680, lr=3.8024e-05, gnorm=1.093, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14015
2023-02-22 15:26:56 - progress_bar.py[line:274] - INFO: epoch 002:   2285 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=111.2, bsz=40, num_updates=5690, lr=3.79983e-05, gnorm=0.802, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14026
2023-02-22 15:27:07 - progress_bar.py[line:274] - INFO: epoch 002:   2295 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=5700, lr=3.79725e-05, gnorm=1.067, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14037
2023-02-22 15:27:18 - progress_bar.py[line:274] - INFO: epoch 002:   2305 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.89, wpb=112.6, bsz=40, num_updates=5710, lr=3.79468e-05, gnorm=0.971, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14048
2023-02-22 15:27:30 - progress_bar.py[line:274] - INFO: epoch 002:   2315 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.87, wpb=112.5, bsz=40, num_updates=5720, lr=3.79211e-05, gnorm=0.938, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14060
2023-02-22 15:27:41 - progress_bar.py[line:274] - INFO: epoch 002:   2325 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.9, wpb=112.4, bsz=40, num_updates=5730, lr=3.78954e-05, gnorm=1.056, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14071
2023-02-22 15:27:52 - progress_bar.py[line:274] - INFO: epoch 002:   2335 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=5740, lr=3.78697e-05, gnorm=0.939, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14082
2023-02-22 15:28:03 - progress_bar.py[line:274] - INFO: epoch 002:   2345 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.93, wpb=110.4, bsz=40, num_updates=5750, lr=3.7844e-05, gnorm=0.741, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14093
2023-02-22 15:28:14 - progress_bar.py[line:274] - INFO: epoch 002:   2355 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.91, wpb=110.9, bsz=40, num_updates=5760, lr=3.78182e-05, gnorm=1.074, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14104
2023-02-22 15:28:25 - progress_bar.py[line:274] - INFO: epoch 002:   2365 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=5770, lr=3.77925e-05, gnorm=0.922, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14115
2023-02-22 15:28:36 - progress_bar.py[line:274] - INFO: epoch 002:   2375 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=5780, lr=3.77668e-05, gnorm=0.868, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14126
2023-02-22 15:28:47 - progress_bar.py[line:274] - INFO: epoch 002:   2385 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=111.2, bsz=40, num_updates=5790, lr=3.77411e-05, gnorm=1.235, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14137
2023-02-22 15:28:58 - progress_bar.py[line:274] - INFO: epoch 002:   2395 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.4, bsz=40, num_updates=5800, lr=3.77154e-05, gnorm=0.798, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14148
2023-02-22 15:29:09 - progress_bar.py[line:274] - INFO: epoch 002:   2405 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=112.4, bsz=40, num_updates=5810, lr=3.76897e-05, gnorm=0.774, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14160
2023-02-22 15:29:20 - progress_bar.py[line:274] - INFO: epoch 002:   2415 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=5820, lr=3.76639e-05, gnorm=0.961, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14170
2023-02-22 15:29:32 - progress_bar.py[line:274] - INFO: epoch 002:   2425 / 3411 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.88, wpb=112.5, bsz=40, num_updates=5830, lr=3.76382e-05, gnorm=0.834, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14182
2023-02-22 15:29:43 - progress_bar.py[line:274] - INFO: epoch 002:   2435 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.87, wpb=112, bsz=40, num_updates=5840, lr=3.76125e-05, gnorm=0.83, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14193
2023-02-22 15:29:54 - progress_bar.py[line:274] - INFO: epoch 002:   2445 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=5850, lr=3.75868e-05, gnorm=0.78, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14205
2023-02-22 15:30:05 - progress_bar.py[line:274] - INFO: epoch 002:   2455 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.7, ups=0.94, wpb=111.3, bsz=40, num_updates=5860, lr=3.75611e-05, gnorm=0.93, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14215
2023-02-22 15:30:16 - progress_bar.py[line:274] - INFO: epoch 002:   2465 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=5870, lr=3.75354e-05, gnorm=0.947, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14226
2023-02-22 15:30:27 - progress_bar.py[line:274] - INFO: epoch 002:   2475 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.91, wpb=111.8, bsz=40, num_updates=5880, lr=3.75096e-05, gnorm=0.97, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14237
2023-02-22 15:30:38 - progress_bar.py[line:274] - INFO: epoch 002:   2485 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.91, wpb=112.4, bsz=40, num_updates=5890, lr=3.74839e-05, gnorm=0.947, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=14248
2023-02-22 15:30:50 - progress_bar.py[line:274] - INFO: epoch 002:   2495 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=5900, lr=3.74582e-05, gnorm=0.863, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14260
2023-02-22 15:31:01 - progress_bar.py[line:274] - INFO: epoch 002:   2505 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=111.8, bsz=40, num_updates=5910, lr=3.74325e-05, gnorm=0.74, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14271
2023-02-22 15:31:12 - progress_bar.py[line:274] - INFO: epoch 002:   2515 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=5920, lr=3.74068e-05, gnorm=0.822, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14282
2023-02-22 15:31:23 - progress_bar.py[line:274] - INFO: epoch 002:   2525 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.9, ups=0.87, wpb=110.6, bsz=40, num_updates=5930, lr=3.73811e-05, gnorm=0.787, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14294
2023-02-22 15:31:35 - progress_bar.py[line:274] - INFO: epoch 002:   2535 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.9, wpb=110.3, bsz=40, num_updates=5940, lr=3.73553e-05, gnorm=0.861, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14305
2023-02-22 15:31:46 - progress_bar.py[line:274] - INFO: epoch 002:   2545 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=110.7, bsz=40, num_updates=5950, lr=3.73296e-05, gnorm=0.876, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14316
2023-02-22 15:31:57 - progress_bar.py[line:274] - INFO: epoch 002:   2555 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.8, bsz=40, num_updates=5960, lr=3.73039e-05, gnorm=0.907, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14327
2023-02-22 15:32:08 - progress_bar.py[line:274] - INFO: epoch 002:   2565 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.91, wpb=113.1, bsz=40, num_updates=5970, lr=3.72782e-05, gnorm=0.902, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=14338
2023-02-22 15:32:18 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 15:32:20 - progress_bar.py[line:274] - INFO: epoch 002:   2576 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91.7, ups=0.83, wpb=110.9, bsz=40, num_updates=5980, lr=3.72525e-05, gnorm=1.058, clip=40, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=14350
2023-02-22 15:32:32 - progress_bar.py[line:274] - INFO: epoch 002:   2586 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=5990, lr=3.72268e-05, gnorm=0.868, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14362
2023-02-22 15:32:43 - progress_bar.py[line:274] - INFO: epoch 002:   2596 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=6000, lr=3.7201e-05, gnorm=0.948, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14373
2023-02-22 15:32:43 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 15:32:44 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 15:32:44 - train.py[line:551] - INFO: load:0.83 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 15:34:47 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 15:34:47 - train.py[line:551] - INFO: load:0.86 valid_run:123.12 task_valid:120.00 collect_output:2.04
2023-02-22 15:36:47 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 15:36:47 - train.py[line:551] - INFO: load:0.89 valid_run:243.12 task_valid:236.09 collect_output:4.92
2023-02-22 15:38:50 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 15:38:50 - train.py[line:551] - INFO: load:0.91 valid_run:365.31 task_valid:353.01 collect_output:9.14
2023-02-22 15:40:52 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 15:40:52 - train.py[line:551] - INFO: load:0.94 valid_run:487.54 task_valid:467.13 collect_output:16.21
2023-02-22 15:42:53 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 15:42:53 - train.py[line:551] - INFO: load:0.96 valid_run:608.24 task_valid:584.81 collect_output:18.23
2023-02-22 15:44:56 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 15:44:56 - train.py[line:551] - INFO: load:0.99 valid_run:731.39 task_valid:703.94 collect_output:21.19
2023-02-22 15:46:59 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 15:46:59 - train.py[line:551] - INFO: load:1.01 valid_run:854.45 task_valid:822.38 collect_output:24.78
2023-02-22 15:49:01 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 15:49:01 - train.py[line:551] - INFO: load:1.04 valid_run:976.58 task_valid:939.45 collect_output:28.81
2023-02-22 15:51:05 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 15:51:05 - train.py[line:551] - INFO: load:1.06 valid_run:1100.64 task_valid:1057.24 collect_output:34.02
2023-02-22 15:53:07 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 15:53:07 - train.py[line:551] - INFO: load:1.09 valid_run:1222.25 task_valid:1170.36 collect_output:41.48
2023-02-22 15:55:08 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 15:55:08 - train.py[line:551] - INFO: load:1.11 valid_run:1342.89 task_valid:1286.68 collect_output:44.71
2023-02-22 15:57:10 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 15:57:10 - train.py[line:551] - INFO: load:1.14 valid_run:1464.70 task_valid:1404.18 collect_output:47.97
2023-02-22 15:59:09 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 15:59:09 - train.py[line:551] - INFO: load:1.17 valid_run:1583.92 task_valid:1518.46 collect_output:51.85
2023-02-22 16:01:10 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 16:01:10 - train.py[line:551] - INFO: load:1.19 valid_run:1705.31 task_valid:1636.77 collect_output:53.89
2023-02-22 16:03:11 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 16:03:11 - train.py[line:551] - INFO: load:1.22 valid_run:1826.42 task_valid:1753.37 collect_output:57.36
2023-02-22 16:05:13 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 16:05:13 - train.py[line:551] - INFO: load:1.24 valid_run:1947.73 task_valid:1867.92 collect_output:63.09
2023-02-22 16:07:15 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 16:07:15 - train.py[line:551] - INFO: load:1.27 valid_run:2069.48 task_valid:1984.71 collect_output:67.01
2023-02-22 16:09:16 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 16:09:16 - train.py[line:551] - INFO: load:1.30 valid_run:2190.59 task_valid:2103.16 collect_output:68.64
2023-02-22 16:11:17 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 16:11:17 - train.py[line:551] - INFO: load:1.32 valid_run:2312.08 task_valid:2220.58 collect_output:71.66
2023-02-22 16:13:18 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 16:13:18 - train.py[line:551] - INFO: load:1.35 valid_run:2432.69 task_valid:2337.66 collect_output:74.14
2023-02-22 16:15:20 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 16:15:20 - train.py[line:551] - INFO: load:1.37 valid_run:2554.74 task_valid:2454.81 collect_output:77.98
2023-02-22 16:17:23 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 16:17:23 - train.py[line:551] - INFO: load:1.40 valid_run:2677.10 task_valid:2574.28 collect_output:79.82
2023-02-22 16:19:23 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 16:19:23 - train.py[line:551] - INFO: load:1.42 valid_run:2797.82 task_valid:2689.13 collect_output:84.65
2023-02-22 16:21:23 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 16:21:23 - train.py[line:551] - INFO: load:1.45 valid_run:2917.85 task_valid:2805.84 collect_output:86.95
2023-02-22 16:23:25 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 16:23:25 - train.py[line:551] - INFO: load:1.47 valid_run:3039.88 task_valid:2922.64 collect_output:91.10
2023-02-22 16:25:29 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 16:25:29 - train.py[line:551] - INFO: load:1.50 valid_run:3162.90 task_valid:3039.29 collect_output:96.41
2023-02-22 16:27:29 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 16:27:29 - train.py[line:551] - INFO: load:1.52 valid_run:3282.79 task_valid:3153.94 collect_output:100.62
2023-02-22 16:29:31 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 16:29:31 - train.py[line:551] - INFO: load:1.55 valid_run:3404.95 task_valid:3273.80 collect_output:101.89
2023-02-22 16:31:33 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 16:31:33 - train.py[line:551] - INFO: load:1.57 valid_run:3526.71 task_valid:3389.76 collect_output:106.65
2023-02-22 16:33:35 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 16:33:35 - train.py[line:551] - INFO: load:1.60 valid_run:3648.98 task_valid:3508.67 collect_output:109.00
2023-02-22 16:35:36 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 16:35:36 - train.py[line:551] - INFO: load:1.62 valid_run:3770.51 task_valid:3627.62 collect_output:110.56

====================================================================================================
SGG eval:     R @ 50: 0.6480;     R @ 100: 0.6860;     R @ 500: 0.7138;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4419;    mR @ 100: 0.4846;    mR @ 500: 0.5249;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.5625) (covering:0.2286) (eating:0.7647) (flying in:1.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9583) (playing:0.0000) (riding:0.9265) (says:0.0000) (sitting on:0.7126) (standing on:0.5327) (using:0.5500) (walking in:0.0000) (walking on:0.5405) (watching:0.6528) 
--------------------------------------------------------
====================================================================================================

2023-02-22 16:36:07 - train.py[line:487] - INFO: 0.6859801120448179
2023-02-22 16:36:07 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 16:36:07 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.25 | loss_v1 0 | loss_v2 0 | nll_loss 0.081 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.68598 | ppl 1.06 | vqa_score 0.5282 | wps 118 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.68598
2023-02-22 16:36:07 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 6000 updates
2023-02-22 16:36:07 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_2_6000.pt

====================================================================================================
SGG eval:     R @ 50: 0.6480;     R @ 100: 0.6860;     R @ 500: 0.7138;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4419;    mR @ 100: 0.4846;    mR @ 500: 0.5249;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.5625) (covering:0.2286) (eating:0.7647) (flying in:1.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9583) (playing:0.0000) (riding:0.9265) (says:0.0000) (sitting on:0.7126) (standing on:0.5327) (using:0.5500) (walking in:0.0000) (walking on:0.5405) (watching:0.6528) 
--------------------------------------------------------
====================================================================================================

2023-02-22 16:36:13 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_2_6000.pt
2023-02-22 16:36:18 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_2_6000.pt (epoch 2 @ 6000 updates, score 0.6859801120448179) (writing took 10.891049252822995 seconds)
2023-02-22 16:36:29 - progress_bar.py[line:274] - INFO: epoch 002:   2606 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=112, bsz=40, num_updates=6010, lr=3.71753e-05, gnorm=0.755, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18200
2023-02-22 16:36:40 - progress_bar.py[line:274] - INFO: epoch 002:   2616 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.9, wpb=113.7, bsz=40, num_updates=6020, lr=3.71496e-05, gnorm=0.918, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18211
2023-02-22 16:36:52 - progress_bar.py[line:274] - INFO: epoch 002:   2626 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=112.8, bsz=40, num_updates=6030, lr=3.71239e-05, gnorm=0.958, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18222
2023-02-22 16:37:03 - progress_bar.py[line:274] - INFO: epoch 002:   2636 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=6040, lr=3.70982e-05, gnorm=0.921, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18233
2023-02-22 16:37:14 - progress_bar.py[line:274] - INFO: epoch 002:   2646 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=6050, lr=3.70725e-05, gnorm=0.973, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18244
2023-02-22 16:37:26 - progress_bar.py[line:274] - INFO: epoch 002:   2656 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=6060, lr=3.70468e-05, gnorm=1.149, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18256
2023-02-22 16:37:36 - progress_bar.py[line:274] - INFO: epoch 002:   2666 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.9, ups=0.92, wpb=112.1, bsz=40, num_updates=6070, lr=3.7021e-05, gnorm=0.857, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18267
2023-02-22 16:37:48 - progress_bar.py[line:274] - INFO: epoch 002:   2676 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=6080, lr=3.69953e-05, gnorm=0.706, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18278
2023-02-22 16:37:59 - progress_bar.py[line:274] - INFO: epoch 002:   2686 / 3411 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=6090, lr=3.69696e-05, gnorm=0.786, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18289
2023-02-22 16:38:10 - progress_bar.py[line:274] - INFO: epoch 002:   2696 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=6100, lr=3.69439e-05, gnorm=0.92, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18300
2023-02-22 16:38:22 - progress_bar.py[line:274] - INFO: epoch 002:   2706 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111.8, bsz=40, num_updates=6110, lr=3.69182e-05, gnorm=0.982, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=18312
2023-02-22 16:38:33 - progress_bar.py[line:274] - INFO: epoch 002:   2716 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=112.4, bsz=40, num_updates=6120, lr=3.68925e-05, gnorm=1.117, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18323
2023-02-22 16:38:44 - progress_bar.py[line:274] - INFO: epoch 002:   2726 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.9, wpb=113.6, bsz=40, num_updates=6130, lr=3.68667e-05, gnorm=1.003, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18334
2023-02-22 16:38:58 - progress_bar.py[line:274] - INFO: epoch 002:   2736 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.4, ups=0.7, wpb=112.3, bsz=40, num_updates=6140, lr=3.6841e-05, gnorm=0.759, clip=10, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18348
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 16:39:09 - progress_bar.py[line:274] - INFO: epoch 002:   2746 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=111, bsz=40, num_updates=6150, lr=3.68153e-05, gnorm=0.883, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18359
2023-02-22 16:39:21 - progress_bar.py[line:274] - INFO: epoch 002:   2756 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.88, wpb=111.4, bsz=40, num_updates=6160, lr=3.67896e-05, gnorm=0.962, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18371
2023-02-22 16:39:32 - progress_bar.py[line:274] - INFO: epoch 002:   2766 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.2, ups=0.86, wpb=110.6, bsz=40, num_updates=6170, lr=3.67639e-05, gnorm=0.828, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=18382
2023-02-22 16:39:45 - progress_bar.py[line:274] - INFO: epoch 002:   2776 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=92.6, ups=0.82, wpb=113, bsz=40, num_updates=6180, lr=3.67382e-05, gnorm=1.307, clip=80, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=18395
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 16:39:57 - progress_bar.py[line:274] - INFO: epoch 002:   2786 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=89.1, ups=0.8, wpb=110.8, bsz=40, num_updates=6190, lr=3.67124e-05, gnorm=0.981, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18407
2023-02-22 16:40:12 - progress_bar.py[line:274] - INFO: epoch 002:   2796 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=75, ups=0.67, wpb=112.4, bsz=40, num_updates=6200, lr=3.66867e-05, gnorm=0.809, clip=30, loss_scale=512, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=18422
2023-02-22 16:40:27 - progress_bar.py[line:274] - INFO: epoch 002:   2806 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=77.4, ups=0.69, wpb=112.8, bsz=40, num_updates=6210, lr=3.6661e-05, gnorm=0.779, clip=40, loss_scale=512, train_wall=14, gb_free=9, ema_decay=0.9999, wall=18437
2023-02-22 16:40:41 - progress_bar.py[line:274] - INFO: epoch 002:   2816 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=76.6, ups=0.68, wpb=112.4, bsz=40, num_updates=6220, lr=3.66353e-05, gnorm=0.884, clip=40, loss_scale=512, train_wall=15, gb_free=10.7, ema_decay=0.9999, wall=18451
2023-02-22 16:40:56 - progress_bar.py[line:274] - INFO: epoch 002:   2826 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=74.8, ups=0.67, wpb=112, bsz=40, num_updates=6230, lr=3.66096e-05, gnorm=0.839, clip=10, loss_scale=512, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=18466
2023-02-22 16:41:11 - progress_bar.py[line:274] - INFO: epoch 002:   2836 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.2, ups=0.67, wpb=113.1, bsz=40, num_updates=6240, lr=3.65839e-05, gnorm=1.258, clip=40, loss_scale=512, train_wall=15, gb_free=10.3, ema_decay=0.9999, wall=18481
2023-02-22 16:41:26 - progress_bar.py[line:274] - INFO: epoch 002:   2846 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=75.3, ups=0.67, wpb=112, bsz=40, num_updates=6250, lr=3.65581e-05, gnorm=1.432, clip=50, loss_scale=512, train_wall=15, gb_free=10.7, ema_decay=0.9999, wall=18496
2023-02-22 16:41:40 - progress_bar.py[line:274] - INFO: epoch 002:   2856 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=80.2, ups=0.71, wpb=113.8, bsz=40, num_updates=6260, lr=3.65324e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18510
2023-02-22 16:41:54 - progress_bar.py[line:274] - INFO: epoch 002:   2866 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.8, ups=0.71, wpb=111.3, bsz=40, num_updates=6270, lr=3.65067e-05, gnorm=0.742, clip=20, loss_scale=512, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=18524
2023-02-22 16:42:09 - progress_bar.py[line:274] - INFO: epoch 002:   2876 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.3, ups=0.69, wpb=114.1, bsz=40, num_updates=6280, lr=3.6481e-05, gnorm=1.037, clip=50, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18539
2023-02-22 16:42:24 - progress_bar.py[line:274] - INFO: epoch 002:   2886 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.6, ups=0.68, wpb=112.7, bsz=40, num_updates=6290, lr=3.64553e-05, gnorm=0.764, clip=30, loss_scale=512, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=18554
2023-02-22 16:42:38 - progress_bar.py[line:274] - INFO: epoch 002:   2896 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=77.7, ups=0.7, wpb=111.3, bsz=40, num_updates=6300, lr=3.64296e-05, gnorm=0.93, clip=50, loss_scale=512, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=18568
2023-02-22 16:42:52 - progress_bar.py[line:274] - INFO: epoch 002:   2906 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.3, ups=0.7, wpb=112, bsz=40, num_updates=6310, lr=3.64038e-05, gnorm=0.968, clip=40, loss_scale=512, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=18583
2023-02-22 16:43:07 - progress_bar.py[line:274] - INFO: epoch 002:   2916 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=78, ups=0.69, wpb=112.3, bsz=40, num_updates=6320, lr=3.63781e-05, gnorm=1.102, clip=30, loss_scale=512, train_wall=14, gb_free=10.2, ema_decay=0.9999, wall=18597
2023-02-22 16:43:21 - progress_bar.py[line:274] - INFO: epoch 002:   2926 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.7, ups=0.69, wpb=110.8, bsz=40, num_updates=6330, lr=3.63524e-05, gnorm=0.808, clip=30, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18611
2023-02-22 16:43:36 - progress_bar.py[line:274] - INFO: epoch 002:   2936 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=74.3, ups=0.67, wpb=111, bsz=40, num_updates=6340, lr=3.63267e-05, gnorm=0.779, clip=40, loss_scale=512, train_wall=15, gb_free=10.5, ema_decay=0.9999, wall=18626
2023-02-22 16:43:52 - progress_bar.py[line:274] - INFO: epoch 002:   2946 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=73.8, ups=0.65, wpb=113.5, bsz=40, num_updates=6350, lr=3.6301e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=15, gb_free=10.2, ema_decay=0.9999, wall=18642
2023-02-22 16:44:06 - progress_bar.py[line:274] - INFO: epoch 002:   2956 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=77.5, ups=0.69, wpb=111.8, bsz=40, num_updates=6360, lr=3.62753e-05, gnorm=0.638, clip=20, loss_scale=512, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=18656
2023-02-22 16:44:21 - progress_bar.py[line:274] - INFO: epoch 002:   2966 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.1, ups=0.67, wpb=112.8, bsz=40, num_updates=6370, lr=3.62495e-05, gnorm=0.937, clip=30, loss_scale=512, train_wall=15, gb_free=10.5, ema_decay=0.9999, wall=18671
2023-02-22 16:44:36 - progress_bar.py[line:274] - INFO: epoch 002:   2976 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=75, ups=0.67, wpb=112.4, bsz=40, num_updates=6380, lr=3.62238e-05, gnorm=0.937, clip=20, loss_scale=512, train_wall=15, gb_free=10.4, ema_decay=0.9999, wall=18686
2023-02-22 16:44:51 - progress_bar.py[line:274] - INFO: epoch 002:   2986 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.1, ups=0.69, wpb=113.2, bsz=40, num_updates=6390, lr=3.61981e-05, gnorm=0.888, clip=30, loss_scale=512, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=18701
2023-02-22 16:45:06 - progress_bar.py[line:274] - INFO: epoch 002:   2996 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=75.9, ups=0.68, wpb=111.1, bsz=40, num_updates=6400, lr=3.61724e-05, gnorm=0.781, clip=10, loss_scale=512, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=18716
2023-02-22 16:45:20 - progress_bar.py[line:274] - INFO: epoch 002:   3006 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=78.4, ups=0.71, wpb=111, bsz=40, num_updates=6410, lr=3.61467e-05, gnorm=0.912, clip=40, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18730
2023-02-22 16:45:34 - progress_bar.py[line:274] - INFO: epoch 002:   3016 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=77.1, ups=0.69, wpb=111.3, bsz=40, num_updates=6420, lr=3.6121e-05, gnorm=1.089, clip=40, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18744
2023-02-22 16:45:48 - progress_bar.py[line:274] - INFO: epoch 002:   3026 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=80.5, ups=0.72, wpb=111, bsz=40, num_updates=6430, lr=3.60953e-05, gnorm=0.895, clip=40, loss_scale=512, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=18758
2023-02-22 16:46:02 - progress_bar.py[line:274] - INFO: epoch 002:   3036 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=81.3, ups=0.72, wpb=112.7, bsz=40, num_updates=6440, lr=3.60695e-05, gnorm=0.874, clip=40, loss_scale=512, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=18772
2023-02-22 16:46:17 - progress_bar.py[line:274] - INFO: epoch 002:   3046 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.7, ups=0.69, wpb=110.7, bsz=40, num_updates=6450, lr=3.60438e-05, gnorm=0.834, clip=20, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18787
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 16:46:30 - progress_bar.py[line:274] - INFO: epoch 002:   3056 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=83, ups=0.74, wpb=112.1, bsz=40, num_updates=6460, lr=3.60181e-05, gnorm=0.837, clip=30, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=18800
2023-02-22 16:46:45 - progress_bar.py[line:274] - INFO: epoch 002:   3066 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.7, ups=0.68, wpb=112.8, bsz=40, num_updates=6470, lr=3.59924e-05, gnorm=1.005, clip=50, loss_scale=512, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=18815
2023-02-22 16:46:59 - progress_bar.py[line:274] - INFO: epoch 002:   3076 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=78.9, ups=0.71, wpb=110.6, bsz=40, num_updates=6480, lr=3.59667e-05, gnorm=1.234, clip=50, loss_scale=512, train_wall=14, gb_free=10, ema_decay=0.9999, wall=18829
2023-02-22 16:47:13 - progress_bar.py[line:274] - INFO: epoch 002:   3086 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.7, ups=0.71, wpb=110.6, bsz=40, num_updates=6490, lr=3.5941e-05, gnorm=0.771, clip=20, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18843
2023-02-22 16:47:28 - progress_bar.py[line:274] - INFO: epoch 002:   3096 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.7, ups=0.69, wpb=111.7, bsz=40, num_updates=6500, lr=3.59152e-05, gnorm=0.942, clip=40, loss_scale=1024, train_wall=14, gb_free=10.2, ema_decay=0.9999, wall=18858
2023-02-22 16:47:42 - progress_bar.py[line:274] - INFO: epoch 002:   3106 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=80, ups=0.72, wpb=110.8, bsz=40, num_updates=6510, lr=3.58895e-05, gnorm=0.879, clip=40, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18872
2023-02-22 16:47:56 - progress_bar.py[line:274] - INFO: epoch 002:   3116 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.8, ups=0.7, wpb=113.3, bsz=40, num_updates=6520, lr=3.58638e-05, gnorm=0.719, clip=30, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18886
2023-02-22 16:48:10 - progress_bar.py[line:274] - INFO: epoch 002:   3126 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=80.9, ups=0.73, wpb=110.5, bsz=40, num_updates=6530, lr=3.58381e-05, gnorm=0.727, clip=20, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18900
2023-02-22 16:48:24 - progress_bar.py[line:274] - INFO: epoch 002:   3136 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=77, ups=0.69, wpb=110.8, bsz=40, num_updates=6540, lr=3.58124e-05, gnorm=1.066, clip=60, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18915
2023-02-22 16:48:39 - progress_bar.py[line:274] - INFO: epoch 002:   3146 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=80.6, ups=0.71, wpb=113.8, bsz=40, num_updates=6550, lr=3.57867e-05, gnorm=0.698, clip=0, loss_scale=1024, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=18929
2023-02-22 16:48:53 - progress_bar.py[line:274] - INFO: epoch 002:   3156 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=81.9, ups=0.72, wpb=113.3, bsz=40, num_updates=6560, lr=3.57609e-05, gnorm=0.993, clip=30, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18943
2023-02-22 16:49:07 - progress_bar.py[line:274] - INFO: epoch 002:   3166 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.2, ups=0.7, wpb=111.7, bsz=40, num_updates=6570, lr=3.57352e-05, gnorm=0.764, clip=20, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18957
2023-02-22 16:49:22 - progress_bar.py[line:274] - INFO: epoch 002:   3176 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=75.4, ups=0.68, wpb=110.1, bsz=40, num_updates=6580, lr=3.57095e-05, gnorm=0.894, clip=30, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18972
2023-02-22 16:49:36 - progress_bar.py[line:274] - INFO: epoch 002:   3186 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.4, ups=0.7, wpb=111.4, bsz=40, num_updates=6590, lr=3.56838e-05, gnorm=0.798, clip=30, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18986
2023-02-22 16:49:50 - progress_bar.py[line:274] - INFO: epoch 002:   3196 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.5, ups=0.7, wpb=112.9, bsz=40, num_updates=6600, lr=3.56581e-05, gnorm=1.052, clip=40, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=19000
2023-02-22 16:50:05 - progress_bar.py[line:274] - INFO: epoch 002:   3206 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76, ups=0.68, wpb=111.5, bsz=40, num_updates=6610, lr=3.56324e-05, gnorm=0.816, clip=30, loss_scale=1024, train_wall=15, gb_free=9.9, ema_decay=0.9999, wall=19015
2023-02-22 16:50:20 - progress_bar.py[line:274] - INFO: epoch 002:   3216 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.3, ups=0.69, wpb=110.9, bsz=40, num_updates=6620, lr=3.56066e-05, gnorm=0.742, clip=20, loss_scale=1024, train_wall=14, gb_free=10.3, ema_decay=0.9999, wall=19030
2023-02-22 16:50:34 - progress_bar.py[line:274] - INFO: epoch 002:   3226 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.9, ups=0.7, wpb=112, bsz=40, num_updates=6630, lr=3.55809e-05, gnorm=0.838, clip=30, loss_scale=1024, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=19044
2023-02-22 16:50:46 - progress_bar.py[line:274] - INFO: epoch 002:   3236 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91.7, ups=0.82, wpb=112.4, bsz=40, num_updates=6640, lr=3.55552e-05, gnorm=1.257, clip=50, loss_scale=1024, train_wall=12, gb_free=9.5, ema_decay=0.9999, wall=19056
2023-02-22 16:50:58 - progress_bar.py[line:274] - INFO: epoch 002:   3246 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91.1, ups=0.82, wpb=111.2, bsz=40, num_updates=6650, lr=3.55295e-05, gnorm=0.927, clip=40, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19069
2023-02-22 16:51:12 - progress_bar.py[line:274] - INFO: epoch 002:   3256 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=83.2, ups=0.74, wpb=112.4, bsz=40, num_updates=6660, lr=3.55038e-05, gnorm=0.736, clip=40, loss_scale=1024, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=19082
2023-02-22 16:51:26 - progress_bar.py[line:274] - INFO: epoch 002:   3266 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=81.1, ups=0.72, wpb=113, bsz=40, num_updates=6670, lr=3.54781e-05, gnorm=0.712, clip=30, loss_scale=1024, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=19096
2023-02-22 16:51:40 - progress_bar.py[line:274] - INFO: epoch 002:   3276 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.6, ups=0.7, wpb=112.6, bsz=40, num_updates=6680, lr=3.54523e-05, gnorm=0.721, clip=10, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=19110
2023-02-22 16:51:55 - progress_bar.py[line:274] - INFO: epoch 002:   3286 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=76.3, ups=0.68, wpb=111.6, bsz=40, num_updates=6690, lr=3.54266e-05, gnorm=0.715, clip=20, loss_scale=1024, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=19125
2023-02-22 16:52:09 - progress_bar.py[line:274] - INFO: epoch 002:   3296 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=82.8, ups=0.74, wpb=112.4, bsz=40, num_updates=6700, lr=3.54009e-05, gnorm=1.015, clip=40, loss_scale=1024, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19139
2023-02-22 16:52:22 - progress_bar.py[line:274] - INFO: epoch 002:   3306 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=82.6, ups=0.73, wpb=112.7, bsz=40, num_updates=6710, lr=3.53752e-05, gnorm=0.74, clip=30, loss_scale=1024, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=19152
2023-02-22 16:52:36 - progress_bar.py[line:274] - INFO: epoch 002:   3316 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=81.2, ups=0.73, wpb=111.8, bsz=40, num_updates=6720, lr=3.53495e-05, gnorm=1.171, clip=30, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=19166
2023-02-22 16:52:50 - progress_bar.py[line:274] - INFO: epoch 002:   3326 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=81.9, ups=0.73, wpb=112, bsz=40, num_updates=6730, lr=3.53238e-05, gnorm=1.179, clip=40, loss_scale=1024, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=19180
2023-02-22 16:53:04 - progress_bar.py[line:274] - INFO: epoch 002:   3336 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=80.6, ups=0.72, wpb=112.3, bsz=40, num_updates=6740, lr=3.52981e-05, gnorm=0.746, clip=20, loss_scale=1024, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=19194
2023-02-22 16:53:18 - progress_bar.py[line:274] - INFO: epoch 002:   3346 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=78.8, ups=0.7, wpb=112.7, bsz=40, num_updates=6750, lr=3.52723e-05, gnorm=0.816, clip=10, loss_scale=1024, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=19208
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 16:53:31 - progress_bar.py[line:274] - INFO: epoch 002:   3356 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=88.5, ups=0.78, wpb=113, bsz=40, num_updates=6760, lr=3.52466e-05, gnorm=0.872, clip=50, loss_scale=1024, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19221
2023-02-22 16:53:43 - progress_bar.py[line:274] - INFO: epoch 002:   3366 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=88.9, ups=0.8, wpb=111.5, bsz=40, num_updates=6770, lr=3.52209e-05, gnorm=0.941, clip=40, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19234
2023-02-22 16:53:56 - progress_bar.py[line:274] - INFO: epoch 002:   3376 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=87, ups=0.78, wpb=111.6, bsz=40, num_updates=6780, lr=3.51952e-05, gnorm=0.951, clip=30, loss_scale=1024, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=19246
2023-02-22 16:54:01 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 16:54:10 - progress_bar.py[line:274] - INFO: epoch 002:   3387 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=82, ups=0.72, wpb=113.5, bsz=40, num_updates=6790, lr=3.51695e-05, gnorm=0.754, clip=40, loss_scale=512, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=19260
2023-02-22 16:54:23 - progress_bar.py[line:274] - INFO: epoch 002:   3397 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=85.4, ups=0.76, wpb=111.9, bsz=40, num_updates=6800, lr=3.51438e-05, gnorm=1.567, clip=60, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19273
2023-02-22 16:54:38 - progress_bar.py[line:274] - INFO: epoch 002:   3407 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=77.1, ups=0.7, wpb=110.2, bsz=40, num_updates=6810, lr=3.5118e-05, gnorm=0.888, clip=50, loss_scale=512, train_wall=14, gb_free=10.1, ema_decay=0.9999, wall=19288
2023-02-22 16:54:43 - train.py[line:339] - INFO: end of epoch 2 (average epoch stats below)
2023-02-22 16:54:43 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.271 | loss_v1 0 | loss_v2 0 | nll_loss 0.092 | ntokens 111.864 | nsentences 39.998 | sample_size 111.864 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.07 | wps 32.7 | ups 0.29 | wpb 111.9 | bsz 40 | num_updates 6814 | lr 3.51078e-05 | gnorm 0.989 | clip 40.6 | loss_scale 512 | train_wall 3972 | gb_free 14.2 | ema_decay 0.9999 | wall 19293
2023-02-22 16:54:43 - trainer.py[line:694] - INFO: loading train data for epoch 3
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E2.tsv slice_id 1 row count 68217 total row count 136434
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E2.tsv slice_id 0 row count 68217 total row count 136434
2023-02-22 16:54:43 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 16:54:43 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 16:54:45 - trainer.py[line:758] - INFO: begin training epoch 3
2023-02-22 16:54:45 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 16:54:57 - progress_bar.py[line:274] - INFO: epoch 003:      6 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=109.1, nsentences=39.4, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=56.7, ups=0.52, wpb=109.1, bsz=39.4, num_updates=6820, lr=3.50923e-05, gnorm=0.661, clip=10, loss_scale=512, train_wall=14, gb_free=10.3, ema_decay=0.9999, wall=19307
2023-02-22 16:55:10 - progress_bar.py[line:274] - INFO: epoch 003:     16 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=86.5, ups=0.77, wpb=112.8, bsz=40, num_updates=6830, lr=3.50666e-05, gnorm=0.647, clip=10, loss_scale=512, train_wall=13, gb_free=9.5, ema_decay=0.9999, wall=19320
2023-02-22 16:55:23 - progress_bar.py[line:274] - INFO: epoch 003:     26 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=87.1, ups=0.78, wpb=111.6, bsz=40, num_updates=6840, lr=3.50409e-05, gnorm=0.788, clip=30, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=19333
2023-02-22 16:55:36 - progress_bar.py[line:274] - INFO: epoch 003:     36 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=85.4, ups=0.76, wpb=111.8, bsz=40, num_updates=6850, lr=3.50152e-05, gnorm=0.768, clip=30, loss_scale=512, train_wall=13, gb_free=9.9, ema_decay=0.9999, wall=19346
2023-02-22 16:55:49 - progress_bar.py[line:274] - INFO: epoch 003:     46 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=86.1, ups=0.78, wpb=111, bsz=40, num_updates=6860, lr=3.49895e-05, gnorm=0.995, clip=40, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19359
2023-02-22 16:56:02 - progress_bar.py[line:274] - INFO: epoch 003:     56 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=85.2, ups=0.77, wpb=111.3, bsz=40, num_updates=6870, lr=3.49637e-05, gnorm=0.881, clip=40, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=19372
2023-02-22 16:56:14 - progress_bar.py[line:274] - INFO: epoch 003:     66 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=89.8, ups=0.81, wpb=111.3, bsz=40, num_updates=6880, lr=3.4938e-05, gnorm=0.972, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19385
2023-02-22 16:56:28 - progress_bar.py[line:274] - INFO: epoch 003:     76 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=85.2, ups=0.76, wpb=111.8, bsz=40, num_updates=6890, lr=3.49123e-05, gnorm=0.823, clip=50, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=19398
2023-02-22 16:56:40 - progress_bar.py[line:274] - INFO: epoch 003:     86 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=92, ups=0.82, wpb=111.8, bsz=40, num_updates=6900, lr=3.48866e-05, gnorm=0.964, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19410
2023-02-22 16:56:52 - progress_bar.py[line:274] - INFO: epoch 003:     96 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=88.9, ups=0.8, wpb=111.2, bsz=40, num_updates=6910, lr=3.48609e-05, gnorm=0.845, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19422
2023-02-22 16:57:05 - progress_bar.py[line:274] - INFO: epoch 003:    106 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=90.4, ups=0.8, wpb=112.4, bsz=40, num_updates=6920, lr=3.48352e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19435
2023-02-22 16:57:17 - progress_bar.py[line:274] - INFO: epoch 003:    116 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=89, ups=0.8, wpb=110.6, bsz=40, num_updates=6930, lr=3.48094e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19447
2023-02-22 16:57:29 - progress_bar.py[line:274] - INFO: epoch 003:    126 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.7, ups=0.85, wpb=113.1, bsz=40, num_updates=6940, lr=3.47837e-05, gnorm=0.814, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19459
2023-02-22 16:57:41 - progress_bar.py[line:274] - INFO: epoch 003:    136 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.87, wpb=112.8, bsz=40, num_updates=6950, lr=3.4758e-05, gnorm=1.204, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19471
2023-02-22 16:57:52 - progress_bar.py[line:274] - INFO: epoch 003:    146 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=112.6, bsz=40, num_updates=6960, lr=3.47323e-05, gnorm=0.94, clip=40, loss_scale=512, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=19482
2023-02-22 16:58:03 - progress_bar.py[line:274] - INFO: epoch 003:    156 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.9, bsz=40, num_updates=6970, lr=3.47066e-05, gnorm=1.089, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19493
2023-02-22 16:58:14 - progress_bar.py[line:274] - INFO: epoch 003:    166 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=6980, lr=3.46809e-05, gnorm=0.938, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19504
2023-02-22 16:58:26 - progress_bar.py[line:274] - INFO: epoch 003:    176 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.9, ups=0.87, wpb=110.6, bsz=40, num_updates=6990, lr=3.46551e-05, gnorm=0.791, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19516
2023-02-22 16:58:37 - progress_bar.py[line:274] - INFO: epoch 003:    186 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.9, wpb=113.2, bsz=40, num_updates=7000, lr=3.46294e-05, gnorm=0.604, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19527
2023-02-22 16:58:48 - progress_bar.py[line:274] - INFO: epoch 003:    196 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.9, ups=0.91, wpb=113, bsz=40, num_updates=7010, lr=3.46037e-05, gnorm=1.048, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19538
2023-02-22 16:58:59 - progress_bar.py[line:274] - INFO: epoch 003:    206 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.9, wpb=113.8, bsz=40, num_updates=7020, lr=3.4578e-05, gnorm=0.963, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19549
2023-02-22 16:59:10 - progress_bar.py[line:274] - INFO: epoch 003:    216 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=111.9, bsz=40, num_updates=7030, lr=3.45523e-05, gnorm=0.808, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19560
2023-02-22 16:59:21 - progress_bar.py[line:274] - INFO: epoch 003:    226 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.89, wpb=110.6, bsz=40, num_updates=7040, lr=3.45266e-05, gnorm=0.999, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19571
2023-02-22 16:59:32 - progress_bar.py[line:274] - INFO: epoch 003:    236 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=7050, lr=3.45008e-05, gnorm=0.697, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19583
2023-02-22 16:59:44 - progress_bar.py[line:274] - INFO: epoch 003:    246 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.9, wpb=111, bsz=40, num_updates=7060, lr=3.44751e-05, gnorm=0.838, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19594
2023-02-22 16:59:55 - progress_bar.py[line:274] - INFO: epoch 003:    256 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.87, wpb=112.8, bsz=40, num_updates=7070, lr=3.44494e-05, gnorm=0.842, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19605
2023-02-22 17:00:06 - progress_bar.py[line:274] - INFO: epoch 003:    266 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.89, wpb=112.6, bsz=40, num_updates=7080, lr=3.44237e-05, gnorm=0.733, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19616
2023-02-22 17:00:18 - progress_bar.py[line:274] - INFO: epoch 003:    276 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.88, wpb=111.8, bsz=40, num_updates=7090, lr=3.4398e-05, gnorm=0.811, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19628
2023-02-22 17:00:29 - progress_bar.py[line:274] - INFO: epoch 003:    286 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.3, bsz=40, num_updates=7100, lr=3.43723e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19639
2023-02-22 17:00:40 - progress_bar.py[line:274] - INFO: epoch 003:    296 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=112, bsz=40, num_updates=7110, lr=3.43466e-05, gnorm=0.878, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19650
2023-02-22 17:00:51 - progress_bar.py[line:274] - INFO: epoch 003:    306 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.6, bsz=40, num_updates=7120, lr=3.43208e-05, gnorm=0.924, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19662
2023-02-22 17:01:04 - progress_bar.py[line:274] - INFO: epoch 003:    316 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=7130, lr=3.42951e-05, gnorm=0.762, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19673
2023-02-22 17:01:15 - progress_bar.py[line:274] - INFO: epoch 003:    326 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=114.2, bsz=40, num_updates=7140, lr=3.42694e-05, gnorm=1.047, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19686
2023-02-22 17:01:26 - progress_bar.py[line:274] - INFO: epoch 003:    336 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.9, wpb=111, bsz=40, num_updates=7150, lr=3.42437e-05, gnorm=1.091, clip=40, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=19697
2023-02-22 17:01:38 - progress_bar.py[line:274] - INFO: epoch 003:    346 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=7160, lr=3.4218e-05, gnorm=0.719, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19708
2023-02-22 17:01:49 - progress_bar.py[line:274] - INFO: epoch 003:    356 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.3, bsz=40, num_updates=7170, lr=3.41923e-05, gnorm=1, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19719
2023-02-22 17:02:00 - progress_bar.py[line:274] - INFO: epoch 003:    366 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.88, wpb=112.2, bsz=40, num_updates=7180, lr=3.41665e-05, gnorm=0.973, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19730
2023-02-22 17:02:12 - progress_bar.py[line:274] - INFO: epoch 003:    376 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=113.2, bsz=40, num_updates=7190, lr=3.41408e-05, gnorm=0.62, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19742
2023-02-22 17:02:23 - progress_bar.py[line:274] - INFO: epoch 003:    386 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.9, wpb=111.2, bsz=40, num_updates=7200, lr=3.41151e-05, gnorm=0.663, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19753
2023-02-22 17:02:34 - progress_bar.py[line:274] - INFO: epoch 003:    396 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=113.2, bsz=40, num_updates=7210, lr=3.40894e-05, gnorm=1.206, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19764
2023-02-22 17:02:46 - progress_bar.py[line:274] - INFO: epoch 003:    406 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.87, wpb=111.2, bsz=40, num_updates=7220, lr=3.40637e-05, gnorm=0.927, clip=60, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19776
2023-02-22 17:02:57 - progress_bar.py[line:274] - INFO: epoch 003:    416 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97, ups=0.89, wpb=109.4, bsz=40, num_updates=7230, lr=3.4038e-05, gnorm=0.648, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19787
2023-02-22 17:03:08 - progress_bar.py[line:274] - INFO: epoch 003:    426 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.9, wpb=110.7, bsz=40, num_updates=7240, lr=3.40122e-05, gnorm=0.824, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19798
2023-02-22 17:03:20 - progress_bar.py[line:274] - INFO: epoch 003:    436 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.87, wpb=112.7, bsz=40, num_updates=7250, lr=3.39865e-05, gnorm=0.814, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=19810
2023-02-22 17:03:31 - progress_bar.py[line:274] - INFO: epoch 003:    446 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=112.3, bsz=40, num_updates=7260, lr=3.39608e-05, gnorm=0.53, clip=10, loss_scale=512, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=19821
2023-02-22 17:03:42 - progress_bar.py[line:274] - INFO: epoch 003:    456 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=112.2, bsz=40, num_updates=7270, lr=3.39351e-05, gnorm=0.6, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19832
2023-02-22 17:03:53 - progress_bar.py[line:274] - INFO: epoch 003:    466 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.4, bsz=40, num_updates=7280, lr=3.39094e-05, gnorm=0.727, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19843
2023-02-22 17:04:04 - progress_bar.py[line:274] - INFO: epoch 003:    476 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=111.2, bsz=40, num_updates=7290, lr=3.38837e-05, gnorm=0.813, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19855
2023-02-22 17:04:16 - progress_bar.py[line:274] - INFO: epoch 003:    486 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=7300, lr=3.38579e-05, gnorm=1.441, clip=40, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19866
2023-02-22 17:04:27 - progress_bar.py[line:274] - INFO: epoch 003:    496 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=7310, lr=3.38322e-05, gnorm=0.839, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19877
2023-02-22 17:04:38 - progress_bar.py[line:274] - INFO: epoch 003:    506 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=112.3, bsz=40, num_updates=7320, lr=3.38065e-05, gnorm=0.699, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19888
2023-02-22 17:04:49 - progress_bar.py[line:274] - INFO: epoch 003:    516 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=112.8, bsz=40, num_updates=7330, lr=3.37808e-05, gnorm=0.787, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19899
2023-02-22 17:05:01 - progress_bar.py[line:274] - INFO: epoch 003:    526 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=7340, lr=3.37551e-05, gnorm=1.078, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19911
2023-02-22 17:05:12 - progress_bar.py[line:274] - INFO: epoch 003:    536 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=7350, lr=3.37294e-05, gnorm=1, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19922
2023-02-22 17:05:23 - progress_bar.py[line:274] - INFO: epoch 003:    546 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.91, wpb=111.9, bsz=40, num_updates=7360, lr=3.37036e-05, gnorm=1.067, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19933
2023-02-22 17:05:34 - progress_bar.py[line:274] - INFO: epoch 003:    556 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.5, ups=0.91, wpb=112.3, bsz=40, num_updates=7370, lr=3.36779e-05, gnorm=0.755, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19944
2023-02-22 17:05:45 - progress_bar.py[line:274] - INFO: epoch 003:    566 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.4, bsz=40, num_updates=7380, lr=3.36522e-05, gnorm=0.85, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19955
2023-02-22 17:05:56 - progress_bar.py[line:274] - INFO: epoch 003:    576 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.9, bsz=40, num_updates=7390, lr=3.36265e-05, gnorm=0.722, clip=20, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=19966
2023-02-22 17:06:07 - progress_bar.py[line:274] - INFO: epoch 003:    586 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.87, wpb=112.2, bsz=40, num_updates=7400, lr=3.36008e-05, gnorm=0.849, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19977
2023-02-22 17:06:18 - progress_bar.py[line:274] - INFO: epoch 003:    596 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=112.1, bsz=40, num_updates=7410, lr=3.35751e-05, gnorm=1.309, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19989
2023-02-22 17:06:30 - progress_bar.py[line:274] - INFO: epoch 003:    606 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111.4, bsz=40, num_updates=7420, lr=3.35493e-05, gnorm=0.738, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20000
2023-02-22 17:06:41 - progress_bar.py[line:274] - INFO: epoch 003:    616 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.9, wpb=113.5, bsz=40, num_updates=7430, lr=3.35236e-05, gnorm=1.433, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20011
2023-02-22 17:06:52 - progress_bar.py[line:274] - INFO: epoch 003:    626 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=113.1, bsz=40, num_updates=7440, lr=3.34979e-05, gnorm=0.988, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20022
2023-02-22 17:07:05 - progress_bar.py[line:274] - INFO: epoch 003:    636 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.89, wpb=109.6, bsz=40, num_updates=7450, lr=3.34722e-05, gnorm=1.005, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20034
2023-02-22 17:07:16 - progress_bar.py[line:274] - INFO: epoch 003:    646 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.9, wpb=110.4, bsz=40, num_updates=7460, lr=3.34465e-05, gnorm=0.884, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20046
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 17:07:27 - progress_bar.py[line:274] - INFO: epoch 003:    656 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.9, wpb=111.1, bsz=40, num_updates=7470, lr=3.34208e-05, gnorm=0.819, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20057
2023-02-22 17:07:39 - progress_bar.py[line:274] - INFO: epoch 003:    666 / 3411 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.88, wpb=110.5, bsz=40, num_updates=7480, lr=3.33951e-05, gnorm=1.025, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20069
2023-02-22 17:07:50 - progress_bar.py[line:274] - INFO: epoch 003:    676 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=112.1, bsz=40, num_updates=7490, lr=3.33693e-05, gnorm=0.925, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20080
2023-02-22 17:08:01 - progress_bar.py[line:274] - INFO: epoch 003:    686 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.93, wpb=112, bsz=40, num_updates=7500, lr=3.33436e-05, gnorm=0.776, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20091
2023-02-22 17:08:12 - progress_bar.py[line:274] - INFO: epoch 003:    696 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.87, wpb=112.8, bsz=40, num_updates=7510, lr=3.33179e-05, gnorm=0.991, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20102
2023-02-22 17:08:23 - progress_bar.py[line:274] - INFO: epoch 003:    706 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=110.7, bsz=40, num_updates=7520, lr=3.32922e-05, gnorm=0.794, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20114
2023-02-22 17:08:35 - progress_bar.py[line:274] - INFO: epoch 003:    716 / 3411 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.88, wpb=112.6, bsz=40, num_updates=7530, lr=3.32665e-05, gnorm=0.815, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20125
2023-02-22 17:08:46 - progress_bar.py[line:274] - INFO: epoch 003:    726 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.9, wpb=114, bsz=40, num_updates=7540, lr=3.32408e-05, gnorm=0.712, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20136
2023-02-22 17:08:57 - progress_bar.py[line:274] - INFO: epoch 003:    736 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=7550, lr=3.3215e-05, gnorm=0.702, clip=10, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=20147
2023-02-22 17:09:08 - progress_bar.py[line:274] - INFO: epoch 003:    746 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.1, bsz=40, num_updates=7560, lr=3.31893e-05, gnorm=0.8, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20158
2023-02-22 17:09:20 - progress_bar.py[line:274] - INFO: epoch 003:    756 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.86, wpb=112.3, bsz=40, num_updates=7570, lr=3.31636e-05, gnorm=0.714, clip=20, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=20170
2023-02-22 17:09:31 - progress_bar.py[line:274] - INFO: epoch 003:    766 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.8, bsz=40, num_updates=7580, lr=3.31379e-05, gnorm=0.983, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20181
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 17:09:42 - progress_bar.py[line:274] - INFO: epoch 003:    776 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.93, wpb=111.6, bsz=40, num_updates=7590, lr=3.31122e-05, gnorm=0.63, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20192
2023-02-22 17:09:53 - progress_bar.py[line:274] - INFO: epoch 003:    786 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.3, bsz=40, num_updates=7600, lr=3.30865e-05, gnorm=0.831, clip=20, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=20203
2023-02-22 17:10:04 - progress_bar.py[line:274] - INFO: epoch 003:    796 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.8, bsz=40, num_updates=7610, lr=3.30607e-05, gnorm=0.625, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20215
2023-02-22 17:10:15 - progress_bar.py[line:274] - INFO: epoch 003:    806 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.92, wpb=111.7, bsz=40, num_updates=7620, lr=3.3035e-05, gnorm=0.67, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20225
2023-02-22 17:10:27 - progress_bar.py[line:274] - INFO: epoch 003:    816 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112.3, bsz=40, num_updates=7630, lr=3.30093e-05, gnorm=0.7, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20237
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 17:10:37 - progress_bar.py[line:274] - INFO: epoch 003:    826 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.3, ups=0.93, wpb=112.7, bsz=40, num_updates=7640, lr=3.29836e-05, gnorm=0.826, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20248
2023-02-22 17:10:49 - progress_bar.py[line:274] - INFO: epoch 003:    836 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=7650, lr=3.29579e-05, gnorm=0.862, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20259
2023-02-22 17:11:00 - progress_bar.py[line:274] - INFO: epoch 003:    846 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=111.3, bsz=40, num_updates=7660, lr=3.29322e-05, gnorm=0.763, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20270
2023-02-22 17:11:11 - progress_bar.py[line:274] - INFO: epoch 003:    856 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=112.7, bsz=40, num_updates=7670, lr=3.29064e-05, gnorm=0.673, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20281
2023-02-22 17:11:22 - progress_bar.py[line:274] - INFO: epoch 003:    866 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=7680, lr=3.28807e-05, gnorm=0.845, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20293
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 17:11:34 - progress_bar.py[line:274] - INFO: epoch 003:    876 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.5, bsz=40, num_updates=7690, lr=3.2855e-05, gnorm=0.795, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20304
2023-02-22 17:11:45 - progress_bar.py[line:274] - INFO: epoch 003:    886 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.91, wpb=112, bsz=40, num_updates=7700, lr=3.28293e-05, gnorm=0.846, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20315
2023-02-22 17:11:56 - progress_bar.py[line:274] - INFO: epoch 003:    896 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.88, wpb=110.6, bsz=40, num_updates=7710, lr=3.28036e-05, gnorm=0.605, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20326
2023-02-22 17:12:08 - progress_bar.py[line:274] - INFO: epoch 003:    906 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97, ups=0.87, wpb=111.1, bsz=40, num_updates=7720, lr=3.27779e-05, gnorm=0.824, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20338
2023-02-22 17:12:18 - progress_bar.py[line:274] - INFO: epoch 003:    916 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.91, wpb=111.9, bsz=40, num_updates=7730, lr=3.27521e-05, gnorm=0.791, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20349
2023-02-22 17:12:29 - progress_bar.py[line:274] - INFO: epoch 003:    926 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.93, wpb=111.6, bsz=40, num_updates=7740, lr=3.27264e-05, gnorm=0.595, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20359
2023-02-22 17:12:41 - progress_bar.py[line:274] - INFO: epoch 003:    936 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=111.4, bsz=40, num_updates=7750, lr=3.27007e-05, gnorm=0.628, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20371
2023-02-22 17:12:52 - progress_bar.py[line:274] - INFO: epoch 003:    946 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=111, bsz=40, num_updates=7760, lr=3.2675e-05, gnorm=0.945, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20382
2023-02-22 17:13:03 - progress_bar.py[line:274] - INFO: epoch 003:    956 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.87, wpb=112.3, bsz=40, num_updates=7770, lr=3.26493e-05, gnorm=0.756, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20394
2023-02-22 17:13:14 - progress_bar.py[line:274] - INFO: epoch 003:    966 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.7, ups=0.94, wpb=111.6, bsz=40, num_updates=7780, lr=3.26236e-05, gnorm=0.817, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20404
2023-02-22 17:13:25 - progress_bar.py[line:274] - INFO: epoch 003:    976 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=7790, lr=3.25979e-05, gnorm=0.782, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20416
2023-02-22 17:13:37 - progress_bar.py[line:274] - INFO: epoch 003:    986 / 3411 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=7800, lr=3.25721e-05, gnorm=0.819, clip=10, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=20427
2023-02-22 17:13:39 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 17:13:49 - progress_bar.py[line:274] - INFO: epoch 003:    997 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=92.1, ups=0.82, wpb=112.5, bsz=40, num_updates=7810, lr=3.25464e-05, gnorm=0.89, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=20439
2023-02-22 17:14:00 - progress_bar.py[line:274] - INFO: epoch 003:   1007 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=7820, lr=3.25207e-05, gnorm=0.712, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20450
2023-02-22 17:14:11 - progress_bar.py[line:274] - INFO: epoch 003:   1017 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=111.7, bsz=40, num_updates=7830, lr=3.2495e-05, gnorm=0.752, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20461
2023-02-22 17:14:22 - progress_bar.py[line:274] - INFO: epoch 003:   1027 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.1, bsz=40, num_updates=7840, lr=3.24693e-05, gnorm=1.001, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20472
2023-02-22 17:14:33 - progress_bar.py[line:274] - INFO: epoch 003:   1037 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.89, wpb=110.1, bsz=40, num_updates=7850, lr=3.24436e-05, gnorm=0.907, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20484
2023-02-22 17:14:45 - progress_bar.py[line:274] - INFO: epoch 003:   1047 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=7860, lr=3.24178e-05, gnorm=1.027, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20495
2023-02-22 17:14:56 - progress_bar.py[line:274] - INFO: epoch 003:   1057 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=7870, lr=3.23921e-05, gnorm=0.698, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20506
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 17:15:07 - progress_bar.py[line:274] - INFO: epoch 003:   1067 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.88, wpb=112.4, bsz=40, num_updates=7880, lr=3.23664e-05, gnorm=0.711, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20517
2023-02-22 17:15:18 - progress_bar.py[line:274] - INFO: epoch 003:   1077 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=7890, lr=3.23407e-05, gnorm=1.03, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20529
2023-02-22 17:15:30 - progress_bar.py[line:274] - INFO: epoch 003:   1087 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=7900, lr=3.2315e-05, gnorm=0.83, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20540
2023-02-22 17:15:40 - progress_bar.py[line:274] - INFO: epoch 003:   1097 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.9, ups=0.94, wpb=111.7, bsz=40, num_updates=7910, lr=3.22893e-05, gnorm=0.912, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20550
2023-02-22 17:15:52 - progress_bar.py[line:274] - INFO: epoch 003:   1107 / 3411 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=7920, lr=3.22635e-05, gnorm=1.075, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20562
2023-02-22 17:16:03 - progress_bar.py[line:274] - INFO: epoch 003:   1117 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=110.8, bsz=40, num_updates=7930, lr=3.22378e-05, gnorm=0.865, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20573
2023-02-22 17:16:14 - progress_bar.py[line:274] - INFO: epoch 003:   1127 / 3411 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.2, ups=0.88, wpb=109.3, bsz=40, num_updates=7940, lr=3.22121e-05, gnorm=1.214, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20584
2023-02-22 17:16:25 - progress_bar.py[line:274] - INFO: epoch 003:   1137 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=7950, lr=3.21864e-05, gnorm=0.905, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20596
2023-02-22 17:16:37 - progress_bar.py[line:274] - INFO: epoch 003:   1147 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=7960, lr=3.21607e-05, gnorm=1.019, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20607
2023-02-22 17:16:47 - progress_bar.py[line:274] - INFO: epoch 003:   1157 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.94, wpb=111.2, bsz=40, num_updates=7970, lr=3.2135e-05, gnorm=0.476, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20617
2023-02-22 17:16:58 - progress_bar.py[line:274] - INFO: epoch 003:   1167 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.8, ups=0.93, wpb=113.2, bsz=40, num_updates=7980, lr=3.21092e-05, gnorm=0.831, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20628
2023-02-22 17:17:09 - progress_bar.py[line:274] - INFO: epoch 003:   1177 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.92, wpb=111.4, bsz=40, num_updates=7990, lr=3.20835e-05, gnorm=0.816, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20639
2023-02-22 17:17:20 - progress_bar.py[line:274] - INFO: epoch 003:   1187 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=8000, lr=3.20578e-05, gnorm=0.699, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20651
2023-02-22 17:17:20 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 17:17:22 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 17:17:22 - train.py[line:551] - INFO: load:1.03 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 17:19:24 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 17:19:24 - train.py[line:551] - INFO: load:1.05 valid_run:122.53 task_valid:119.45 collect_output:1.93
2023-02-22 17:21:25 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 17:21:25 - train.py[line:551] - INFO: load:1.08 valid_run:243.17 task_valid:235.60 collect_output:5.35
2023-02-22 17:23:28 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 17:23:28 - train.py[line:551] - INFO: load:1.10 valid_run:366.34 task_valid:352.42 collect_output:10.68
2023-02-22 17:25:30 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 17:25:30 - train.py[line:551] - INFO: load:1.12 valid_run:488.44 task_valid:466.30 collect_output:17.87
2023-02-22 17:27:31 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 17:27:31 - train.py[line:551] - INFO: load:1.15 valid_run:609.31 task_valid:583.98 collect_output:20.05
2023-02-22 17:29:35 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 17:29:35 - train.py[line:551] - INFO: load:1.17 valid_run:732.38 task_valid:702.86 collect_output:23.20
2023-02-22 17:31:38 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 17:31:38 - train.py[line:551] - INFO: load:1.20 valid_run:855.66 task_valid:821.08 collect_output:27.23
2023-02-22 17:33:40 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 17:33:40 - train.py[line:551] - INFO: load:1.22 valid_run:977.72 task_valid:937.77 collect_output:31.56
2023-02-22 17:35:44 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 17:35:44 - train.py[line:551] - INFO: load:1.24 valid_run:1101.53 task_valid:1055.23 collect_output:36.86
2023-02-22 17:37:46 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 17:37:46 - train.py[line:551] - INFO: load:1.27 valid_run:1223.75 task_valid:1168.14 collect_output:45.12
2023-02-22 17:39:47 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 17:39:47 - train.py[line:551] - INFO: load:1.29 valid_run:1344.25 task_valid:1283.98 collect_output:48.75
2023-02-22 17:41:48 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 17:41:48 - train.py[line:551] - INFO: load:1.31 valid_run:1465.98 task_valid:1401.10 collect_output:52.35
2023-02-22 17:43:48 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 17:43:48 - train.py[line:551] - INFO: load:1.34 valid_run:1585.11 task_valid:1515.09 collect_output:56.46
2023-02-22 17:45:49 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 17:45:49 - train.py[line:551] - INFO: load:1.36 valid_run:1706.62 task_valid:1633.33 collect_output:58.70
2023-02-22 17:47:50 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 17:47:50 - train.py[line:551] - INFO: load:1.39 valid_run:1827.76 task_valid:1749.74 collect_output:62.41
2023-02-22 17:49:52 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 17:49:52 - train.py[line:551] - INFO: load:1.41 valid_run:1949.03 task_valid:1864.14 collect_output:68.23
2023-02-22 17:51:54 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 17:51:54 - train.py[line:551] - INFO: load:1.43 valid_run:2070.72 task_valid:1980.50 collect_output:72.54
2023-02-22 17:53:54 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 17:53:54 - train.py[line:551] - INFO: load:1.46 valid_run:2191.64 task_valid:2098.72 collect_output:74.20
2023-02-22 17:55:56 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 17:55:56 - train.py[line:551] - INFO: load:1.48 valid_run:2313.25 task_valid:2216.05 collect_output:77.44
2023-02-22 17:57:57 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 17:57:57 - train.py[line:551] - INFO: load:1.51 valid_run:2433.94 task_valid:2333.01 collect_output:80.15
2023-02-22 17:59:59 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 17:59:59 - train.py[line:551] - INFO: load:1.53 valid_run:2555.86 task_valid:2449.87 collect_output:84.17
2023-02-22 18:02:01 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 18:02:01 - train.py[line:551] - INFO: load:1.56 valid_run:2678.05 task_valid:2568.92 collect_output:86.29
2023-02-22 18:04:02 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 18:04:02 - train.py[line:551] - INFO: load:1.58 valid_run:2798.59 task_valid:2683.38 collect_output:91.34
2023-02-22 18:06:02 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 18:06:02 - train.py[line:551] - INFO: load:1.60 valid_run:2918.68 task_valid:2799.67 collect_output:94.12
2023-02-22 18:08:04 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 18:08:04 - train.py[line:551] - INFO: load:1.63 valid_run:3040.62 task_valid:2916.06 collect_output:98.63
2023-02-22 18:10:07 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 18:10:07 - train.py[line:551] - INFO: load:1.65 valid_run:3163.65 task_valid:3032.25 collect_output:104.45
2023-02-22 18:12:07 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 18:12:07 - train.py[line:551] - INFO: load:1.68 valid_run:3283.20 task_valid:3146.51 collect_output:108.73
2023-02-22 18:14:09 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 18:14:09 - train.py[line:551] - INFO: load:1.70 valid_run:3405.32 task_valid:3266.23 collect_output:110.11
2023-02-22 18:16:11 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 18:16:11 - train.py[line:551] - INFO: load:1.73 valid_run:3527.02 task_valid:3381.97 collect_output:115.06
2023-02-22 18:18:13 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 18:18:13 - train.py[line:551] - INFO: load:1.75 valid_run:3649.13 task_valid:3500.70 collect_output:117.44
2023-02-22 18:20:14 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 18:20:14 - train.py[line:551] - INFO: load:1.77 valid_run:3770.28 task_valid:3619.28 collect_output:118.98

====================================================================================================
SGG eval:     R @ 50: 0.6520;     R @ 100: 0.6867;     R @ 500: 0.7175;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4459;    mR @ 100: 0.4852;    mR @ 500: 0.5338;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7195) (covered in:0.6250) (covering:0.2286) (eating:0.7647) (flying in:0.9091) (growing on:0.2500) (hanging from:0.3871) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9363) (says:0.0000) (sitting on:0.6905) (standing on:0.5693) (using:0.6000) (walking in:0.0000) (walking on:0.5135) (watching:0.6944) 
--------------------------------------------------------
====================================================================================================

2023-02-22 18:20:45 - train.py[line:487] - INFO: 0.6867355233002291

====================================================================================================
SGG eval:     R @ 50: 0.6520;     R @ 100: 0.6867;     R @ 500: 0.7175;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4459;    mR @ 100: 0.4852;    mR @ 500: 0.5338;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7195) (covered in:0.6250) (covering:0.2286) (eating:0.7647) (flying in:0.9091) (growing on:0.2500) (hanging from:0.3871) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9363) (says:0.0000) (sitting on:0.6905) (standing on:0.5693) (using:0.6000) (walking in:0.0000) (walking on:0.5135) (watching:0.6944) 
--------------------------------------------------------
====================================================================================================

2023-02-22 18:20:45 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 18:20:45 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.25 | loss_v1 0 | loss_v2 0 | nll_loss 0.085 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.686736 | ppl 1.06 | vqa_score 0.5079 | wps 118 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.686736
2023-02-22 18:20:45 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 8000 updates
2023-02-22 18:20:45 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_3_8000.pt
2023-02-22 18:20:53 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_3_8000.pt
2023-02-22 18:20:58 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_3_8000.pt (epoch 3 @ 8000 updates, score 0.6867355233002291) (writing took 12.304685276001692 seconds)
2023-02-22 18:21:09 - progress_bar.py[line:274] - INFO: epoch 003:   1197 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=113.1, bsz=40, num_updates=8010, lr=3.20321e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24479
2023-02-22 18:21:20 - progress_bar.py[line:274] - INFO: epoch 003:   1207 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=8020, lr=3.20064e-05, gnorm=0.648, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24491
2023-02-22 18:21:32 - progress_bar.py[line:274] - INFO: epoch 003:   1217 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.2, bsz=40, num_updates=8030, lr=3.19807e-05, gnorm=0.961, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24502
2023-02-22 18:21:43 - progress_bar.py[line:274] - INFO: epoch 003:   1227 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=111.8, bsz=40, num_updates=8040, lr=3.19549e-05, gnorm=1.013, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24513
2023-02-22 18:21:54 - progress_bar.py[line:274] - INFO: epoch 003:   1237 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=8050, lr=3.19292e-05, gnorm=0.891, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=24524
2023-02-22 18:22:06 - progress_bar.py[line:274] - INFO: epoch 003:   1247 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.87, wpb=111.8, bsz=40, num_updates=8060, lr=3.19035e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24536
2023-02-22 18:22:17 - progress_bar.py[line:274] - INFO: epoch 003:   1257 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=111.9, bsz=40, num_updates=8070, lr=3.18778e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24547
2023-02-22 18:22:28 - progress_bar.py[line:274] - INFO: epoch 003:   1267 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.7, bsz=40, num_updates=8080, lr=3.18521e-05, gnorm=0.533, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24558
2023-02-22 18:22:39 - progress_bar.py[line:274] - INFO: epoch 003:   1277 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=8090, lr=3.18264e-05, gnorm=0.696, clip=20, loss_scale=512, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=24569
2023-02-22 18:22:50 - progress_bar.py[line:274] - INFO: epoch 003:   1287 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.91, wpb=113, bsz=40, num_updates=8100, lr=3.18006e-05, gnorm=0.904, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24580
2023-02-22 18:23:02 - progress_bar.py[line:274] - INFO: epoch 003:   1297 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.8, bsz=40, num_updates=8110, lr=3.17749e-05, gnorm=0.901, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24592
2023-02-22 18:23:14 - progress_bar.py[line:274] - INFO: epoch 003:   1307 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.9, wpb=109.7, bsz=40, num_updates=8120, lr=3.17492e-05, gnorm=0.708, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24603
2023-02-22 18:23:25 - progress_bar.py[line:274] - INFO: epoch 003:   1317 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=111.3, bsz=40, num_updates=8130, lr=3.17235e-05, gnorm=0.554, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24616
2023-02-22 18:23:37 - progress_bar.py[line:274] - INFO: epoch 003:   1327 / 3411 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112.5, bsz=40, num_updates=8140, lr=3.16978e-05, gnorm=0.959, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24627
2023-02-22 18:23:48 - progress_bar.py[line:274] - INFO: epoch 003:   1337 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=112.2, bsz=40, num_updates=8150, lr=3.16721e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24638
2023-02-22 18:24:00 - progress_bar.py[line:274] - INFO: epoch 003:   1347 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.87, wpb=112.2, bsz=40, num_updates=8160, lr=3.16464e-05, gnorm=0.856, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24650
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:24:11 - progress_bar.py[line:274] - INFO: epoch 003:   1357 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.87, wpb=112.1, bsz=40, num_updates=8170, lr=3.16206e-05, gnorm=0.904, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24661
2023-02-22 18:24:23 - progress_bar.py[line:274] - INFO: epoch 003:   1367 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.88, wpb=110.9, bsz=40, num_updates=8180, lr=3.15949e-05, gnorm=0.55, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24673
2023-02-22 18:24:34 - progress_bar.py[line:274] - INFO: epoch 003:   1377 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.7, bsz=40, num_updates=8190, lr=3.15692e-05, gnorm=0.99, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24684
2023-02-22 18:24:45 - progress_bar.py[line:274] - INFO: epoch 003:   1387 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=112.6, bsz=40, num_updates=8200, lr=3.15435e-05, gnorm=1.088, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24695
2023-02-22 18:24:56 - progress_bar.py[line:274] - INFO: epoch 003:   1397 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.88, wpb=112.3, bsz=40, num_updates=8210, lr=3.15178e-05, gnorm=1.075, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24706
2023-02-22 18:25:07 - progress_bar.py[line:274] - INFO: epoch 003:   1407 / 3411 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112.5, bsz=40, num_updates=8220, lr=3.14921e-05, gnorm=0.952, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24718
2023-02-22 18:25:19 - progress_bar.py[line:274] - INFO: epoch 003:   1417 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=8230, lr=3.14663e-05, gnorm=0.793, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24729
2023-02-22 18:25:30 - progress_bar.py[line:274] - INFO: epoch 003:   1427 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.7, bsz=40, num_updates=8240, lr=3.14406e-05, gnorm=0.962, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24740
2023-02-22 18:25:41 - progress_bar.py[line:274] - INFO: epoch 003:   1437 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.91, wpb=112.5, bsz=40, num_updates=8250, lr=3.14149e-05, gnorm=0.815, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24751
2023-02-22 18:25:52 - progress_bar.py[line:274] - INFO: epoch 003:   1447 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=8260, lr=3.13892e-05, gnorm=0.892, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24763
2023-02-22 18:26:04 - progress_bar.py[line:274] - INFO: epoch 003:   1457 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.87, wpb=112.7, bsz=40, num_updates=8270, lr=3.13635e-05, gnorm=1.019, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24774
2023-02-22 18:26:15 - progress_bar.py[line:274] - INFO: epoch 003:   1467 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=113.1, bsz=40, num_updates=8280, lr=3.13378e-05, gnorm=0.865, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24785
2023-02-22 18:26:26 - progress_bar.py[line:274] - INFO: epoch 003:   1477 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.5, bsz=40, num_updates=8290, lr=3.1312e-05, gnorm=0.835, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24796
2023-02-22 18:26:37 - progress_bar.py[line:274] - INFO: epoch 003:   1487 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.7, bsz=40, num_updates=8300, lr=3.12863e-05, gnorm=0.86, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24808
2023-02-22 18:26:49 - progress_bar.py[line:274] - INFO: epoch 003:   1497 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=8310, lr=3.12606e-05, gnorm=0.836, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24819
2023-02-22 18:26:59 - progress_bar.py[line:274] - INFO: epoch 003:   1507 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.92, wpb=112.5, bsz=40, num_updates=8320, lr=3.12349e-05, gnorm=0.823, clip=30, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=24830
2023-02-22 18:27:08 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 18:27:12 - progress_bar.py[line:274] - INFO: epoch 003:   1518 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=91.8, ups=0.82, wpb=112.1, bsz=40, num_updates=8330, lr=3.12092e-05, gnorm=1.136, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=24842
2023-02-22 18:27:23 - progress_bar.py[line:274] - INFO: epoch 003:   1528 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=8340, lr=3.11835e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24853
2023-02-22 18:27:34 - progress_bar.py[line:274] - INFO: epoch 003:   1538 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=111.8, bsz=40, num_updates=8350, lr=3.11577e-05, gnorm=0.744, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24864
2023-02-22 18:27:45 - progress_bar.py[line:274] - INFO: epoch 003:   1548 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.3, bsz=40, num_updates=8360, lr=3.1132e-05, gnorm=1.218, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24875
2023-02-22 18:27:56 - progress_bar.py[line:274] - INFO: epoch 003:   1558 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.9, wpb=113, bsz=40, num_updates=8370, lr=3.11063e-05, gnorm=1.147, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24886
2023-02-22 18:28:07 - progress_bar.py[line:274] - INFO: epoch 003:   1568 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=8380, lr=3.10806e-05, gnorm=0.818, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24897
2023-02-22 18:28:18 - progress_bar.py[line:274] - INFO: epoch 003:   1578 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.91, wpb=111.4, bsz=40, num_updates=8390, lr=3.10549e-05, gnorm=0.871, clip=30, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=24909
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:28:30 - progress_bar.py[line:274] - INFO: epoch 003:   1588 / 3411 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.88, wpb=112.5, bsz=40, num_updates=8400, lr=3.10292e-05, gnorm=0.485, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24920
2023-02-22 18:28:41 - progress_bar.py[line:274] - INFO: epoch 003:   1598 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=8410, lr=3.10034e-05, gnorm=0.798, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24931
2023-02-22 18:28:52 - progress_bar.py[line:274] - INFO: epoch 003:   1608 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112, bsz=40, num_updates=8420, lr=3.09777e-05, gnorm=0.559, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24942
2023-02-22 18:29:03 - progress_bar.py[line:274] - INFO: epoch 003:   1618 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.9, wpb=113.2, bsz=40, num_updates=8430, lr=3.0952e-05, gnorm=1.053, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24953
2023-02-22 18:29:14 - progress_bar.py[line:274] - INFO: epoch 003:   1628 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=8440, lr=3.09263e-05, gnorm=0.829, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24964
2023-02-22 18:29:25 - progress_bar.py[line:274] - INFO: epoch 003:   1638 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=113, bsz=40, num_updates=8450, lr=3.09006e-05, gnorm=0.772, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24975
2023-02-22 18:29:36 - progress_bar.py[line:274] - INFO: epoch 003:   1648 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=111.5, bsz=40, num_updates=8460, lr=3.08749e-05, gnorm=0.72, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24987
2023-02-22 18:29:48 - progress_bar.py[line:274] - INFO: epoch 003:   1658 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=8470, lr=3.08491e-05, gnorm=0.722, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24998
2023-02-22 18:29:59 - progress_bar.py[line:274] - INFO: epoch 003:   1668 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97, ups=0.87, wpb=111.2, bsz=40, num_updates=8480, lr=3.08234e-05, gnorm=1.173, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25009
2023-02-22 18:30:10 - progress_bar.py[line:274] - INFO: epoch 003:   1678 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.9, bsz=40, num_updates=8490, lr=3.07977e-05, gnorm=1.041, clip=50, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=25021
2023-02-22 18:30:22 - progress_bar.py[line:274] - INFO: epoch 003:   1688 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=112.2, bsz=40, num_updates=8500, lr=3.0772e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25032
2023-02-22 18:30:33 - progress_bar.py[line:274] - INFO: epoch 003:   1698 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=8510, lr=3.07463e-05, gnorm=0.538, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25043
2023-02-22 18:30:44 - progress_bar.py[line:274] - INFO: epoch 003:   1708 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=8520, lr=3.07206e-05, gnorm=0.646, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25054
2023-02-22 18:30:55 - progress_bar.py[line:274] - INFO: epoch 003:   1718 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.88, wpb=112.9, bsz=40, num_updates=8530, lr=3.06949e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25065
2023-02-22 18:31:07 - progress_bar.py[line:274] - INFO: epoch 003:   1728 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.8, ups=0.87, wpb=111.3, bsz=40, num_updates=8540, lr=3.06691e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25077
2023-02-22 18:31:19 - progress_bar.py[line:274] - INFO: epoch 003:   1738 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=8550, lr=3.06434e-05, gnorm=0.899, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25089
2023-02-22 18:31:30 - progress_bar.py[line:274] - INFO: epoch 003:   1748 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.6, bsz=40, num_updates=8560, lr=3.06177e-05, gnorm=0.59, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25100
2023-02-22 18:31:41 - progress_bar.py[line:274] - INFO: epoch 003:   1758 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.6, ups=0.94, wpb=111.2, bsz=40, num_updates=8570, lr=3.0592e-05, gnorm=0.777, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25111
2023-02-22 18:31:52 - progress_bar.py[line:274] - INFO: epoch 003:   1768 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.92, wpb=111.7, bsz=40, num_updates=8580, lr=3.05663e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25122
2023-02-22 18:32:03 - progress_bar.py[line:274] - INFO: epoch 003:   1778 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.4, bsz=40, num_updates=8590, lr=3.05406e-05, gnorm=0.719, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25133
2023-02-22 18:32:14 - progress_bar.py[line:274] - INFO: epoch 003:   1788 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=8600, lr=3.05148e-05, gnorm=0.73, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25144
2023-02-22 18:32:26 - progress_bar.py[line:274] - INFO: epoch 003:   1798 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=8610, lr=3.04891e-05, gnorm=0.742, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25155
2023-02-22 18:32:36 - progress_bar.py[line:274] - INFO: epoch 003:   1808 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=111.2, bsz=40, num_updates=8620, lr=3.04634e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25167
2023-02-22 18:32:47 - progress_bar.py[line:274] - INFO: epoch 003:   1818 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104, ups=0.94, wpb=110.9, bsz=40, num_updates=8630, lr=3.04377e-05, gnorm=1.141, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25177
2023-02-22 18:32:58 - progress_bar.py[line:274] - INFO: epoch 003:   1828 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.88, wpb=112.6, bsz=40, num_updates=8640, lr=3.0412e-05, gnorm=0.755, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25189
2023-02-22 18:33:10 - progress_bar.py[line:274] - INFO: epoch 003:   1838 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=8650, lr=3.03863e-05, gnorm=0.615, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25200
2023-02-22 18:33:21 - progress_bar.py[line:274] - INFO: epoch 003:   1848 / 3411 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=8660, lr=3.03605e-05, gnorm=0.809, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25211
2023-02-22 18:33:33 - progress_bar.py[line:274] - INFO: epoch 003:   1858 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.5, bsz=40, num_updates=8670, lr=3.03348e-05, gnorm=0.715, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25222
2023-02-22 18:33:44 - progress_bar.py[line:274] - INFO: epoch 003:   1868 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.93, wpb=111, bsz=40, num_updates=8680, lr=3.03091e-05, gnorm=0.705, clip=30, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=25234
2023-02-22 18:33:55 - progress_bar.py[line:274] - INFO: epoch 003:   1878 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.91, wpb=112.9, bsz=40, num_updates=8690, lr=3.02834e-05, gnorm=0.686, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25245
2023-02-22 18:34:07 - progress_bar.py[line:274] - INFO: epoch 003:   1888 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=112, bsz=40, num_updates=8700, lr=3.02577e-05, gnorm=0.535, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25257
2023-02-22 18:34:18 - progress_bar.py[line:274] - INFO: epoch 003:   1898 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.88, wpb=112.5, bsz=40, num_updates=8710, lr=3.0232e-05, gnorm=0.843, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25268
2023-02-22 18:34:29 - progress_bar.py[line:274] - INFO: epoch 003:   1908 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.91, wpb=109.3, bsz=40, num_updates=8720, lr=3.02062e-05, gnorm=0.828, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25279
2023-02-22 18:34:41 - progress_bar.py[line:274] - INFO: epoch 003:   1918 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.88, wpb=112.3, bsz=40, num_updates=8730, lr=3.01805e-05, gnorm=1.041, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25291
2023-02-22 18:34:52 - progress_bar.py[line:274] - INFO: epoch 003:   1928 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=111.4, bsz=40, num_updates=8740, lr=3.01548e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25302
2023-02-22 18:35:03 - progress_bar.py[line:274] - INFO: epoch 003:   1938 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=110.9, bsz=40, num_updates=8750, lr=3.01291e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25313
2023-02-22 18:35:14 - progress_bar.py[line:274] - INFO: epoch 003:   1948 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=8760, lr=3.01034e-05, gnorm=0.973, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25324
2023-02-22 18:35:25 - progress_bar.py[line:274] - INFO: epoch 003:   1958 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.93, wpb=111, bsz=40, num_updates=8770, lr=3.00777e-05, gnorm=0.865, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25335
2023-02-22 18:35:36 - progress_bar.py[line:274] - INFO: epoch 003:   1968 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.87, wpb=110.7, bsz=40, num_updates=8780, lr=3.00519e-05, gnorm=0.539, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25347
2023-02-22 18:35:47 - progress_bar.py[line:274] - INFO: epoch 003:   1978 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=112.7, bsz=40, num_updates=8790, lr=3.00262e-05, gnorm=0.653, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25358
2023-02-22 18:35:59 - progress_bar.py[line:274] - INFO: epoch 003:   1988 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.2, bsz=40, num_updates=8800, lr=3.00005e-05, gnorm=0.88, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25369
2023-02-22 18:36:10 - progress_bar.py[line:274] - INFO: epoch 003:   1998 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=8810, lr=2.99748e-05, gnorm=0.742, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25380
2023-02-22 18:36:21 - progress_bar.py[line:274] - INFO: epoch 003:   2008 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.5, bsz=40, num_updates=8820, lr=2.99491e-05, gnorm=0.783, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25391
2023-02-22 18:36:32 - progress_bar.py[line:274] - INFO: epoch 003:   2018 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=111.7, bsz=40, num_updates=8830, lr=2.99234e-05, gnorm=0.803, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25402
2023-02-22 18:36:43 - progress_bar.py[line:274] - INFO: epoch 003:   2028 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=111.9, bsz=40, num_updates=8840, lr=2.98976e-05, gnorm=0.635, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25413
2023-02-22 18:36:55 - progress_bar.py[line:274] - INFO: epoch 003:   2038 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.8, bsz=40, num_updates=8850, lr=2.98719e-05, gnorm=0.775, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25425
2023-02-22 18:37:06 - progress_bar.py[line:274] - INFO: epoch 003:   2048 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=113.2, bsz=40, num_updates=8860, lr=2.98462e-05, gnorm=0.742, clip=40, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=25436
2023-02-22 18:37:17 - progress_bar.py[line:274] - INFO: epoch 003:   2058 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=8870, lr=2.98205e-05, gnorm=0.538, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25447
2023-02-22 18:37:28 - progress_bar.py[line:274] - INFO: epoch 003:   2068 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.7, ups=0.94, wpb=112, bsz=40, num_updates=8880, lr=2.97948e-05, gnorm=0.818, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25458
2023-02-22 18:37:38 - progress_bar.py[line:274] - INFO: epoch 003:   2078 / 3411 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.5, ups=0.95, wpb=111.6, bsz=40, num_updates=8890, lr=2.97691e-05, gnorm=0.909, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25469
2023-02-22 18:37:50 - progress_bar.py[line:274] - INFO: epoch 003:   2088 / 3411 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=8900, lr=2.97434e-05, gnorm=1.106, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25480
2023-02-22 18:38:01 - progress_bar.py[line:274] - INFO: epoch 003:   2098 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.2, bsz=40, num_updates=8910, lr=2.97176e-05, gnorm=0.882, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25491
2023-02-22 18:38:12 - progress_bar.py[line:274] - INFO: epoch 003:   2108 / 3411 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.4, bsz=40, num_updates=8920, lr=2.96919e-05, gnorm=0.475, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25502
2023-02-22 18:38:23 - progress_bar.py[line:274] - INFO: epoch 003:   2118 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.92, wpb=111.4, bsz=40, num_updates=8930, lr=2.96662e-05, gnorm=0.753, clip=30, loss_scale=1024, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=25513
2023-02-22 18:38:34 - progress_bar.py[line:274] - INFO: epoch 003:   2128 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.88, wpb=112.6, bsz=40, num_updates=8940, lr=2.96405e-05, gnorm=0.541, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25524
@@@@ ERROR IN DATA @@@@ play
2023-02-22 18:38:46 - progress_bar.py[line:274] - INFO: epoch 003:   2138 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=8950, lr=2.96148e-05, gnorm=0.578, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25536
2023-02-22 18:38:57 - progress_bar.py[line:274] - INFO: epoch 003:   2148 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=8960, lr=2.95891e-05, gnorm=0.481, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25547
2023-02-22 18:39:08 - progress_bar.py[line:274] - INFO: epoch 003:   2158 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.93, wpb=111.5, bsz=40, num_updates=8970, lr=2.95633e-05, gnorm=0.468, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25558
2023-02-22 18:39:19 - progress_bar.py[line:274] - INFO: epoch 003:   2168 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=8980, lr=2.95376e-05, gnorm=0.797, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25569
2023-02-22 18:39:30 - progress_bar.py[line:274] - INFO: epoch 003:   2178 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=8990, lr=2.95119e-05, gnorm=0.82, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25580
2023-02-22 18:39:41 - progress_bar.py[line:274] - INFO: epoch 003:   2188 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.6, bsz=40, num_updates=9000, lr=2.94862e-05, gnorm=0.794, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25591
2023-02-22 18:39:52 - progress_bar.py[line:274] - INFO: epoch 003:   2198 / 3411 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.91, wpb=111.6, bsz=40, num_updates=9010, lr=2.94605e-05, gnorm=0.99, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25602
2023-02-22 18:40:03 - progress_bar.py[line:274] - INFO: epoch 003:   2208 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=112.1, bsz=40, num_updates=9020, lr=2.94348e-05, gnorm=0.955, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25613
2023-02-22 18:40:14 - progress_bar.py[line:274] - INFO: epoch 003:   2218 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=112.5, bsz=40, num_updates=9030, lr=2.9409e-05, gnorm=0.573, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25625
2023-02-22 18:40:15 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 18:40:27 - progress_bar.py[line:274] - INFO: epoch 003:   2229 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=89.1, ups=0.8, wpb=111.7, bsz=40, num_updates=9040, lr=2.93833e-05, gnorm=0.672, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=25637
2023-02-22 18:40:38 - progress_bar.py[line:274] - INFO: epoch 003:   2239 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=112.4, bsz=40, num_updates=9050, lr=2.93576e-05, gnorm=0.737, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25648
2023-02-22 18:40:49 - progress_bar.py[line:274] - INFO: epoch 003:   2249 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.93, wpb=111.7, bsz=40, num_updates=9060, lr=2.93319e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25659
2023-02-22 18:41:00 - progress_bar.py[line:274] - INFO: epoch 003:   2259 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.8, bsz=40, num_updates=9070, lr=2.93062e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25670
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:41:11 - progress_bar.py[line:274] - INFO: epoch 003:   2269 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=9080, lr=2.92805e-05, gnorm=0.671, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25681
2023-02-22 18:41:23 - progress_bar.py[line:274] - INFO: epoch 003:   2279 / 3411 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.88, wpb=111.5, bsz=40, num_updates=9090, lr=2.92547e-05, gnorm=0.891, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25693
2023-02-22 18:41:34 - progress_bar.py[line:274] - INFO: epoch 003:   2289 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=9100, lr=2.9229e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25704
2023-02-22 18:41:45 - progress_bar.py[line:274] - INFO: epoch 003:   2299 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.4, bsz=40, num_updates=9110, lr=2.92033e-05, gnorm=0.652, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25715
2023-02-22 18:41:56 - progress_bar.py[line:274] - INFO: epoch 003:   2309 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=113.1, bsz=40, num_updates=9120, lr=2.91776e-05, gnorm=0.581, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25726
2023-02-22 18:42:07 - progress_bar.py[line:274] - INFO: epoch 003:   2319 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.92, wpb=111.2, bsz=40, num_updates=9130, lr=2.91519e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25737
2023-02-22 18:42:18 - progress_bar.py[line:274] - INFO: epoch 003:   2329 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.3, bsz=40, num_updates=9140, lr=2.91262e-05, gnorm=0.722, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=25749
2023-02-22 18:42:30 - progress_bar.py[line:274] - INFO: epoch 003:   2339 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=9150, lr=2.91004e-05, gnorm=0.667, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25760
2023-02-22 18:42:41 - progress_bar.py[line:274] - INFO: epoch 003:   2349 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.91, wpb=112.5, bsz=40, num_updates=9160, lr=2.90747e-05, gnorm=0.902, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25771
2023-02-22 18:42:52 - progress_bar.py[line:274] - INFO: epoch 003:   2359 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.2, bsz=40, num_updates=9170, lr=2.9049e-05, gnorm=0.629, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25782
2023-02-22 18:43:03 - progress_bar.py[line:274] - INFO: epoch 003:   2369 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=9180, lr=2.90233e-05, gnorm=0.672, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25793
2023-02-22 18:43:14 - progress_bar.py[line:274] - INFO: epoch 003:   2379 / 3411 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.5, bsz=40, num_updates=9190, lr=2.89976e-05, gnorm=0.582, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25804
2023-02-22 18:43:25 - progress_bar.py[line:274] - INFO: epoch 003:   2389 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111, bsz=40, num_updates=9200, lr=2.89719e-05, gnorm=0.732, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25816
2023-02-22 18:43:36 - progress_bar.py[line:274] - INFO: epoch 003:   2399 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=9210, lr=2.89462e-05, gnorm=0.904, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25827
2023-02-22 18:43:48 - progress_bar.py[line:274] - INFO: epoch 003:   2409 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.86, wpb=113.2, bsz=40, num_updates=9220, lr=2.89204e-05, gnorm=0.513, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25838
2023-02-22 18:44:00 - progress_bar.py[line:274] - INFO: epoch 003:   2419 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.87, wpb=112.3, bsz=40, num_updates=9230, lr=2.88947e-05, gnorm=0.788, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25850
2023-02-22 18:44:11 - progress_bar.py[line:274] - INFO: epoch 003:   2429 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.9, bsz=40, num_updates=9240, lr=2.8869e-05, gnorm=0.782, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25861
2023-02-22 18:44:22 - progress_bar.py[line:274] - INFO: epoch 003:   2439 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.6, bsz=40, num_updates=9250, lr=2.88433e-05, gnorm=0.817, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25873
2023-02-22 18:44:34 - progress_bar.py[line:274] - INFO: epoch 003:   2449 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.87, wpb=112.7, bsz=40, num_updates=9260, lr=2.88176e-05, gnorm=0.578, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25884
2023-02-22 18:44:45 - progress_bar.py[line:274] - INFO: epoch 003:   2459 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=9270, lr=2.87919e-05, gnorm=0.604, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25895
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:44:56 - progress_bar.py[line:274] - INFO: epoch 003:   2469 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.92, wpb=112.4, bsz=40, num_updates=9280, lr=2.87661e-05, gnorm=0.716, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25906
2023-02-22 18:45:07 - progress_bar.py[line:274] - INFO: epoch 003:   2479 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.91, wpb=113.1, bsz=40, num_updates=9290, lr=2.87404e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25917
2023-02-22 18:45:18 - progress_bar.py[line:274] - INFO: epoch 003:   2489 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=9300, lr=2.87147e-05, gnorm=0.945, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25929
2023-02-22 18:45:30 - progress_bar.py[line:274] - INFO: epoch 003:   2499 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=9310, lr=2.8689e-05, gnorm=0.732, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25940
2023-02-22 18:45:41 - progress_bar.py[line:274] - INFO: epoch 003:   2509 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.91, wpb=110.5, bsz=40, num_updates=9320, lr=2.86633e-05, gnorm=0.898, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25951
2023-02-22 18:45:52 - progress_bar.py[line:274] - INFO: epoch 003:   2519 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=110.5, bsz=40, num_updates=9330, lr=2.86376e-05, gnorm=0.873, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25962
2023-02-22 18:46:04 - progress_bar.py[line:274] - INFO: epoch 003:   2529 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.88, wpb=111.5, bsz=40, num_updates=9340, lr=2.86118e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=9.4, ema_decay=0.9999, wall=25974
2023-02-22 18:46:15 - progress_bar.py[line:274] - INFO: epoch 003:   2539 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=112, bsz=40, num_updates=9350, lr=2.85861e-05, gnorm=0.886, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25985
2023-02-22 18:46:26 - progress_bar.py[line:274] - INFO: epoch 003:   2549 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=9360, lr=2.85604e-05, gnorm=0.794, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25996
2023-02-22 18:46:37 - progress_bar.py[line:274] - INFO: epoch 003:   2559 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.1, bsz=40, num_updates=9370, lr=2.85347e-05, gnorm=0.656, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26007
2023-02-22 18:46:48 - progress_bar.py[line:274] - INFO: epoch 003:   2569 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=9380, lr=2.8509e-05, gnorm=0.899, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26018
2023-02-22 18:46:59 - progress_bar.py[line:274] - INFO: epoch 003:   2579 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.7, bsz=40, num_updates=9390, lr=2.84833e-05, gnorm=0.758, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26029
2023-02-22 18:47:10 - progress_bar.py[line:274] - INFO: epoch 003:   2589 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=112, bsz=40, num_updates=9400, lr=2.84575e-05, gnorm=0.609, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26040
2023-02-22 18:47:22 - progress_bar.py[line:274] - INFO: epoch 003:   2599 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=9410, lr=2.84318e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26052
2023-02-22 18:47:33 - progress_bar.py[line:274] - INFO: epoch 003:   2609 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=9420, lr=2.84061e-05, gnorm=0.8, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26063
2023-02-22 18:47:44 - progress_bar.py[line:274] - INFO: epoch 003:   2619 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=9430, lr=2.83804e-05, gnorm=0.643, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26074
2023-02-22 18:47:55 - progress_bar.py[line:274] - INFO: epoch 003:   2629 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=9440, lr=2.83547e-05, gnorm=0.756, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26086
2023-02-22 18:48:07 - progress_bar.py[line:274] - INFO: epoch 003:   2639 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=111.8, bsz=40, num_updates=9450, lr=2.8329e-05, gnorm=0.625, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26097
2023-02-22 18:48:18 - progress_bar.py[line:274] - INFO: epoch 003:   2649 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.5, bsz=40, num_updates=9460, lr=2.83032e-05, gnorm=0.735, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26108
2023-02-22 18:48:30 - progress_bar.py[line:274] - INFO: epoch 003:   2659 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.9, wpb=113.8, bsz=40, num_updates=9470, lr=2.82775e-05, gnorm=0.743, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26120
2023-02-22 18:48:41 - progress_bar.py[line:274] - INFO: epoch 003:   2669 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.5, bsz=40, num_updates=9480, lr=2.82518e-05, gnorm=0.626, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26131
2023-02-22 18:48:52 - progress_bar.py[line:274] - INFO: epoch 003:   2679 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.2, bsz=40, num_updates=9490, lr=2.82261e-05, gnorm=0.75, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26142
2023-02-22 18:49:03 - progress_bar.py[line:274] - INFO: epoch 003:   2689 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=112.1, bsz=40, num_updates=9500, lr=2.82004e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26153
2023-02-22 18:49:14 - progress_bar.py[line:274] - INFO: epoch 003:   2699 / 3411 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=111.8, bsz=40, num_updates=9510, lr=2.81747e-05, gnorm=0.809, clip=40, loss_scale=512, train_wall=11, gb_free=9.1, ema_decay=0.9999, wall=26164
2023-02-22 18:49:25 - progress_bar.py[line:274] - INFO: epoch 003:   2709 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.91, wpb=112.4, bsz=40, num_updates=9520, lr=2.81489e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26175
2023-02-22 18:49:36 - progress_bar.py[line:274] - INFO: epoch 003:   2719 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=112, bsz=40, num_updates=9530, lr=2.81232e-05, gnorm=0.659, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26186
2023-02-22 18:49:48 - progress_bar.py[line:274] - INFO: epoch 003:   2729 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.88, wpb=111.3, bsz=40, num_updates=9540, lr=2.80975e-05, gnorm=0.917, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26198
2023-02-22 18:49:59 - progress_bar.py[line:274] - INFO: epoch 003:   2739 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.87, wpb=112.4, bsz=40, num_updates=9550, lr=2.80718e-05, gnorm=0.66, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26209
2023-02-22 18:50:11 - progress_bar.py[line:274] - INFO: epoch 003:   2749 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.89, wpb=111.3, bsz=40, num_updates=9560, lr=2.80461e-05, gnorm=0.737, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26221
2023-02-22 18:50:22 - progress_bar.py[line:274] - INFO: epoch 003:   2759 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.7, bsz=40, num_updates=9570, lr=2.80204e-05, gnorm=0.819, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26232
2023-02-22 18:50:33 - progress_bar.py[line:274] - INFO: epoch 003:   2769 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=112.1, bsz=40, num_updates=9580, lr=2.79947e-05, gnorm=0.656, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26243
2023-02-22 18:50:44 - progress_bar.py[line:274] - INFO: epoch 003:   2779 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.9, wpb=113, bsz=40, num_updates=9590, lr=2.79689e-05, gnorm=0.757, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26254
2023-02-22 18:50:56 - progress_bar.py[line:274] - INFO: epoch 003:   2789 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=112, bsz=40, num_updates=9600, lr=2.79432e-05, gnorm=0.968, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26266
2023-02-22 18:51:06 - progress_bar.py[line:274] - INFO: epoch 003:   2799 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.4, ups=0.92, wpb=113, bsz=40, num_updates=9610, lr=2.79175e-05, gnorm=1.146, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26277
2023-02-22 18:51:17 - progress_bar.py[line:274] - INFO: epoch 003:   2809 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104, ups=0.92, wpb=113.6, bsz=40, num_updates=9620, lr=2.78918e-05, gnorm=0.613, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26287
2023-02-22 18:51:28 - progress_bar.py[line:274] - INFO: epoch 003:   2819 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=9630, lr=2.78661e-05, gnorm=0.748, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26299
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:51:40 - progress_bar.py[line:274] - INFO: epoch 003:   2829 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=9640, lr=2.78404e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26310
2023-02-22 18:51:51 - progress_bar.py[line:274] - INFO: epoch 003:   2839 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=113.5, bsz=40, num_updates=9650, lr=2.78146e-05, gnorm=0.88, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26321
2023-02-22 18:52:02 - progress_bar.py[line:274] - INFO: epoch 003:   2849 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.9, wpb=110.7, bsz=40, num_updates=9660, lr=2.77889e-05, gnorm=0.768, clip=30, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=26332
2023-02-22 18:52:13 - progress_bar.py[line:274] - INFO: epoch 003:   2859 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=9670, lr=2.77632e-05, gnorm=0.731, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26343
2023-02-22 18:52:24 - progress_bar.py[line:274] - INFO: epoch 003:   2869 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=9680, lr=2.77375e-05, gnorm=0.644, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26355
2023-02-22 18:52:36 - progress_bar.py[line:274] - INFO: epoch 003:   2879 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=9690, lr=2.77118e-05, gnorm=0.848, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26366
@@@@ ERROR IN DATA @@@@ play
2023-02-22 18:52:47 - progress_bar.py[line:274] - INFO: epoch 003:   2889 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=9700, lr=2.76861e-05, gnorm=0.417, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26377
2023-02-22 18:52:58 - progress_bar.py[line:274] - INFO: epoch 003:   2899 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=9710, lr=2.76603e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26388
2023-02-22 18:53:10 - progress_bar.py[line:274] - INFO: epoch 003:   2909 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=112, bsz=40, num_updates=9720, lr=2.76346e-05, gnorm=0.992, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26400
2023-02-22 18:53:21 - progress_bar.py[line:274] - INFO: epoch 003:   2919 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=9730, lr=2.76089e-05, gnorm=0.457, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26411
2023-02-22 18:53:32 - progress_bar.py[line:274] - INFO: epoch 003:   2929 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.5, ups=0.93, wpb=112.7, bsz=40, num_updates=9740, lr=2.75832e-05, gnorm=0.919, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26422
2023-02-22 18:53:43 - progress_bar.py[line:274] - INFO: epoch 003:   2939 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=111.6, bsz=40, num_updates=9750, lr=2.75575e-05, gnorm=0.539, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26433
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:53:55 - progress_bar.py[line:274] - INFO: epoch 003:   2949 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111.8, bsz=40, num_updates=9760, lr=2.75318e-05, gnorm=0.531, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26445
2023-02-22 18:54:05 - progress_bar.py[line:274] - INFO: epoch 003:   2959 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112.5, bsz=40, num_updates=9770, lr=2.7506e-05, gnorm=0.518, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26456
2023-02-22 18:54:16 - progress_bar.py[line:274] - INFO: epoch 003:   2969 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.3, ups=0.94, wpb=111.9, bsz=40, num_updates=9780, lr=2.74803e-05, gnorm=0.801, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26466
2023-02-22 18:54:28 - progress_bar.py[line:274] - INFO: epoch 003:   2979 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.3, ups=0.87, wpb=110.1, bsz=40, num_updates=9790, lr=2.74546e-05, gnorm=1.095, clip=30, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=26478
2023-02-22 18:54:39 - progress_bar.py[line:274] - INFO: epoch 003:   2989 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.9, wpb=109.2, bsz=40, num_updates=9800, lr=2.74289e-05, gnorm=0.612, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26489
2023-02-22 18:54:50 - progress_bar.py[line:274] - INFO: epoch 003:   2999 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.9, wpb=112, bsz=40, num_updates=9810, lr=2.74032e-05, gnorm=0.546, clip=10, loss_scale=1024, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=26500
2023-02-22 18:55:01 - progress_bar.py[line:274] - INFO: epoch 003:   3009 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112, bsz=40, num_updates=9820, lr=2.73775e-05, gnorm=0.708, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26511
2023-02-22 18:55:12 - progress_bar.py[line:274] - INFO: epoch 003:   3019 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=112, bsz=40, num_updates=9830, lr=2.73517e-05, gnorm=0.92, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26522
2023-02-22 18:55:23 - progress_bar.py[line:274] - INFO: epoch 003:   3029 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.93, wpb=111.3, bsz=40, num_updates=9840, lr=2.7326e-05, gnorm=0.923, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26533
2023-02-22 18:55:34 - progress_bar.py[line:274] - INFO: epoch 003:   3039 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.8, bsz=40, num_updates=9850, lr=2.73003e-05, gnorm=0.847, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26544
2023-02-22 18:55:45 - progress_bar.py[line:274] - INFO: epoch 003:   3049 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.92, wpb=109.8, bsz=40, num_updates=9860, lr=2.72746e-05, gnorm=1.075, clip=50, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26555
2023-02-22 18:55:58 - progress_bar.py[line:274] - INFO: epoch 003:   3059 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.6, bsz=40, num_updates=9870, lr=2.72489e-05, gnorm=0.95, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26567
2023-02-22 18:56:09 - progress_bar.py[line:274] - INFO: epoch 003:   3069 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.8, bsz=40, num_updates=9880, lr=2.72232e-05, gnorm=0.58, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26579
2023-02-22 18:56:20 - progress_bar.py[line:274] - INFO: epoch 003:   3079 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=9890, lr=2.71974e-05, gnorm=0.885, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26590
2023-02-22 18:56:31 - progress_bar.py[line:274] - INFO: epoch 003:   3089 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=9900, lr=2.71717e-05, gnorm=0.611, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26601
2023-02-22 18:56:44 - progress_bar.py[line:274] - INFO: epoch 003:   3099 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=9910, lr=2.7146e-05, gnorm=0.93, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26612
2023-02-22 18:56:55 - progress_bar.py[line:274] - INFO: epoch 003:   3109 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.7, bsz=40, num_updates=9920, lr=2.71203e-05, gnorm=0.65, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26625
2023-02-22 18:57:06 - progress_bar.py[line:274] - INFO: epoch 003:   3119 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=9930, lr=2.70946e-05, gnorm=0.709, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26636
2023-02-22 18:57:17 - progress_bar.py[line:274] - INFO: epoch 003:   3129 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=113, bsz=40, num_updates=9940, lr=2.70689e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26648
2023-02-22 18:57:29 - progress_bar.py[line:274] - INFO: epoch 003:   3139 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=9950, lr=2.70432e-05, gnorm=0.763, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26659
2023-02-22 18:57:40 - progress_bar.py[line:274] - INFO: epoch 003:   3149 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.9, bsz=40, num_updates=9960, lr=2.70174e-05, gnorm=1.039, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26670
2023-02-22 18:57:51 - progress_bar.py[line:274] - INFO: epoch 003:   3159 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.88, wpb=111.9, bsz=40, num_updates=9970, lr=2.69917e-05, gnorm=0.52, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26681
2023-02-22 18:58:03 - progress_bar.py[line:274] - INFO: epoch 003:   3169 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=9980, lr=2.6966e-05, gnorm=0.754, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26693
2023-02-22 18:58:14 - progress_bar.py[line:274] - INFO: epoch 003:   3179 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=9990, lr=2.69403e-05, gnorm=0.691, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26704
2023-02-22 18:58:25 - progress_bar.py[line:274] - INFO: epoch 003:   3189 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=111.6, bsz=40, num_updates=10000, lr=2.69146e-05, gnorm=0.772, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26715
2023-02-22 18:58:25 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 18:58:26 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 18:58:26 - train.py[line:551] - INFO: load:0.90 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 19:00:29 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 19:00:29 - train.py[line:551] - INFO: load:0.92 valid_run:122.38 task_valid:119.33 collect_output:1.96
2023-02-22 19:02:29 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 19:02:29 - train.py[line:551] - INFO: load:0.95 valid_run:242.51 task_valid:235.30 collect_output:5.09
2023-02-22 19:04:32 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 19:04:32 - train.py[line:551] - INFO: load:0.97 valid_run:365.00 task_valid:352.01 collect_output:9.84
2023-02-22 19:06:34 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 19:06:34 - train.py[line:551] - INFO: load:0.99 valid_run:486.99 task_valid:465.90 collect_output:16.93
2023-02-22 19:08:34 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 19:08:34 - train.py[line:551] - INFO: load:1.02 valid_run:607.63 task_valid:583.39 collect_output:19.08
2023-02-22 19:10:38 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 19:10:38 - train.py[line:551] - INFO: load:1.04 valid_run:730.84 task_valid:702.20 collect_output:22.47
2023-02-22 19:12:41 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 19:12:41 - train.py[line:551] - INFO: load:1.06 valid_run:854.08 task_valid:820.47 collect_output:26.44
2023-02-22 19:14:43 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 19:14:43 - train.py[line:551] - INFO: load:1.09 valid_run:975.95 task_valid:937.08 collect_output:30.69
2023-02-22 19:16:47 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 19:16:47 - train.py[line:551] - INFO: load:1.11 valid_run:1099.80 task_valid:1054.44 collect_output:36.16
2023-02-22 19:18:49 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 19:18:49 - train.py[line:551] - INFO: load:1.14 valid_run:1221.75 task_valid:1167.34 collect_output:44.20
2023-02-22 19:20:49 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 19:20:49 - train.py[line:551] - INFO: load:1.16 valid_run:1341.99 task_valid:1283.08 collect_output:47.68
2023-02-22 19:22:51 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 19:22:51 - train.py[line:551] - INFO: load:1.18 valid_run:1463.71 task_valid:1400.25 collect_output:51.22
2023-02-22 19:24:52 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 19:24:52 - train.py[line:551] - INFO: load:1.21 valid_run:1584.89 task_valid:1516.87 collect_output:54.66
2023-02-22 19:27:05 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 19:27:05 - train.py[line:551] - INFO: load:1.24 valid_run:1717.98 task_valid:1647.96 collect_output:55.19
2023-02-22 19:29:17 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 19:29:17 - train.py[line:551] - INFO: load:1.27 valid_run:1849.74 task_valid:1777.52 collect_output:55.90
2023-02-22 19:31:27 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 19:31:27 - train.py[line:551] - INFO: load:1.31 valid_run:1979.72 task_valid:1904.92 collect_output:57.01
2023-02-22 19:33:39 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 19:33:39 - train.py[line:551] - INFO: load:1.34 valid_run:2111.41 task_valid:2034.39 collect_output:57.75
2023-02-22 19:35:52 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 19:35:52 - train.py[line:551] - INFO: load:1.37 valid_run:2244.23 task_valid:2165.19 collect_output:58.29
2023-02-22 19:38:04 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 19:38:04 - train.py[line:551] - INFO: load:1.40 valid_run:2376.56 task_valid:2295.32 collect_output:59.04
2023-02-22 19:40:16 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 19:40:16 - train.py[line:551] - INFO: load:1.44 valid_run:2508.26 task_valid:2425.07 collect_output:59.56
2023-02-22 19:42:28 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 19:42:28 - train.py[line:551] - INFO: load:1.47 valid_run:2640.61 task_valid:2555.30 collect_output:60.17
2023-02-22 19:44:43 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 19:44:43 - train.py[line:551] - INFO: load:1.50 valid_run:2774.86 task_valid:2687.54 collect_output:60.69
2023-02-22 19:46:52 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 19:46:52 - train.py[line:551] - INFO: load:1.53 valid_run:2904.51 task_valid:2814.99 collect_output:61.43
2023-02-22 19:49:04 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 19:49:04 - train.py[line:551] - INFO: load:1.57 valid_run:3035.95 task_valid:2944.37 collect_output:61.98
2023-02-22 19:51:16 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 19:51:16 - train.py[line:551] - INFO: load:1.60 valid_run:3167.94 task_valid:3074.03 collect_output:62.85
2023-02-22 19:53:28 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 19:53:28 - train.py[line:551] - INFO: load:1.63 valid_run:3299.41 task_valid:3203.40 collect_output:63.46
2023-02-22 19:55:30 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 19:55:30 - train.py[line:551] - INFO: load:1.66 valid_run:3421.31 task_valid:3320.38 collect_output:67.30
2023-02-22 19:57:32 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 19:57:32 - train.py[line:551] - INFO: load:1.68 valid_run:3543.30 task_valid:3440.01 collect_output:68.64
2023-02-22 19:59:35 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 19:59:35 - train.py[line:551] - INFO: load:1.71 valid_run:3666.97 task_valid:3558.36 collect_output:72.86
2023-02-22 20:01:39 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 20:01:39 - train.py[line:551] - INFO: load:1.74 valid_run:3791.05 task_valid:3679.37 collect_output:74.80
2023-02-22 20:03:54 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 20:03:54 - train.py[line:551] - INFO: load:1.77 valid_run:3925.87 task_valid:3812.09 collect_output:75.41

====================================================================================================
SGG eval:     R @ 50: 0.6609;     R @ 100: 0.6914;     R @ 500: 0.7268;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4367;    mR @ 100: 0.4797;    mR @ 500: 0.5656;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7195) (covered in:0.6250) (covering:0.0857) (eating:0.7647) (flying in:0.8182) (growing on:0.2500) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.1429) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9363) (says:0.0000) (sitting on:0.7049) (standing on:0.5793) (using:0.6000) (walking in:0.0000) (walking on:0.5000) (watching:0.6806) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6609;     R @ 100: 0.6914;     R @ 500: 0.7268;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4367;    mR @ 100: 0.4797;    mR @ 500: 0.5656;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7195) (covered in:0.6250) (covering:0.0857) (eating:0.7647) (flying in:0.8182) (growing on:0.2500) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.1429) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9363) (says:0.0000) (sitting on:0.7049) (standing on:0.5793) (using:0.6000) (walking in:0.0000) (walking on:0.5000) (watching:0.6806) 
--------------------------------------------------------
====================================================================================================

2023-02-22 20:04:36 - train.py[line:487] - INFO: 0.6914052202699262
2023-02-22 20:04:37 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 20:04:37 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.244 | loss_v1 0 | loss_v2 0 | nll_loss 0.074 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.691405 | ppl 1.05 | vqa_score 0.5158 | wps 113 | wpb 72 | bsz 24 | num_updates 10000 | best_R@100 0.691405
2023-02-22 20:04:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 10000 updates
2023-02-22 20:04:37 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_3_10000.pt
2023-02-22 20:04:46 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_3_10000.pt
2023-02-22 20:04:54 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_4vg/1_B20_A1_E6_0.05_5e-5_480/checkpoint_3_10000.pt (epoch 3 @ 10000 updates, score 0.6914052202699262) (writing took 16.975211272016168 seconds)
2023-02-22 20:05:06 - progress_bar.py[line:274] - INFO: epoch 003:   3199 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=111.8, bsz=40, num_updates=10010, lr=2.68889e-05, gnorm=0.698, clip=30, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30716
2023-02-22 20:05:17 - progress_bar.py[line:274] - INFO: epoch 003:   3209 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=93.9, ups=0.84, wpb=111.5, bsz=40, num_updates=10020, lr=2.68631e-05, gnorm=0.826, clip=30, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30728
2023-02-22 20:05:29 - progress_bar.py[line:274] - INFO: epoch 003:   3219 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=93.5, ups=0.84, wpb=111.3, bsz=40, num_updates=10030, lr=2.68374e-05, gnorm=0.515, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30740
2023-02-22 20:05:41 - progress_bar.py[line:274] - INFO: epoch 003:   3229 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.5, ups=0.86, wpb=112.7, bsz=40, num_updates=10040, lr=2.68117e-05, gnorm=0.612, clip=20, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30751
2023-02-22 20:05:54 - progress_bar.py[line:274] - INFO: epoch 003:   3239 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=84.4, ups=0.76, wpb=111.7, bsz=40, num_updates=10050, lr=2.6786e-05, gnorm=0.576, clip=20, loss_scale=1024, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=30764
2023-02-22 20:06:06 - progress_bar.py[line:274] - INFO: epoch 003:   3249 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=93.5, ups=0.83, wpb=112.3, bsz=40, num_updates=10060, lr=2.67603e-05, gnorm=0.975, clip=20, loss_scale=2048, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=30777
2023-02-22 20:06:18 - progress_bar.py[line:274] - INFO: epoch 003:   3259 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.8, ups=0.85, wpb=112.3, bsz=40, num_updates=10070, lr=2.67346e-05, gnorm=0.635, clip=10, loss_scale=2048, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30788
2023-02-22 20:06:30 - progress_bar.py[line:274] - INFO: epoch 003:   3269 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96, ups=0.86, wpb=112.2, bsz=40, num_updates=10080, lr=2.67088e-05, gnorm=0.75, clip=30, loss_scale=2048, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30800
2023-02-22 20:06:41 - progress_bar.py[line:274] - INFO: epoch 003:   3279 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.87, wpb=111.3, bsz=40, num_updates=10090, lr=2.66831e-05, gnorm=0.626, clip=20, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=30812
2023-02-22 20:06:53 - progress_bar.py[line:274] - INFO: epoch 003:   3289 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=94.5, ups=0.85, wpb=111.7, bsz=40, num_updates=10100, lr=2.66574e-05, gnorm=0.635, clip=10, loss_scale=2048, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30823
2023-02-22 20:06:56 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 20:07:06 - progress_bar.py[line:274] - INFO: epoch 003:   3300 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=89.2, ups=0.8, wpb=111.3, bsz=40, num_updates=10110, lr=2.66317e-05, gnorm=0.491, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30836
2023-02-22 20:07:18 - progress_bar.py[line:274] - INFO: epoch 003:   3310 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=93.6, ups=0.84, wpb=111.9, bsz=40, num_updates=10120, lr=2.6606e-05, gnorm=0.749, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30848
2023-02-22 20:07:29 - progress_bar.py[line:274] - INFO: epoch 003:   3320 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96, ups=0.86, wpb=111.6, bsz=40, num_updates=10130, lr=2.65803e-05, gnorm=0.615, clip=10, loss_scale=1024, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=30860
2023-02-22 20:07:41 - progress_bar.py[line:274] - INFO: epoch 003:   3330 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.87, wpb=112, bsz=40, num_updates=10140, lr=2.65545e-05, gnorm=0.695, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=30871
2023-02-22 20:07:53 - progress_bar.py[line:274] - INFO: epoch 003:   3340 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=93.8, ups=0.83, wpb=112.5, bsz=40, num_updates=10150, lr=2.65288e-05, gnorm=0.547, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30883
2023-02-22 20:08:05 - progress_bar.py[line:274] - INFO: epoch 003:   3350 / 3411 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.86, wpb=113.4, bsz=40, num_updates=10160, lr=2.65031e-05, gnorm=0.653, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30895
2023-02-22 20:08:16 - progress_bar.py[line:274] - INFO: epoch 003:   3360 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.6, ups=0.86, wpb=110.8, bsz=40, num_updates=10170, lr=2.64774e-05, gnorm=0.674, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30906
2023-02-22 20:08:28 - progress_bar.py[line:274] - INFO: epoch 003:   3370 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=90.8, ups=0.82, wpb=111.1, bsz=40, num_updates=10180, lr=2.64517e-05, gnorm=0.58, clip=10, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30919
2023-02-22 20:08:41 - progress_bar.py[line:274] - INFO: epoch 003:   3380 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=90.9, ups=0.81, wpb=112.6, bsz=40, num_updates=10190, lr=2.6426e-05, gnorm=0.806, clip=20, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30931
2023-02-22 20:08:53 - progress_bar.py[line:274] - INFO: epoch 003:   3390 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.86, wpb=112.4, bsz=40, num_updates=10200, lr=2.64002e-05, gnorm=0.93, clip=40, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30943
2023-02-22 20:09:04 - progress_bar.py[line:274] - INFO: epoch 003:   3400 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=10210, lr=2.63745e-05, gnorm=0.566, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30954
2023-02-22 20:09:15 - progress_bar.py[line:274] - INFO: epoch 003:   3410 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.9, wpb=110.7, bsz=40, num_updates=10220, lr=2.63488e-05, gnorm=1.003, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30965
2023-02-22 20:09:16 - train.py[line:339] - INFO: end of epoch 3 (average epoch stats below)
2023-02-22 20:09:16 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.257 | loss_v1 0 | loss_v2 0 | nll_loss 0.077 | ntokens 111.862 | nsentences 39.998 | sample_size 111.862 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 32.6 | ups 0.29 | wpb 111.9 | bsz 40 | num_updates 10221 | lr 2.63462e-05 | gnorm 0.789 | clip 25.5 | loss_scale 1024 | train_wall 3825 | gb_free 14.3 | ema_decay 0.9999 | wall 30966
2023-02-22 20:09:16 - trainer.py[line:694] - INFO: loading train data for epoch 4
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E3.tsv slice_id 1 row count 68217 total row count 136434
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_4vg_train_NA1_E3.tsv slice_id 0 row count 68217 total row count 136434
2023-02-22 20:09:18 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 20:09:18 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 20:09:18 - trainer.py[line:758] - INFO: begin training epoch 4
2023-02-22 20:09:18 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 20:09:30 - progress_bar.py[line:274] - INFO: epoch 004:      9 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.1, nsentences=39.4, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=72.1, ups=0.65, wpb=110.1, bsz=39.4, num_updates=10230, lr=2.63231e-05, gnorm=0.978, clip=30, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30980
2023-02-22 20:09:41 - progress_bar.py[line:274] - INFO: epoch 004:     19 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.93, wpb=112.6, bsz=40, num_updates=10240, lr=2.62974e-05, gnorm=0.343, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30991
2023-02-22 20:09:52 - progress_bar.py[line:274] - INFO: epoch 004:     29 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.88, wpb=111.1, bsz=40, num_updates=10250, lr=2.62717e-05, gnorm=0.61, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31003
2023-02-22 20:10:03 - progress_bar.py[line:274] - INFO: epoch 004:     39 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.3, ups=0.92, wpb=114, bsz=40, num_updates=10260, lr=2.62459e-05, gnorm=0.69, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31013
2023-02-22 20:10:14 - progress_bar.py[line:274] - INFO: epoch 004:     49 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.93, wpb=113.6, bsz=40, num_updates=10270, lr=2.62202e-05, gnorm=0.552, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31024
2023-02-22 20:10:25 - progress_bar.py[line:274] - INFO: epoch 004:     59 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.4, bsz=40, num_updates=10280, lr=2.61945e-05, gnorm=0.827, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31035
2023-02-22 20:10:37 - progress_bar.py[line:274] - INFO: epoch 004:     69 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.5, ups=0.86, wpb=111.6, bsz=40, num_updates=10290, lr=2.61688e-05, gnorm=0.765, clip=20, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=31047
2023-02-22 20:10:48 - progress_bar.py[line:274] - INFO: epoch 004:     79 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.7, bsz=40, num_updates=10300, lr=2.61431e-05, gnorm=0.949, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31058
2023-02-22 20:11:00 - progress_bar.py[line:274] - INFO: epoch 004:     89 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=112.3, bsz=40, num_updates=10310, lr=2.61174e-05, gnorm=0.585, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31070
2023-02-22 20:11:11 - progress_bar.py[line:274] - INFO: epoch 004:     99 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.88, wpb=110.9, bsz=40, num_updates=10320, lr=2.60917e-05, gnorm=0.669, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31081
2023-02-22 20:11:22 - progress_bar.py[line:274] - INFO: epoch 004:    109 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=110.9, bsz=40, num_updates=10330, lr=2.60659e-05, gnorm=0.624, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31092
2023-02-22 20:11:33 - progress_bar.py[line:274] - INFO: epoch 004:    119 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.91, wpb=110.6, bsz=40, num_updates=10340, lr=2.60402e-05, gnorm=0.914, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31103
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 20:11:45 - progress_bar.py[line:274] - INFO: epoch 004:    129 / 3411 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.87, wpb=110.7, bsz=40, num_updates=10350, lr=2.60145e-05, gnorm=0.846, clip=40, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31115
2023-02-22 20:11:56 - progress_bar.py[line:274] - INFO: epoch 004:    139 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=10360, lr=2.59888e-05, gnorm=0.664, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31126
2023-02-22 20:12:07 - progress_bar.py[line:274] - INFO: epoch 004:    149 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=10370, lr=2.59631e-05, gnorm=0.678, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31137
2023-02-22 20:12:18 - progress_bar.py[line:274] - INFO: epoch 004:    159 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.92, wpb=111.6, bsz=40, num_updates=10380, lr=2.59374e-05, gnorm=0.856, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31148
2023-02-22 20:12:29 - progress_bar.py[line:274] - INFO: epoch 004:    169 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=10390, lr=2.59116e-05, gnorm=0.626, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31159
2023-02-22 20:12:40 - progress_bar.py[line:274] - INFO: epoch 004:    179 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.6, bsz=40, num_updates=10400, lr=2.58859e-05, gnorm=0.588, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31170
2023-02-22 20:12:51 - progress_bar.py[line:274] - INFO: epoch 004:    189 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.6, bsz=40, num_updates=10410, lr=2.58602e-05, gnorm=0.561, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31182
2023-02-22 20:13:03 - progress_bar.py[line:274] - INFO: epoch 004:    199 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.9, wpb=110.4, bsz=40, num_updates=10420, lr=2.58345e-05, gnorm=0.808, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31193
2023-02-22 20:13:14 - progress_bar.py[line:274] - INFO: epoch 004:    209 / 3411 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.92, wpb=110.4, bsz=40, num_updates=10430, lr=2.58088e-05, gnorm=0.761, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31204
2023-02-22 20:13:25 - progress_bar.py[line:274] - INFO: epoch 004:    219 / 3411 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=10440, lr=2.57831e-05, gnorm=0.711, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31215
2023-02-22 20:13:36 - progress_bar.py[line:274] - INFO: epoch 004:    229 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=10450, lr=2.57573e-05, gnorm=0.722, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31226
2023-02-22 20:13:47 - progress_bar.py[line:274] - INFO: epoch 004:    239 / 3411 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.6, bsz=40, num_updates=10460, lr=2.57316e-05, gnorm=0.495, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31237
2023-02-22 20:13:58 - progress_bar.py[line:274] - INFO: epoch 004:    249 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.1, bsz=40, num_updates=10470, lr=2.57059e-05, gnorm=0.594, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31248
2023-02-22 20:14:10 - progress_bar.py[line:274] - INFO: epoch 004:    259 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.2, bsz=40, num_updates=10480, lr=2.56802e-05, gnorm=0.71, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31260
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:14:21 - progress_bar.py[line:274] - INFO: epoch 004:    269 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.7, ups=0.88, wpb=110.2, bsz=40, num_updates=10490, lr=2.56545e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31271
2023-02-22 20:14:32 - progress_bar.py[line:274] - INFO: epoch 004:    279 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=112.1, bsz=40, num_updates=10500, lr=2.56288e-05, gnorm=0.535, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31282
2023-02-22 20:14:43 - progress_bar.py[line:274] - INFO: epoch 004:    289 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.91, wpb=111.1, bsz=40, num_updates=10510, lr=2.5603e-05, gnorm=0.724, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31294
2023-02-22 20:14:55 - progress_bar.py[line:274] - INFO: epoch 004:    299 / 3411 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=112, bsz=40, num_updates=10520, lr=2.55773e-05, gnorm=0.856, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31305
2023-02-22 20:15:06 - progress_bar.py[line:274] - INFO: epoch 004:    309 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.88, wpb=113.5, bsz=40, num_updates=10530, lr=2.55516e-05, gnorm=0.565, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31316
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 20:15:17 - progress_bar.py[line:274] - INFO: epoch 004:    319 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=112.6, bsz=40, num_updates=10540, lr=2.55259e-05, gnorm=0.979, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31327
2023-02-22 20:15:29 - progress_bar.py[line:274] - INFO: epoch 004:    329 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=112.3, bsz=40, num_updates=10550, lr=2.55002e-05, gnorm=0.675, clip=10, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=31339
2023-02-22 20:15:39 - progress_bar.py[line:274] - INFO: epoch 004:    339 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.92, wpb=111, bsz=40, num_updates=10560, lr=2.54745e-05, gnorm=0.688, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31350
2023-02-22 20:15:51 - progress_bar.py[line:274] - INFO: epoch 004:    349 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=10570, lr=2.54487e-05, gnorm=0.811, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31361
2023-02-22 20:16:02 - progress_bar.py[line:274] - INFO: epoch 004:    359 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.91, wpb=110.1, bsz=40, num_updates=10580, lr=2.5423e-05, gnorm=0.577, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31372
2023-02-22 20:16:13 - progress_bar.py[line:274] - INFO: epoch 004:    369 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.5, ups=0.87, wpb=111.2, bsz=40, num_updates=10590, lr=2.53973e-05, gnorm=0.759, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31383
2023-02-22 20:16:24 - progress_bar.py[line:274] - INFO: epoch 004:    379 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.93, wpb=111.7, bsz=40, num_updates=10600, lr=2.53716e-05, gnorm=0.448, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31394
2023-02-22 20:16:35 - progress_bar.py[line:274] - INFO: epoch 004:    389 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=113.2, bsz=40, num_updates=10610, lr=2.53459e-05, gnorm=0.507, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31405
2023-02-22 20:16:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 20:16:47 - progress_bar.py[line:274] - INFO: epoch 004:    400 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=92.6, ups=0.83, wpb=112.1, bsz=40, num_updates=10620, lr=2.53202e-05, gnorm=0.853, clip=40, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31418
2023-02-22 20:16:59 - progress_bar.py[line:274] - INFO: epoch 004:    410 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.2, ups=0.87, wpb=112.2, bsz=40, num_updates=10630, lr=2.52945e-05, gnorm=0.566, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31429
2023-02-22 20:17:10 - progress_bar.py[line:274] - INFO: epoch 004:    420 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=10640, lr=2.52687e-05, gnorm=0.739, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31440
2023-02-22 20:17:21 - progress_bar.py[line:274] - INFO: epoch 004:    430 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=10650, lr=2.5243e-05, gnorm=0.718, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31452
2023-02-22 20:17:33 - progress_bar.py[line:274] - INFO: epoch 004:    440 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=10660, lr=2.52173e-05, gnorm=0.513, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31463
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:17:44 - progress_bar.py[line:274] - INFO: epoch 004:    450 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.93, wpb=112.2, bsz=40, num_updates=10670, lr=2.51916e-05, gnorm=0.591, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31474
2023-02-22 20:17:54 - progress_bar.py[line:274] - INFO: epoch 004:    460 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.92, wpb=111.4, bsz=40, num_updates=10680, lr=2.51659e-05, gnorm=0.613, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31485
2023-02-22 20:18:06 - progress_bar.py[line:274] - INFO: epoch 004:    470 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.89, wpb=113.2, bsz=40, num_updates=10690, lr=2.51402e-05, gnorm=0.855, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31496
2023-02-22 20:18:17 - progress_bar.py[line:274] - INFO: epoch 004:    480 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.9, bsz=40, num_updates=10700, lr=2.51144e-05, gnorm=1.04, clip=60, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31507
2023-02-22 20:18:28 - progress_bar.py[line:274] - INFO: epoch 004:    490 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.5, bsz=40, num_updates=10710, lr=2.50887e-05, gnorm=0.864, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31519
2023-02-22 20:18:40 - progress_bar.py[line:274] - INFO: epoch 004:    500 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=112.3, bsz=40, num_updates=10720, lr=2.5063e-05, gnorm=0.685, clip=40, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=31530
2023-02-22 20:18:51 - progress_bar.py[line:274] - INFO: epoch 004:    510 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.93, wpb=112.1, bsz=40, num_updates=10730, lr=2.50373e-05, gnorm=0.86, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31541
2023-02-22 20:19:02 - progress_bar.py[line:274] - INFO: epoch 004:    520 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=112.8, bsz=40, num_updates=10740, lr=2.50116e-05, gnorm=0.806, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31552
2023-02-22 20:19:13 - progress_bar.py[line:274] - INFO: epoch 004:    530 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.5, bsz=40, num_updates=10750, lr=2.49859e-05, gnorm=0.346, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31563
2023-02-22 20:19:24 - progress_bar.py[line:274] - INFO: epoch 004:    540 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=10760, lr=2.49601e-05, gnorm=0.947, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31574
2023-02-22 20:19:35 - progress_bar.py[line:274] - INFO: epoch 004:    550 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.9, wpb=112.9, bsz=40, num_updates=10770, lr=2.49344e-05, gnorm=0.676, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31585
2023-02-22 20:19:46 - progress_bar.py[line:274] - INFO: epoch 004:    560 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105, ups=0.95, wpb=111, bsz=40, num_updates=10780, lr=2.49087e-05, gnorm=0.56, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31596
2023-02-22 20:19:56 - progress_bar.py[line:274] - INFO: epoch 004:    570 / 3411 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.92, wpb=112.2, bsz=40, num_updates=10790, lr=2.4883e-05, gnorm=0.599, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31607
2023-02-22 20:20:08 - progress_bar.py[line:274] - INFO: epoch 004:    580 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=112.1, bsz=40, num_updates=10800, lr=2.48573e-05, gnorm=0.334, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31618
@@@@ ERROR IN DATA @@@@ play
2023-02-22 20:20:19 - progress_bar.py[line:274] - INFO: epoch 004:    590 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.3, bsz=40, num_updates=10810, lr=2.48316e-05, gnorm=0.884, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31629
2023-02-22 20:20:30 - progress_bar.py[line:274] - INFO: epoch 004:    600 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.89, wpb=111.4, bsz=40, num_updates=10820, lr=2.48058e-05, gnorm=0.482, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31640
2023-02-22 20:20:42 - progress_bar.py[line:274] - INFO: epoch 004:    610 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=10830, lr=2.47801e-05, gnorm=0.647, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31652
2023-02-22 20:20:53 - progress_bar.py[line:274] - INFO: epoch 004:    620 / 3411 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.91, wpb=111.8, bsz=40, num_updates=10840, lr=2.47544e-05, gnorm=0.484, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31663
2023-02-22 20:21:04 - progress_bar.py[line:274] - INFO: epoch 004:    630 / 3411 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.4, bsz=40, num_updates=10850, lr=2.47287e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31674
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 20:21:15 - progress_bar.py[line:274] - INFO: epoch 004:    640 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.92, wpb=111.7, bsz=40, num_updates=10860, lr=2.4703e-05, gnorm=1.094, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31685
2023-02-22 20:21:26 - progress_bar.py[line:274] - INFO: epoch 004:    650 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=10870, lr=2.46773e-05, gnorm=0.465, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31696
2023-02-22 20:21:38 - progress_bar.py[line:274] - INFO: epoch 004:    660 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.9, bsz=40, num_updates=10880, lr=2.46515e-05, gnorm=0.604, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31708
2023-02-22 20:21:43 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 20:21:50 - progress_bar.py[line:274] - INFO: epoch 004:    671 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=92.9, ups=0.83, wpb=111.7, bsz=40, num_updates=10890, lr=2.46258e-05, gnorm=0.919, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31720
2023-02-22 20:22:01 - progress_bar.py[line:274] - INFO: epoch 004:    681 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=10900, lr=2.46001e-05, gnorm=1.433, clip=30, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=31731
2023-02-22 20:22:12 - progress_bar.py[line:274] - INFO: epoch 004:    691 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.9, wpb=113.4, bsz=40, num_updates=10910, lr=2.45744e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31742
2023-02-22 20:22:23 - progress_bar.py[line:274] - INFO: epoch 004:    701 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.89, wpb=113.6, bsz=40, num_updates=10920, lr=2.45487e-05, gnorm=0.893, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31753
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:22:34 - progress_bar.py[line:274] - INFO: epoch 004:    711 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=10930, lr=2.4523e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31764
2023-02-22 20:22:45 - progress_bar.py[line:274] - INFO: epoch 004:    721 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.9, bsz=40, num_updates=10940, lr=2.44972e-05, gnorm=0.778, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31776
2023-02-22 20:22:57 - progress_bar.py[line:274] - INFO: epoch 004:    731 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.7, bsz=40, num_updates=10950, lr=2.44715e-05, gnorm=0.736, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31787
2023-02-22 20:23:08 - progress_bar.py[line:274] - INFO: epoch 004:    741 / 3411 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=10960, lr=2.44458e-05, gnorm=0.797, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31798
2023-02-22 20:23:19 - progress_bar.py[line:274] - INFO: epoch 004:    751 / 3411 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=10970, lr=2.44201e-05, gnorm=0.552, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31809
2023-02-22 20:23:31 - progress_bar.py[line:274] - INFO: epoch 004:    761 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.9, bsz=40, num_updates=10980, lr=2.43944e-05, gnorm=0.593, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31821
2023-02-22 20:23:42 - progress_bar.py[line:274] - INFO: epoch 004:    771 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.6, bsz=40, num_updates=10990, lr=2.43687e-05, gnorm=0.567, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31832
2023-02-22 20:23:53 - progress_bar.py[line:274] - INFO: epoch 004:    781 / 3411 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.4, bsz=40, num_updates=11000, lr=2.4343e-05, gnorm=0.584, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31844
2023-02-22 20:24:05 - progress_bar.py[line:274] - INFO: epoch 004:    791 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=112.5, bsz=40, num_updates=11010, lr=2.43172e-05, gnorm=0.746, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31855
2023-02-22 20:24:15 - progress_bar.py[line:274] - INFO: epoch 004:    801 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=111.8, bsz=40, num_updates=11020, lr=2.42915e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31866
2023-02-22 20:24:27 - progress_bar.py[line:274] - INFO: epoch 004:    811 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=11030, lr=2.42658e-05, gnorm=0.504, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31877
2023-02-22 20:24:38 - progress_bar.py[line:274] - INFO: epoch 004:    821 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.87, wpb=112.5, bsz=40, num_updates=11040, lr=2.42401e-05, gnorm=0.639, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31888
2023-02-22 20:24:50 - progress_bar.py[line:274] - INFO: epoch 004:    831 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.88, wpb=110.9, bsz=40, num_updates=11050, lr=2.42144e-05, gnorm=0.57, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31900
2023-02-22 20:25:01 - progress_bar.py[line:274] - INFO: epoch 004:    841 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=112, bsz=40, num_updates=11060, lr=2.41887e-05, gnorm=0.663, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31911
2023-02-22 20:25:12 - progress_bar.py[line:274] - INFO: epoch 004:    851 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.8, bsz=40, num_updates=11070, lr=2.41629e-05, gnorm=0.84, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31923
2023-02-22 20:25:23 - progress_bar.py[line:274] - INFO: epoch 004:    861 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.5, ups=0.94, wpb=111.1, bsz=40, num_updates=11080, lr=2.41372e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31933
2023-02-22 20:25:34 - progress_bar.py[line:274] - INFO: epoch 004:    871 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=11090, lr=2.41115e-05, gnorm=0.79, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31944
2023-02-22 20:25:45 - progress_bar.py[line:274] - INFO: epoch 004:    881 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.92, wpb=109.5, bsz=40, num_updates=11100, lr=2.40858e-05, gnorm=0.616, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31955
2023-02-22 20:25:56 - progress_bar.py[line:274] - INFO: epoch 004:    891 / 3411 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107.2, ups=0.95, wpb=112.4, bsz=40, num_updates=11110, lr=2.40601e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=10, gb_free=10.5, ema_decay=0.9999, wall=31966
2023-02-22 20:26:06 - progress_bar.py[line:274] - INFO: epoch 004:    901 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.93, wpb=111.8, bsz=40, num_updates=11120, lr=2.40344e-05, gnorm=0.616, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31977
2023-02-22 20:26:17 - progress_bar.py[line:274] - INFO: epoch 004:    911 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.3, bsz=40, num_updates=11130, lr=2.40086e-05, gnorm=0.647, clip=20, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=31988
2023-02-22 20:26:29 - progress_bar.py[line:274] - INFO: epoch 004:    921 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.8, bsz=40, num_updates=11140, lr=2.39829e-05, gnorm=0.605, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31999
2023-02-22 20:26:40 - progress_bar.py[line:274] - INFO: epoch 004:    931 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=11150, lr=2.39572e-05, gnorm=0.714, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32010
2023-02-22 20:26:51 - progress_bar.py[line:274] - INFO: epoch 004:    941 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=111.8, bsz=40, num_updates=11160, lr=2.39315e-05, gnorm=0.693, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32021
2023-02-22 20:27:02 - progress_bar.py[line:274] - INFO: epoch 004:    951 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.3, ups=0.93, wpb=112.3, bsz=40, num_updates=11170, lr=2.39058e-05, gnorm=0.752, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32032
2023-02-22 20:27:13 - progress_bar.py[line:274] - INFO: epoch 004:    961 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=11180, lr=2.38801e-05, gnorm=0.907, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32043
2023-02-22 20:27:24 - progress_bar.py[line:274] - INFO: epoch 004:    971 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=11190, lr=2.38543e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32055
2023-02-22 20:27:35 - progress_bar.py[line:274] - INFO: epoch 004:    981 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=111.2, bsz=40, num_updates=11200, lr=2.38286e-05, gnorm=0.614, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32066
2023-02-22 20:27:47 - progress_bar.py[line:274] - INFO: epoch 004:    991 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112, bsz=40, num_updates=11210, lr=2.38029e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32077
2023-02-22 20:27:58 - progress_bar.py[line:274] - INFO: epoch 004:   1001 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.9, wpb=110.2, bsz=40, num_updates=11220, lr=2.37772e-05, gnorm=0.397, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32088
2023-02-22 20:28:09 - progress_bar.py[line:274] - INFO: epoch 004:   1011 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.88, wpb=112.6, bsz=40, num_updates=11230, lr=2.37515e-05, gnorm=0.515, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32099
2023-02-22 20:28:20 - progress_bar.py[line:274] - INFO: epoch 004:   1021 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=11240, lr=2.37258e-05, gnorm=0.639, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32110
2023-02-22 20:28:31 - progress_bar.py[line:274] - INFO: epoch 004:   1031 / 3411 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.5, ups=0.94, wpb=112.4, bsz=40, num_updates=11250, lr=2.37e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=32121
2023-02-22 20:28:42 - progress_bar.py[line:274] - INFO: epoch 004:   1041 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=11260, lr=2.36743e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32132
2023-02-22 20:28:54 - progress_bar.py[line:274] - INFO: epoch 004:   1051 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=111.1, bsz=40, num_updates=11270, lr=2.36486e-05, gnorm=0.539, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32144
2023-02-22 20:29:05 - progress_bar.py[line:274] - INFO: epoch 004:   1061 / 3411 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.87, wpb=111.8, bsz=40, num_updates=11280, lr=2.36229e-05, gnorm=0.431, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32155
2023-02-22 20:29:16 - progress_bar.py[line:274] - INFO: epoch 004:   1071 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=112, bsz=40, num_updates=11290, lr=2.35972e-05, gnorm=0.978, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32166
2023-02-22 20:29:27 - progress_bar.py[line:274] - INFO: epoch 004:   1081 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=11300, lr=2.35715e-05, gnorm=0.704, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32177
2023-02-22 20:29:38 - progress_bar.py[line:274] - INFO: epoch 004:   1091 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=11310, lr=2.35457e-05, gnorm=1.038, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32189
2023-02-22 20:29:49 - progress_bar.py[line:274] - INFO: epoch 004:   1101 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=112.7, bsz=40, num_updates=11320, lr=2.352e-05, gnorm=0.61, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32200
2023-02-22 20:30:01 - progress_bar.py[line:274] - INFO: epoch 004:   1111 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.8, bsz=40, num_updates=11330, lr=2.34943e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32211
2023-02-22 20:30:12 - progress_bar.py[line:274] - INFO: epoch 004:   1121 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.91, wpb=112, bsz=40, num_updates=11340, lr=2.34686e-05, gnorm=0.467, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32222
2023-02-22 20:30:23 - progress_bar.py[line:274] - INFO: epoch 004:   1131 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=111.7, bsz=40, num_updates=11350, lr=2.34429e-05, gnorm=0.632, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32233
2023-02-22 20:30:34 - progress_bar.py[line:274] - INFO: epoch 004:   1141 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=11360, lr=2.34172e-05, gnorm=0.519, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32244
2023-02-22 20:30:45 - progress_bar.py[line:274] - INFO: epoch 004:   1151 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.2, bsz=40, num_updates=11370, lr=2.33915e-05, gnorm=0.55, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32256
2023-02-22 20:30:57 - progress_bar.py[line:274] - INFO: epoch 004:   1161 / 3411 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.4, ups=0.87, wpb=111, bsz=40, num_updates=11380, lr=2.33657e-05, gnorm=0.651, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32267
2023-02-22 20:31:08 - progress_bar.py[line:274] - INFO: epoch 004:   1171 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=11390, lr=2.334e-05, gnorm=0.508, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32278
2023-02-22 20:31:19 - progress_bar.py[line:274] - INFO: epoch 004:   1181 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.9, bsz=40, num_updates=11400, lr=2.33143e-05, gnorm=0.715, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32289
2023-02-22 20:31:30 - progress_bar.py[line:274] - INFO: epoch 004:   1191 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.91, wpb=112.3, bsz=40, num_updates=11410, lr=2.32886e-05, gnorm=0.912, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32300
2023-02-22 20:31:41 - progress_bar.py[line:274] - INFO: epoch 004:   1201 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113.5, bsz=40, num_updates=11420, lr=2.32629e-05, gnorm=1.056, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32312
2023-02-22 20:31:53 - progress_bar.py[line:274] - INFO: epoch 004:   1211 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.6, ups=0.87, wpb=110, bsz=40, num_updates=11430, lr=2.32372e-05, gnorm=0.803, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32323
2023-02-22 20:32:05 - progress_bar.py[line:274] - INFO: epoch 004:   1221 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.87, wpb=112.1, bsz=40, num_updates=11440, lr=2.32114e-05, gnorm=0.829, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32335
2023-02-22 20:32:16 - progress_bar.py[line:274] - INFO: epoch 004:   1231 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.1, ups=0.87, wpb=110.2, bsz=40, num_updates=11450, lr=2.31857e-05, gnorm=0.686, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32346
2023-02-22 20:32:27 - progress_bar.py[line:274] - INFO: epoch 004:   1241 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=11460, lr=2.316e-05, gnorm=0.548, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32357
2023-02-22 20:32:38 - progress_bar.py[line:274] - INFO: epoch 004:   1251 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.89, wpb=110.4, bsz=40, num_updates=11470, lr=2.31343e-05, gnorm=0.65, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32368
2023-02-22 20:32:49 - progress_bar.py[line:274] - INFO: epoch 004:   1261 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=11480, lr=2.31086e-05, gnorm=0.579, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32380
2023-02-22 20:33:01 - progress_bar.py[line:274] - INFO: epoch 004:   1271 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=11490, lr=2.30829e-05, gnorm=0.639, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32391
2023-02-22 20:33:12 - progress_bar.py[line:274] - INFO: epoch 004:   1281 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111.1, bsz=40, num_updates=11500, lr=2.30571e-05, gnorm=0.55, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32402
2023-02-22 20:33:23 - progress_bar.py[line:274] - INFO: epoch 004:   1291 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=11510, lr=2.30314e-05, gnorm=0.614, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32413
2023-02-22 20:33:34 - progress_bar.py[line:274] - INFO: epoch 004:   1301 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=11520, lr=2.30057e-05, gnorm=0.766, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32424
2023-02-22 20:33:46 - progress_bar.py[line:274] - INFO: epoch 004:   1311 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.5, bsz=40, num_updates=11530, lr=2.298e-05, gnorm=0.832, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32436
2023-02-22 20:33:57 - progress_bar.py[line:274] - INFO: epoch 004:   1321 / 3411 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112.4, bsz=40, num_updates=11540, lr=2.29543e-05, gnorm=0.67, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32447
2023-02-22 20:34:08 - progress_bar.py[line:274] - INFO: epoch 004:   1331 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.2, bsz=40, num_updates=11550, lr=2.29286e-05, gnorm=0.987, clip=50, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=32458
2023-02-22 20:34:19 - progress_bar.py[line:274] - INFO: epoch 004:   1341 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.5, bsz=40, num_updates=11560, lr=2.29028e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32469
2023-02-22 20:34:30 - progress_bar.py[line:274] - INFO: epoch 004:   1351 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=11570, lr=2.28771e-05, gnorm=0.562, clip=10, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=32481
2023-02-22 20:34:42 - progress_bar.py[line:274] - INFO: epoch 004:   1361 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=11580, lr=2.28514e-05, gnorm=0.511, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32492
2023-02-22 20:34:53 - progress_bar.py[line:274] - INFO: epoch 004:   1371 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=113.1, bsz=40, num_updates=11590, lr=2.28257e-05, gnorm=0.474, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32503
2023-02-22 20:35:04 - progress_bar.py[line:274] - INFO: epoch 004:   1381 / 3411 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=11600, lr=2.28e-05, gnorm=0.602, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32514
2023-02-22 20:35:15 - progress_bar.py[line:274] - INFO: epoch 004:   1391 / 3411 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.93, wpb=112.2, bsz=40, num_updates=11610, lr=2.27743e-05, gnorm=0.833, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32525
2023-02-22 20:35:26 - progress_bar.py[line:274] - INFO: epoch 004:   1401 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=11620, lr=2.27485e-05, gnorm=0.75, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32536
2023-02-22 20:35:37 - progress_bar.py[line:274] - INFO: epoch 004:   1411 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=112.8, bsz=40, num_updates=11630, lr=2.27228e-05, gnorm=0.912, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32547
2023-02-22 20:35:49 - progress_bar.py[line:274] - INFO: epoch 004:   1421 / 3411 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.87, wpb=112.3, bsz=40, num_updates=11640, lr=2.26971e-05, gnorm=0.357, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32559
2023-02-22 20:36:00 - progress_bar.py[line:274] - INFO: epoch 004:   1431 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.88, wpb=111, bsz=40, num_updates=11650, lr=2.26714e-05, gnorm=0.532, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32570
2023-02-22 20:36:11 - progress_bar.py[line:274] - INFO: epoch 004:   1441 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.4, ups=0.93, wpb=112.2, bsz=40, num_updates=11660, lr=2.26457e-05, gnorm=0.485, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32581
2023-02-22 20:36:22 - progress_bar.py[line:274] - INFO: epoch 004:   1451 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112.2, bsz=40, num_updates=11670, lr=2.262e-05, gnorm=1.181, clip=30, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=32592
2023-02-22 20:36:33 - progress_bar.py[line:274] - INFO: epoch 004:   1461 / 3411 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=112.9, bsz=40, num_updates=11680, lr=2.25942e-05, gnorm=0.526, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32603
2023-02-22 20:36:44 - progress_bar.py[line:274] - INFO: epoch 004:   1471 / 3411 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.6, bsz=40, num_updates=11690, lr=2.25685e-05, gnorm=0.844, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32615
2023-02-22 20:36:55 - progress_bar.py[line:274] - INFO: epoch 004:   1481 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.3, bsz=40, num_updates=11700, lr=2.25428e-05, gnorm=0.55, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32626
2023-02-22 20:37:07 - progress_bar.py[line:274] - INFO: epoch 004:   1491 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112, bsz=40, num_updates=11710, lr=2.25171e-05, gnorm=0.679, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32637
2023-02-22 20:37:18 - progress_bar.py[line:274] - INFO: epoch 004:   1501 / 3411 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.5, bsz=40, num_updates=11720, lr=2.24914e-05, gnorm=0.522, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32648
2023-02-22 20:37:29 - progress_bar.py[line:274] - INFO: epoch 004:   1511 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.9, bsz=40, num_updates=11730, lr=2.24657e-05, gnorm=0.615, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32659
2023-02-22 20:37:41 - progress_bar.py[line:274] - INFO: epoch 004:   1521 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.87, wpb=111.9, bsz=40, num_updates=11740, lr=2.244e-05, gnorm=0.56, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32671
2023-02-22 20:37:52 - progress_bar.py[line:274] - INFO: epoch 004:   1531 / 3411 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=11750, lr=2.24142e-05, gnorm=0.527, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32682
2023-02-22 20:38:03 - progress_bar.py[line:274] - INFO: epoch 004:   1541 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=112, bsz=40, num_updates=11760, lr=2.23885e-05, gnorm=0.777, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32693
2023-02-22 20:38:14 - progress_bar.py[line:274] - INFO: epoch 004:   1551 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=111.2, bsz=40, num_updates=11770, lr=2.23628e-05, gnorm=0.344, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32704
2023-02-22 20:38:25 - progress_bar.py[line:274] - INFO: epoch 004:   1561 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=11780, lr=2.23371e-05, gnorm=0.658, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32715
2023-02-22 20:38:36 - progress_bar.py[line:274] - INFO: epoch 004:   1571 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.87, wpb=111.9, bsz=40, num_updates=11790, lr=2.23114e-05, gnorm=0.536, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32727
2023-02-22 20:38:48 - progress_bar.py[line:274] - INFO: epoch 004:   1581 / 3411 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=11800, lr=2.22857e-05, gnorm=0.75, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32738
2023-02-22 20:38:59 - progress_bar.py[line:274] - INFO: epoch 004:   1591 / 3411 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.92, wpb=110.8, bsz=40, num_updates=11810, lr=2.22599e-05, gnorm=0.695, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32749
2023-02-22 20:39:10 - progress_bar.py[line:274] - INFO: epoch 004:   1601 / 3411 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.2, bsz=40, num_updates=11820, lr=2.22342e-05, gnorm=0.625, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32760
2023-02-22 20:39:21 - progress_bar.py[line:274] - INFO: epoch 004:   1611 / 3411 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.88, wpb=112.3, bsz=40, num_updates=11830, lr=2.22085e-05, gnorm=0.606, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32771
2023-02-22 20:39:32 - progress_bar.py[line:274] - INFO: epoch 004:   1621 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.4, bsz=40, num_updates=11840, lr=2.21828e-05, gnorm=0.51, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32782
2023-02-22 20:39:43 - progress_bar.py[line:274] - INFO: epoch 004:   1631 / 3411 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.92, wpb=111.3, bsz=40, num_updates=11850, lr=2.21571e-05, gnorm=0.752, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32793
2023-02-22 20:39:55 - progress_bar.py[line:274] - INFO: epoch 004:   1641 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=113.1, bsz=40, num_updates=11860, lr=2.21314e-05, gnorm=0.933, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32805
2023-02-22 20:40:06 - progress_bar.py[line:274] - INFO: epoch 004:   1651 / 3411 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.8, bsz=40, num_updates=11870, lr=2.21056e-05, gnorm=0.52, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32816
2023-02-22 20:40:17 - progress_bar.py[line:274] - INFO: epoch 004:   1661 / 3411 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.9, ups=0.93, wpb=111.4, bsz=40, num_updates=11880, lr=2.20799e-05, gnorm=0.817, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32827
2023-02-22 20:40:28 - progress_bar.py[line:274] - INFO: epoch 004:   1671 / 3411 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.9, wpb=113.9, bsz=40, num_updates=11890, lr=2.20542e-05, gnorm=0.598, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32838
2023-02-22 20:40:39 - progress_bar.py[line:274] - INFO: epoch 004:   1681 / 3411 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.9, wpb=110.2, bsz=40, num_updates=11900, lr=2.20285e-05, gnorm=0.654, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32849
2023-02-22 20:40:50 - progress_bar.py[line:274] - INFO: epoch 004:   1691 / 3411 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.89, wpb=114, bsz=40, num_updates=11910, lr=2.20028e-05, gnorm=0.754, clip=30, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32860
2023-02-22 20:40:51 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 20:41:02 - progress_bar.py[line:274] - INFO: epoch 004:   1702 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=91, ups=0.82, wpb=110.5, bsz=40, num_updates=11920, lr=2.19771e-05, gnorm=0.665, clip=20, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=32872
2023-02-22 20:41:13 - progress_bar.py[line:274] - INFO: epoch 004:   1712 / 3411 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=11930, lr=2.19513e-05, gnorm=0.872, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32884
2023-02-22 20:41:25 - progress_bar.py[line:274] - INFO: epoch 004:   1722 / 3411 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=11940, lr=2.19256e-05, gnorm=0.711, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32895
2023-02-22 20:41:35 - progress_bar.py[line:274] - INFO: epoch 004:   1732 / 3411 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.92, wpb=112, bsz=40, num_updates=11950, lr=2.18999e-05, gnorm=0.997, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32906
2023-02-22 20:41:46 - progress_bar.py[line:274] - INFO: epoch 004:   1742 / 3411 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.5, ups=0.94, wpb=113.5, bsz=40, num_updates=11960, lr=2.18742e-05, gnorm=0.721, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32916
2023-02-22 20:41:57 - progress_bar.py[line:274] - INFO: epoch 004:   1752 / 3411 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.92, wpb=111.6, bsz=40, num_updates=11970, lr=2.18485e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32927
2023-02-22 20:42:08 - progress_bar.py[line:274] - INFO: epoch 004:   1762 / 3411 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.2, ups=0.95, wpb=112.4, bsz=40, num_updates=11980, lr=2.18228e-05, gnorm=0.934, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32938
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 20:42:19 - progress_bar.py[line:274] - INFO: epoch 004:   1772 / 3411 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=111.4, bsz=40, num_updates=11990, lr=2.1797e-05, gnorm=0.79, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32949
2023-02-22 20:42:30 - progress_bar.py[line:274] - INFO: epoch 004:   1782 / 3411 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.9, bsz=40, num_updates=12000, lr=2.17713e-05, gnorm=0.655, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32960
2023-02-22 20:42:30 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 20:42:31 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 20:42:31 - train.py[line:551] - INFO: load:0.82 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 20:44:33 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 20:44:33 - train.py[line:551] - INFO: load:0.84 valid_run:122.39 task_valid:119.50 collect_output:1.78
2023-02-22 20:46:33 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 20:46:33 - train.py[line:551] - INFO: load:0.87 valid_run:242.50 task_valid:235.56 collect_output:4.75
2023-02-22 20:48:35 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 20:48:35 - train.py[line:551] - INFO: load:0.89 valid_run:364.47 task_valid:352.26 collect_output:9.00
2023-02-22 20:50:38 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 20:50:38 - train.py[line:551] - INFO: load:0.92 valid_run:486.52 task_valid:466.09 collect_output:16.20
2023-02-22 20:52:38 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 20:52:38 - train.py[line:551] - INFO: load:0.94 valid_run:607.01 task_valid:583.54 collect_output:18.23
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 62425
Killing subprocess 62427
Main process received SIGINT, exiting
