2022-10-11 17:08:03 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-10-11 17:08:03 - utils.py[line:261] - INFO: Start init
2022-10-11 17:08:03 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-10-11 17:08:03 - utils.py[line:261] - INFO: Start init
2022-10-11 17:08:04 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-10-11 17:08:04 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-10-11 17:08:04 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2022-10-11 17:08:04 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2022-10-11 17:08:10 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 5, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=50, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=5, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2022-10-11 17:08:10 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2022-10-11 17:08:10 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2022-10-11 17:08:14 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-10-11 17:08:14 - train.py[line:118] - INFO: task: VqaGenTask
2022-10-11 17:08:14 - train.py[line:119] - INFO: model: OFAModel
2022-10-11 17:08:14 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2022-10-11 17:08:14 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2022-10-11 17:08:14 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-10-11 17:08:15 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-10-11 17:08:15 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-10-11 17:08:16 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-11 17:08:16 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-11 17:08:16 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-11 17:08:16 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-11 17:08:16 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2022-10-11 17:08:16 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2022-10-11 17:08:16 - trainer.py[line:458] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt
2022-10-11 17:08:42 - trainer.py[line:605] - INFO: Loading EMA from checkpoint
2022-10-11 17:08:42 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2022-10-11 17:08:43 - trainer.py[line:273] - INFO: Exponential Moving Average Shadow Model is initialized.
2022-10-11 17:08:43 - trainer.py[line:612] - INFO: Loading EMA fp32 params from checkpoint
2022-10-11 17:08:43 - trainer.py[line:623] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt (epoch 8 @ 0 updates)
2022-10-11 17:08:43 - trainer.py[line:643] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 0 row count 578200 total row count 1156400
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 1 row count 578200 total row count 1156400
2022-10-11 17:08:44 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 1445500, warmup steps 57820, warmup_factor 1.7295053614666207e-05
Total steps 1445500, warmup steps 57820, warmup_factor 1.7295053614666207e-05
2022-10-11 17:08:46 - trainer.py[line:707] - INFO: begin training epoch 1
2022-10-11 17:08:46 - train.py[line:312] - INFO: Start iterating over samples
2022-10-11 17:09:03 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 28910 loss=0.765, loss_v1=0, loss_v2=0, nll_loss=0.669, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=86, ups=0.78, wpb=110.8, bsz=40, num_updates=10, lr=8.64753e-09, gnorm=2.464, clip=100, loss_scale=128, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=48
2022-10-11 17:09:15 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 28910 loss=0.781, loss_v1=0, loss_v2=0, nll_loss=0.682, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=95.9, ups=0.88, wpb=109.6, bsz=40, num_updates=20, lr=1.72951e-08, gnorm=2.531, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59
2022-10-11 17:09:27 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 28910 loss=0.792, loss_v1=0, loss_v2=0, nll_loss=0.702, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=95.8, ups=0.86, wpb=111.1, bsz=40, num_updates=30, lr=2.59426e-08, gnorm=2.481, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=71
2022-10-11 17:09:38 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 28910 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.656, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=97.7, ups=0.88, wpb=111.6, bsz=40, num_updates=40, lr=3.45901e-08, gnorm=2.317, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=82
2022-10-11 17:09:50 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 28910 loss=0.76, loss_v1=0, loss_v2=0, nll_loss=0.661, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=94, ups=0.86, wpb=109.4, bsz=40, num_updates=50, lr=4.32376e-08, gnorm=2.649, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=94
2022-10-11 17:10:02 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 28910 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.644, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=91.4, ups=0.82, wpb=111.1, bsz=40, num_updates=60, lr=5.18852e-08, gnorm=2.369, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=106
2022-10-11 17:10:13 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 28910 loss=0.717, loss_v1=0, loss_v2=0, nll_loss=0.616, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=94.7, ups=0.86, wpb=109.8, bsz=40, num_updates=70, lr=6.05327e-08, gnorm=2.347, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=118
2022-10-11 17:10:24 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 28910 loss=0.739, loss_v1=0, loss_v2=0, nll_loss=0.644, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=100.3, ups=0.91, wpb=110, bsz=40, num_updates=80, lr=6.91802e-08, gnorm=2.304, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=129
2022-10-11 17:10:36 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 28910 loss=0.749, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=97.8, ups=0.88, wpb=111, bsz=40, num_updates=90, lr=7.78277e-08, gnorm=2.232, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=140
2022-10-11 17:10:47 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 28910 loss=0.765, loss_v1=0, loss_v2=0, nll_loss=0.672, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=96.1, ups=0.87, wpb=110.1, bsz=40, num_updates=100, lr=8.64753e-08, gnorm=2.802, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=152
2022-10-11 17:10:58 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 28910 loss=0.803, loss_v1=0, loss_v2=0, nll_loss=0.711, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=110, lr=9.51228e-08, gnorm=2.831, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=163
2022-10-11 17:11:10 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 28910 loss=0.726, loss_v1=0, loss_v2=0, nll_loss=0.63, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=97.2, ups=0.87, wpb=111.2, bsz=40, num_updates=120, lr=1.0377e-07, gnorm=2.259, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=174
2022-10-11 17:11:21 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 28910 loss=0.802, loss_v1=0, loss_v2=0, nll_loss=0.711, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=130, lr=1.12418e-07, gnorm=2.611, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=185
2022-10-11 17:11:32 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 28910 loss=0.743, loss_v1=0, loss_v2=0, nll_loss=0.648, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=140, lr=1.21065e-07, gnorm=2.421, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=197
2022-10-11 17:11:44 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=96.4, ups=0.87, wpb=110.6, bsz=40, num_updates=150, lr=1.29713e-07, gnorm=2.416, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=208
2022-10-11 17:11:55 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 28910 loss=0.764, loss_v1=0, loss_v2=0, nll_loss=0.667, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=101.6, ups=0.92, wpb=110.5, bsz=40, num_updates=160, lr=1.3836e-07, gnorm=2.505, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=219
2022-10-11 17:12:06 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 28910 loss=0.852, loss_v1=0, loss_v2=0, nll_loss=0.764, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=170, lr=1.47008e-07, gnorm=2.758, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=230
2022-10-11 17:12:17 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 28910 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.596, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=180, lr=1.55655e-07, gnorm=2.149, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=241
2022-10-11 17:12:28 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 28910 loss=0.694, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=101.8, ups=0.91, wpb=111.8, bsz=40, num_updates=190, lr=1.64303e-07, gnorm=2.392, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=252
2022-10-11 17:12:39 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 28910 loss=0.818, loss_v1=0, loss_v2=0, nll_loss=0.728, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=97.6, ups=0.89, wpb=109.9, bsz=40, num_updates=200, lr=1.72951e-07, gnorm=2.601, clip=100, loss_scale=128, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=264
2022-10-11 17:12:50 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.649, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=210, lr=1.81598e-07, gnorm=2.262, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=275
2022-10-11 17:13:02 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 28910 loss=0.687, loss_v1=0, loss_v2=0, nll_loss=0.586, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=99, ups=0.9, wpb=109.8, bsz=40, num_updates=220, lr=1.90246e-07, gnorm=2.287, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=286
2022-10-11 17:13:13 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 28910 loss=0.738, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=99.8, ups=0.9, wpb=110.9, bsz=40, num_updates=230, lr=1.98893e-07, gnorm=2.358, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=297
2022-10-11 17:13:24 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 28910 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=100.4, ups=0.9, wpb=111.4, bsz=40, num_updates=240, lr=2.07541e-07, gnorm=2.015, clip=100, loss_scale=128, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=308
2022-10-11 17:13:35 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 28910 loss=0.741, loss_v1=0, loss_v2=0, nll_loss=0.648, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=250, lr=2.16188e-07, gnorm=2.17, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=319
2022-10-11 17:13:46 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 28910 loss=0.73, loss_v1=0, loss_v2=0, nll_loss=0.634, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=102.4, ups=0.93, wpb=110.5, bsz=40, num_updates=260, lr=2.24836e-07, gnorm=2.317, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=330
2022-10-11 17:13:57 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 28910 loss=0.828, loss_v1=0, loss_v2=0, nll_loss=0.735, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=98.9, ups=0.91, wpb=108.9, bsz=40, num_updates=270, lr=2.33483e-07, gnorm=2.455, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=341
2022-10-11 17:14:08 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 28910 loss=0.766, loss_v1=0, loss_v2=0, nll_loss=0.676, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=280, lr=2.42131e-07, gnorm=2.419, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=352
2022-10-11 17:14:19 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 28910 loss=0.722, loss_v1=0, loss_v2=0, nll_loss=0.631, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=97.2, ups=0.88, wpb=110, bsz=40, num_updates=290, lr=2.50778e-07, gnorm=2.211, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=364
2022-10-11 17:14:30 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 28910 loss=0.712, loss_v1=0, loss_v2=0, nll_loss=0.623, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=99.6, ups=0.91, wpb=109.8, bsz=40, num_updates=300, lr=2.59426e-07, gnorm=2.033, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=375
2022-10-11 17:14:42 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 28910 loss=0.674, loss_v1=0, loss_v2=0, nll_loss=0.577, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=96.8, ups=0.88, wpb=110.2, bsz=40, num_updates=310, lr=2.68073e-07, gnorm=2.079, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=386
2022-10-11 17:14:53 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 28910 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.583, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=101.2, ups=0.91, wpb=111.3, bsz=40, num_updates=320, lr=2.76721e-07, gnorm=2.244, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=397
2022-10-11 17:15:04 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 28910 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.639, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=98.9, ups=0.9, wpb=110.5, bsz=40, num_updates=330, lr=2.85368e-07, gnorm=2.054, clip=90, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=408
2022-10-11 17:15:15 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.666, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=104.5, ups=0.94, wpb=111.2, bsz=40, num_updates=340, lr=2.94016e-07, gnorm=1.999, clip=90, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=419
2022-10-11 17:15:26 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 28910 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=101.6, ups=0.91, wpb=111.8, bsz=40, num_updates=350, lr=3.02663e-07, gnorm=1.693, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=430
2022-10-11 17:15:37 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 28910 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=98.7, ups=0.9, wpb=110.2, bsz=40, num_updates=360, lr=3.11311e-07, gnorm=2.104, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=441
2022-10-11 17:15:48 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 28910 loss=0.735, loss_v1=0, loss_v2=0, nll_loss=0.651, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=99, ups=0.9, wpb=110.4, bsz=40, num_updates=370, lr=3.19958e-07, gnorm=2.107, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=452
2022-10-11 17:15:59 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.662, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=99.7, ups=0.91, wpb=109.9, bsz=40, num_updates=380, lr=3.28606e-07, gnorm=1.9, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=463
2022-10-11 17:16:11 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 28910 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.55, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=390, lr=3.37254e-07, gnorm=1.842, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=475
2022-10-11 17:16:22 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 28910 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=400, lr=3.45901e-07, gnorm=1.498, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=487
2022-10-11 17:16:33 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 28910 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.577, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=410, lr=3.54549e-07, gnorm=1.729, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=498
2022-10-11 17:16:44 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 28910 loss=0.674, loss_v1=0, loss_v2=0, nll_loss=0.587, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=101.1, ups=0.92, wpb=110.4, bsz=40, num_updates=420, lr=3.63196e-07, gnorm=1.848, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=509
2022-10-11 17:16:55 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 28910 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.56, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=100.9, ups=0.91, wpb=111.3, bsz=40, num_updates=430, lr=3.71844e-07, gnorm=1.651, clip=70, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=520
2022-10-11 17:17:07 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 28910 loss=0.732, loss_v1=0, loss_v2=0, nll_loss=0.649, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.4, ups=0.9, wpb=109.6, bsz=40, num_updates=440, lr=3.80491e-07, gnorm=1.918, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=531
2022-10-11 17:17:18 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 28910 loss=0.668, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=96.6, ups=0.87, wpb=110.8, bsz=40, num_updates=450, lr=3.89139e-07, gnorm=1.628, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=542
2022-10-11 17:17:29 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 28910 loss=0.725, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.1, ups=0.89, wpb=109.6, bsz=40, num_updates=460, lr=3.97786e-07, gnorm=1.881, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=554
2022-10-11 17:17:40 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 28910 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=97.7, ups=0.89, wpb=110.3, bsz=40, num_updates=470, lr=4.06434e-07, gnorm=1.375, clip=90, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=565
2022-10-11 17:17:51 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 28910 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.59, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=100.4, ups=0.92, wpb=109.2, bsz=40, num_updates=480, lr=4.15081e-07, gnorm=1.711, clip=80, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=576
2022-10-11 17:18:03 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 28910 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=490, lr=4.23729e-07, gnorm=1.523, clip=80, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=587
2022-10-11 17:18:14 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 28910 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=97.2, ups=0.88, wpb=110, bsz=40, num_updates=500, lr=4.32376e-07, gnorm=1.445, clip=70, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=598
2022-10-11 17:18:25 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 28910 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.588, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=97, ups=0.89, wpb=108.7, bsz=40, num_updates=510, lr=4.41024e-07, gnorm=1.814, clip=90, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=610
2022-10-11 17:18:36 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 28910 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=100.8, ups=0.9, wpb=111.5, bsz=40, num_updates=520, lr=4.49671e-07, gnorm=1.522, clip=80, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=621
2022-10-11 17:18:47 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 28910 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=103.5, ups=0.93, wpb=111, bsz=40, num_updates=530, lr=4.58319e-07, gnorm=1.252, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=632
2022-10-11 17:18:58 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 28910 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=540, lr=4.66966e-07, gnorm=1.688, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=643
2022-10-11 17:19:09 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 28910 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.567, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=100.7, ups=0.91, wpb=110.9, bsz=40, num_updates=550, lr=4.75614e-07, gnorm=1.435, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=654
2022-10-11 17:19:20 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 28910 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.595, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=102.4, ups=0.93, wpb=110.1, bsz=40, num_updates=560, lr=4.84262e-07, gnorm=1.639, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=664
2022-10-11 17:19:31 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 28910 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=98.5, ups=0.89, wpb=110.1, bsz=40, num_updates=570, lr=4.92909e-07, gnorm=1.381, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=676
2022-10-11 17:19:42 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 28910 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=98.5, ups=0.89, wpb=111.1, bsz=40, num_updates=580, lr=5.01557e-07, gnorm=1.466, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=687
2022-10-11 17:19:54 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 28910 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=101.2, ups=0.9, wpb=112.1, bsz=40, num_updates=590, lr=5.10204e-07, gnorm=1.508, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=698
2022-10-11 17:20:05 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 28910 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=96.3, ups=0.87, wpb=110.4, bsz=40, num_updates=600, lr=5.18852e-07, gnorm=1.626, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=709
2022-10-11 17:20:19 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 28910 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=76.9, ups=0.7, wpb=109.4, bsz=40, num_updates=610, lr=5.27499e-07, gnorm=1.635, clip=100, loss_scale=256, train_wall=14, gb_free=10.8, ema_decay=0.9999, wall=724
2022-10-11 17:20:34 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 28910 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=76.2, ups=0.7, wpb=109.6, bsz=40, num_updates=620, lr=5.36147e-07, gnorm=1.129, clip=60, loss_scale=256, train_wall=14, gb_free=11, ema_decay=0.9999, wall=738
2022-10-11 17:20:46 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 28910 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=93.2, ups=0.83, wpb=111.7, bsz=40, num_updates=630, lr=5.44794e-07, gnorm=1.547, clip=90, loss_scale=256, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=750
2022-10-11 17:20:59 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 28910 loss=0.682, loss_v1=0, loss_v2=0, nll_loss=0.61, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=82.5, ups=0.75, wpb=109.8, bsz=40, num_updates=640, lr=5.53442e-07, gnorm=1.805, clip=100, loss_scale=256, train_wall=13, gb_free=11, ema_decay=0.9999, wall=763
2022-10-11 17:21:11 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 28910 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=91.6, ups=0.83, wpb=110.1, bsz=40, num_updates=650, lr=5.62089e-07, gnorm=1.4, clip=90, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=775
2022-10-11 17:21:24 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.539, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=83, ups=0.75, wpb=110.6, bsz=40, num_updates=660, lr=5.70737e-07, gnorm=1.323, clip=90, loss_scale=256, train_wall=13, gb_free=10.8, ema_decay=0.9999, wall=789
2022-10-11 17:21:39 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 28910 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=75.4, ups=0.69, wpb=109.8, bsz=40, num_updates=670, lr=5.79384e-07, gnorm=1.397, clip=60, loss_scale=256, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=803
2022-10-11 17:21:53 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 28910 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.616, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=75.8, ups=0.69, wpb=109.6, bsz=40, num_updates=680, lr=5.88032e-07, gnorm=1.342, clip=80, loss_scale=256, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=818
2022-10-11 17:22:07 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 28910 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=78.8, ups=0.72, wpb=109.9, bsz=40, num_updates=690, lr=5.96679e-07, gnorm=1.122, clip=80, loss_scale=256, train_wall=14, gb_free=11.1, ema_decay=0.9999, wall=832
2022-10-11 17:22:21 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 28910 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=83.6, ups=0.74, wpb=112.4, bsz=40, num_updates=700, lr=6.05327e-07, gnorm=1.095, clip=60, loss_scale=256, train_wall=13, gb_free=10.7, ema_decay=0.9999, wall=845
2022-10-11 17:22:35 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 28910 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.546, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=77.7, ups=0.71, wpb=110, bsz=40, num_updates=710, lr=6.13974e-07, gnorm=1.234, clip=70, loss_scale=256, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=859
2022-10-11 17:22:49 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 28910 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=76.8, ups=0.69, wpb=110.5, bsz=40, num_updates=720, lr=6.22622e-07, gnorm=1.132, clip=70, loss_scale=256, train_wall=14, gb_free=10.8, ema_decay=0.9999, wall=874
2022-10-11 17:23:03 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 28910 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=78.1, ups=0.71, wpb=110.5, bsz=40, num_updates=730, lr=6.31269e-07, gnorm=1.278, clip=80, loss_scale=256, train_wall=14, gb_free=10.7, ema_decay=0.9999, wall=888
2022-10-11 17:23:18 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 28910 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=76, ups=0.69, wpb=109.9, bsz=40, num_updates=740, lr=6.39917e-07, gnorm=1.317, clip=60, loss_scale=256, train_wall=14, gb_free=10.3, ema_decay=0.9999, wall=902
2022-10-11 17:23:33 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 28910 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=76.7, ups=0.69, wpb=111.6, bsz=40, num_updates=750, lr=6.48565e-07, gnorm=1.312, clip=70, loss_scale=256, train_wall=14, gb_free=10.8, ema_decay=0.9999, wall=917
2022-10-11 17:23:47 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 28910 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=75.4, ups=0.69, wpb=109.4, bsz=40, num_updates=760, lr=6.57212e-07, gnorm=1.247, clip=80, loss_scale=256, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=931
2022-10-11 17:24:01 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 28910 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=77.6, ups=0.69, wpb=111.8, bsz=40, num_updates=770, lr=6.6586e-07, gnorm=1.41, clip=100, loss_scale=256, train_wall=14, gb_free=11.1, ema_decay=0.9999, wall=946
2022-10-11 17:24:16 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 28910 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.528, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=76.2, ups=0.68, wpb=111.6, bsz=40, num_updates=780, lr=6.74507e-07, gnorm=1.37, clip=80, loss_scale=256, train_wall=15, gb_free=10.5, ema_decay=0.9999, wall=961
2022-10-11 17:24:31 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 28910 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.562, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=76.7, ups=0.69, wpb=110.5, bsz=40, num_updates=790, lr=6.83155e-07, gnorm=1.282, clip=70, loss_scale=256, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=975
2022-10-11 17:24:45 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 28910 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.527, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=73.4, ups=0.68, wpb=108.7, bsz=40, num_updates=800, lr=6.91802e-07, gnorm=1.325, clip=80, loss_scale=256, train_wall=15, gb_free=10.4, ema_decay=0.9999, wall=990
2022-10-11 17:25:00 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 28910 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=76.2, ups=0.69, wpb=110.1, bsz=40, num_updates=810, lr=7.0045e-07, gnorm=1.163, clip=80, loss_scale=256, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=1004
2022-10-11 17:25:14 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.539, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=79.3, ups=0.72, wpb=110.3, bsz=40, num_updates=820, lr=7.09097e-07, gnorm=1.268, clip=90, loss_scale=256, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=1018
2022-10-11 17:25:28 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 28910 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=78.3, ups=0.7, wpb=111.5, bsz=40, num_updates=830, lr=7.17745e-07, gnorm=1.411, clip=80, loss_scale=256, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=1032
2022-10-11 17:25:45 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 28910 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.493, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=63.8, ups=0.58, wpb=110.4, bsz=40, num_updates=840, lr=7.26392e-07, gnorm=1.174, clip=60, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=1050
2022-10-11 17:26:07 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=51.4, ups=0.47, wpb=109.9, bsz=40, num_updates=850, lr=7.3504e-07, gnorm=1.403, clip=100, loss_scale=256, train_wall=21, gb_free=10.6, ema_decay=0.9999, wall=1071
2022-10-11 17:26:26 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 28910 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=57.7, ups=0.52, wpb=111, bsz=40, num_updates=860, lr=7.43687e-07, gnorm=1.309, clip=60, loss_scale=256, train_wall=19, gb_free=10.7, ema_decay=0.9999, wall=1090
2022-10-11 17:26:45 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 28910 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=57.1, ups=0.52, wpb=109.8, bsz=40, num_updates=870, lr=7.52335e-07, gnorm=1.171, clip=50, loss_scale=256, train_wall=19, gb_free=10.5, ema_decay=0.9999, wall=1110
2022-10-11 17:27:04 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 28910 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=58.1, ups=0.53, wpb=109.8, bsz=40, num_updates=880, lr=7.60982e-07, gnorm=1.369, clip=90, loss_scale=256, train_wall=19, gb_free=10.7, ema_decay=0.9999, wall=1129
2022-10-11 17:27:19 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 28910 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=75.3, ups=0.67, wpb=112.1, bsz=40, num_updates=890, lr=7.6963e-07, gnorm=1.28, clip=80, loss_scale=256, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=1144
2022-10-11 17:27:36 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 28910 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.516, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=64.3, ups=0.58, wpb=110.2, bsz=40, num_updates=900, lr=7.78277e-07, gnorm=1.283, clip=80, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=1161
2022-10-11 17:27:54 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 28910 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.537, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=63.3, ups=0.57, wpb=110.6, bsz=40, num_updates=910, lr=7.86925e-07, gnorm=1.217, clip=60, loss_scale=256, train_wall=17, gb_free=11.1, ema_decay=0.9999, wall=1178
2022-10-11 17:28:10 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 28910 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=70.1, ups=0.63, wpb=111.7, bsz=40, num_updates=920, lr=7.95572e-07, gnorm=1.145, clip=50, loss_scale=256, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=1194
2022-10-11 17:28:22 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 28910 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=93.8, ups=0.84, wpb=111.3, bsz=40, num_updates=930, lr=8.0422e-07, gnorm=1.29, clip=70, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1206
2022-10-11 17:28:33 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 28910 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=940, lr=8.12868e-07, gnorm=1.332, clip=90, loss_scale=256, train_wall=11, gb_free=10, ema_decay=0.9999, wall=1217
2022-10-11 17:28:43 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 28910 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=100.9, ups=0.92, wpb=109.8, bsz=40, num_updates=950, lr=8.21515e-07, gnorm=1.211, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1228
2022-10-11 17:28:55 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 28910 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.545, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=96.5, ups=0.89, wpb=109, bsz=40, num_updates=960, lr=8.30163e-07, gnorm=1.079, clip=50, loss_scale=256, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=1239
2022-10-11 17:29:06 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 28910 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.549, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=97.6, ups=0.88, wpb=110.6, bsz=40, num_updates=970, lr=8.3881e-07, gnorm=1.194, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1250
2022-10-11 17:29:18 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 28910 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=95.5, ups=0.87, wpb=109.6, bsz=40, num_updates=980, lr=8.47458e-07, gnorm=1.22, clip=60, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1262
2022-10-11 17:29:28 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 28910 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=103.7, ups=0.93, wpb=111.3, bsz=40, num_updates=990, lr=8.56105e-07, gnorm=1.184, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1273
2022-10-11 17:29:39 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 28910 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.503, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=1000, lr=8.64753e-07, gnorm=1.082, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1284
2022-10-11 17:29:39 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-11 17:29:39 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2022-10-11 17:29:41 - train.py[line:549] - INFO: 0 / 4988
2022-10-11 17:29:41 - train.py[line:551] - INFO: load:1.00 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-11 17:32:14 - train.py[line:549] - INFO: 200 / 4988
2022-10-11 17:32:14 - train.py[line:551] - INFO: load:1.02 valid_run:153.27 task_valid:149.39 collect_output:2.76
2022-10-11 17:34:42 - train.py[line:549] - INFO: 400 / 4988
2022-10-11 17:34:42 - train.py[line:551] - INFO: load:1.05 valid_run:301.21 task_valid:291.46 collect_output:7.61
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1647307
Killing subprocess 1647308
Main process received SIGINT, exiting
2022-10-11 17:37:13 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-10-11 17:37:13 - utils.py[line:261] - INFO: Start init
2022-10-11 17:37:13 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-10-11 17:37:13 - utils.py[line:261] - INFO: Start init
2022-10-11 17:37:13 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-10-11 17:37:13 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-10-11 17:37:13 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2022-10-11 17:37:13 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2022-10-11 17:37:18 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 5, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=50, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=5, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2022-10-11 17:37:19 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2022-10-11 17:37:19 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2022-10-11 17:37:23 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-10-11 17:37:23 - train.py[line:118] - INFO: task: VqaGenTask
2022-10-11 17:37:23 - train.py[line:119] - INFO: model: OFAModel
2022-10-11 17:37:23 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2022-10-11 17:37:23 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2022-10-11 17:37:23 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-10-11 17:37:23 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-10-11 17:37:23 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-10-11 17:37:24 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-11 17:37:24 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-11 17:37:24 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-11 17:37:24 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-11 17:37:24 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2022-10-11 17:37:24 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2022-10-11 17:37:24 - trainer.py[line:458] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt
2022-10-11 17:37:50 - trainer.py[line:605] - INFO: Loading EMA from checkpoint
2022-10-11 17:37:51 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2022-10-11 17:37:51 - trainer.py[line:273] - INFO: Exponential Moving Average Shadow Model is initialized.
2022-10-11 17:37:51 - trainer.py[line:612] - INFO: Loading EMA fp32 params from checkpoint
2022-10-11 17:37:51 - trainer.py[line:623] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt (epoch 8 @ 0 updates)
2022-10-11 17:37:51 - trainer.py[line:643] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 1 row count 578200 total row count 1156400
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 0 row count 578200 total row count 1156400
2022-10-11 17:37:52 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 1445500, warmup steps 57820, warmup_factor 1.7295053614666207e-05
Total steps 1445500, warmup steps 57820, warmup_factor 1.7295053614666207e-05
2022-10-11 17:37:53 - trainer.py[line:707] - INFO: begin training epoch 1
2022-10-11 17:37:53 - train.py[line:312] - INFO: Start iterating over samples
2022-10-11 17:38:10 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 28910 loss=0.765, loss_v1=0, loss_v2=0, nll_loss=0.669, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=93, ups=0.84, wpb=110.8, bsz=40, num_updates=10, lr=8.64753e-09, gnorm=2.464, clip=100, loss_scale=128, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=46
2022-10-11 17:38:21 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 28910 loss=0.781, loss_v1=0, loss_v2=0, nll_loss=0.682, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=94.9, ups=0.87, wpb=109.6, bsz=40, num_updates=20, lr=1.72951e-08, gnorm=2.531, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=57
2022-10-11 17:38:33 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 28910 loss=0.792, loss_v1=0, loss_v2=0, nll_loss=0.702, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=96.8, ups=0.87, wpb=111.1, bsz=40, num_updates=30, lr=2.59426e-08, gnorm=2.48, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69
2022-10-11 17:38:44 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 28910 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.656, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=98.7, ups=0.88, wpb=111.6, bsz=40, num_updates=40, lr=3.45901e-08, gnorm=2.316, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=80
2022-10-11 17:38:56 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 28910 loss=0.76, loss_v1=0, loss_v2=0, nll_loss=0.661, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=93.4, ups=0.85, wpb=109.4, bsz=40, num_updates=50, lr=4.32376e-08, gnorm=2.648, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=92
2022-10-11 17:39:08 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 28910 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.644, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=92.7, ups=0.83, wpb=111.1, bsz=40, num_updates=60, lr=5.18852e-08, gnorm=2.37, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=104
2022-10-11 17:39:20 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 28910 loss=0.717, loss_v1=0, loss_v2=0, nll_loss=0.616, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=94.4, ups=0.86, wpb=109.8, bsz=40, num_updates=70, lr=6.05327e-08, gnorm=2.345, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=116
2022-10-11 17:39:31 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 28910 loss=0.739, loss_v1=0, loss_v2=0, nll_loss=0.644, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=99.1, ups=0.9, wpb=110, bsz=40, num_updates=80, lr=6.91802e-08, gnorm=2.302, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=127
2022-10-11 17:39:42 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 28910 loss=0.749, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=97.6, ups=0.88, wpb=111, bsz=40, num_updates=90, lr=7.78277e-08, gnorm=2.231, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=138
2022-10-11 17:39:53 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 28910 loss=0.765, loss_v1=0, loss_v2=0, nll_loss=0.672, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=96.2, ups=0.87, wpb=110.1, bsz=40, num_updates=100, lr=8.64753e-08, gnorm=2.812, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=150
2022-10-11 17:40:05 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 28910 loss=0.803, loss_v1=0, loss_v2=0, nll_loss=0.711, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=110, lr=9.51228e-08, gnorm=2.829, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=161
2022-10-11 17:40:16 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 28910 loss=0.726, loss_v1=0, loss_v2=0, nll_loss=0.63, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=96.5, ups=0.87, wpb=111.2, bsz=40, num_updates=120, lr=1.0377e-07, gnorm=2.262, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=172
2022-10-11 17:40:27 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 28910 loss=0.802, loss_v1=0, loss_v2=0, nll_loss=0.711, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=130, lr=1.12418e-07, gnorm=2.61, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=184
2022-10-11 17:40:39 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 28910 loss=0.743, loss_v1=0, loss_v2=0, nll_loss=0.648, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=140, lr=1.21065e-07, gnorm=2.42, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=195
2022-10-11 17:40:50 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=96.9, ups=0.88, wpb=110.6, bsz=40, num_updates=150, lr=1.29713e-07, gnorm=2.415, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=206
2022-10-11 17:41:01 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 28910 loss=0.764, loss_v1=0, loss_v2=0, nll_loss=0.667, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=102, ups=0.92, wpb=110.5, bsz=40, num_updates=160, lr=1.3836e-07, gnorm=2.502, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=217
2022-10-11 17:41:12 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 28910 loss=0.852, loss_v1=0, loss_v2=0, nll_loss=0.764, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=170, lr=1.47008e-07, gnorm=2.76, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=228
2022-10-11 17:41:23 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 28910 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.596, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=99.1, ups=0.89, wpb=111.4, bsz=40, num_updates=180, lr=1.55655e-07, gnorm=2.15, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=240
2022-10-11 17:41:35 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 28910 loss=0.694, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=100.9, ups=0.9, wpb=111.8, bsz=40, num_updates=190, lr=1.64303e-07, gnorm=2.394, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=251
2022-10-11 17:41:46 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 28910 loss=0.818, loss_v1=0, loss_v2=0, nll_loss=0.728, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=200, lr=1.72951e-07, gnorm=2.602, clip=100, loss_scale=128, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=262
2022-10-11 17:41:57 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.649, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98, ups=0.89, wpb=110.7, bsz=40, num_updates=210, lr=1.81598e-07, gnorm=2.263, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=273
2022-10-11 17:42:08 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 28910 loss=0.687, loss_v1=0, loss_v2=0, nll_loss=0.586, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=98.4, ups=0.9, wpb=109.8, bsz=40, num_updates=220, lr=1.90246e-07, gnorm=2.289, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=284
2022-10-11 17:42:19 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 28910 loss=0.738, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=230, lr=1.98893e-07, gnorm=2.364, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=296
2022-10-11 17:42:31 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 28910 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=240, lr=2.07541e-07, gnorm=2.017, clip=100, loss_scale=128, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=307
2022-10-11 17:42:42 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 28910 loss=0.741, loss_v1=0, loss_v2=0, nll_loss=0.648, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=250, lr=2.16188e-07, gnorm=2.17, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=318
2022-10-11 17:42:53 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 28910 loss=0.73, loss_v1=0, loss_v2=0, nll_loss=0.634, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=101, ups=0.91, wpb=110.5, bsz=40, num_updates=260, lr=2.24836e-07, gnorm=2.316, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=329
2022-10-11 17:43:04 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 28910 loss=0.828, loss_v1=0, loss_v2=0, nll_loss=0.735, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=97.9, ups=0.9, wpb=108.9, bsz=40, num_updates=270, lr=2.33483e-07, gnorm=2.454, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=340
2022-10-11 17:43:15 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 28910 loss=0.766, loss_v1=0, loss_v2=0, nll_loss=0.676, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=280, lr=2.42131e-07, gnorm=2.419, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=351
2022-10-11 17:43:27 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 28910 loss=0.722, loss_v1=0, loss_v2=0, nll_loss=0.631, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=95.9, ups=0.87, wpb=110, bsz=40, num_updates=290, lr=2.50778e-07, gnorm=2.211, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=363
2022-10-11 17:43:38 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 28910 loss=0.712, loss_v1=0, loss_v2=0, nll_loss=0.623, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=98.9, ups=0.9, wpb=109.8, bsz=40, num_updates=300, lr=2.59426e-07, gnorm=2.029, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=374
2022-10-11 17:43:49 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 28910 loss=0.674, loss_v1=0, loss_v2=0, nll_loss=0.577, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=96.7, ups=0.88, wpb=110.2, bsz=40, num_updates=310, lr=2.68073e-07, gnorm=2.08, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=385
2022-10-11 17:44:00 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 28910 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.583, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=100.3, ups=0.9, wpb=111.3, bsz=40, num_updates=320, lr=2.76721e-07, gnorm=2.247, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=396
2022-10-11 17:44:11 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 28910 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.639, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=98.9, ups=0.89, wpb=110.5, bsz=40, num_updates=330, lr=2.85368e-07, gnorm=2.055, clip=90, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=407
2022-10-11 17:44:22 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.666, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=104, ups=0.94, wpb=111.2, bsz=40, num_updates=340, lr=2.94016e-07, gnorm=2, clip=90, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=418
2022-10-11 17:44:33 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 28910 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=100.5, ups=0.9, wpb=111.8, bsz=40, num_updates=350, lr=3.02663e-07, gnorm=1.692, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=429
2022-10-11 17:44:45 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 28910 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.566, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=97.4, ups=0.88, wpb=110.2, bsz=40, num_updates=360, lr=3.11311e-07, gnorm=2.106, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=441
2022-10-11 17:44:56 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 28910 loss=0.735, loss_v1=0, loss_v2=0, nll_loss=0.651, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=99.3, ups=0.9, wpb=110.4, bsz=40, num_updates=370, lr=3.19958e-07, gnorm=2.108, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=452
2022-10-11 17:45:07 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 28910 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.662, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=95, ups=0.86, wpb=109.9, bsz=40, num_updates=380, lr=3.28606e-07, gnorm=1.9, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=463
2022-10-11 17:45:18 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 28910 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.55, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=99.8, ups=0.9, wpb=111, bsz=40, num_updates=390, lr=3.37254e-07, gnorm=1.843, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=474
2022-10-11 17:45:29 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 28910 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=400, lr=3.45901e-07, gnorm=1.497, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=486
2022-10-11 17:45:41 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 28910 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.577, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=96.3, ups=0.88, wpb=109.5, bsz=40, num_updates=410, lr=3.54549e-07, gnorm=1.728, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=497
2022-10-11 17:45:52 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 28910 loss=0.674, loss_v1=0, loss_v2=0, nll_loss=0.587, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=420, lr=3.63196e-07, gnorm=1.852, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=508
2022-10-11 17:46:03 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 28910 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.56, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=430, lr=3.71844e-07, gnorm=1.651, clip=70, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=519
2022-10-11 17:46:14 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 28910 loss=0.732, loss_v1=0, loss_v2=0, nll_loss=0.649, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=440, lr=3.80491e-07, gnorm=1.915, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=530
2022-10-11 17:46:26 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 28910 loss=0.668, loss_v1=0, loss_v2=0, nll_loss=0.593, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=450, lr=3.89139e-07, gnorm=1.628, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=542
2022-10-11 17:46:37 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 28910 loss=0.725, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=460, lr=3.97786e-07, gnorm=1.887, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=553
2022-10-11 17:46:48 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 28910 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=470, lr=4.06434e-07, gnorm=1.373, clip=90, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=564
2022-10-11 17:46:59 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 28910 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.59, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=480, lr=4.15081e-07, gnorm=1.705, clip=80, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=575
2022-10-11 17:47:11 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 28910 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=96.9, ups=0.88, wpb=110.4, bsz=40, num_updates=490, lr=4.23729e-07, gnorm=1.52, clip=80, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=587
2022-10-11 17:47:23 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 28910 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=97.5, ups=0.89, wpb=110, bsz=40, num_updates=500, lr=4.32376e-07, gnorm=1.443, clip=70, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=599
2022-10-11 17:47:34 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 28910 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.588, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=510, lr=4.41024e-07, gnorm=1.819, clip=90, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=610
2022-10-11 17:47:45 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 28910 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=100.4, ups=0.9, wpb=111.5, bsz=40, num_updates=520, lr=4.49671e-07, gnorm=1.519, clip=80, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=621
2022-10-11 17:47:56 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 28910 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=102.9, ups=0.93, wpb=111, bsz=40, num_updates=530, lr=4.58319e-07, gnorm=1.249, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=632
2022-10-11 17:48:07 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 28910 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=99.3, ups=0.9, wpb=110.1, bsz=40, num_updates=540, lr=4.66966e-07, gnorm=1.692, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=643
2022-10-11 17:48:18 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 28910 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.567, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=550, lr=4.75614e-07, gnorm=1.44, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=654
2022-10-11 17:48:29 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 28910 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.595, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=102.4, ups=0.93, wpb=110.1, bsz=40, num_updates=560, lr=4.84262e-07, gnorm=1.639, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=665
2022-10-11 17:48:40 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 28910 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=570, lr=4.92909e-07, gnorm=1.381, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=676
2022-10-11 17:48:51 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 28910 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=98.1, ups=0.88, wpb=111.1, bsz=40, num_updates=580, lr=5.01557e-07, gnorm=1.465, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=687
2022-10-11 17:49:02 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 28910 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=100.8, ups=0.9, wpb=112.1, bsz=40, num_updates=590, lr=5.10204e-07, gnorm=1.503, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=698
2022-10-11 17:49:14 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 28910 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=95.1, ups=0.86, wpb=110.4, bsz=40, num_updates=600, lr=5.18852e-07, gnorm=1.63, clip=80, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=710
2022-10-11 17:49:25 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 28910 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=96.5, ups=0.88, wpb=109.4, bsz=40, num_updates=610, lr=5.27499e-07, gnorm=1.639, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=721
2022-10-11 17:49:36 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 28910 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=98.4, ups=0.9, wpb=109.6, bsz=40, num_updates=620, lr=5.36147e-07, gnorm=1.131, clip=60, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=732
2022-10-11 17:49:48 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 28910 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=98.2, ups=0.88, wpb=111.7, bsz=40, num_updates=630, lr=5.44794e-07, gnorm=1.55, clip=90, loss_scale=256, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=744
2022-10-11 17:49:59 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 28910 loss=0.682, loss_v1=0, loss_v2=0, nll_loss=0.61, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=95.2, ups=0.87, wpb=109.8, bsz=40, num_updates=640, lr=5.53442e-07, gnorm=1.8, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=755
2022-10-11 17:50:11 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 28910 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=650, lr=5.62089e-07, gnorm=1.401, clip=90, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=767
2022-10-11 17:50:22 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.539, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=96.3, ups=0.87, wpb=110.6, bsz=40, num_updates=660, lr=5.70737e-07, gnorm=1.327, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=778
2022-10-11 17:50:33 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 28910 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=98.9, ups=0.9, wpb=109.8, bsz=40, num_updates=670, lr=5.79384e-07, gnorm=1.395, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=789
2022-10-11 17:50:45 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 28910 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.616, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=96.7, ups=0.88, wpb=109.6, bsz=40, num_updates=680, lr=5.88032e-07, gnorm=1.346, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=801
2022-10-11 17:50:55 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 28910 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=101.7, ups=0.93, wpb=109.9, bsz=40, num_updates=690, lr=5.96679e-07, gnorm=1.121, clip=80, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=812
2022-10-11 17:51:06 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 28910 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=105.8, ups=0.94, wpb=112.4, bsz=40, num_updates=700, lr=6.05327e-07, gnorm=1.096, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=822
2022-10-11 17:51:17 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 28910 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.546, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=97.6, ups=0.89, wpb=110, bsz=40, num_updates=710, lr=6.13974e-07, gnorm=1.231, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=833
2022-10-11 17:51:29 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 28910 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=720, lr=6.22622e-07, gnorm=1.133, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=845
2022-10-11 17:51:40 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 28910 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=99.1, ups=0.9, wpb=110.5, bsz=40, num_updates=730, lr=6.31269e-07, gnorm=1.276, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=856
2022-10-11 17:51:51 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 28910 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=740, lr=6.39917e-07, gnorm=1.322, clip=60, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=867
2022-10-11 17:52:02 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 28910 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=100.3, ups=0.9, wpb=111.6, bsz=40, num_updates=750, lr=6.48565e-07, gnorm=1.315, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=879
2022-10-11 17:52:13 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 28910 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=100.3, ups=0.92, wpb=109.4, bsz=40, num_updates=760, lr=6.57212e-07, gnorm=1.256, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=889
2022-10-11 17:52:25 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 28910 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=97.1, ups=0.87, wpb=111.8, bsz=40, num_updates=770, lr=6.6586e-07, gnorm=1.408, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=901
2022-10-11 17:52:36 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 28910 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.528, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.7, ups=0.88, wpb=111.6, bsz=40, num_updates=780, lr=6.74507e-07, gnorm=1.371, clip=80, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=912
2022-10-11 17:52:47 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 28910 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.562, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=100, ups=0.9, wpb=110.5, bsz=40, num_updates=790, lr=6.83155e-07, gnorm=1.293, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=923
2022-10-11 17:52:58 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 28910 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.527, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=97.6, ups=0.9, wpb=108.7, bsz=40, num_updates=800, lr=6.91802e-07, gnorm=1.318, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=934
2022-10-11 17:53:09 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 28910 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=100, ups=0.91, wpb=110.1, bsz=40, num_updates=810, lr=7.0045e-07, gnorm=1.165, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=946
2022-10-11 17:53:20 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.539, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=103.9, ups=0.94, wpb=110.3, bsz=40, num_updates=820, lr=7.09097e-07, gnorm=1.274, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=956
2022-10-11 17:53:31 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 28910 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=99.5, ups=0.89, wpb=111.5, bsz=40, num_updates=830, lr=7.17745e-07, gnorm=1.409, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=967
2022-10-11 17:53:42 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 28910 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.493, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=840, lr=7.26392e-07, gnorm=1.176, clip=60, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=979
2022-10-11 17:53:54 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=96.2, ups=0.88, wpb=109.9, bsz=40, num_updates=850, lr=7.3504e-07, gnorm=1.401, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=990
2022-10-11 17:54:05 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 28910 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=97.7, ups=0.88, wpb=111, bsz=40, num_updates=860, lr=7.43687e-07, gnorm=1.312, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1001
2022-10-11 17:54:16 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 28910 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=870, lr=7.52335e-07, gnorm=1.173, clip=50, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1013
2022-10-11 17:54:28 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 28910 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=97.6, ups=0.89, wpb=109.8, bsz=40, num_updates=880, lr=7.60982e-07, gnorm=1.364, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1024
2022-10-11 17:54:39 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 28910 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=98.3, ups=0.88, wpb=112.1, bsz=40, num_updates=890, lr=7.6963e-07, gnorm=1.274, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1035
2022-10-11 17:54:50 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 28910 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.516, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.1, ups=0.9, wpb=110.2, bsz=40, num_updates=900, lr=7.78277e-07, gnorm=1.284, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1046
2022-10-11 17:55:02 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 28910 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.537, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=910, lr=7.86925e-07, gnorm=1.219, clip=60, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1058
2022-10-11 17:55:13 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 28910 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=99.5, ups=0.89, wpb=111.7, bsz=40, num_updates=920, lr=7.95572e-07, gnorm=1.146, clip=50, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1069
2022-10-11 17:55:24 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 28910 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=930, lr=8.0422e-07, gnorm=1.282, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1080
2022-10-11 17:55:35 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 28910 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=940, lr=8.12868e-07, gnorm=1.316, clip=90, loss_scale=256, train_wall=11, gb_free=10, ema_decay=0.9999, wall=1091
2022-10-11 17:55:46 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 28910 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=100.4, ups=0.91, wpb=109.8, bsz=40, num_updates=950, lr=8.21515e-07, gnorm=1.212, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1102
2022-10-11 17:55:58 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 28910 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.545, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=96.3, ups=0.88, wpb=109, bsz=40, num_updates=960, lr=8.30163e-07, gnorm=1.078, clip=50, loss_scale=256, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=1114
2022-10-11 17:56:09 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 28910 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.549, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=96.3, ups=0.87, wpb=110.6, bsz=40, num_updates=970, lr=8.3881e-07, gnorm=1.202, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1125
2022-10-11 17:56:21 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 28910 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=95, ups=0.87, wpb=109.6, bsz=40, num_updates=980, lr=8.47458e-07, gnorm=1.215, clip=60, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1137
2022-10-11 17:56:31 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 28910 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=103.3, ups=0.93, wpb=111.3, bsz=40, num_updates=990, lr=8.56105e-07, gnorm=1.187, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1148
2022-10-11 17:56:42 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 28910 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.503, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=101.1, ups=0.9, wpb=112.1, bsz=40, num_updates=1000, lr=8.64753e-07, gnorm=1.082, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1159
2022-10-11 17:56:42 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-11 17:56:43 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2022-10-11 17:56:44 - train.py[line:549] - INFO: 0 / 4988
2022-10-11 17:56:44 - train.py[line:551] - INFO: load:1.35 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-11 17:59:19 - train.py[line:549] - INFO: 200 / 4988
2022-10-11 17:59:19 - train.py[line:551] - INFO: load:1.37 valid_run:155.07 task_valid:151.67 collect_output:2.22
2022-10-11 18:01:49 - train.py[line:549] - INFO: 400 / 4988
2022-10-11 18:01:49 - train.py[line:551] - INFO: load:1.40 valid_run:304.55 task_valid:295.72 collect_output:6.47
2022-10-11 18:04:21 - train.py[line:549] - INFO: 600 / 4988
2022-10-11 18:04:21 - train.py[line:551] - INFO: load:1.42 valid_run:456.74 task_valid:438.96 collect_output:14.41
2022-10-11 18:06:50 - train.py[line:549] - INFO: 800 / 4988
2022-10-11 18:06:50 - train.py[line:551] - INFO: load:1.45 valid_run:605.65 task_valid:584.12 collect_output:17.04
2022-10-11 18:09:22 - train.py[line:549] - INFO: 1000 / 4988
2022-10-11 18:09:22 - train.py[line:551] - INFO: load:1.47 valid_run:757.70 task_valid:731.73 collect_output:20.42
2022-10-11 18:11:54 - train.py[line:549] - INFO: 1200 / 4988
2022-10-11 18:11:54 - train.py[line:551] - INFO: load:1.50 valid_run:909.12 task_valid:877.41 collect_output:25.12
2022-10-11 18:14:26 - train.py[line:549] - INFO: 1400 / 4988
2022-10-11 18:14:26 - train.py[line:551] - INFO: load:1.52 valid_run:1061.67 task_valid:1023.71 collect_output:30.31
2022-10-11 18:16:57 - train.py[line:549] - INFO: 1600 / 4988
2022-10-11 18:16:57 - train.py[line:551] - INFO: load:1.55 valid_run:1212.62 task_valid:1165.60 collect_output:38.27
2022-10-11 18:19:27 - train.py[line:549] - INFO: 1800 / 4988
2022-10-11 18:19:27 - train.py[line:551] - INFO: load:1.58 valid_run:1362.29 task_valid:1310.73 collect_output:41.74
2022-10-11 18:21:55 - train.py[line:549] - INFO: 2000 / 4988
2022-10-11 18:21:55 - train.py[line:551] - INFO: load:1.60 valid_run:1510.41 task_valid:1454.02 collect_output:45.51
2022-10-11 18:24:25 - train.py[line:549] - INFO: 2200 / 4988
2022-10-11 18:24:25 - train.py[line:551] - INFO: load:1.63 valid_run:1659.73 task_valid:1598.94 collect_output:48.90
2022-10-11 18:26:55 - train.py[line:549] - INFO: 2400 / 4988
2022-10-11 18:26:55 - train.py[line:551] - INFO: load:1.65 valid_run:1809.58 task_valid:1744.22 collect_output:52.42
2022-10-11 18:29:24 - train.py[line:549] - INFO: 2600 / 4988
2022-10-11 18:29:24 - train.py[line:551] - INFO: load:1.68 valid_run:1959.01 task_valid:1886.13 collect_output:58.91
2022-10-11 18:31:54 - train.py[line:549] - INFO: 2800 / 4988
2022-10-11 18:31:54 - train.py[line:551] - INFO: load:1.70 valid_run:2109.19 task_valid:2031.70 collect_output:62.48
2022-10-11 18:34:24 - train.py[line:549] - INFO: 3000 / 4988
2022-10-11 18:34:24 - train.py[line:551] - INFO: load:1.73 valid_run:2259.15 task_valid:2178.35 collect_output:64.73
2022-10-11 18:36:54 - train.py[line:549] - INFO: 3200 / 4988
2022-10-11 18:36:54 - train.py[line:551] - INFO: load:1.76 valid_run:2409.25 task_valid:2323.19 collect_output:68.93
2022-10-11 18:39:26 - train.py[line:549] - INFO: 3400 / 4988
2022-10-11 18:39:26 - train.py[line:551] - INFO: load:1.78 valid_run:2560.67 task_valid:2469.06 collect_output:73.41
2022-10-11 18:41:57 - train.py[line:549] - INFO: 3600 / 4988
2022-10-11 18:41:57 - train.py[line:551] - INFO: load:1.81 valid_run:2711.20 task_valid:2616.30 collect_output:75.64
2022-10-11 18:44:25 - train.py[line:549] - INFO: 3800 / 4988
2022-10-11 18:44:25 - train.py[line:551] - INFO: load:1.83 valid_run:2859.25 task_valid:2758.15 collect_output:80.79
2022-10-11 18:46:55 - train.py[line:549] - INFO: 4000 / 4988
2022-10-11 18:46:55 - train.py[line:551] - INFO: load:1.86 valid_run:3009.35 task_valid:2903.54 collect_output:84.44
2022-10-11 18:49:26 - train.py[line:549] - INFO: 4200 / 4988
2022-10-11 18:49:26 - train.py[line:551] - INFO: load:1.88 valid_run:3160.97 task_valid:3048.54 collect_output:89.86
2022-10-11 18:51:56 - train.py[line:549] - INFO: 4400 / 4988
2022-10-11 18:51:56 - train.py[line:551] - INFO: load:1.91 valid_run:3310.84 task_valid:3193.88 collect_output:93.17
2022-10-11 18:54:28 - train.py[line:549] - INFO: 4600 / 4988
2022-10-11 18:54:28 - train.py[line:551] - INFO: load:1.93 valid_run:3462.40 task_valid:3340.79 collect_output:96.69
2022-10-11 18:57:00 - train.py[line:549] - INFO: 4800 / 4988
2022-10-11 18:57:00 - train.py[line:551] - INFO: load:1.96 valid_run:3614.15 task_valid:3488.13 collect_output:99.91

====================================================================================================
SGG eval:     R @ 50: 0.6418;     R @ 100: 0.6811;     R @ 500: 0.7114;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4349;    mR @ 100: 0.4837;    mR @ 500: 0.5289;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7195) (covered in:0.3542) (covering:0.3714) (eating:0.7647) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5323) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.9624) (says:0.0000) (sitting on:0.6774) (standing on:0.5113) (using:0.4500) (walking in:0.0000) (walking on:0.6216) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6418;     R @ 100: 0.6811;     R @ 500: 0.7114;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4349;    mR @ 100: 0.4837;    mR @ 500: 0.5289;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7195) (covered in:0.3542) (covering:0.3714) (eating:0.7647) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5323) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.9624) (says:0.0000) (sitting on:0.6774) (standing on:0.5113) (using:0.4500) (walking in:0.0000) (walking on:0.6216) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================

2022-10-11 18:59:32 - train.py[line:487] - INFO: 0.6811467787114845
2022-10-11 18:59:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-11 18:59:32 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.325 | loss_v1 0 | loss_v2 0 | nll_loss 0.212 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.681147 | ppl 1.16 | vqa_score 0.4696 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 1000
2022-10-11 18:59:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1000 updates
2022-10-11 18:59:32 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_1000.pt
2022-10-11 18:59:38 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_1000.pt
2022-10-11 18:59:43 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 0.6811467787114845) (writing took 10.75139603484422 seconds)
2022-10-11 18:59:53 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 28910 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=0.3, ups=0, wpb=109.3, bsz=40, num_updates=1010, lr=8.734e-07, gnorm=1.236, clip=80, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4949
2022-10-11 19:00:05 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 28910 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=1020, lr=8.82048e-07, gnorm=1.27, clip=70, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=4961
2022-10-11 19:00:16 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 28910 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=1030, lr=8.90695e-07, gnorm=1.233, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4972
2022-10-11 19:00:28 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 28910 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=1040, lr=8.99343e-07, gnorm=1.132, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4984
2022-10-11 19:00:39 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 28910 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=1050, lr=9.0799e-07, gnorm=1.187, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4995
2022-10-11 19:00:50 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 28910 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=97, ups=0.89, wpb=108.6, bsz=40, num_updates=1060, lr=9.16638e-07, gnorm=1.044, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5006
2022-10-11 19:01:02 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=97.8, ups=0.88, wpb=111, bsz=40, num_updates=1070, lr=9.25285e-07, gnorm=1.113, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5018
2022-10-11 19:01:13 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 28910 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.5, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.4, ups=0.88, wpb=110.8, bsz=40, num_updates=1080, lr=9.33933e-07, gnorm=1.159, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5029
2022-10-11 19:01:24 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 28910 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=100.7, ups=0.92, wpb=109.7, bsz=40, num_updates=1090, lr=9.4258e-07, gnorm=1.264, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5040
2022-10-11 19:01:35 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 28910 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.516, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=1100, lr=9.51228e-07, gnorm=1.164, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5051
2022-10-11 19:01:46 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 28910 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=1110, lr=9.59875e-07, gnorm=1.375, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5062
2022-10-11 19:01:57 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 28910 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=98.8, ups=0.9, wpb=110, bsz=40, num_updates=1120, lr=9.68523e-07, gnorm=1.101, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5074
2022-10-11 19:02:08 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 28910 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=102.9, ups=0.93, wpb=110.7, bsz=40, num_updates=1130, lr=9.77171e-07, gnorm=1.212, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5084
2022-10-11 19:02:19 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 28910 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.514, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.7, ups=0.9, wpb=110.6, bsz=40, num_updates=1140, lr=9.85818e-07, gnorm=1.199, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5095
2022-10-11 19:02:30 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 28910 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.513, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=1150, lr=9.94466e-07, gnorm=1.11, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5107
2022-10-11 19:02:41 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 28910 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=102.1, ups=0.93, wpb=110, bsz=40, num_updates=1160, lr=1.00311e-06, gnorm=1.192, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5117
2022-10-11 19:02:52 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 28910 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=98.9, ups=0.9, wpb=110.3, bsz=40, num_updates=1170, lr=1.01176e-06, gnorm=1.134, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5129
2022-10-11 19:03:04 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 28910 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=1180, lr=1.02041e-06, gnorm=1.328, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5140
2022-10-11 19:03:15 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 28910 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=94.7, ups=0.87, wpb=108.9, bsz=40, num_updates=1190, lr=1.02906e-06, gnorm=1.138, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5151
2022-10-11 19:03:26 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 28910 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=1200, lr=1.0377e-06, gnorm=1.335, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5162
2022-10-11 19:03:37 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 28910 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=97.7, ups=0.9, wpb=109.1, bsz=40, num_updates=1210, lr=1.04635e-06, gnorm=1.35, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5173
2022-10-11 19:03:49 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 28910 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=1220, lr=1.055e-06, gnorm=1.201, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5185
2022-10-11 19:04:00 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 28910 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=98, ups=0.88, wpb=111.3, bsz=40, num_updates=1230, lr=1.06365e-06, gnorm=1.195, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5196
2022-10-11 19:04:11 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 28910 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=97.1, ups=0.89, wpb=108.6, bsz=40, num_updates=1240, lr=1.07229e-06, gnorm=1.201, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5207
2022-10-11 19:04:23 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 28910 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=95.2, ups=0.88, wpb=108.5, bsz=40, num_updates=1250, lr=1.08094e-06, gnorm=1.107, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5219
2022-10-11 19:04:34 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 28910 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=95.3, ups=0.87, wpb=109.5, bsz=40, num_updates=1260, lr=1.08959e-06, gnorm=1.199, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5230
2022-10-11 19:04:45 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 28910 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=1270, lr=1.09824e-06, gnorm=0.998, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5241
2022-10-11 19:04:56 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 28910 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.533, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=98.5, ups=0.9, wpb=110, bsz=40, num_updates=1280, lr=1.10688e-06, gnorm=1.097, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5252
2022-10-11 19:05:08 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 28910 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98, ups=0.9, wpb=109.4, bsz=40, num_updates=1290, lr=1.11553e-06, gnorm=1.183, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5264
2022-10-11 19:05:19 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 28910 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=97.7, ups=0.87, wpb=111.9, bsz=40, num_updates=1300, lr=1.12418e-06, gnorm=1.238, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5275
2022-10-11 19:05:30 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 28910 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98.8, ups=0.89, wpb=110.4, bsz=40, num_updates=1310, lr=1.13283e-06, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5286
2022-10-11 19:05:41 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 28910 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=100.6, ups=0.92, wpb=109.6, bsz=40, num_updates=1320, lr=1.14147e-06, gnorm=1.097, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5297
2022-10-11 19:05:52 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 28910 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=1330, lr=1.15012e-06, gnorm=1.246, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5308
2022-10-11 19:06:03 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 28910 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.7, ups=0.9, wpb=112.1, bsz=40, num_updates=1340, lr=1.15877e-06, gnorm=1.062, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5319
2022-10-11 19:06:15 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 28910 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=97.2, ups=0.88, wpb=110.5, bsz=40, num_updates=1350, lr=1.16742e-06, gnorm=1.16, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5331
2022-10-11 19:06:26 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 28910 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=100.5, ups=0.91, wpb=111, bsz=40, num_updates=1360, lr=1.17606e-06, gnorm=1.074, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5342
2022-10-11 19:06:37 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 28910 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=98.8, ups=0.9, wpb=109.7, bsz=40, num_updates=1370, lr=1.18471e-06, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5353
2022-10-11 19:06:47 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 28910 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=105.2, ups=0.95, wpb=110.2, bsz=40, num_updates=1380, lr=1.19336e-06, gnorm=1.198, clip=80, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=5363
2022-10-11 19:06:59 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 28910 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=96.8, ups=0.88, wpb=109.7, bsz=40, num_updates=1390, lr=1.20201e-06, gnorm=1.072, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5375
2022-10-11 19:07:10 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 28910 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.3, ups=0.88, wpb=109.3, bsz=40, num_updates=1400, lr=1.21065e-06, gnorm=1.123, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5386
2022-10-11 19:07:21 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 28910 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.5, ups=0.91, wpb=110.9, bsz=40, num_updates=1410, lr=1.2193e-06, gnorm=1.063, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5397
2022-10-11 19:07:32 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 28910 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=1420, lr=1.22795e-06, gnorm=1.122, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5408
2022-10-11 19:07:43 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 28910 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=102.6, ups=0.93, wpb=109.8, bsz=40, num_updates=1430, lr=1.2366e-06, gnorm=1.114, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5419
2022-10-11 19:07:54 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 28910 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=98.4, ups=0.91, wpb=108.6, bsz=40, num_updates=1440, lr=1.24524e-06, gnorm=1.201, clip=80, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=5430
2022-10-11 19:08:05 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 28910 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.9, ups=0.9, wpb=110.9, bsz=40, num_updates=1450, lr=1.25389e-06, gnorm=1.29, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5441
2022-10-11 19:08:16 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 28910 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=96.9, ups=0.89, wpb=109.1, bsz=40, num_updates=1460, lr=1.26254e-06, gnorm=1.2, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5452
2022-10-11 19:08:27 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 28910 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100.4, ups=0.91, wpb=110.7, bsz=40, num_updates=1470, lr=1.27119e-06, gnorm=1.112, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5463
2022-10-11 19:08:39 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 28910 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=98.9, ups=0.9, wpb=110.5, bsz=40, num_updates=1480, lr=1.27983e-06, gnorm=1.04, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5475
2022-10-11 19:08:49 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 28910 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=1490, lr=1.28848e-06, gnorm=1.071, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5486
2022-10-11 19:09:01 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 28910 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=1500, lr=1.29713e-06, gnorm=1.037, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5497
2022-10-11 19:09:12 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 28910 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=1510, lr=1.30578e-06, gnorm=1.129, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5508
2022-10-11 19:09:23 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 28910 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=102.8, ups=0.93, wpb=110.1, bsz=40, num_updates=1520, lr=1.31442e-06, gnorm=1.158, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5519
2022-10-11 19:09:33 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 28910 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=103.8, ups=0.93, wpb=111.6, bsz=40, num_updates=1530, lr=1.32307e-06, gnorm=1.057, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5530
2022-10-11 19:09:45 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 28910 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=96.1, ups=0.87, wpb=110.2, bsz=40, num_updates=1540, lr=1.33172e-06, gnorm=1.137, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5541
2022-10-11 19:09:56 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 28910 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.9, ups=0.92, wpb=111, bsz=40, num_updates=1550, lr=1.34037e-06, gnorm=1.184, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5552
2022-10-11 19:10:07 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 28910 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=1560, lr=1.34901e-06, gnorm=1.164, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5563
2022-10-11 19:10:18 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 28910 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.7, ups=0.91, wpb=109.7, bsz=40, num_updates=1570, lr=1.35766e-06, gnorm=1.289, clip=90, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5574
2022-10-11 19:10:29 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 28910 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.5, ups=0.9, wpb=110.9, bsz=40, num_updates=1580, lr=1.36631e-06, gnorm=1.055, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5585
2022-10-11 19:10:40 - progress_bar.py[line:274] - INFO: epoch 001:   1590 / 28910 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=1590, lr=1.37496e-06, gnorm=1.116, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5596
2022-10-11 19:10:51 - progress_bar.py[line:274] - INFO: epoch 001:   1600 / 28910 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=96.6, ups=0.87, wpb=111.1, bsz=40, num_updates=1600, lr=1.3836e-06, gnorm=1.066, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5608
2022-10-11 19:11:03 - progress_bar.py[line:274] - INFO: epoch 001:   1610 / 28910 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.2, ups=0.88, wpb=109.8, bsz=40, num_updates=1610, lr=1.39225e-06, gnorm=1.14, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5619
2022-10-11 19:11:14 - progress_bar.py[line:274] - INFO: epoch 001:   1620 / 28910 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.9, ups=0.91, wpb=110, bsz=40, num_updates=1620, lr=1.4009e-06, gnorm=1.106, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5630
2022-10-11 19:11:25 - progress_bar.py[line:274] - INFO: epoch 001:   1630 / 28910 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.9, ups=0.91, wpb=111.1, bsz=40, num_updates=1630, lr=1.40955e-06, gnorm=1.127, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5641
2022-10-11 19:11:36 - progress_bar.py[line:274] - INFO: epoch 001:   1640 / 28910 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.6, ups=0.92, wpb=110.7, bsz=40, num_updates=1640, lr=1.41819e-06, gnorm=1.094, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5652
2022-10-11 19:11:47 - progress_bar.py[line:274] - INFO: epoch 001:   1650 / 28910 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.6, ups=0.92, wpb=110.3, bsz=40, num_updates=1650, lr=1.42684e-06, gnorm=1.154, clip=80, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5663
2022-10-11 19:11:58 - progress_bar.py[line:274] - INFO: epoch 001:   1660 / 28910 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98, ups=0.88, wpb=110.9, bsz=40, num_updates=1660, lr=1.43549e-06, gnorm=1.228, clip=90, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5674
2022-10-11 19:12:09 - progress_bar.py[line:274] - INFO: epoch 001:   1670 / 28910 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.4, ups=0.9, wpb=111, bsz=40, num_updates=1670, lr=1.44414e-06, gnorm=1.059, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5685
2022-10-11 19:12:21 - progress_bar.py[line:274] - INFO: epoch 001:   1680 / 28910 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=96.5, ups=0.88, wpb=109.1, bsz=40, num_updates=1680, lr=1.45278e-06, gnorm=1.17, clip=80, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5697
2022-10-11 19:12:32 - progress_bar.py[line:274] - INFO: epoch 001:   1690 / 28910 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=1690, lr=1.46143e-06, gnorm=1.216, clip=90, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5708
2022-10-11 19:12:43 - progress_bar.py[line:274] - INFO: epoch 001:   1700 / 28910 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.1, ups=0.91, wpb=109.3, bsz=40, num_updates=1700, lr=1.47008e-06, gnorm=1.082, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5719
2022-10-11 19:12:49 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-11 19:12:55 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 28910 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=93.8, ups=0.85, wpb=110.3, bsz=40, num_updates=1710, lr=1.47873e-06, gnorm=1.245, clip=90, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=5731
2022-10-11 19:13:06 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 28910 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=102.3, ups=0.92, wpb=110.6, bsz=40, num_updates=1720, lr=1.48737e-06, gnorm=0.959, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5742
2022-10-11 19:13:17 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 28910 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=1730, lr=1.49602e-06, gnorm=1.11, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5753
2022-10-11 19:13:28 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 28910 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=1740, lr=1.50467e-06, gnorm=1, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5764
2022-10-11 19:13:39 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 28910 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.9, ups=0.89, wpb=110.6, bsz=40, num_updates=1750, lr=1.51332e-06, gnorm=1.166, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5775
2022-10-11 19:13:51 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 28910 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.4, ups=0.88, wpb=111.2, bsz=40, num_updates=1760, lr=1.52196e-06, gnorm=1.058, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5787
2022-10-11 19:14:02 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 28910 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.7, ups=0.94, wpb=110.3, bsz=40, num_updates=1770, lr=1.53061e-06, gnorm=1.051, clip=50, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=5797
2022-10-11 19:14:13 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 28910 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.1, ups=0.87, wpb=110.6, bsz=40, num_updates=1780, lr=1.53926e-06, gnorm=1.153, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5809
2022-10-11 19:14:24 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 28910 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.2, ups=0.88, wpb=111.2, bsz=40, num_updates=1790, lr=1.54791e-06, gnorm=1, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5821
2022-10-11 19:14:36 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 28910 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=96.5, ups=0.88, wpb=109.6, bsz=40, num_updates=1800, lr=1.55655e-06, gnorm=1.153, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5832
2022-10-11 19:14:47 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 28910 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=96.9, ups=0.89, wpb=108.4, bsz=40, num_updates=1810, lr=1.5652e-06, gnorm=1.097, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5843
2022-10-11 19:14:58 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 28910 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=1820, lr=1.57385e-06, gnorm=1.086, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5854
2022-10-11 19:15:09 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 28910 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.4, ups=0.9, wpb=108.3, bsz=40, num_updates=1830, lr=1.5825e-06, gnorm=1.146, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5865
2022-10-11 19:15:21 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 28910 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=95.5, ups=0.87, wpb=110.4, bsz=40, num_updates=1840, lr=1.59114e-06, gnorm=1.317, clip=90, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5877
2022-10-11 19:15:32 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 28910 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=104.3, ups=0.93, wpb=111.9, bsz=40, num_updates=1850, lr=1.59979e-06, gnorm=0.948, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5888
2022-10-11 19:15:43 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 28910 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.2, ups=0.91, wpb=109, bsz=40, num_updates=1860, lr=1.60844e-06, gnorm=1.066, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5899
2022-10-11 19:15:54 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 28910 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=1870, lr=1.61709e-06, gnorm=1.038, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5910
2022-10-11 19:16:05 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 28910 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=1880, lr=1.62574e-06, gnorm=1.047, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5921
2022-10-11 19:16:16 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 28910 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.7, ups=0.91, wpb=111.1, bsz=40, num_updates=1890, lr=1.63438e-06, gnorm=0.991, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5932
2022-10-11 19:16:27 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 28910 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=97.7, ups=0.89, wpb=109.2, bsz=40, num_updates=1900, lr=1.64303e-06, gnorm=1.117, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5943
2022-10-11 19:16:38 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 28910 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.4, ups=0.89, wpb=109.2, bsz=40, num_updates=1910, lr=1.65168e-06, gnorm=1.061, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5954
2022-10-11 19:16:49 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 28910 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=1920, lr=1.66033e-06, gnorm=1.045, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5965
2022-10-11 19:17:00 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 28910 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=1930, lr=1.66897e-06, gnorm=1.033, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5976
2022-10-11 19:17:12 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 28910 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=1940, lr=1.67762e-06, gnorm=1.101, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5988
2022-10-11 19:17:23 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 28910 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.7, ups=0.91, wpb=110.2, bsz=40, num_updates=1950, lr=1.68627e-06, gnorm=1.043, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5999
2022-10-11 19:17:34 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 28910 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.1, ups=0.91, wpb=108.9, bsz=40, num_updates=1960, lr=1.69492e-06, gnorm=1.064, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6010
2022-10-11 19:17:45 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 28910 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.5, ups=0.88, wpb=110.3, bsz=40, num_updates=1970, lr=1.70356e-06, gnorm=1.095, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6021
2022-10-11 19:17:56 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 28910 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.4, ups=0.9, wpb=111.8, bsz=40, num_updates=1980, lr=1.71221e-06, gnorm=0.966, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6032
2022-10-11 19:18:07 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 28910 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=104.7, ups=0.94, wpb=111.2, bsz=40, num_updates=1990, lr=1.72086e-06, gnorm=1.037, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6043
2022-10-11 19:18:18 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 28910 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98, ups=0.9, wpb=109.5, bsz=40, num_updates=2000, lr=1.72951e-06, gnorm=1.034, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6054
2022-10-11 19:18:18 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-11 19:18:20 - train.py[line:549] - INFO: 0 / 4988
2022-10-11 19:18:20 - train.py[line:551] - INFO: load:1.16 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-11 19:18:20 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 8.87 GiB already allocated; 3.18 GiB free; 33.92 GiB reserved in total by PyTorch)
2022-10-11 19:18:20 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-11 19:18:20 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9078 MB |   10302 MB |     880 TB |     880 TB |
|       from large pool |    8933 MB |   10157 MB |     879 TB |     879 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9078 MB |   10302 MB |     880 TB |     880 TB |
|       from large pool |    8933 MB |   10157 MB |     879 TB |     879 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   34738 MB |   36468 MB |  129810 MB |   95072 MB |
|       from large pool |   34592 MB |   36316 MB |  129554 MB |   94962 MB |
|       from small pool |     146 MB |     152 MB |     256 MB |     110 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25659 MB |   25659 MB |     953 TB |     953 TB |
|       from large pool |   25658 MB |   25658 MB |     953 TB |     953 TB |
|       from small pool |       1 MB |       2 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |   38496 K  |   38492 K  |
|       from large pool |     563    |     575    |   13267 K  |   13266 K  |
|       from small pool |    3095    |    3114    |   25228 K  |   25225 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |   38496 K  |   38492 K  |
|       from large pool |     563    |     575    |   13267 K  |   13266 K  |
|       from small pool |    3095    |    3114    |   25228 K  |   25225 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     196    |     205    |     456    |     260    |
|       from large pool |     123    |     129    |     328    |     205    |
|       from small pool |      73    |      76    |     128    |      55    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     146    |   26712 K  |   26712 K  |
|       from large pool |      95    |      97    |    5376 K  |    5376 K  |
|       from small pool |      46    |      53    |   21336 K  |   21336 K  |
|===========================================================================|

2022-10-11 19:18:20 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-11 19:20:53 - train.py[line:549] - INFO: 200 / 4988
2022-10-11 19:20:53 - train.py[line:551] - INFO: load:1.18 valid_run:153.44 task_valid:149.26 collect_output:3.09
2022-10-11 19:23:22 - train.py[line:549] - INFO: 400 / 4988
2022-10-11 19:23:22 - train.py[line:551] - INFO: load:1.21 valid_run:301.87 task_valid:292.91 collect_output:6.80
2022-10-11 19:25:54 - train.py[line:549] - INFO: 600 / 4988
2022-10-11 19:25:54 - train.py[line:551] - INFO: load:1.23 valid_run:453.94 task_valid:436.31 collect_output:14.38
2022-10-11 19:28:23 - train.py[line:549] - INFO: 800 / 4988
2022-10-11 19:28:23 - train.py[line:551] - INFO: load:1.26 valid_run:602.76 task_valid:581.46 collect_output:16.99
2022-10-11 19:30:55 - train.py[line:549] - INFO: 1000 / 4988
2022-10-11 19:30:55 - train.py[line:551] - INFO: load:1.29 valid_run:754.79 task_valid:729.12 collect_output:20.34
2022-10-11 19:33:26 - train.py[line:549] - INFO: 1200 / 4988
2022-10-11 19:33:26 - train.py[line:551] - INFO: load:1.31 valid_run:906.16 task_valid:874.93 collect_output:24.85
2022-10-11 19:35:59 - train.py[line:549] - INFO: 1400 / 4988
2022-10-11 19:35:59 - train.py[line:551] - INFO: load:1.34 valid_run:1058.77 task_valid:1021.09 collect_output:30.28
2022-10-11 19:38:30 - train.py[line:549] - INFO: 1600 / 4988
2022-10-11 19:38:30 - train.py[line:551] - INFO: load:1.36 valid_run:1209.44 task_valid:1162.45 collect_output:38.54
2022-10-11 19:40:59 - train.py[line:549] - INFO: 1800 / 4988
2022-10-11 19:40:59 - train.py[line:551] - INFO: load:1.39 valid_run:1358.99 task_valid:1307.53 collect_output:41.97
2022-10-11 19:43:28 - train.py[line:549] - INFO: 2000 / 4988
2022-10-11 19:43:28 - train.py[line:551] - INFO: load:1.41 valid_run:1507.39 task_valid:1451.19 collect_output:45.66
2022-10-11 19:45:57 - train.py[line:549] - INFO: 2200 / 4988
2022-10-11 19:45:57 - train.py[line:551] - INFO: load:1.44 valid_run:1656.78 task_valid:1596.13 collect_output:49.09
2022-10-11 19:48:27 - train.py[line:549] - INFO: 2400 / 4988
2022-10-11 19:48:27 - train.py[line:551] - INFO: load:1.46 valid_run:1806.38 task_valid:1741.08 collect_output:52.71
2022-10-11 19:50:57 - train.py[line:549] - INFO: 2600 / 4988
2022-10-11 19:50:57 - train.py[line:551] - INFO: load:1.49 valid_run:1956.23 task_valid:1883.45 collect_output:59.11
2022-10-11 19:53:27 - train.py[line:549] - INFO: 2800 / 4988
2022-10-11 19:53:27 - train.py[line:551] - INFO: load:1.51 valid_run:2106.17 task_valid:2028.75 collect_output:62.73
2022-10-11 19:55:57 - train.py[line:549] - INFO: 3000 / 4988
2022-10-11 19:55:57 - train.py[line:551] - INFO: load:1.54 valid_run:2256.41 task_valid:2175.41 collect_output:65.24
2022-10-11 19:58:28 - train.py[line:549] - INFO: 3200 / 4988
2022-10-11 19:58:28 - train.py[line:551] - INFO: load:1.56 valid_run:2406.82 task_valid:2320.13 collect_output:69.86
2022-10-11 20:00:59 - train.py[line:549] - INFO: 3400 / 4988
2022-10-11 20:00:59 - train.py[line:551] - INFO: load:1.59 valid_run:2558.28 task_valid:2465.92 collect_output:74.44
2022-10-11 20:03:30 - train.py[line:549] - INFO: 3600 / 4988
2022-10-11 20:03:30 - train.py[line:551] - INFO: load:1.61 valid_run:2709.40 task_valid:2613.62 collect_output:76.70
2022-10-11 20:05:59 - train.py[line:549] - INFO: 3800 / 4988
2022-10-11 20:05:59 - train.py[line:551] - INFO: load:1.64 valid_run:2857.85 task_valid:2755.55 collect_output:82.14
2022-10-11 20:08:29 - train.py[line:549] - INFO: 4000 / 4988
2022-10-11 20:08:29 - train.py[line:551] - INFO: load:1.66 valid_run:3008.10 task_valid:2900.98 collect_output:85.87
2022-10-11 20:11:01 - train.py[line:549] - INFO: 4200 / 4988
2022-10-11 20:11:01 - train.py[line:551] - INFO: load:1.69 valid_run:3159.77 task_valid:3045.83 collect_output:91.54
2022-10-11 20:13:31 - train.py[line:549] - INFO: 4400 / 4988
2022-10-11 20:13:31 - train.py[line:551] - INFO: load:1.71 valid_run:3309.42 task_valid:3190.87 collect_output:95.04
2022-10-11 20:16:02 - train.py[line:549] - INFO: 4600 / 4988
2022-10-11 20:16:02 - train.py[line:551] - INFO: load:1.74 valid_run:3460.69 task_valid:3337.59 collect_output:98.46
2022-10-11 20:18:33 - train.py[line:549] - INFO: 4800 / 4988
2022-10-11 20:18:33 - train.py[line:551] - INFO: load:1.77 valid_run:3611.81 task_valid:3484.15 collect_output:102.00

====================================================================================================
SGG eval:     R @ 50: 0.6575;     R @ 100: 0.6985;     R @ 500: 0.7237;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4510;    mR @ 100: 0.5130;    mR @ 500: 0.5401;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6875) (covering:0.5143) (eating:0.8235) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4677) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:1.0000) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.7134) (standing on:0.4813) (using:0.5000) (walking in:0.0000) (walking on:0.6757) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6575;     R @ 100: 0.6985;     R @ 500: 0.7237;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4510;    mR @ 100: 0.5130;    mR @ 500: 0.5401;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6875) (covering:0.5143) (eating:0.8235) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4677) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:1.0000) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.7134) (standing on:0.4813) (using:0.5000) (walking in:0.0000) (walking on:0.6757) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================

2022-10-11 20:21:04 - train.py[line:487] - INFO: 0.698480112044818
2022-10-11 20:21:04 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-11 20:21:04 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.327 | loss_v1 0 | loss_v2 0 | nll_loss 0.196 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.69848 | ppl 1.15 | vqa_score 0.5203 | wps 119.2 | wpb 89.9 | bsz 30 | num_updates 2000 | best_R@100 0.69848
2022-10-11 20:21:04 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2022-10-11 20:21:04 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_2000.pt
2022-10-11 20:21:11 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_2000.pt
2022-10-11 20:21:16 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.698480112044818) (writing took 11.20223046373576 seconds)
2022-10-11 20:21:27 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 28910 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=0.3, ups=0, wpb=109.9, bsz=40, num_updates=2010, lr=1.73815e-06, gnorm=1.071, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=9843
2022-10-11 20:21:38 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 28910 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.3, ups=0.88, wpb=109.3, bsz=40, num_updates=2020, lr=1.7468e-06, gnorm=1.044, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=9854
2022-10-11 20:21:49 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 28910 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=2030, lr=1.75545e-06, gnorm=1.017, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9866
2022-10-11 20:22:00 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 28910 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=2040, lr=1.7641e-06, gnorm=1.091, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9876
2022-10-11 20:22:12 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 28910 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.2, ups=0.88, wpb=109.7, bsz=40, num_updates=2050, lr=1.77274e-06, gnorm=0.992, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9888
2022-10-11 20:22:23 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 28910 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.8, ups=0.91, wpb=110.7, bsz=40, num_updates=2060, lr=1.78139e-06, gnorm=1.113, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9899
2022-10-11 20:22:34 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 28910 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.2, ups=0.9, wpb=108.5, bsz=40, num_updates=2070, lr=1.79004e-06, gnorm=1.093, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9910
2022-10-11 20:22:45 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 28910 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=2080, lr=1.79869e-06, gnorm=1.1, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9921
2022-10-11 20:22:57 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 28910 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=2090, lr=1.80733e-06, gnorm=1.114, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9933
2022-10-11 20:23:08 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 28910 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.3, ups=0.89, wpb=111.1, bsz=40, num_updates=2100, lr=1.81598e-06, gnorm=1.081, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9944
2022-10-11 20:23:19 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 28910 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=2110, lr=1.82463e-06, gnorm=1.124, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9955
2022-10-11 20:23:30 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 28910 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=95.9, ups=0.88, wpb=108.7, bsz=40, num_updates=2120, lr=1.83328e-06, gnorm=1.148, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=9967
2022-10-11 20:23:42 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 28910 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.6, ups=0.9, wpb=108.8, bsz=40, num_updates=2130, lr=1.84192e-06, gnorm=1.118, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9978
2022-10-11 20:23:53 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 28910 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=95.8, ups=0.88, wpb=108.5, bsz=40, num_updates=2140, lr=1.85057e-06, gnorm=1.218, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9989
2022-10-11 20:24:04 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 28910 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.91, wpb=110.2, bsz=40, num_updates=2150, lr=1.85922e-06, gnorm=0.986, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10000
2022-10-11 20:24:15 - progress_bar.py[line:274] - INFO: epoch 001:   2161 / 28910 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.1, ups=0.9, wpb=109.3, bsz=40, num_updates=2160, lr=1.86787e-06, gnorm=1.147, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10011
2022-10-11 20:24:26 - progress_bar.py[line:274] - INFO: epoch 001:   2171 / 28910 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=2170, lr=1.87651e-06, gnorm=1.165, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10022
2022-10-11 20:24:37 - progress_bar.py[line:274] - INFO: epoch 001:   2181 / 28910 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=2180, lr=1.88516e-06, gnorm=1.126, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10033
2022-10-11 20:24:48 - progress_bar.py[line:274] - INFO: epoch 001:   2191 / 28910 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99, ups=0.9, wpb=110.1, bsz=40, num_updates=2190, lr=1.89381e-06, gnorm=1.232, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10044
2022-10-11 20:24:59 - progress_bar.py[line:274] - INFO: epoch 001:   2201 / 28910 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.5, ups=0.92, wpb=110.3, bsz=40, num_updates=2200, lr=1.90246e-06, gnorm=1.113, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10055
2022-10-11 20:25:11 - progress_bar.py[line:274] - INFO: epoch 001:   2211 / 28910 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.8, ups=0.87, wpb=110.9, bsz=40, num_updates=2210, lr=1.9111e-06, gnorm=1.082, clip=50, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10067
2022-10-11 20:25:22 - progress_bar.py[line:274] - INFO: epoch 001:   2221 / 28910 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.7, ups=0.9, wpb=109.9, bsz=40, num_updates=2220, lr=1.91975e-06, gnorm=1.049, clip=60, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10078
2022-10-11 20:25:33 - progress_bar.py[line:274] - INFO: epoch 001:   2231 / 28910 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.4, ups=0.89, wpb=110, bsz=40, num_updates=2230, lr=1.9284e-06, gnorm=1.149, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10090
2022-10-11 20:25:44 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-11 20:25:46 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 28910 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=87.1, ups=0.8, wpb=108.9, bsz=40, num_updates=2240, lr=1.93705e-06, gnorm=1.09, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=10102
2022-10-11 20:25:57 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 28910 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.9, ups=0.9, wpb=110.4, bsz=40, num_updates=2250, lr=1.94569e-06, gnorm=1.218, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10113
2022-10-11 20:26:08 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 28910 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=2260, lr=1.95434e-06, gnorm=1.068, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10125
2022-10-11 20:26:19 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 28910 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.7, ups=0.91, wpb=111.1, bsz=40, num_updates=2270, lr=1.96299e-06, gnorm=1.102, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10136
2022-10-11 20:26:31 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 28910 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.5, ups=0.89, wpb=111.1, bsz=40, num_updates=2280, lr=1.97164e-06, gnorm=1.07, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10147
2022-10-11 20:26:42 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 28910 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.8, ups=0.93, wpb=110.8, bsz=40, num_updates=2290, lr=1.98028e-06, gnorm=0.996, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10158
2022-10-11 20:26:53 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 28910 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=95.8, ups=0.88, wpb=109.4, bsz=40, num_updates=2300, lr=1.98893e-06, gnorm=1.103, clip=80, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=10169
2022-10-11 20:27:04 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 28910 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.91, wpb=110.3, bsz=40, num_updates=2310, lr=1.99758e-06, gnorm=1.148, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10180
2022-10-11 20:27:15 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 28910 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.4, ups=0.89, wpb=108.9, bsz=40, num_updates=2320, lr=2.00623e-06, gnorm=1.076, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10192
2022-10-11 20:27:27 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 28910 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.8, ups=0.89, wpb=109.3, bsz=40, num_updates=2330, lr=2.01487e-06, gnorm=1.142, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10203
2022-10-11 20:27:38 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 28910 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.93, wpb=110.4, bsz=40, num_updates=2340, lr=2.02352e-06, gnorm=1.045, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10214
2022-10-11 20:27:49 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 28910 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.1, ups=0.87, wpb=110.2, bsz=40, num_updates=2350, lr=2.03217e-06, gnorm=1.065, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10225
2022-10-11 20:28:00 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 28910 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=2360, lr=2.04082e-06, gnorm=1.107, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10236
2022-10-11 20:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 28910 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.6, ups=0.9, wpb=111.2, bsz=40, num_updates=2370, lr=2.04946e-06, gnorm=1.016, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10248
2022-10-11 20:28:23 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 28910 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=2380, lr=2.05811e-06, gnorm=1.158, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10259
2022-10-11 20:28:34 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 28910 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=2390, lr=2.06676e-06, gnorm=1.069, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10270
2022-10-11 20:28:45 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 28910 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98, ups=0.9, wpb=109.5, bsz=40, num_updates=2400, lr=2.07541e-06, gnorm=1.163, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10281
2022-10-11 20:28:56 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 28910 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.3, ups=0.91, wpb=110.2, bsz=40, num_updates=2410, lr=2.08405e-06, gnorm=1.023, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10292
2022-10-11 20:29:07 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 28910 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.4, ups=0.92, wpb=110.2, bsz=40, num_updates=2420, lr=2.0927e-06, gnorm=1.267, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10303
2022-10-11 20:29:19 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 28910 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=2430, lr=2.10135e-06, gnorm=1.047, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10315
2022-10-11 20:29:30 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 28910 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.2, ups=0.9, wpb=111.9, bsz=40, num_updates=2440, lr=2.11e-06, gnorm=1.121, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10326
2022-10-11 20:29:41 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 28910 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.7, ups=0.9, wpb=109.5, bsz=40, num_updates=2450, lr=2.11864e-06, gnorm=1.089, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10337
2022-10-11 20:29:52 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 28910 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.2, ups=0.93, wpb=110.9, bsz=40, num_updates=2460, lr=2.12729e-06, gnorm=1.071, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10348
2022-10-11 20:30:03 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 28910 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.2, ups=0.91, wpb=111.8, bsz=40, num_updates=2470, lr=2.13594e-06, gnorm=1.142, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10359
2022-10-11 20:30:14 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 28910 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=2480, lr=2.14459e-06, gnorm=1.072, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10370
2022-10-11 20:30:27 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 28910 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.2, ups=0.87, wpb=111.5, bsz=40, num_updates=2490, lr=2.15323e-06, gnorm=1.042, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10381
2022-10-11 20:30:38 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 28910 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=2500, lr=2.16188e-06, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10394
2022-10-11 20:30:49 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 28910 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=2510, lr=2.17053e-06, gnorm=1.35, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10405
2022-10-11 20:31:01 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 28910 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.1, ups=0.88, wpb=109.9, bsz=40, num_updates=2520, lr=2.17918e-06, gnorm=1.162, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10417
2022-10-11 20:31:12 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 28910 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=2530, lr=2.18782e-06, gnorm=1.071, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10428
2022-10-11 20:31:23 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 28910 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=104.1, ups=0.94, wpb=111, bsz=40, num_updates=2540, lr=2.19647e-06, gnorm=1.07, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10439
2022-10-11 20:31:34 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 28910 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.8, ups=0.92, wpb=108.7, bsz=40, num_updates=2550, lr=2.20512e-06, gnorm=1.148, clip=80, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10450
2022-10-11 20:31:45 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 28910 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.9, ups=0.89, wpb=111.7, bsz=40, num_updates=2560, lr=2.21377e-06, gnorm=1.095, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10461
2022-10-11 20:31:57 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 28910 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.8, ups=0.89, wpb=111.3, bsz=40, num_updates=2570, lr=2.22241e-06, gnorm=1.102, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10472
2022-10-11 20:32:08 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 28910 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.9, ups=0.9, wpb=110, bsz=40, num_updates=2580, lr=2.23106e-06, gnorm=1.018, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10484
2022-10-11 20:32:19 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 28910 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.1, ups=0.9, wpb=109.8, bsz=40, num_updates=2590, lr=2.23971e-06, gnorm=1.069, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10495
2022-10-11 20:32:31 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 28910 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.6, ups=0.88, wpb=111.2, bsz=40, num_updates=2600, lr=2.24836e-06, gnorm=1.08, clip=60, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=10507
2022-10-11 20:32:42 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 28910 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.1, ups=0.88, wpb=111.2, bsz=40, num_updates=2610, lr=2.257e-06, gnorm=1.08, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10518
2022-10-11 20:32:53 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 28910 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.8, ups=0.87, wpb=111.9, bsz=40, num_updates=2620, lr=2.26565e-06, gnorm=1.078, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10529
2022-10-11 20:33:05 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 28910 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.9, ups=0.88, wpb=109.6, bsz=40, num_updates=2630, lr=2.2743e-06, gnorm=1.12, clip=70, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=10541
2022-10-11 20:33:16 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 28910 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.4, ups=0.88, wpb=111.3, bsz=40, num_updates=2640, lr=2.28295e-06, gnorm=1.176, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10552
2022-10-11 20:33:27 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 28910 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=2650, lr=2.29159e-06, gnorm=1.184, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10563
2022-10-11 20:33:38 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 28910 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.4, ups=0.9, wpb=109.8, bsz=40, num_updates=2660, lr=2.30024e-06, gnorm=1.189, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10574
2022-10-11 20:33:49 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 28910 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99, ups=0.92, wpb=107.3, bsz=40, num_updates=2670, lr=2.30889e-06, gnorm=1.271, clip=80, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10585
2022-10-11 20:34:01 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 28910 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=2680, lr=2.31754e-06, gnorm=1.101, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10597
2022-10-11 20:34:11 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-11 20:34:13 - progress_bar.py[line:274] - INFO: epoch 001:   2693 / 28910 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=88, ups=0.8, wpb=110.6, bsz=40, num_updates=2690, lr=2.32618e-06, gnorm=1.157, clip=80, loss_scale=256, train_wall=13, gb_free=11.3, ema_decay=0.9999, wall=10609
2022-10-11 20:34:25 - progress_bar.py[line:274] - INFO: epoch 001:   2703 / 28910 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.9, ups=0.88, wpb=111.1, bsz=40, num_updates=2700, lr=2.33483e-06, gnorm=1.075, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10621
2022-10-11 20:34:36 - progress_bar.py[line:274] - INFO: epoch 001:   2713 / 28910 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.6, ups=0.9, wpb=110.1, bsz=40, num_updates=2710, lr=2.34348e-06, gnorm=1.063, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10632
2022-10-11 20:34:47 - progress_bar.py[line:274] - INFO: epoch 001:   2723 / 28910 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.3, ups=0.92, wpb=110.5, bsz=40, num_updates=2720, lr=2.35213e-06, gnorm=1.12, clip=70, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10643
2022-10-11 20:34:58 - progress_bar.py[line:274] - INFO: epoch 001:   2733 / 28910 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=94.5, ups=0.87, wpb=108.2, bsz=40, num_updates=2730, lr=2.36077e-06, gnorm=1.069, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10654
2022-10-11 20:35:10 - progress_bar.py[line:274] - INFO: epoch 001:   2743 / 28910 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=2740, lr=2.36942e-06, gnorm=1.119, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10666
2022-10-11 20:35:20 - progress_bar.py[line:274] - INFO: epoch 001:   2753 / 28910 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=2750, lr=2.37807e-06, gnorm=1.171, clip=80, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10677
2022-10-11 20:35:32 - progress_bar.py[line:274] - INFO: epoch 001:   2763 / 28910 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.6, ups=0.89, wpb=109.6, bsz=40, num_updates=2760, lr=2.38672e-06, gnorm=1.152, clip=90, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10688
2022-10-11 20:35:43 - progress_bar.py[line:274] - INFO: epoch 001:   2773 / 28910 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=2770, lr=2.39536e-06, gnorm=1.075, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10699
2022-10-11 20:35:54 - progress_bar.py[line:274] - INFO: epoch 001:   2783 / 28910 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=2780, lr=2.40401e-06, gnorm=1.24, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10710
2022-10-11 20:36:05 - progress_bar.py[line:274] - INFO: epoch 001:   2793 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.3, ups=0.91, wpb=111.8, bsz=40, num_updates=2790, lr=2.41266e-06, gnorm=1.062, clip=60, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10721
2022-10-11 20:36:16 - progress_bar.py[line:274] - INFO: epoch 001:   2803 / 28910 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=2800, lr=2.42131e-06, gnorm=1.108, clip=70, loss_scale=256, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=10732
2022-10-11 20:36:27 - progress_bar.py[line:274] - INFO: epoch 001:   2813 / 28910 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.7, ups=0.88, wpb=111.7, bsz=40, num_updates=2810, lr=2.42996e-06, gnorm=1.071, clip=50, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10743
2022-10-11 20:36:38 - progress_bar.py[line:274] - INFO: epoch 001:   2823 / 28910 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102, ups=0.92, wpb=111.1, bsz=40, num_updates=2820, lr=2.4386e-06, gnorm=1.223, clip=70, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10754
2022-10-11 20:36:49 - progress_bar.py[line:274] - INFO: epoch 001:   2833 / 28910 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.4, ups=0.93, wpb=110.1, bsz=40, num_updates=2830, lr=2.44725e-06, gnorm=1.129, clip=60, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10765
2022-10-11 20:37:00 - progress_bar.py[line:274] - INFO: epoch 001:   2843 / 28910 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.3, ups=0.92, wpb=111.7, bsz=40, num_updates=2840, lr=2.4559e-06, gnorm=1.046, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10776
2022-10-11 20:37:11 - progress_bar.py[line:274] - INFO: epoch 001:   2853 / 28910 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102, ups=0.92, wpb=110.4, bsz=40, num_updates=2850, lr=2.46455e-06, gnorm=1.168, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10787
2022-10-11 20:37:22 - progress_bar.py[line:274] - INFO: epoch 001:   2863 / 28910 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.5, ups=0.89, wpb=109, bsz=40, num_updates=2860, lr=2.47319e-06, gnorm=1.169, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10798
2022-10-11 20:37:33 - progress_bar.py[line:274] - INFO: epoch 001:   2873 / 28910 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.2, ups=0.9, wpb=109.7, bsz=40, num_updates=2870, lr=2.48184e-06, gnorm=1.05, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10809
2022-10-11 20:37:45 - progress_bar.py[line:274] - INFO: epoch 001:   2883 / 28910 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.2, ups=0.87, wpb=110.6, bsz=40, num_updates=2880, lr=2.49049e-06, gnorm=1.088, clip=80, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10821
2022-10-11 20:37:56 - progress_bar.py[line:274] - INFO: epoch 001:   2893 / 28910 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=2890, lr=2.49914e-06, gnorm=1.083, clip=60, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10832
2022-10-11 20:38:07 - progress_bar.py[line:274] - INFO: epoch 001:   2903 / 28910 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.1, ups=0.9, wpb=111.7, bsz=40, num_updates=2900, lr=2.50778e-06, gnorm=1.113, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10843
2022-10-11 20:38:19 - progress_bar.py[line:274] - INFO: epoch 001:   2913 / 28910 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.8, ups=0.87, wpb=110.9, bsz=40, num_updates=2910, lr=2.51643e-06, gnorm=1.183, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10855
2022-10-11 20:38:30 - progress_bar.py[line:274] - INFO: epoch 001:   2923 / 28910 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.9, ups=0.91, wpb=110.6, bsz=40, num_updates=2920, lr=2.52508e-06, gnorm=1.089, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10866
2022-10-11 20:38:41 - progress_bar.py[line:274] - INFO: epoch 001:   2933 / 28910 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.6, ups=0.92, wpb=110.5, bsz=40, num_updates=2930, lr=2.53373e-06, gnorm=1.096, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10877
2022-10-11 20:38:52 - progress_bar.py[line:274] - INFO: epoch 001:   2943 / 28910 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100, ups=0.91, wpb=110.5, bsz=40, num_updates=2940, lr=2.54237e-06, gnorm=1.071, clip=80, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10888
2022-10-11 20:39:03 - progress_bar.py[line:274] - INFO: epoch 001:   2953 / 28910 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=2950, lr=2.55102e-06, gnorm=1.069, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10899
2022-10-11 20:39:14 - progress_bar.py[line:274] - INFO: epoch 001:   2963 / 28910 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=2960, lr=2.55967e-06, gnorm=1.186, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10910
2022-10-11 20:39:25 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 28910 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.9, ups=0.9, wpb=109.3, bsz=40, num_updates=2970, lr=2.56832e-06, gnorm=1.14, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10921
2022-10-11 20:39:36 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 28910 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=2980, lr=2.57696e-06, gnorm=1.142, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10933
2022-10-11 20:39:48 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 28910 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.3, ups=0.9, wpb=110.9, bsz=40, num_updates=2990, lr=2.58561e-06, gnorm=1.081, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10944
2022-10-11 20:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 28910 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=3000, lr=2.59426e-06, gnorm=1.243, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10955
2022-10-11 20:39:59 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-11 20:40:00 - train.py[line:549] - INFO: 0 / 4988
2022-10-11 20:40:00 - train.py[line:551] - INFO: load:1.16 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-11 20:42:32 - train.py[line:549] - INFO: 200 / 4988
2022-10-11 20:42:32 - train.py[line:551] - INFO: load:1.18 valid_run:151.68 task_valid:148.33 collect_output:2.27
2022-10-11 20:45:00 - train.py[line:549] - INFO: 400 / 4988
2022-10-11 20:45:00 - train.py[line:551] - INFO: load:1.21 valid_run:299.92 task_valid:291.46 collect_output:6.32
2022-10-11 20:47:33 - train.py[line:549] - INFO: 600 / 4988
2022-10-11 20:47:33 - train.py[line:551] - INFO: load:1.24 valid_run:452.74 task_valid:435.10 collect_output:14.42
2022-10-11 20:50:02 - train.py[line:549] - INFO: 800 / 4988
2022-10-11 20:50:02 - train.py[line:551] - INFO: load:1.26 valid_run:601.49 task_valid:580.15 collect_output:17.09
2022-10-11 20:52:34 - train.py[line:549] - INFO: 1000 / 4988
2022-10-11 20:52:34 - train.py[line:551] - INFO: load:1.29 valid_run:753.41 task_valid:727.52 collect_output:20.62
2022-10-11 20:55:05 - train.py[line:549] - INFO: 1200 / 4988
2022-10-11 20:55:05 - train.py[line:551] - INFO: load:1.31 valid_run:904.68 task_valid:873.19 collect_output:25.17
2022-10-11 20:57:38 - train.py[line:549] - INFO: 1400 / 4988
2022-10-11 20:57:38 - train.py[line:551] - INFO: load:1.34 valid_run:1057.19 task_valid:1019.22 collect_output:30.63
2022-10-11 21:00:09 - train.py[line:549] - INFO: 1600 / 4988
2022-10-11 21:00:09 - train.py[line:551] - INFO: load:1.36 valid_run:1207.87 task_valid:1160.51 collect_output:38.97
2022-10-11 21:02:38 - train.py[line:549] - INFO: 1800 / 4988
2022-10-11 21:02:38 - train.py[line:551] - INFO: load:1.39 valid_run:1357.44 task_valid:1305.64 collect_output:42.33
2022-10-11 21:05:07 - train.py[line:549] - INFO: 2000 / 4988
2022-10-11 21:05:07 - train.py[line:551] - INFO: load:1.41 valid_run:1506.11 task_valid:1449.28 collect_output:46.19
2022-10-11 21:07:37 - train.py[line:549] - INFO: 2200 / 4988
2022-10-11 21:07:37 - train.py[line:551] - INFO: load:1.44 valid_run:1655.65 task_valid:1594.29 collect_output:49.59
2022-10-11 21:10:06 - train.py[line:549] - INFO: 2400 / 4988
2022-10-11 21:10:06 - train.py[line:551] - INFO: load:1.46 valid_run:1805.39 task_valid:1739.42 collect_output:53.11
2022-10-11 21:12:36 - train.py[line:549] - INFO: 2600 / 4988
2022-10-11 21:12:36 - train.py[line:551] - INFO: load:1.49 valid_run:1955.18 task_valid:1881.80 collect_output:59.32
2022-10-11 21:15:07 - train.py[line:549] - INFO: 2800 / 4988
2022-10-11 21:15:07 - train.py[line:551] - INFO: load:1.51 valid_run:2105.76 task_valid:2027.65 collect_output:62.96
2022-10-11 21:17:37 - train.py[line:549] - INFO: 3000 / 4988
2022-10-11 21:17:37 - train.py[line:551] - INFO: load:1.54 valid_run:2256.17 task_valid:2174.66 collect_output:65.27
2022-10-11 21:20:08 - train.py[line:549] - INFO: 3200 / 4988
2022-10-11 21:20:08 - train.py[line:551] - INFO: load:1.56 valid_run:2406.55 task_valid:2319.75 collect_output:69.42
2022-10-11 21:22:39 - train.py[line:549] - INFO: 3400 / 4988
2022-10-11 21:22:39 - train.py[line:551] - INFO: load:1.59 valid_run:2557.77 task_valid:2465.42 collect_output:73.93
2022-10-11 21:25:09 - train.py[line:549] - INFO: 3600 / 4988
2022-10-11 21:25:09 - train.py[line:551] - INFO: load:1.61 valid_run:2708.01 task_valid:2612.31 collect_output:76.25
2022-10-11 21:27:37 - train.py[line:549] - INFO: 3800 / 4988
2022-10-11 21:27:37 - train.py[line:551] - INFO: load:1.64 valid_run:2855.85 task_valid:2753.99 collect_output:81.37
2022-10-11 21:30:07 - train.py[line:549] - INFO: 4000 / 4988
2022-10-11 21:30:07 - train.py[line:551] - INFO: load:1.66 valid_run:3005.86 task_valid:2899.31 collect_output:85.02
2022-10-11 21:32:39 - train.py[line:549] - INFO: 4200 / 4988
2022-10-11 21:32:39 - train.py[line:551] - INFO: load:1.69 valid_run:3157.22 task_valid:3043.97 collect_output:90.67
2022-10-11 21:35:08 - train.py[line:549] - INFO: 4400 / 4988
2022-10-11 21:35:08 - train.py[line:551] - INFO: load:1.71 valid_run:3306.17 task_valid:3188.48 collect_output:94.08
2022-10-11 21:37:39 - train.py[line:549] - INFO: 4600 / 4988
2022-10-11 21:37:39 - train.py[line:551] - INFO: load:1.74 valid_run:3457.08 task_valid:3334.86 collect_output:97.56
2022-10-11 21:40:10 - train.py[line:549] - INFO: 4800 / 4988
2022-10-11 21:40:10 - train.py[line:551] - INFO: load:1.76 valid_run:3608.23 task_valid:3481.55 collect_output:100.97

====================================================================================================
SGG eval:     R @ 50: 0.6572;     R @ 100: 0.6946;     R @ 500: 0.7235;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4681;    mR @ 100: 0.5129;    mR @ 500: 0.5484;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6875) (covering:0.5143) (eating:0.8235) (flying in:0.9091) (growing on:0.3750) (hanging from:0.4677) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.7202) (standing on:0.4413) (using:0.5500) (walking in:0.0000) (walking on:0.6216) (watching:0.6250) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6572;     R @ 100: 0.6946;     R @ 500: 0.7235;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4681;    mR @ 100: 0.5129;    mR @ 500: 0.5484;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6875) (covering:0.5143) (eating:0.8235) (flying in:0.9091) (growing on:0.3750) (hanging from:0.4677) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.7202) (standing on:0.4413) (using:0.5500) (walking in:0.0000) (walking on:0.6216) (watching:0.6250) 
--------------------------------------------------------
====================================================================================================

2022-10-11 21:42:41 - train.py[line:487] - INFO: 0.6946498090145149
2022-10-11 21:42:41 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-11 21:42:41 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.341 | loss_v1 0 | loss_v2 0 | nll_loss 0.199 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.69465 | ppl 1.15 | vqa_score 0.5349 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 3000 | best_R@100 0.69848
2022-10-11 21:42:41 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3000 updates
2022-10-11 21:42:41 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_3000.pt
2022-10-11 21:42:47 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_3000.pt
2022-10-11 21:42:49 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 0.6946498090145149) (writing took 8.290404716040939 seconds)
2022-10-11 21:43:00 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 28910 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=0.3, ups=0, wpb=109.9, bsz=40, num_updates=3010, lr=2.60291e-06, gnorm=1.112, clip=80, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14736
2022-10-11 21:43:12 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 28910 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=3020, lr=2.61155e-06, gnorm=1.096, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14748
2022-10-11 21:43:23 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 28910 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=102.1, ups=0.92, wpb=111.5, bsz=40, num_updates=3030, lr=2.6202e-06, gnorm=1.077, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14759
2022-10-11 21:43:34 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 28910 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.8, ups=0.91, wpb=109, bsz=40, num_updates=3040, lr=2.62885e-06, gnorm=1.068, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14770
2022-10-11 21:43:44 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 28910 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.6, ups=0.93, wpb=110.5, bsz=40, num_updates=3050, lr=2.6375e-06, gnorm=1.149, clip=80, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14780
2022-10-11 21:43:56 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=3060, lr=2.64614e-06, gnorm=1.061, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14792
2022-10-11 21:44:06 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 28910 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.5, ups=0.92, wpb=109.5, bsz=40, num_updates=3070, lr=2.65479e-06, gnorm=1.04, clip=80, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14803
2022-10-11 21:44:18 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 28910 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=3080, lr=2.66344e-06, gnorm=1.079, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14814
2022-10-11 21:44:29 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 28910 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.3, ups=0.92, wpb=110.1, bsz=40, num_updates=3090, lr=2.67209e-06, gnorm=1.046, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14825
2022-10-11 21:44:40 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 28910 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.3, ups=0.91, wpb=109.7, bsz=40, num_updates=3100, lr=2.68073e-06, gnorm=1.095, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14836
2022-10-11 21:44:51 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 28910 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.4, ups=0.89, wpb=109, bsz=40, num_updates=3110, lr=2.68938e-06, gnorm=1.065, clip=70, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14847
2022-10-11 21:45:02 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 28910 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=3120, lr=2.69803e-06, gnorm=1.205, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14858
2022-10-11 21:45:13 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=103.3, ups=0.93, wpb=111.2, bsz=40, num_updates=3130, lr=2.70668e-06, gnorm=1.018, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14869
2022-10-11 21:45:25 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 28910 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.8, ups=0.88, wpb=109.8, bsz=40, num_updates=3140, lr=2.71532e-06, gnorm=1.09, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14881
2022-10-11 21:45:35 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 28910 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.5, ups=0.92, wpb=109.9, bsz=40, num_updates=3150, lr=2.72397e-06, gnorm=1.03, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14892
2022-10-11 21:45:47 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 28910 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=94.7, ups=0.86, wpb=109.5, bsz=40, num_updates=3160, lr=2.73262e-06, gnorm=1.21, clip=90, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=14903
2022-10-11 21:45:58 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 28910 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=3170, lr=2.74127e-06, gnorm=1.196, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14914
2022-10-11 21:46:09 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 28910 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.7, ups=0.89, wpb=110.4, bsz=40, num_updates=3180, lr=2.74991e-06, gnorm=1.102, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14926
2022-10-11 21:46:20 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 28910 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.5, ups=0.92, wpb=109.4, bsz=40, num_updates=3190, lr=2.75856e-06, gnorm=1.097, clip=70, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14936
2022-10-11 21:46:31 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 28910 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.7, ups=0.92, wpb=109.6, bsz=40, num_updates=3200, lr=2.76721e-06, gnorm=1.199, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14947
2022-10-11 21:46:43 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 28910 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.7, ups=0.88, wpb=109.4, bsz=40, num_updates=3210, lr=2.77586e-06, gnorm=1.304, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14959
2022-10-11 21:46:54 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 28910 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.9, ups=0.89, wpb=108.4, bsz=40, num_updates=3220, lr=2.7845e-06, gnorm=1.188, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14970
2022-10-11 21:47:05 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 28910 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.3, ups=0.91, wpb=111.7, bsz=40, num_updates=3230, lr=2.79315e-06, gnorm=1.095, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14981
2022-10-11 21:47:16 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 28910 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.2, ups=0.9, wpb=109.3, bsz=40, num_updates=3240, lr=2.8018e-06, gnorm=1.16, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14992
2022-10-11 21:47:27 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.6, ups=0.89, wpb=110.2, bsz=40, num_updates=3250, lr=2.81045e-06, gnorm=1.069, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15003
2022-10-11 21:47:39 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 28910 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.8, ups=0.89, wpb=109.3, bsz=40, num_updates=3260, lr=2.81909e-06, gnorm=1.094, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15015
2022-10-11 21:47:50 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 28910 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.5, ups=0.92, wpb=109.8, bsz=40, num_updates=3270, lr=2.82774e-06, gnorm=1.179, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15026
2022-10-11 21:48:01 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 28910 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=3280, lr=2.83639e-06, gnorm=0.958, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15037
2022-10-11 21:48:11 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 28910 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=104.9, ups=0.96, wpb=109.5, bsz=40, num_updates=3290, lr=2.84504e-06, gnorm=1.148, clip=80, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=15048
2022-10-11 21:48:23 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 28910 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.1, ups=0.91, wpb=110.6, bsz=40, num_updates=3300, lr=2.85368e-06, gnorm=1.165, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15059
2022-10-11 21:48:34 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 28910 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=3310, lr=2.86233e-06, gnorm=1.136, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15070
2022-10-11 21:48:45 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 28910 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.1, ups=0.89, wpb=110.7, bsz=40, num_updates=3320, lr=2.87098e-06, gnorm=1.037, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15081
2022-10-11 21:48:56 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 28910 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.1, ups=0.91, wpb=111.1, bsz=40, num_updates=3330, lr=2.87963e-06, gnorm=1.237, clip=80, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=15092
2022-10-11 21:49:08 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 28910 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.6, ups=0.88, wpb=109.4, bsz=40, num_updates=3340, lr=2.88827e-06, gnorm=1.19, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15104
2022-10-11 21:49:19 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 28910 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=3350, lr=2.89692e-06, gnorm=1.049, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15115
2022-10-11 21:49:30 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 28910 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.1, ups=0.91, wpb=111.5, bsz=40, num_updates=3360, lr=2.90557e-06, gnorm=1.108, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15126
2022-10-11 21:49:41 - progress_bar.py[line:274] - INFO: epoch 001:   3373 / 28910 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.8, ups=0.9, wpb=110.2, bsz=40, num_updates=3370, lr=2.91422e-06, gnorm=1.169, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15137
2022-10-11 21:49:52 - progress_bar.py[line:274] - INFO: epoch 001:   3383 / 28910 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.5, ups=0.91, wpb=108.6, bsz=40, num_updates=3380, lr=2.92286e-06, gnorm=1.269, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15148
2022-10-11 21:50:04 - progress_bar.py[line:274] - INFO: epoch 001:   3393 / 28910 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.88, wpb=111.4, bsz=40, num_updates=3390, lr=2.93151e-06, gnorm=1.01, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15160
2022-10-11 21:50:15 - progress_bar.py[line:274] - INFO: epoch 001:   3403 / 28910 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.6, ups=0.91, wpb=109.6, bsz=40, num_updates=3400, lr=2.94016e-06, gnorm=1.185, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15171
2022-10-11 21:50:26 - progress_bar.py[line:274] - INFO: epoch 001:   3413 / 28910 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=3410, lr=2.94881e-06, gnorm=1.162, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15182
2022-10-11 21:50:37 - progress_bar.py[line:274] - INFO: epoch 001:   3423 / 28910 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=3420, lr=2.95745e-06, gnorm=1.162, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15193
2022-10-11 21:50:48 - progress_bar.py[line:274] - INFO: epoch 001:   3433 / 28910 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=3430, lr=2.9661e-06, gnorm=1.135, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15204
2022-10-11 21:50:59 - progress_bar.py[line:274] - INFO: epoch 001:   3443 / 28910 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.1, ups=0.92, wpb=111, bsz=40, num_updates=3440, lr=2.97475e-06, gnorm=1.19, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15215
2022-10-11 21:51:11 - progress_bar.py[line:274] - INFO: epoch 001:   3453 / 28910 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.4, ups=0.87, wpb=109.5, bsz=40, num_updates=3450, lr=2.9834e-06, gnorm=1.138, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15227
2022-10-11 21:51:21 - progress_bar.py[line:274] - INFO: epoch 001:   3463 / 28910 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=103.1, ups=0.93, wpb=110.6, bsz=40, num_updates=3460, lr=2.99204e-06, gnorm=1.09, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15237
2022-10-11 21:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   3473 / 28910 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.1, ups=0.91, wpb=110.3, bsz=40, num_updates=3470, lr=3.00069e-06, gnorm=1.221, clip=70, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=15248
2022-10-11 21:51:44 - progress_bar.py[line:274] - INFO: epoch 001:   3483 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.3, ups=0.9, wpb=112.3, bsz=40, num_updates=3480, lr=3.00934e-06, gnorm=1.03, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15260
2022-10-11 21:51:55 - progress_bar.py[line:274] - INFO: epoch 001:   3493 / 28910 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.8, ups=0.86, wpb=110.9, bsz=40, num_updates=3490, lr=3.01799e-06, gnorm=1.087, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15271
2022-10-11 21:52:06 - progress_bar.py[line:274] - INFO: epoch 001:   3503 / 28910 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.4, ups=0.93, wpb=109.2, bsz=40, num_updates=3500, lr=3.02663e-06, gnorm=1.155, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=15282
2022-10-11 21:52:17 - progress_bar.py[line:274] - INFO: epoch 001:   3513 / 28910 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.1, ups=0.89, wpb=109.5, bsz=40, num_updates=3510, lr=3.03528e-06, gnorm=1.252, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15293
2022-10-11 21:52:28 - progress_bar.py[line:274] - INFO: epoch 001:   3523 / 28910 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.8, ups=0.91, wpb=108.8, bsz=40, num_updates=3520, lr=3.04393e-06, gnorm=1.195, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15304
2022-10-11 21:52:39 - progress_bar.py[line:274] - INFO: epoch 001:   3533 / 28910 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=3530, lr=3.05258e-06, gnorm=1.145, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15315
2022-10-11 21:52:51 - progress_bar.py[line:274] - INFO: epoch 001:   3543 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.9, ups=0.88, wpb=109.5, bsz=40, num_updates=3540, lr=3.06122e-06, gnorm=1.214, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15327
2022-10-11 21:53:02 - progress_bar.py[line:274] - INFO: epoch 001:   3553 / 28910 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.9, ups=0.91, wpb=110.2, bsz=40, num_updates=3550, lr=3.06987e-06, gnorm=1.209, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15338
2022-10-11 21:53:13 - progress_bar.py[line:274] - INFO: epoch 001:   3563 / 28910 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=3560, lr=3.07852e-06, gnorm=1.107, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15349
2022-10-11 21:53:24 - progress_bar.py[line:274] - INFO: epoch 001:   3573 / 28910 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=3570, lr=3.08717e-06, gnorm=1.215, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15360
2022-10-11 21:53:35 - progress_bar.py[line:274] - INFO: epoch 001:   3583 / 28910 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=3580, lr=3.09581e-06, gnorm=1.086, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15372
2022-10-11 21:53:47 - progress_bar.py[line:274] - INFO: epoch 001:   3593 / 28910 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.2, ups=0.88, wpb=111.2, bsz=40, num_updates=3590, lr=3.10446e-06, gnorm=1.158, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15383
2022-10-11 21:53:58 - progress_bar.py[line:274] - INFO: epoch 001:   3603 / 28910 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=3600, lr=3.11311e-06, gnorm=1.137, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15394
2022-10-11 21:54:10 - progress_bar.py[line:274] - INFO: epoch 001:   3613 / 28910 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.1, ups=0.87, wpb=109.9, bsz=40, num_updates=3610, lr=3.12176e-06, gnorm=1.131, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15406
2022-10-11 21:54:21 - progress_bar.py[line:274] - INFO: epoch 001:   3623 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.6, ups=0.89, wpb=109.1, bsz=40, num_updates=3620, lr=3.1304e-06, gnorm=1.153, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15417
2022-10-11 21:54:32 - progress_bar.py[line:274] - INFO: epoch 001:   3633 / 28910 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97, ups=0.88, wpb=110.5, bsz=40, num_updates=3630, lr=3.13905e-06, gnorm=1.174, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15428
2022-10-11 21:54:44 - progress_bar.py[line:274] - INFO: epoch 001:   3643 / 28910 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.6, ups=0.88, wpb=112.1, bsz=40, num_updates=3640, lr=3.1477e-06, gnorm=1.155, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15440
2022-10-11 21:54:55 - progress_bar.py[line:274] - INFO: epoch 001:   3653 / 28910 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=3650, lr=3.15635e-06, gnorm=1.114, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15451
2022-10-11 21:55:06 - progress_bar.py[line:274] - INFO: epoch 001:   3663 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.1, ups=0.9, wpb=111.9, bsz=40, num_updates=3660, lr=3.16499e-06, gnorm=1.153, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15462
2022-10-11 21:55:17 - progress_bar.py[line:274] - INFO: epoch 001:   3673 / 28910 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=3670, lr=3.17364e-06, gnorm=1.174, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15473
2022-10-11 21:55:29 - progress_bar.py[line:274] - INFO: epoch 001:   3683 / 28910 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=3680, lr=3.18229e-06, gnorm=1.244, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15485
2022-10-11 21:55:40 - progress_bar.py[line:274] - INFO: epoch 001:   3693 / 28910 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.3, ups=0.91, wpb=110.9, bsz=40, num_updates=3690, lr=3.19094e-06, gnorm=1.322, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15496
2022-10-11 21:55:51 - progress_bar.py[line:274] - INFO: epoch 001:   3703 / 28910 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=3700, lr=3.19958e-06, gnorm=1.084, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15507
2022-10-11 21:56:02 - progress_bar.py[line:274] - INFO: epoch 001:   3713 / 28910 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.7, ups=0.88, wpb=110.5, bsz=40, num_updates=3710, lr=3.20823e-06, gnorm=1.169, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15518
2022-10-11 21:56:13 - progress_bar.py[line:274] - INFO: epoch 001:   3723 / 28910 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=102.2, ups=0.92, wpb=111.4, bsz=40, num_updates=3720, lr=3.21688e-06, gnorm=1.125, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15529
2022-10-11 21:56:24 - progress_bar.py[line:274] - INFO: epoch 001:   3733 / 28910 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.7, ups=0.87, wpb=109.9, bsz=40, num_updates=3730, lr=3.22553e-06, gnorm=1.112, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15541
2022-10-11 21:56:35 - progress_bar.py[line:274] - INFO: epoch 001:   3743 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.7, ups=0.92, wpb=111, bsz=40, num_updates=3740, lr=3.23418e-06, gnorm=1.112, clip=70, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15552
2022-10-11 21:56:47 - progress_bar.py[line:274] - INFO: epoch 001:   3753 / 28910 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.6, ups=0.88, wpb=111.1, bsz=40, num_updates=3750, lr=3.24282e-06, gnorm=1.166, clip=60, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15563
2022-10-11 21:56:58 - progress_bar.py[line:274] - INFO: epoch 001:   3763 / 28910 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.9, ups=0.88, wpb=109.7, bsz=40, num_updates=3760, lr=3.25147e-06, gnorm=1.171, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15574
2022-10-11 21:57:03 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-11 21:57:10 - progress_bar.py[line:274] - INFO: epoch 001:   3774 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=91, ups=0.82, wpb=111, bsz=40, num_updates=3770, lr=3.26012e-06, gnorm=1.167, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15586
2022-10-11 21:57:22 - progress_bar.py[line:274] - INFO: epoch 001:   3784 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=3780, lr=3.26877e-06, gnorm=1.146, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15598
2022-10-11 21:57:33 - progress_bar.py[line:274] - INFO: epoch 001:   3794 / 28910 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97, ups=0.88, wpb=109.8, bsz=40, num_updates=3790, lr=3.27741e-06, gnorm=1.091, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15609
2022-10-11 21:57:44 - progress_bar.py[line:274] - INFO: epoch 001:   3804 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=102.2, ups=0.92, wpb=111.2, bsz=40, num_updates=3800, lr=3.28606e-06, gnorm=1.032, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15620
2022-10-11 21:57:55 - progress_bar.py[line:274] - INFO: epoch 001:   3814 / 28910 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.1, ups=0.87, wpb=109, bsz=40, num_updates=3810, lr=3.29471e-06, gnorm=1.219, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15632
2022-10-11 21:58:06 - progress_bar.py[line:274] - INFO: epoch 001:   3824 / 28910 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.4, ups=0.93, wpb=110.8, bsz=40, num_updates=3820, lr=3.30336e-06, gnorm=1.318, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15642
2022-10-11 21:58:17 - progress_bar.py[line:274] - INFO: epoch 001:   3834 / 28910 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=3830, lr=3.312e-06, gnorm=1.052, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15653
2022-10-11 21:58:29 - progress_bar.py[line:274] - INFO: epoch 001:   3844 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.1, ups=0.86, wpb=110.2, bsz=40, num_updates=3840, lr=3.32065e-06, gnorm=1.151, clip=90, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15665
2022-10-11 21:58:40 - progress_bar.py[line:274] - INFO: epoch 001:   3854 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=3850, lr=3.3293e-06, gnorm=1.16, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15676
2022-10-11 21:58:52 - progress_bar.py[line:274] - INFO: epoch 001:   3864 / 28910 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.5, ups=0.88, wpb=109.3, bsz=40, num_updates=3860, lr=3.33795e-06, gnorm=1.159, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15688
2022-10-11 21:59:03 - progress_bar.py[line:274] - INFO: epoch 001:   3874 / 28910 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.4, ups=0.88, wpb=110.2, bsz=40, num_updates=3870, lr=3.34659e-06, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=15699
2022-10-11 21:59:14 - progress_bar.py[line:274] - INFO: epoch 001:   3884 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.6, ups=0.91, wpb=109.8, bsz=40, num_updates=3880, lr=3.35524e-06, gnorm=1.148, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15710
2022-10-11 21:59:25 - progress_bar.py[line:274] - INFO: epoch 001:   3894 / 28910 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=3890, lr=3.36389e-06, gnorm=1.086, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15721
2022-10-11 21:59:36 - progress_bar.py[line:274] - INFO: epoch 001:   3904 / 28910 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.3, ups=0.91, wpb=110.1, bsz=40, num_updates=3900, lr=3.37254e-06, gnorm=1.183, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15732
2022-10-11 21:59:47 - progress_bar.py[line:274] - INFO: epoch 001:   3914 / 28910 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.4, ups=0.91, wpb=109.5, bsz=40, num_updates=3910, lr=3.38118e-06, gnorm=1.153, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15743
2022-10-11 21:59:58 - progress_bar.py[line:274] - INFO: epoch 001:   3924 / 28910 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=103.4, ups=0.93, wpb=110.9, bsz=40, num_updates=3920, lr=3.38983e-06, gnorm=1.109, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15754
2022-10-11 22:00:09 - progress_bar.py[line:274] - INFO: epoch 001:   3934 / 28910 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.5, ups=0.89, wpb=110, bsz=40, num_updates=3930, lr=3.39848e-06, gnorm=1.188, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15765
2022-10-11 22:00:20 - progress_bar.py[line:274] - INFO: epoch 001:   3944 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.7, ups=0.91, wpb=112.1, bsz=40, num_updates=3940, lr=3.40713e-06, gnorm=1.1, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15776
2022-10-11 22:00:32 - progress_bar.py[line:274] - INFO: epoch 001:   3954 / 28910 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.8, ups=0.92, wpb=110.9, bsz=40, num_updates=3950, lr=3.41577e-06, gnorm=1.176, clip=80, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15787
2022-10-11 22:00:42 - progress_bar.py[line:274] - INFO: epoch 001:   3964 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=102.3, ups=0.94, wpb=109.2, bsz=40, num_updates=3960, lr=3.42442e-06, gnorm=1.196, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15798
2022-10-11 22:00:53 - progress_bar.py[line:274] - INFO: epoch 001:   3974 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=3970, lr=3.43307e-06, gnorm=1.273, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15810
2022-10-11 22:01:05 - progress_bar.py[line:274] - INFO: epoch 001:   3984 / 28910 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.1, ups=0.88, wpb=108.7, bsz=40, num_updates=3980, lr=3.44172e-06, gnorm=1.169, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15821
2022-10-11 22:01:16 - progress_bar.py[line:274] - INFO: epoch 001:   3994 / 28910 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.88, wpb=110.5, bsz=40, num_updates=3990, lr=3.45036e-06, gnorm=1.124, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15832
2022-10-11 22:01:27 - progress_bar.py[line:274] - INFO: epoch 001:   4004 / 28910 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.2, ups=0.93, wpb=109.8, bsz=40, num_updates=4000, lr=3.45901e-06, gnorm=1.05, clip=70, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=15843
2022-10-11 22:01:27 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-11 22:01:28 - train.py[line:549] - INFO: 0 / 4988
2022-10-11 22:01:28 - train.py[line:551] - INFO: load:1.24 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-11 22:04:00 - train.py[line:549] - INFO: 200 / 4988
2022-10-11 22:04:00 - train.py[line:551] - INFO: load:1.27 valid_run:151.13 task_valid:148.10 collect_output:1.98
2022-10-11 22:06:28 - train.py[line:549] - INFO: 400 / 4988
2022-10-11 22:06:28 - train.py[line:551] - INFO: load:1.29 valid_run:299.18 task_valid:291.31 collect_output:5.81
2022-10-11 22:09:00 - train.py[line:549] - INFO: 600 / 4988
2022-10-11 22:09:00 - train.py[line:551] - INFO: load:1.32 valid_run:451.17 task_valid:434.48 collect_output:13.52
2022-10-11 22:11:29 - train.py[line:549] - INFO: 800 / 4988
2022-10-11 22:11:29 - train.py[line:551] - INFO: load:1.34 valid_run:600.40 task_valid:579.87 collect_output:16.20
2022-10-11 22:14:01 - train.py[line:549] - INFO: 1000 / 4988
2022-10-11 22:14:01 - train.py[line:551] - INFO: load:1.37 valid_run:752.70 task_valid:727.66 collect_output:19.61
2022-10-11 22:16:33 - train.py[line:549] - INFO: 1200 / 4988
2022-10-11 22:16:33 - train.py[line:551] - INFO: load:1.40 valid_run:904.33 task_valid:873.40 collect_output:24.41
2022-10-11 22:19:06 - train.py[line:549] - INFO: 1400 / 4988
2022-10-11 22:19:06 - train.py[line:551] - INFO: load:1.43 valid_run:1057.34 task_valid:1019.77 collect_output:29.91
2022-10-11 22:21:37 - train.py[line:549] - INFO: 1600 / 4988
2022-10-11 22:21:37 - train.py[line:551] - INFO: load:1.46 valid_run:1208.35 task_valid:1161.33 collect_output:38.24
2022-10-11 22:24:07 - train.py[line:549] - INFO: 1800 / 4988
2022-10-11 22:24:07 - train.py[line:551] - INFO: load:1.48 valid_run:1357.90 task_valid:1306.33 collect_output:41.70
2022-10-11 22:26:35 - train.py[line:549] - INFO: 2000 / 4988
2022-10-11 22:26:35 - train.py[line:551] - INFO: load:1.51 valid_run:1506.46 task_valid:1449.60 collect_output:45.93
2022-10-11 22:29:05 - train.py[line:549] - INFO: 2200 / 4988
2022-10-11 22:29:05 - train.py[line:551] - INFO: load:1.53 valid_run:1655.79 task_valid:1594.58 collect_output:49.23
2022-10-11 22:31:35 - train.py[line:549] - INFO: 2400 / 4988
2022-10-11 22:31:35 - train.py[line:551] - INFO: load:1.56 valid_run:1805.50 task_valid:1739.74 collect_output:52.72
2022-10-11 22:34:04 - train.py[line:549] - INFO: 2600 / 4988
2022-10-11 22:34:04 - train.py[line:551] - INFO: load:1.58 valid_run:1955.14 task_valid:1881.46 collect_output:59.59
2022-10-11 22:36:35 - train.py[line:549] - INFO: 2800 / 4988
2022-10-11 22:36:35 - train.py[line:551] - INFO: load:1.61 valid_run:2105.28 task_valid:2026.88 collect_output:63.30
2022-10-11 22:39:04 - train.py[line:549] - INFO: 3000 / 4988
2022-10-11 22:39:04 - train.py[line:551] - INFO: load:1.63 valid_run:2255.12 task_valid:2173.36 collect_output:65.61
2022-10-11 22:41:34 - train.py[line:549] - INFO: 3200 / 4988
2022-10-11 22:41:34 - train.py[line:551] - INFO: load:1.66 valid_run:2404.90 task_valid:2317.76 collect_output:69.93
2022-10-11 22:44:06 - train.py[line:549] - INFO: 3400 / 4988
2022-10-11 22:44:06 - train.py[line:551] - INFO: load:1.68 valid_run:2556.14 task_valid:2463.35 collect_output:74.52
2022-10-11 22:46:36 - train.py[line:549] - INFO: 3600 / 4988
2022-10-11 22:46:36 - train.py[line:551] - INFO: load:1.71 valid_run:2706.38 task_valid:2610.26 collect_output:76.81
2022-10-11 22:49:04 - train.py[line:549] - INFO: 3800 / 4988
2022-10-11 22:49:04 - train.py[line:551] - INFO: load:1.73 valid_run:2854.22 task_valid:2752.06 collect_output:81.82
2022-10-11 22:51:34 - train.py[line:549] - INFO: 4000 / 4988
2022-10-11 22:51:34 - train.py[line:551] - INFO: load:1.76 valid_run:3004.19 task_valid:2897.21 collect_output:85.60
2022-10-11 22:54:05 - train.py[line:549] - INFO: 4200 / 4988
2022-10-11 22:54:05 - train.py[line:551] - INFO: load:1.78 valid_run:3155.40 task_valid:3041.71 collect_output:91.28
2022-10-11 22:56:34 - train.py[line:549] - INFO: 4400 / 4988
2022-10-11 22:56:34 - train.py[line:551] - INFO: load:1.81 valid_run:3304.54 task_valid:3186.31 collect_output:94.77
2022-10-11 22:59:06 - train.py[line:549] - INFO: 4600 / 4988
2022-10-11 22:59:06 - train.py[line:551] - INFO: load:1.83 valid_run:3455.81 task_valid:3332.86 collect_output:98.43
2022-10-11 23:01:36 - train.py[line:549] - INFO: 4800 / 4988
2022-10-11 23:01:36 - train.py[line:551] - INFO: load:1.86 valid_run:3606.64 task_valid:3479.28 collect_output:101.82

====================================================================================================
SGG eval:     R @ 50: 0.6677;     R @ 100: 0.6958;     R @ 500: 0.7207;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4721;    mR @ 100: 0.5103;    mR @ 500: 0.5517;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6875) (covering:0.3714) (eating:0.8235) (flying in:0.7727) (growing on:0.5000) (hanging from:0.5000) (lying on:0.4667) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9583) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.7245) (standing on:0.4443) (using:0.5500) (walking in:0.0000) (walking on:0.6216) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2022-10-11 23:04:07 - train.py[line:487] - INFO: 0.6958043544690603

====================================================================================================
SGG eval:     R @ 50: 0.6677;     R @ 100: 0.6958;     R @ 500: 0.7207;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4721;    mR @ 100: 0.5103;    mR @ 500: 0.5517;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6875) (covering:0.3714) (eating:0.8235) (flying in:0.7727) (growing on:0.5000) (hanging from:0.5000) (lying on:0.4667) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9583) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.7245) (standing on:0.4443) (using:0.5500) (walking in:0.0000) (walking on:0.6216) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2022-10-11 23:04:07 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-11 23:04:07 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.34 | loss_v1 0 | loss_v2 0 | nll_loss 0.187 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.695804 | ppl 1.14 | vqa_score 0.5507 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 4000 | best_R@100 0.69848
2022-10-11 23:04:07 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2022-10-11 23:04:07 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_4000.pt
2022-10-11 23:04:13 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_4000.pt
2022-10-11 23:04:16 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.6958043544690603) (writing took 8.187721136026084 seconds)
2022-10-11 23:04:27 - progress_bar.py[line:274] - INFO: epoch 001:   4014 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=0.3, ups=0, wpb=109, bsz=40, num_updates=4010, lr=3.46766e-06, gnorm=1.161, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19623
2022-10-11 23:04:38 - progress_bar.py[line:274] - INFO: epoch 001:   4024 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.92, wpb=110.7, bsz=40, num_updates=4020, lr=3.47631e-06, gnorm=1.121, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19634
2022-10-11 23:04:49 - progress_bar.py[line:274] - INFO: epoch 001:   4034 / 28910 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.2, ups=0.91, wpb=111.4, bsz=40, num_updates=4030, lr=3.48495e-06, gnorm=1.182, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19645
2022-10-11 23:05:01 - progress_bar.py[line:274] - INFO: epoch 001:   4044 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.5, ups=0.88, wpb=111, bsz=40, num_updates=4040, lr=3.4936e-06, gnorm=1.007, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19657
2022-10-11 23:05:11 - progress_bar.py[line:274] - INFO: epoch 001:   4054 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.2, ups=0.93, wpb=111.2, bsz=40, num_updates=4050, lr=3.50225e-06, gnorm=1.054, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19667
2022-10-11 23:05:22 - progress_bar.py[line:274] - INFO: epoch 001:   4064 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.8, ups=0.92, wpb=109.8, bsz=40, num_updates=4060, lr=3.5109e-06, gnorm=1.169, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19678
2022-10-11 23:05:33 - progress_bar.py[line:274] - INFO: epoch 001:   4074 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.3, ups=0.92, wpb=110.4, bsz=40, num_updates=4070, lr=3.51954e-06, gnorm=1.239, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19689
2022-10-11 23:05:44 - progress_bar.py[line:274] - INFO: epoch 001:   4084 / 28910 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.9, ups=0.92, wpb=110.2, bsz=40, num_updates=4080, lr=3.52819e-06, gnorm=1.173, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19700
2022-10-11 23:05:55 - progress_bar.py[line:274] - INFO: epoch 001:   4094 / 28910 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.1, ups=0.88, wpb=109.9, bsz=40, num_updates=4090, lr=3.53684e-06, gnorm=1.135, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19712
2022-10-11 23:06:07 - progress_bar.py[line:274] - INFO: epoch 001:   4104 / 28910 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=4100, lr=3.54549e-06, gnorm=1.215, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19723
2022-10-11 23:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   4114 / 28910 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.2, ups=0.92, wpb=110, bsz=40, num_updates=4110, lr=3.55413e-06, gnorm=1.165, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19734
2022-10-11 23:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   4124 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.1, ups=0.89, wpb=109.5, bsz=40, num_updates=4120, lr=3.56278e-06, gnorm=1.23, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19745
2022-10-11 23:06:40 - progress_bar.py[line:274] - INFO: epoch 001:   4134 / 28910 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.9, wpb=110.9, bsz=40, num_updates=4130, lr=3.57143e-06, gnorm=1.073, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19756
2022-10-11 23:06:51 - progress_bar.py[line:274] - INFO: epoch 001:   4144 / 28910 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.9, ups=0.92, wpb=110, bsz=40, num_updates=4140, lr=3.58008e-06, gnorm=1.196, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19767
2022-10-11 23:07:02 - progress_bar.py[line:274] - INFO: epoch 001:   4154 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=103, ups=0.93, wpb=110.7, bsz=40, num_updates=4150, lr=3.58872e-06, gnorm=1.166, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19778
2022-10-11 23:07:13 - progress_bar.py[line:274] - INFO: epoch 001:   4164 / 28910 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.7, ups=0.91, wpb=109.9, bsz=40, num_updates=4160, lr=3.59737e-06, gnorm=1.236, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19789
2022-10-11 23:07:24 - progress_bar.py[line:274] - INFO: epoch 001:   4174 / 28910 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.2, ups=0.91, wpb=109.2, bsz=40, num_updates=4170, lr=3.60602e-06, gnorm=1.245, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19800
2022-10-11 23:07:35 - progress_bar.py[line:274] - INFO: epoch 001:   4184 / 28910 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=4180, lr=3.61467e-06, gnorm=1.226, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19811
2022-10-11 23:07:47 - progress_bar.py[line:274] - INFO: epoch 001:   4194 / 28910 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.9, ups=0.89, wpb=111.5, bsz=40, num_updates=4190, lr=3.62331e-06, gnorm=1.065, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19823
2022-10-11 23:07:58 - progress_bar.py[line:274] - INFO: epoch 001:   4204 / 28910 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=4200, lr=3.63196e-06, gnorm=1.115, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19834
2022-10-11 23:08:09 - progress_bar.py[line:274] - INFO: epoch 001:   4214 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.8, ups=0.91, wpb=109.8, bsz=40, num_updates=4210, lr=3.64061e-06, gnorm=1.199, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19845
2022-10-11 23:08:20 - progress_bar.py[line:274] - INFO: epoch 001:   4224 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=4220, lr=3.64926e-06, gnorm=1.183, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19856
2022-10-11 23:08:31 - progress_bar.py[line:274] - INFO: epoch 001:   4234 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97, ups=0.88, wpb=110.2, bsz=40, num_updates=4230, lr=3.6579e-06, gnorm=1.213, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19867
2022-10-11 23:08:42 - progress_bar.py[line:274] - INFO: epoch 001:   4244 / 28910 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=4240, lr=3.66655e-06, gnorm=1.212, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19878
2022-10-11 23:08:53 - progress_bar.py[line:274] - INFO: epoch 001:   4254 / 28910 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.4, ups=0.92, wpb=110.5, bsz=40, num_updates=4250, lr=3.6752e-06, gnorm=1.208, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19889
2022-10-11 23:09:04 - progress_bar.py[line:274] - INFO: epoch 001:   4264 / 28910 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=4260, lr=3.68385e-06, gnorm=1.082, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19900
2022-10-11 23:09:16 - progress_bar.py[line:274] - INFO: epoch 001:   4274 / 28910 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.1, ups=0.9, wpb=110.3, bsz=40, num_updates=4270, lr=3.69249e-06, gnorm=1.209, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19912
2022-10-11 23:09:27 - progress_bar.py[line:274] - INFO: epoch 001:   4284 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.3, ups=0.91, wpb=108.5, bsz=40, num_updates=4280, lr=3.70114e-06, gnorm=1.262, clip=80, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19923
2022-10-11 23:09:38 - progress_bar.py[line:274] - INFO: epoch 001:   4294 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.8, ups=0.94, wpb=110.1, bsz=40, num_updates=4290, lr=3.70979e-06, gnorm=1.184, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19933
2022-10-11 23:09:49 - progress_bar.py[line:274] - INFO: epoch 001:   4304 / 28910 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=4300, lr=3.71844e-06, gnorm=1.229, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19945
2022-10-11 23:10:01 - progress_bar.py[line:274] - INFO: epoch 001:   4314 / 28910 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=4310, lr=3.72708e-06, gnorm=1.253, clip=100, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19957
2022-10-11 23:10:12 - progress_bar.py[line:274] - INFO: epoch 001:   4324 / 28910 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.1, ups=0.89, wpb=110.6, bsz=40, num_updates=4320, lr=3.73573e-06, gnorm=1.212, clip=90, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19968
2022-10-11 23:10:23 - progress_bar.py[line:274] - INFO: epoch 001:   4334 / 28910 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.3, ups=0.88, wpb=109.1, bsz=40, num_updates=4330, lr=3.74438e-06, gnorm=1.263, clip=90, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19979
2022-10-11 23:10:30 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-11 23:10:35 - progress_bar.py[line:274] - INFO: epoch 001:   4345 / 28910 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=90.6, ups=0.82, wpb=110.5, bsz=40, num_updates=4340, lr=3.75303e-06, gnorm=1.251, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19992
2022-10-11 23:10:47 - progress_bar.py[line:274] - INFO: epoch 001:   4355 / 28910 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101, ups=0.91, wpb=111.5, bsz=40, num_updates=4350, lr=3.76167e-06, gnorm=1.265, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20003
2022-10-11 23:10:58 - progress_bar.py[line:274] - INFO: epoch 001:   4365 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.7, ups=0.91, wpb=110.6, bsz=40, num_updates=4360, lr=3.77032e-06, gnorm=1.209, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20014
2022-10-11 23:11:09 - progress_bar.py[line:274] - INFO: epoch 001:   4375 / 28910 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=4370, lr=3.77897e-06, gnorm=1.118, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20025
2022-10-11 23:11:20 - progress_bar.py[line:274] - INFO: epoch 001:   4385 / 28910 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=4380, lr=3.78762e-06, gnorm=1.32, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20036
2022-10-11 23:11:31 - progress_bar.py[line:274] - INFO: epoch 001:   4395 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.89, wpb=110.9, bsz=40, num_updates=4390, lr=3.79626e-06, gnorm=1.065, clip=70, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=20047
2022-10-11 23:11:42 - progress_bar.py[line:274] - INFO: epoch 001:   4405 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=107.1, ups=0.97, wpb=110.9, bsz=40, num_updates=4400, lr=3.80491e-06, gnorm=1.209, clip=90, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=20058
2022-10-11 23:11:53 - progress_bar.py[line:274] - INFO: epoch 001:   4415 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.5, ups=0.9, wpb=110.6, bsz=40, num_updates=4410, lr=3.81356e-06, gnorm=1.142, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20069
2022-10-11 23:12:04 - progress_bar.py[line:274] - INFO: epoch 001:   4425 / 28910 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.6, ups=0.87, wpb=112, bsz=40, num_updates=4420, lr=3.82221e-06, gnorm=1.299, clip=90, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=20080
2022-10-11 23:12:16 - progress_bar.py[line:274] - INFO: epoch 001:   4435 / 28910 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.7, ups=0.87, wpb=110.6, bsz=40, num_updates=4430, lr=3.83085e-06, gnorm=1.208, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20092
2022-10-11 23:12:27 - progress_bar.py[line:274] - INFO: epoch 001:   4445 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=4440, lr=3.8395e-06, gnorm=1.232, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20103
2022-10-11 23:12:38 - progress_bar.py[line:274] - INFO: epoch 001:   4455 / 28910 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=4450, lr=3.84815e-06, gnorm=1.156, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20114
2022-10-11 23:12:49 - progress_bar.py[line:274] - INFO: epoch 001:   4465 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=4460, lr=3.8568e-06, gnorm=1.163, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20125
2022-10-11 23:13:00 - progress_bar.py[line:274] - INFO: epoch 001:   4475 / 28910 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.7, ups=0.93, wpb=109.6, bsz=40, num_updates=4470, lr=3.86544e-06, gnorm=1.251, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20136
2022-10-11 23:13:11 - progress_bar.py[line:274] - INFO: epoch 001:   4485 / 28910 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=4480, lr=3.87409e-06, gnorm=1.235, clip=100, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=20147
2022-10-11 23:13:22 - progress_bar.py[line:274] - INFO: epoch 001:   4495 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=4490, lr=3.88274e-06, gnorm=1.179, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20158
2022-10-11 23:13:34 - progress_bar.py[line:274] - INFO: epoch 001:   4505 / 28910 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=4500, lr=3.89139e-06, gnorm=1.289, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20170
2022-10-11 23:13:45 - progress_bar.py[line:274] - INFO: epoch 001:   4515 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=4510, lr=3.90003e-06, gnorm=1.108, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20181
2022-10-11 23:13:56 - progress_bar.py[line:274] - INFO: epoch 001:   4525 / 28910 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.6, ups=0.88, wpb=109.5, bsz=40, num_updates=4520, lr=3.90868e-06, gnorm=1.156, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20192
2022-10-11 23:14:07 - progress_bar.py[line:274] - INFO: epoch 001:   4535 / 28910 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.4, ups=0.91, wpb=108.4, bsz=40, num_updates=4530, lr=3.91733e-06, gnorm=1.253, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20203
2022-10-11 23:14:19 - progress_bar.py[line:274] - INFO: epoch 001:   4545 / 28910 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.8, ups=0.87, wpb=109.9, bsz=40, num_updates=4540, lr=3.92598e-06, gnorm=1.127, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20215
2022-10-11 23:14:30 - progress_bar.py[line:274] - INFO: epoch 001:   4555 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.5, ups=0.89, wpb=110.2, bsz=40, num_updates=4550, lr=3.93462e-06, gnorm=1.21, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20226
2022-10-11 23:14:41 - progress_bar.py[line:274] - INFO: epoch 001:   4565 / 28910 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=4560, lr=3.94327e-06, gnorm=1.178, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20237
2022-10-11 23:14:53 - progress_bar.py[line:274] - INFO: epoch 001:   4575 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.89, wpb=110.1, bsz=40, num_updates=4570, lr=3.95192e-06, gnorm=1.232, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20249
2022-10-11 23:15:04 - progress_bar.py[line:274] - INFO: epoch 001:   4585 / 28910 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=4580, lr=3.96057e-06, gnorm=1.202, clip=90, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=20260
2022-10-11 23:15:15 - progress_bar.py[line:274] - INFO: epoch 001:   4595 / 28910 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=4590, lr=3.96921e-06, gnorm=1.35, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20271
2022-10-11 23:15:26 - progress_bar.py[line:274] - INFO: epoch 001:   4605 / 28910 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.91, wpb=108.6, bsz=40, num_updates=4600, lr=3.97786e-06, gnorm=1.343, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20282
2022-10-11 23:15:37 - progress_bar.py[line:274] - INFO: epoch 001:   4615 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=4610, lr=3.98651e-06, gnorm=1.239, clip=100, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=20293
2022-10-11 23:15:49 - progress_bar.py[line:274] - INFO: epoch 001:   4625 / 28910 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.4, ups=0.9, wpb=108.6, bsz=40, num_updates=4620, lr=3.99516e-06, gnorm=1.386, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20305
2022-10-11 23:16:00 - progress_bar.py[line:274] - INFO: epoch 001:   4635 / 28910 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.91, wpb=109.3, bsz=40, num_updates=4630, lr=4.0038e-06, gnorm=1.242, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20316
2022-10-11 23:16:10 - progress_bar.py[line:274] - INFO: epoch 001:   4645 / 28910 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.2, ups=0.92, wpb=109.4, bsz=40, num_updates=4640, lr=4.01245e-06, gnorm=1.171, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20327
2022-10-11 23:16:22 - progress_bar.py[line:274] - INFO: epoch 001:   4655 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.2, ups=0.89, wpb=109.8, bsz=40, num_updates=4650, lr=4.0211e-06, gnorm=1.24, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20338
2022-10-11 23:16:33 - progress_bar.py[line:274] - INFO: epoch 001:   4665 / 28910 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.9, ups=0.87, wpb=112.1, bsz=40, num_updates=4660, lr=4.02975e-06, gnorm=1.174, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20349
2022-10-11 23:16:45 - progress_bar.py[line:274] - INFO: epoch 001:   4675 / 28910 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=4670, lr=4.0384e-06, gnorm=1.202, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20361
2022-10-11 23:16:55 - progress_bar.py[line:274] - INFO: epoch 001:   4685 / 28910 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=4680, lr=4.04704e-06, gnorm=1.116, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20372
2022-10-11 23:17:07 - progress_bar.py[line:274] - INFO: epoch 001:   4695 / 28910 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=4690, lr=4.05569e-06, gnorm=1.192, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20383
2022-10-11 23:17:18 - progress_bar.py[line:274] - INFO: epoch 001:   4705 / 28910 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.5, ups=0.9, wpb=110.6, bsz=40, num_updates=4700, lr=4.06434e-06, gnorm=1.214, clip=100, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=20394
2022-10-11 23:17:29 - progress_bar.py[line:274] - INFO: epoch 001:   4715 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.8, ups=0.88, wpb=111, bsz=40, num_updates=4710, lr=4.07299e-06, gnorm=1.075, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20405
2022-10-11 23:17:41 - progress_bar.py[line:274] - INFO: epoch 001:   4725 / 28910 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=4720, lr=4.08163e-06, gnorm=1.195, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20417
2022-10-11 23:17:52 - progress_bar.py[line:274] - INFO: epoch 001:   4735 / 28910 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.1, ups=0.9, wpb=110, bsz=40, num_updates=4730, lr=4.09028e-06, gnorm=1.243, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20428
2022-10-11 23:18:03 - progress_bar.py[line:274] - INFO: epoch 001:   4745 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.3, ups=0.88, wpb=111.1, bsz=40, num_updates=4740, lr=4.09893e-06, gnorm=1.182, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20439
2022-10-11 23:18:14 - progress_bar.py[line:274] - INFO: epoch 001:   4755 / 28910 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=4750, lr=4.10758e-06, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20450
2022-10-11 23:18:25 - progress_bar.py[line:274] - INFO: epoch 001:   4765 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=4760, lr=4.11622e-06, gnorm=1.232, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20461
2022-10-11 23:18:36 - progress_bar.py[line:274] - INFO: epoch 001:   4775 / 28910 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=4770, lr=4.12487e-06, gnorm=1.273, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20472
2022-10-11 23:18:48 - progress_bar.py[line:274] - INFO: epoch 001:   4785 / 28910 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=4780, lr=4.13352e-06, gnorm=1.324, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20484
2022-10-11 23:18:59 - progress_bar.py[line:274] - INFO: epoch 001:   4795 / 28910 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=4790, lr=4.14217e-06, gnorm=1.093, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20495
2022-10-11 23:19:10 - progress_bar.py[line:274] - INFO: epoch 001:   4805 / 28910 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=4800, lr=4.15081e-06, gnorm=1.101, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20506
2022-10-11 23:19:21 - progress_bar.py[line:274] - INFO: epoch 001:   4815 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.91, wpb=109.9, bsz=40, num_updates=4810, lr=4.15946e-06, gnorm=1.199, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20517
2022-10-11 23:19:32 - progress_bar.py[line:274] - INFO: epoch 001:   4825 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.3, ups=0.91, wpb=109.4, bsz=40, num_updates=4820, lr=4.16811e-06, gnorm=1.229, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20528
2022-10-11 23:19:43 - progress_bar.py[line:274] - INFO: epoch 001:   4835 / 28910 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.9, wpb=109.5, bsz=40, num_updates=4830, lr=4.17676e-06, gnorm=1.205, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20539
2022-10-11 23:19:54 - progress_bar.py[line:274] - INFO: epoch 001:   4845 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.2, ups=0.9, wpb=112.9, bsz=40, num_updates=4840, lr=4.1854e-06, gnorm=1.114, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20550
2022-10-11 23:20:05 - progress_bar.py[line:274] - INFO: epoch 001:   4855 / 28910 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=102.8, ups=0.93, wpb=110.3, bsz=40, num_updates=4850, lr=4.19405e-06, gnorm=1.217, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20561
2022-10-11 23:20:16 - progress_bar.py[line:274] - INFO: epoch 001:   4865 / 28910 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.5, ups=0.93, wpb=109.6, bsz=40, num_updates=4860, lr=4.2027e-06, gnorm=1.247, clip=90, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20572
2022-10-11 23:20:27 - progress_bar.py[line:274] - INFO: epoch 001:   4875 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=4870, lr=4.21135e-06, gnorm=1.248, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20583
2022-10-11 23:20:38 - progress_bar.py[line:274] - INFO: epoch 001:   4885 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.6, ups=0.91, wpb=110.8, bsz=40, num_updates=4880, lr=4.21999e-06, gnorm=1.15, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20594
2022-10-11 23:20:47 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-11 23:20:50 - progress_bar.py[line:274] - INFO: epoch 001:   4896 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=90.7, ups=0.82, wpb=111.1, bsz=40, num_updates=4890, lr=4.22864e-06, gnorm=1.164, clip=80, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=20606
2022-10-11 23:21:01 - progress_bar.py[line:274] - INFO: epoch 001:   4906 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=104.3, ups=0.94, wpb=110.4, bsz=40, num_updates=4900, lr=4.23729e-06, gnorm=1.208, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20617
2022-10-11 23:21:12 - progress_bar.py[line:274] - INFO: epoch 001:   4916 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=4910, lr=4.24594e-06, gnorm=1.13, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20628
2022-10-11 23:21:23 - progress_bar.py[line:274] - INFO: epoch 001:   4926 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.4, ups=0.91, wpb=112.2, bsz=40, num_updates=4920, lr=4.25458e-06, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20639
2022-10-11 23:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   4936 / 28910 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.89, wpb=110.2, bsz=40, num_updates=4930, lr=4.26323e-06, gnorm=1.227, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20650
2022-10-11 23:21:45 - progress_bar.py[line:274] - INFO: epoch 001:   4946 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=4940, lr=4.27188e-06, gnorm=1.189, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20662
2022-10-11 23:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   4956 / 28910 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=4950, lr=4.28053e-06, gnorm=1.318, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20673
2022-10-11 23:22:08 - progress_bar.py[line:274] - INFO: epoch 001:   4966 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=4960, lr=4.28917e-06, gnorm=1.202, clip=70, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=20684
2022-10-11 23:22:19 - progress_bar.py[line:274] - INFO: epoch 001:   4976 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.6, ups=0.88, wpb=108.7, bsz=40, num_updates=4970, lr=4.29782e-06, gnorm=1.166, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20695
2022-10-11 23:22:30 - progress_bar.py[line:274] - INFO: epoch 001:   4986 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=4980, lr=4.30647e-06, gnorm=1.155, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20706
2022-10-11 23:22:41 - progress_bar.py[line:274] - INFO: epoch 001:   4996 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.92, wpb=109.3, bsz=40, num_updates=4990, lr=4.31512e-06, gnorm=1.254, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20717
2022-10-11 23:22:52 - progress_bar.py[line:274] - INFO: epoch 001:   5006 / 28910 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=5000, lr=4.32376e-06, gnorm=1.225, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20729
2022-10-11 23:22:52 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-11 23:22:54 - train.py[line:549] - INFO: 0 / 4988
2022-10-11 23:22:54 - train.py[line:551] - INFO: load:1.31 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-11 23:22:55 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.37 GiB (GPU 0; 39.59 GiB total capacity; 8.41 GiB already allocated; 2.36 GiB free; 34.75 GiB reserved in total by PyTorch)
2022-10-11 23:22:55 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8614 MB |    9747 MB |    3003 TB |    3003 TB |
|       from large pool |    8470 MB |    9602 MB |    3002 TB |    3002 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8614 MB |    9747 MB |    3003 TB |    3003 TB |
|       from large pool |    8470 MB |    9602 MB |    3002 TB |    3002 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   35584 MB |   36944 MB |  154066 MB |  118482 MB |
|       from large pool |   35438 MB |   36792 MB |  153826 MB |  118388 MB |
|       from small pool |     146 MB |     152 MB |     240 MB |      94 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   26969 MB |   26969 MB |    2975 TB |    2975 TB |
|       from large pool |   26967 MB |   26967 MB |    2974 TB |    2974 TB |
|       from small pool |       1 MB |       1 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  138034 K  |  138030 K  |
|       from large pool |     563    |     575    |   44966 K  |   44965 K  |
|       from small pool |    3095    |    3114    |   93067 K  |   93064 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  138034 K  |  138030 K  |
|       from large pool |     563    |     575    |   44966 K  |   44965 K  |
|       from small pool |    3095    |    3114    |   93067 K  |   93064 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     188    |     196    |     492    |     304    |
|       from large pool |     115    |     120    |     372    |     257    |
|       from small pool |      73    |      76    |     120    |      47    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     121    |     127    |   97560 K  |   97560 K  |
|       from large pool |      86    |      89    |   17428 K  |   17428 K  |
|       from small pool |      35    |      42    |   80132 K  |   80132 K  |
|===========================================================================|

2022-10-11 23:22:55 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-11 23:22:55 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-11 23:25:27 - train.py[line:549] - INFO: 200 / 4988
2022-10-11 23:25:27 - train.py[line:551] - INFO: load:1.33 valid_run:153.00 task_valid:148.45 collect_output:2.08
2022-10-11 23:27:55 - train.py[line:549] - INFO: 400 / 4988
2022-10-11 23:27:55 - train.py[line:551] - INFO: load:1.35 valid_run:301.21 task_valid:291.84 collect_output:5.77
2022-10-11 23:30:28 - train.py[line:549] - INFO: 600 / 4988
2022-10-11 23:30:28 - train.py[line:551] - INFO: load:1.38 valid_run:453.36 task_valid:435.14 collect_output:13.54
2022-10-11 23:32:57 - train.py[line:549] - INFO: 800 / 4988
2022-10-11 23:32:57 - train.py[line:551] - INFO: load:1.40 valid_run:602.37 task_valid:580.43 collect_output:16.18
2022-10-11 23:35:28 - train.py[line:549] - INFO: 1000 / 4988
2022-10-11 23:35:28 - train.py[line:551] - INFO: load:1.43 valid_run:754.09 task_valid:727.74 collect_output:19.58
2022-10-11 23:38:00 - train.py[line:549] - INFO: 1200 / 4988
2022-10-11 23:38:00 - train.py[line:551] - INFO: load:1.45 valid_run:905.23 task_valid:873.08 collect_output:24.38
2022-10-11 23:40:32 - train.py[line:549] - INFO: 1400 / 4988
2022-10-11 23:40:32 - train.py[line:551] - INFO: load:1.47 valid_run:1057.68 task_valid:1019.02 collect_output:29.90
2022-10-11 23:43:03 - train.py[line:549] - INFO: 1600 / 4988
2022-10-11 23:43:03 - train.py[line:551] - INFO: load:1.50 valid_run:1208.16 task_valid:1160.13 collect_output:38.28
2022-10-11 23:45:32 - train.py[line:549] - INFO: 1800 / 4988
2022-10-11 23:45:32 - train.py[line:551] - INFO: load:1.52 valid_run:1357.23 task_valid:1304.67 collect_output:41.81
2022-10-11 23:48:00 - train.py[line:549] - INFO: 2000 / 4988
2022-10-11 23:48:00 - train.py[line:551] - INFO: load:1.54 valid_run:1505.39 task_valid:1447.86 collect_output:45.77
2022-10-11 23:50:30 - train.py[line:549] - INFO: 2200 / 4988
2022-10-11 23:50:30 - train.py[line:551] - INFO: load:1.57 valid_run:1654.87 task_valid:1592.94 collect_output:49.08
2022-10-11 23:52:59 - train.py[line:549] - INFO: 2400 / 4988
2022-10-11 23:52:59 - train.py[line:551] - INFO: load:1.59 valid_run:1804.19 task_valid:1737.62 collect_output:52.71
2022-10-11 23:55:28 - train.py[line:549] - INFO: 2600 / 4988
2022-10-11 23:55:28 - train.py[line:551] - INFO: load:1.62 valid_run:1953.60 task_valid:1879.30 collect_output:59.41
2022-10-11 23:57:59 - train.py[line:549] - INFO: 2800 / 4988
2022-10-11 23:57:59 - train.py[line:551] - INFO: load:1.64 valid_run:2103.75 task_valid:2024.72 collect_output:63.12
2022-10-12 00:00:28 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 00:00:28 - train.py[line:551] - INFO: load:1.66 valid_run:2253.42 task_valid:2170.97 collect_output:65.48
2022-10-12 00:02:58 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 00:02:58 - train.py[line:551] - INFO: load:1.69 valid_run:2403.05 task_valid:2314.99 collect_output:70.08
2022-10-12 00:05:29 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 00:05:29 - train.py[line:551] - INFO: load:1.71 valid_run:2554.07 task_valid:2460.44 collect_output:74.63
2022-10-12 00:08:00 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 00:08:00 - train.py[line:551] - INFO: load:1.74 valid_run:2704.50 task_valid:2607.52 collect_output:76.94
2022-10-12 00:10:28 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 00:10:28 - train.py[line:551] - INFO: load:1.76 valid_run:2852.50 task_valid:2749.12 collect_output:82.27
2022-10-12 00:12:58 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 00:12:58 - train.py[line:551] - INFO: load:1.79 valid_run:3002.43 task_valid:2894.22 collect_output:86.03
2022-10-12 00:15:29 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 00:15:29 - train.py[line:551] - INFO: load:1.81 valid_run:3153.88 task_valid:3038.83 collect_output:91.83
2022-10-12 00:17:58 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 00:17:58 - train.py[line:551] - INFO: load:1.83 valid_run:3303.09 task_valid:3183.54 collect_output:95.25
2022-10-12 00:20:30 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 00:20:30 - train.py[line:551] - INFO: load:1.86 valid_run:3454.35 task_valid:3330.12 collect_output:98.85
2022-10-12 00:23:02 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 00:23:02 - train.py[line:551] - INFO: load:1.88 valid_run:3606.15 task_valid:3477.22 collect_output:102.38

====================================================================================================
SGG eval:     R @ 50: 0.6803;     R @ 100: 0.7039;     R @ 500: 0.7257;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4825;    mR @ 100: 0.5096;    mR @ 500: 0.5603;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8415) (covered in:0.8125) (covering:0.3714) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4355) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9516) (says:0.0000) (sitting on:0.7517) (standing on:0.4343) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2022-10-12 00:25:33 - train.py[line:487] - INFO: 0.7039104150751209

====================================================================================================
SGG eval:     R @ 50: 0.6803;     R @ 100: 0.7039;     R @ 500: 0.7257;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4825;    mR @ 100: 0.5096;    mR @ 500: 0.5603;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8415) (covered in:0.8125) (covering:0.3714) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4355) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9516) (says:0.0000) (sitting on:0.7517) (standing on:0.4343) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2022-10-12 00:25:33 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 00:25:33 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.335 | loss_v1 0 | loss_v2 0 | nll_loss 0.182 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.70391 | ppl 1.13 | vqa_score 0.5755 | wps 119.4 | wpb 89.9 | bsz 30 | num_updates 5000 | best_R@100 0.70391
2022-10-12 00:25:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5000 updates
2022-10-12 00:25:33 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_5000.pt
2022-10-12 00:25:39 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_5000.pt
2022-10-12 00:25:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 0.7039104150751209) (writing took 10.798373258206993 seconds)
2022-10-12 00:25:55 - progress_bar.py[line:274] - INFO: epoch 001:   5016 / 28910 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=0.3, ups=0, wpb=109.8, bsz=40, num_updates=5010, lr=4.33241e-06, gnorm=1.248, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24511
2022-10-12 00:26:06 - progress_bar.py[line:274] - INFO: epoch 001:   5026 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.7, ups=0.88, wpb=110.5, bsz=40, num_updates=5020, lr=4.34106e-06, gnorm=1.184, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24522
2022-10-12 00:26:18 - progress_bar.py[line:274] - INFO: epoch 001:   5036 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.88, wpb=112.1, bsz=40, num_updates=5030, lr=4.34971e-06, gnorm=1.102, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24534
2022-10-12 00:26:29 - progress_bar.py[line:274] - INFO: epoch 001:   5046 / 28910 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=5040, lr=4.35835e-06, gnorm=1.192, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24545
2022-10-12 00:26:40 - progress_bar.py[line:274] - INFO: epoch 001:   5056 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.4, ups=0.91, wpb=110.8, bsz=40, num_updates=5050, lr=4.367e-06, gnorm=1.132, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24556
2022-10-12 00:26:51 - progress_bar.py[line:274] - INFO: epoch 001:   5066 / 28910 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=5060, lr=4.37565e-06, gnorm=1.173, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24567
2022-10-12 00:27:02 - progress_bar.py[line:274] - INFO: epoch 001:   5076 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.88, wpb=111.3, bsz=40, num_updates=5070, lr=4.3843e-06, gnorm=1.172, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24578
2022-10-12 00:27:13 - progress_bar.py[line:274] - INFO: epoch 001:   5086 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=5080, lr=4.39294e-06, gnorm=1.228, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24589
2022-10-12 00:27:24 - progress_bar.py[line:274] - INFO: epoch 001:   5096 / 28910 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.8, ups=0.91, wpb=110.2, bsz=40, num_updates=5090, lr=4.40159e-06, gnorm=1.237, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24600
2022-10-12 00:27:35 - progress_bar.py[line:274] - INFO: epoch 001:   5106 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103, ups=0.93, wpb=111, bsz=40, num_updates=5100, lr=4.41024e-06, gnorm=1.237, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24611
2022-10-12 00:27:46 - progress_bar.py[line:274] - INFO: epoch 001:   5116 / 28910 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.6, ups=0.91, wpb=110, bsz=40, num_updates=5110, lr=4.41889e-06, gnorm=1.227, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24622
2022-10-12 00:27:57 - progress_bar.py[line:274] - INFO: epoch 001:   5126 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.6, ups=0.89, wpb=110.2, bsz=40, num_updates=5120, lr=4.42753e-06, gnorm=1.182, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24633
2022-10-12 00:28:08 - progress_bar.py[line:274] - INFO: epoch 001:   5136 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=5130, lr=4.43618e-06, gnorm=1.184, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24644
2022-10-12 00:28:19 - progress_bar.py[line:274] - INFO: epoch 001:   5146 / 28910 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.6, ups=0.9, wpb=109.1, bsz=40, num_updates=5140, lr=4.44483e-06, gnorm=1.32, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24655
2022-10-12 00:28:31 - progress_bar.py[line:274] - INFO: epoch 001:   5156 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=5150, lr=4.45348e-06, gnorm=1.23, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24667
2022-10-12 00:28:42 - progress_bar.py[line:274] - INFO: epoch 001:   5166 / 28910 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=5160, lr=4.46212e-06, gnorm=1.214, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24678
2022-10-12 00:28:53 - progress_bar.py[line:274] - INFO: epoch 001:   5176 / 28910 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=5170, lr=4.47077e-06, gnorm=1.328, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24689
2022-10-12 00:29:04 - progress_bar.py[line:274] - INFO: epoch 001:   5186 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=5180, lr=4.47942e-06, gnorm=1.148, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24700
2022-10-12 00:29:15 - progress_bar.py[line:274] - INFO: epoch 001:   5196 / 28910 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.8, ups=0.88, wpb=109.7, bsz=40, num_updates=5190, lr=4.48807e-06, gnorm=1.428, clip=90, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=24711
2022-10-12 00:29:26 - progress_bar.py[line:274] - INFO: epoch 001:   5206 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.8, ups=0.92, wpb=110, bsz=40, num_updates=5200, lr=4.49671e-06, gnorm=1.253, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24722
2022-10-12 00:29:37 - progress_bar.py[line:274] - INFO: epoch 001:   5216 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.6, ups=0.92, wpb=112, bsz=40, num_updates=5210, lr=4.50536e-06, gnorm=1.258, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24733
2022-10-12 00:29:48 - progress_bar.py[line:274] - INFO: epoch 001:   5226 / 28910 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.2, ups=0.91, wpb=110, bsz=40, num_updates=5220, lr=4.51401e-06, gnorm=1.255, clip=90, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=24744
2022-10-12 00:30:00 - progress_bar.py[line:274] - INFO: epoch 001:   5236 / 28910 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.5, ups=0.86, wpb=110.5, bsz=40, num_updates=5230, lr=4.52266e-06, gnorm=1.267, clip=100, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=24756
2022-10-12 00:30:11 - progress_bar.py[line:274] - INFO: epoch 001:   5246 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.3, ups=0.89, wpb=109, bsz=40, num_updates=5240, lr=4.5313e-06, gnorm=1.138, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24767
2022-10-12 00:30:22 - progress_bar.py[line:274] - INFO: epoch 001:   5256 / 28910 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=5250, lr=4.53995e-06, gnorm=1.255, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24778
2022-10-12 00:30:33 - progress_bar.py[line:274] - INFO: epoch 001:   5266 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=5260, lr=4.5486e-06, gnorm=1.224, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24789
2022-10-12 00:30:44 - progress_bar.py[line:274] - INFO: epoch 001:   5276 / 28910 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.88, wpb=109.6, bsz=40, num_updates=5270, lr=4.55725e-06, gnorm=1.314, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24800
2022-10-12 00:30:56 - progress_bar.py[line:274] - INFO: epoch 001:   5286 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=94.2, ups=0.87, wpb=108.5, bsz=40, num_updates=5280, lr=4.56589e-06, gnorm=1.274, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24812
2022-10-12 00:31:07 - progress_bar.py[line:274] - INFO: epoch 001:   5296 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.5, ups=0.93, wpb=110.6, bsz=40, num_updates=5290, lr=4.57454e-06, gnorm=1.22, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24823
2022-10-12 00:31:17 - progress_bar.py[line:274] - INFO: epoch 001:   5306 / 28910 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=5300, lr=4.58319e-06, gnorm=1.22, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24834
2022-10-12 00:31:28 - progress_bar.py[line:274] - INFO: epoch 001:   5316 / 28910 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.9, wpb=108.9, bsz=40, num_updates=5310, lr=4.59184e-06, gnorm=1.253, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24845
2022-10-12 00:31:40 - progress_bar.py[line:274] - INFO: epoch 001:   5326 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.6, ups=0.88, wpb=110, bsz=40, num_updates=5320, lr=4.60048e-06, gnorm=1.217, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24856
2022-10-12 00:31:51 - progress_bar.py[line:274] - INFO: epoch 001:   5336 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=5330, lr=4.60913e-06, gnorm=1.087, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24867
2022-10-12 00:32:02 - progress_bar.py[line:274] - INFO: epoch 001:   5346 / 28910 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.9, ups=0.9, wpb=109.5, bsz=40, num_updates=5340, lr=4.61778e-06, gnorm=1.302, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24878
2022-10-12 00:32:13 - progress_bar.py[line:274] - INFO: epoch 001:   5356 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.88, wpb=112.1, bsz=40, num_updates=5350, lr=4.62643e-06, gnorm=1.231, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24889
2022-10-12 00:32:24 - progress_bar.py[line:274] - INFO: epoch 001:   5366 / 28910 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.9, ups=0.9, wpb=111.5, bsz=40, num_updates=5360, lr=4.63507e-06, gnorm=1.275, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24901
2022-10-12 00:32:36 - progress_bar.py[line:274] - INFO: epoch 001:   5376 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=5370, lr=4.64372e-06, gnorm=1.135, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24912
2022-10-12 00:32:47 - progress_bar.py[line:274] - INFO: epoch 001:   5386 / 28910 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.88, wpb=111.5, bsz=40, num_updates=5380, lr=4.65237e-06, gnorm=1.115, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24923
2022-10-12 00:32:59 - progress_bar.py[line:274] - INFO: epoch 001:   5396 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.9, ups=0.87, wpb=110.2, bsz=40, num_updates=5390, lr=4.66102e-06, gnorm=1.227, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24935
2022-10-12 00:33:10 - progress_bar.py[line:274] - INFO: epoch 001:   5406 / 28910 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.9, ups=0.88, wpb=110, bsz=40, num_updates=5400, lr=4.66966e-06, gnorm=1.278, clip=90, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24946
2022-10-12 00:33:21 - progress_bar.py[line:274] - INFO: epoch 001:   5416 / 28910 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.91, wpb=109.1, bsz=40, num_updates=5410, lr=4.67831e-06, gnorm=1.2, clip=80, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24957
2022-10-12 00:33:32 - progress_bar.py[line:274] - INFO: epoch 001:   5426 / 28910 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.1, ups=0.92, wpb=111.4, bsz=40, num_updates=5420, lr=4.68696e-06, gnorm=1.176, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24968
2022-10-12 00:33:43 - progress_bar.py[line:274] - INFO: epoch 001:   5436 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.3, ups=0.87, wpb=109.5, bsz=40, num_updates=5430, lr=4.69561e-06, gnorm=1.336, clip=90, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24979
2022-10-12 00:33:55 - progress_bar.py[line:274] - INFO: epoch 001:   5446 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=5440, lr=4.70425e-06, gnorm=1.241, clip=70, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24991
2022-10-12 00:34:06 - progress_bar.py[line:274] - INFO: epoch 001:   5456 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.8, ups=0.92, wpb=109.5, bsz=40, num_updates=5450, lr=4.7129e-06, gnorm=1.229, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25002
2022-10-12 00:34:17 - progress_bar.py[line:274] - INFO: epoch 001:   5466 / 28910 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.89, wpb=109.8, bsz=40, num_updates=5460, lr=4.72155e-06, gnorm=1.365, clip=90, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25013
2022-10-12 00:34:22 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 00:34:29 - progress_bar.py[line:274] - INFO: epoch 001:   5477 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=90.8, ups=0.82, wpb=110.7, bsz=40, num_updates=5470, lr=4.7302e-06, gnorm=1.214, clip=90, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=25025
2022-10-12 00:34:40 - progress_bar.py[line:274] - INFO: epoch 001:   5487 / 28910 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.89, wpb=109.2, bsz=40, num_updates=5480, lr=4.73884e-06, gnorm=1.233, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25036
2022-10-12 00:34:51 - progress_bar.py[line:274] - INFO: epoch 001:   5497 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=5490, lr=4.74749e-06, gnorm=1.299, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25047
2022-10-12 00:35:03 - progress_bar.py[line:274] - INFO: epoch 001:   5507 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.88, wpb=111.1, bsz=40, num_updates=5500, lr=4.75614e-06, gnorm=1.185, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25059
2022-10-12 00:35:14 - progress_bar.py[line:274] - INFO: epoch 001:   5517 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.1, ups=0.89, wpb=109.2, bsz=40, num_updates=5510, lr=4.76479e-06, gnorm=1.229, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25070
2022-10-12 00:35:25 - progress_bar.py[line:274] - INFO: epoch 001:   5527 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.88, wpb=111.2, bsz=40, num_updates=5520, lr=4.77343e-06, gnorm=1.246, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25081
2022-10-12 00:35:37 - progress_bar.py[line:274] - INFO: epoch 001:   5537 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=5530, lr=4.78208e-06, gnorm=1.218, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25093
2022-10-12 00:35:48 - progress_bar.py[line:274] - INFO: epoch 001:   5547 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=5540, lr=4.79073e-06, gnorm=1.187, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25104
2022-10-12 00:35:59 - progress_bar.py[line:274] - INFO: epoch 001:   5557 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=5550, lr=4.79938e-06, gnorm=1.158, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25115
2022-10-12 00:36:11 - progress_bar.py[line:274] - INFO: epoch 001:   5567 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=5560, lr=4.80802e-06, gnorm=1.089, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25127
2022-10-12 00:36:22 - progress_bar.py[line:274] - INFO: epoch 001:   5577 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=110.4, bsz=40, num_updates=5570, lr=4.81667e-06, gnorm=1.218, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25138
2022-10-12 00:36:33 - progress_bar.py[line:274] - INFO: epoch 001:   5587 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=5580, lr=4.82532e-06, gnorm=1.24, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25149
2022-10-12 00:36:44 - progress_bar.py[line:274] - INFO: epoch 001:   5597 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=5590, lr=4.83397e-06, gnorm=1.138, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25160
2022-10-12 00:36:55 - progress_bar.py[line:274] - INFO: epoch 001:   5607 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.5, ups=0.89, wpb=111.7, bsz=40, num_updates=5600, lr=4.84262e-06, gnorm=1.184, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25171
2022-10-12 00:37:06 - progress_bar.py[line:274] - INFO: epoch 001:   5617 / 28910 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=102, ups=0.92, wpb=111, bsz=40, num_updates=5610, lr=4.85126e-06, gnorm=1.293, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25182
2022-10-12 00:37:17 - progress_bar.py[line:274] - INFO: epoch 001:   5627 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=5620, lr=4.85991e-06, gnorm=1.202, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25194
2022-10-12 00:37:28 - progress_bar.py[line:274] - INFO: epoch 001:   5637 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.9, ups=0.94, wpb=110.5, bsz=40, num_updates=5630, lr=4.86856e-06, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25204
2022-10-12 00:37:39 - progress_bar.py[line:274] - INFO: epoch 001:   5647 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.2, ups=0.88, wpb=111.6, bsz=40, num_updates=5640, lr=4.87721e-06, gnorm=1.142, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25216
2022-10-12 00:37:51 - progress_bar.py[line:274] - INFO: epoch 001:   5657 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=5650, lr=4.88585e-06, gnorm=1.199, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25227
2022-10-12 00:38:02 - progress_bar.py[line:274] - INFO: epoch 001:   5667 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.1, ups=0.9, wpb=110.5, bsz=40, num_updates=5660, lr=4.8945e-06, gnorm=1.111, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25238
2022-10-12 00:38:13 - progress_bar.py[line:274] - INFO: epoch 001:   5677 / 28910 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=5670, lr=4.90315e-06, gnorm=1.267, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25249
2022-10-12 00:38:24 - progress_bar.py[line:274] - INFO: epoch 001:   5687 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.9, ups=0.9, wpb=110.1, bsz=40, num_updates=5680, lr=4.9118e-06, gnorm=1.217, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25260
2022-10-12 00:38:35 - progress_bar.py[line:274] - INFO: epoch 001:   5697 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.88, wpb=112.1, bsz=40, num_updates=5690, lr=4.92044e-06, gnorm=1.217, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25271
2022-10-12 00:38:46 - progress_bar.py[line:274] - INFO: epoch 001:   5707 / 28910 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.7, ups=0.91, wpb=110.1, bsz=40, num_updates=5700, lr=4.92909e-06, gnorm=1.222, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25282
2022-10-12 00:38:57 - progress_bar.py[line:274] - INFO: epoch 001:   5717 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=5710, lr=4.93774e-06, gnorm=1.178, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25293
2022-10-12 00:39:08 - progress_bar.py[line:274] - INFO: epoch 001:   5727 / 28910 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.6, ups=0.9, wpb=110.5, bsz=40, num_updates=5720, lr=4.94639e-06, gnorm=1.177, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25305
2022-10-12 00:39:19 - progress_bar.py[line:274] - INFO: epoch 001:   5737 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.1, ups=0.92, wpb=110.4, bsz=40, num_updates=5730, lr=4.95503e-06, gnorm=1.175, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25316
2022-10-12 00:39:30 - progress_bar.py[line:274] - INFO: epoch 001:   5747 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=5740, lr=4.96368e-06, gnorm=1.196, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25327
2022-10-12 00:39:42 - progress_bar.py[line:274] - INFO: epoch 001:   5757 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=5750, lr=4.97233e-06, gnorm=1.238, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25338
2022-10-12 00:39:53 - progress_bar.py[line:274] - INFO: epoch 001:   5767 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=5760, lr=4.98098e-06, gnorm=1.229, clip=90, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=25349
2022-10-12 00:40:04 - progress_bar.py[line:274] - INFO: epoch 001:   5777 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.9, wpb=109.9, bsz=40, num_updates=5770, lr=4.98962e-06, gnorm=1.331, clip=90, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=25360
2022-10-12 00:40:16 - progress_bar.py[line:274] - INFO: epoch 001:   5787 / 28910 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=5780, lr=4.99827e-06, gnorm=1.163, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25372
2022-10-12 00:40:27 - progress_bar.py[line:274] - INFO: epoch 001:   5797 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.6, ups=0.89, wpb=111.3, bsz=40, num_updates=5790, lr=5.00692e-06, gnorm=1.154, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25383
2022-10-12 00:40:38 - progress_bar.py[line:274] - INFO: epoch 001:   5807 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.4, ups=0.9, wpb=108.8, bsz=40, num_updates=5800, lr=5.01557e-06, gnorm=1.256, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25394
2022-10-12 00:40:49 - progress_bar.py[line:274] - INFO: epoch 001:   5817 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=103.1, ups=0.93, wpb=110.8, bsz=40, num_updates=5810, lr=5.02421e-06, gnorm=1.181, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25405
2022-10-12 00:41:00 - progress_bar.py[line:274] - INFO: epoch 001:   5827 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.89, wpb=110.8, bsz=40, num_updates=5820, lr=5.03286e-06, gnorm=1.175, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25416
2022-10-12 00:41:11 - progress_bar.py[line:274] - INFO: epoch 001:   5837 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.9, wpb=109.8, bsz=40, num_updates=5830, lr=5.04151e-06, gnorm=1.329, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25427
2022-10-12 00:41:22 - progress_bar.py[line:274] - INFO: epoch 001:   5847 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.4, ups=0.91, wpb=110.5, bsz=40, num_updates=5840, lr=5.05016e-06, gnorm=1.066, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25438
2022-10-12 00:41:33 - progress_bar.py[line:274] - INFO: epoch 001:   5857 / 28910 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.3, ups=0.9, wpb=109.8, bsz=40, num_updates=5850, lr=5.0588e-06, gnorm=1.246, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25449
2022-10-12 00:41:44 - progress_bar.py[line:274] - INFO: epoch 001:   5867 / 28910 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100, ups=0.91, wpb=110.2, bsz=40, num_updates=5860, lr=5.06745e-06, gnorm=1.235, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25460
2022-10-12 00:41:56 - progress_bar.py[line:274] - INFO: epoch 001:   5877 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=5870, lr=5.0761e-06, gnorm=1.152, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25472
2022-10-12 00:42:07 - progress_bar.py[line:274] - INFO: epoch 001:   5887 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=5880, lr=5.08475e-06, gnorm=1.187, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25483
2022-10-12 00:42:18 - progress_bar.py[line:274] - INFO: epoch 001:   5897 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.7, ups=0.92, wpb=111.8, bsz=40, num_updates=5890, lr=5.09339e-06, gnorm=1.315, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25494
2022-10-12 00:42:29 - progress_bar.py[line:274] - INFO: epoch 001:   5907 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.3, ups=0.88, wpb=109.1, bsz=40, num_updates=5900, lr=5.10204e-06, gnorm=1.305, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25505
2022-10-12 00:42:40 - progress_bar.py[line:274] - INFO: epoch 001:   5917 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.4, ups=0.91, wpb=112.6, bsz=40, num_updates=5910, lr=5.11069e-06, gnorm=1.204, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25516
2022-10-12 00:42:51 - progress_bar.py[line:274] - INFO: epoch 001:   5927 / 28910 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=5920, lr=5.11934e-06, gnorm=1.31, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25527
2022-10-12 00:43:02 - progress_bar.py[line:274] - INFO: epoch 001:   5937 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.89, wpb=110.5, bsz=40, num_updates=5930, lr=5.12798e-06, gnorm=1.314, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25538
2022-10-12 00:43:14 - progress_bar.py[line:274] - INFO: epoch 001:   5947 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=95.7, ups=0.87, wpb=109.4, bsz=40, num_updates=5940, lr=5.13663e-06, gnorm=1.258, clip=90, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=25550
2022-10-12 00:43:25 - progress_bar.py[line:274] - INFO: epoch 001:   5957 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.2, ups=0.87, wpb=110, bsz=40, num_updates=5950, lr=5.14528e-06, gnorm=1.13, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25561
2022-10-12 00:43:36 - progress_bar.py[line:274] - INFO: epoch 001:   5967 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.9, wpb=108.5, bsz=40, num_updates=5960, lr=5.15393e-06, gnorm=1.315, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25572
2022-10-12 00:43:47 - progress_bar.py[line:274] - INFO: epoch 001:   5977 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=5970, lr=5.16257e-06, gnorm=1.148, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25583
2022-10-12 00:43:59 - progress_bar.py[line:274] - INFO: epoch 001:   5987 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.88, wpb=109.2, bsz=40, num_updates=5980, lr=5.17122e-06, gnorm=1.133, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25595
2022-10-12 00:44:10 - progress_bar.py[line:274] - INFO: epoch 001:   5997 / 28910 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=5990, lr=5.17987e-06, gnorm=1.355, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25606
2022-10-12 00:44:21 - progress_bar.py[line:274] - INFO: epoch 001:   6007 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.9, ups=0.9, wpb=109.3, bsz=40, num_updates=6000, lr=5.18852e-06, gnorm=1.299, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25617
2022-10-12 00:44:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 00:44:22 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 00:44:22 - train.py[line:551] - INFO: load:1.22 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 00:46:55 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 00:46:55 - train.py[line:551] - INFO: load:1.25 valid_run:152.31 task_valid:149.13 collect_output:2.09
2022-10-12 00:49:23 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 00:49:23 - train.py[line:551] - INFO: load:1.27 valid_run:300.60 task_valid:292.38 collect_output:6.10
2022-10-12 00:51:55 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 00:51:55 - train.py[line:551] - INFO: load:1.29 valid_run:452.31 task_valid:435.67 collect_output:13.51
2022-10-12 00:54:24 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 00:54:24 - train.py[line:551] - INFO: load:1.32 valid_run:601.06 task_valid:580.59 collect_output:16.31
2022-10-12 00:56:56 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 00:56:56 - train.py[line:551] - INFO: load:1.34 valid_run:753.24 task_valid:728.07 collect_output:19.98
2022-10-12 00:59:27 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 00:59:27 - train.py[line:551] - INFO: load:1.36 valid_run:904.57 task_valid:873.72 collect_output:24.61
2022-10-12 01:02:00 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 01:02:00 - train.py[line:551] - INFO: load:1.39 valid_run:1057.31 task_valid:1019.84 collect_output:30.24
2022-10-12 01:04:31 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 01:04:31 - train.py[line:551] - INFO: load:1.41 valid_run:1208.11 task_valid:1161.10 collect_output:38.76
2022-10-12 01:07:00 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 01:07:00 - train.py[line:551] - INFO: load:1.44 valid_run:1357.44 task_valid:1305.92 collect_output:42.25
2022-10-12 01:09:29 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 01:09:29 - train.py[line:551] - INFO: load:1.46 valid_run:1505.63 task_valid:1449.01 collect_output:46.31
2022-10-12 01:11:59 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 01:11:59 - train.py[line:551] - INFO: load:1.49 valid_run:1655.42 task_valid:1594.29 collect_output:49.78
2022-10-12 01:14:28 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 01:14:28 - train.py[line:551] - INFO: load:1.51 valid_run:1804.97 task_valid:1738.95 collect_output:53.66
2022-10-12 01:16:58 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 01:16:58 - train.py[line:551] - INFO: load:1.53 valid_run:1954.41 task_valid:1880.91 collect_output:60.12
2022-10-12 01:19:28 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 01:19:28 - train.py[line:551] - INFO: load:1.56 valid_run:2104.62 task_valid:2026.25 collect_output:64.00
2022-10-12 01:21:58 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 01:21:58 - train.py[line:551] - INFO: load:1.58 valid_run:2254.48 task_valid:2172.69 collect_output:66.40
2022-10-12 01:24:28 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 01:24:28 - train.py[line:551] - INFO: load:1.61 valid_run:2404.37 task_valid:2316.93 collect_output:71.00
2022-10-12 01:27:00 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 01:27:00 - train.py[line:551] - INFO: load:1.63 valid_run:2556.26 task_valid:2462.69 collect_output:75.94
2022-10-12 01:29:31 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 01:29:31 - train.py[line:551] - INFO: load:1.66 valid_run:2707.52 task_valid:2610.06 collect_output:78.59
2022-10-12 01:32:00 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 01:32:00 - train.py[line:551] - INFO: load:1.68 valid_run:2856.37 task_valid:2752.35 collect_output:83.92
2022-10-12 01:34:31 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 01:34:31 - train.py[line:551] - INFO: load:1.73 valid_run:3007.56 task_valid:2898.53 collect_output:87.65
2022-10-12 01:37:04 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 01:37:04 - train.py[line:551] - INFO: load:1.75 valid_run:3159.96 task_valid:3043.89 collect_output:93.43
2022-10-12 01:39:34 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 01:39:34 - train.py[line:551] - INFO: load:1.78 valid_run:3310.43 task_valid:3189.43 collect_output:97.08
2022-10-12 01:42:06 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 01:42:06 - train.py[line:551] - INFO: load:1.81 valid_run:3462.40 task_valid:3336.45 collect_output:100.72
2022-10-12 01:44:38 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 01:44:38 - train.py[line:551] - INFO: load:1.83 valid_run:3614.37 task_valid:3483.80 collect_output:104.14

====================================================================================================
SGG eval:     R @ 50: 0.6777;     R @ 100: 0.6959;     R @ 500: 0.7197;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4845;    mR @ 100: 0.5022;    mR @ 500: 0.5590;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8659) (covered in:0.9375) (covering:0.2286) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7517) (standing on:0.4143) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6777;     R @ 100: 0.6959;     R @ 500: 0.7197;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4845;    mR @ 100: 0.5022;    mR @ 500: 0.5590;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8659) (covered in:0.9375) (covering:0.2286) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7517) (standing on:0.4143) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2022-10-12 01:47:10 - train.py[line:487] - INFO: 0.6959104150751209
2022-10-12 01:47:10 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 01:47:10 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.343 | loss_v1 0 | loss_v2 0 | nll_loss 0.189 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.69591 | ppl 1.14 | vqa_score 0.5845 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 6000 | best_R@100 0.70391
2022-10-12 01:47:10 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2022-10-12 01:47:10 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_6000.pt
2022-10-12 01:47:15 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_6000.pt
2022-10-12 01:47:18 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6959104150751209) (writing took 8.254069630987942 seconds)
2022-10-12 01:47:29 - progress_bar.py[line:274] - INFO: epoch 001:   6017 / 28910 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=0.3, ups=0, wpb=110.6, bsz=40, num_updates=6010, lr=5.19716e-06, gnorm=1.238, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29405
2022-10-12 01:47:40 - progress_bar.py[line:274] - INFO: epoch 001:   6027 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.7, ups=0.91, wpb=109.9, bsz=40, num_updates=6020, lr=5.20581e-06, gnorm=1.155, clip=100, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29416
2022-10-12 01:47:51 - progress_bar.py[line:274] - INFO: epoch 001:   6037 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=6030, lr=5.21446e-06, gnorm=1.285, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29428
2022-10-12 01:48:02 - progress_bar.py[line:274] - INFO: epoch 001:   6047 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.2, ups=0.92, wpb=111.2, bsz=40, num_updates=6040, lr=5.22311e-06, gnorm=1.165, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29438
2022-10-12 01:48:13 - progress_bar.py[line:274] - INFO: epoch 001:   6057 / 28910 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=6050, lr=5.23175e-06, gnorm=1.297, clip=90, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29449
2022-10-12 01:48:24 - progress_bar.py[line:274] - INFO: epoch 001:   6067 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99, ups=0.9, wpb=109.6, bsz=40, num_updates=6060, lr=5.2404e-06, gnorm=1.083, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29461
2022-10-12 01:48:36 - progress_bar.py[line:274] - INFO: epoch 001:   6077 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.4, ups=0.87, wpb=109.3, bsz=40, num_updates=6070, lr=5.24905e-06, gnorm=1.385, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29472
2022-10-12 01:48:47 - progress_bar.py[line:274] - INFO: epoch 001:   6087 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102, ups=0.92, wpb=111.3, bsz=40, num_updates=6080, lr=5.2577e-06, gnorm=1.195, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29483
2022-10-12 01:48:49 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 01:48:59 - progress_bar.py[line:274] - INFO: epoch 001:   6098 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=92.4, ups=0.83, wpb=110.7, bsz=40, num_updates=6090, lr=5.26634e-06, gnorm=1.25, clip=90, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29495
2022-10-12 01:49:10 - progress_bar.py[line:274] - INFO: epoch 001:   6108 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.1, ups=0.93, wpb=111, bsz=40, num_updates=6100, lr=5.27499e-06, gnorm=1.333, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29506
2022-10-12 01:49:21 - progress_bar.py[line:274] - INFO: epoch 001:   6118 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.1, ups=0.91, wpb=109.4, bsz=40, num_updates=6110, lr=5.28364e-06, gnorm=1.165, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29517
2022-10-12 01:49:32 - progress_bar.py[line:274] - INFO: epoch 001:   6128 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=6120, lr=5.29229e-06, gnorm=1.329, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29528
2022-10-12 01:49:43 - progress_bar.py[line:274] - INFO: epoch 001:   6138 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=6130, lr=5.30093e-06, gnorm=1.121, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29539
2022-10-12 01:49:54 - progress_bar.py[line:274] - INFO: epoch 001:   6148 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.88, wpb=111, bsz=40, num_updates=6140, lr=5.30958e-06, gnorm=1.229, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29550
2022-10-12 01:50:05 - progress_bar.py[line:274] - INFO: epoch 001:   6158 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=6150, lr=5.31823e-06, gnorm=1.134, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29562
2022-10-12 01:50:16 - progress_bar.py[line:274] - INFO: epoch 001:   6168 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.6, ups=0.94, wpb=110.8, bsz=40, num_updates=6160, lr=5.32688e-06, gnorm=1.167, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29572
2022-10-12 01:50:27 - progress_bar.py[line:274] - INFO: epoch 001:   6178 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=6170, lr=5.33552e-06, gnorm=1.211, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29583
2022-10-12 01:50:39 - progress_bar.py[line:274] - INFO: epoch 001:   6188 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.87, wpb=110.1, bsz=40, num_updates=6180, lr=5.34417e-06, gnorm=1.153, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29595
2022-10-12 01:50:50 - progress_bar.py[line:274] - INFO: epoch 001:   6198 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.88, wpb=109.2, bsz=40, num_updates=6190, lr=5.35282e-06, gnorm=1.185, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29606
2022-10-12 01:51:01 - progress_bar.py[line:274] - INFO: epoch 001:   6208 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.88, wpb=110.5, bsz=40, num_updates=6200, lr=5.36147e-06, gnorm=1.119, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29618
2022-10-12 01:51:13 - progress_bar.py[line:274] - INFO: epoch 001:   6218 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=6210, lr=5.37011e-06, gnorm=1.183, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29629
2022-10-12 01:51:24 - progress_bar.py[line:274] - INFO: epoch 001:   6228 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=6220, lr=5.37876e-06, gnorm=1.159, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29640
2022-10-12 01:51:35 - progress_bar.py[line:274] - INFO: epoch 001:   6238 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.89, wpb=109.3, bsz=40, num_updates=6230, lr=5.38741e-06, gnorm=1.288, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29651
2022-10-12 01:51:47 - progress_bar.py[line:274] - INFO: epoch 001:   6248 / 28910 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.6, ups=0.87, wpb=109.8, bsz=40, num_updates=6240, lr=5.39606e-06, gnorm=1.278, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29663
2022-10-12 01:51:58 - progress_bar.py[line:274] - INFO: epoch 001:   6258 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98, ups=0.89, wpb=109.5, bsz=40, num_updates=6250, lr=5.4047e-06, gnorm=1.151, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29674
2022-10-12 01:52:09 - progress_bar.py[line:274] - INFO: epoch 001:   6268 / 28910 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=6260, lr=5.41335e-06, gnorm=1.244, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29685
2022-10-12 01:52:20 - progress_bar.py[line:274] - INFO: epoch 001:   6278 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.6, ups=0.9, wpb=111.2, bsz=40, num_updates=6270, lr=5.422e-06, gnorm=1.254, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29697
2022-10-12 01:52:32 - progress_bar.py[line:274] - INFO: epoch 001:   6288 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=6280, lr=5.43065e-06, gnorm=1.206, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29708
2022-10-12 01:52:43 - progress_bar.py[line:274] - INFO: epoch 001:   6298 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.8, ups=0.87, wpb=109.5, bsz=40, num_updates=6290, lr=5.43929e-06, gnorm=1.304, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29719
2022-10-12 01:52:55 - progress_bar.py[line:274] - INFO: epoch 001:   6308 / 28910 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.5, ups=0.87, wpb=109.4, bsz=40, num_updates=6300, lr=5.44794e-06, gnorm=1.188, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29731
2022-10-12 01:53:06 - progress_bar.py[line:274] - INFO: epoch 001:   6318 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.91, wpb=109.7, bsz=40, num_updates=6310, lr=5.45659e-06, gnorm=1.319, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29742
2022-10-12 01:53:17 - progress_bar.py[line:274] - INFO: epoch 001:   6328 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.8, ups=0.9, wpb=110.3, bsz=40, num_updates=6320, lr=5.46524e-06, gnorm=1.214, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29753
2022-10-12 01:53:28 - progress_bar.py[line:274] - INFO: epoch 001:   6338 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=6330, lr=5.47388e-06, gnorm=1.228, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29764
2022-10-12 01:53:39 - progress_bar.py[line:274] - INFO: epoch 001:   6348 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.2, ups=0.88, wpb=110.5, bsz=40, num_updates=6340, lr=5.48253e-06, gnorm=1.158, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29776
2022-10-12 01:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   6358 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.92, wpb=109.1, bsz=40, num_updates=6350, lr=5.49118e-06, gnorm=1.289, clip=100, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=29786
2022-10-12 01:54:02 - progress_bar.py[line:274] - INFO: epoch 001:   6368 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.7, ups=0.88, wpb=109.3, bsz=40, num_updates=6360, lr=5.49983e-06, gnorm=1.203, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29798
2022-10-12 01:54:13 - progress_bar.py[line:274] - INFO: epoch 001:   6378 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=6370, lr=5.50847e-06, gnorm=1.252, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29809
2022-10-12 01:54:24 - progress_bar.py[line:274] - INFO: epoch 001:   6388 / 28910 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=6380, lr=5.51712e-06, gnorm=1.297, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29820
2022-10-12 01:54:35 - progress_bar.py[line:274] - INFO: epoch 001:   6398 / 28910 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=6390, lr=5.52577e-06, gnorm=1.212, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29831
2022-10-12 01:54:46 - progress_bar.py[line:274] - INFO: epoch 001:   6408 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=6400, lr=5.53442e-06, gnorm=1.173, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29843
2022-10-12 01:54:57 - progress_bar.py[line:274] - INFO: epoch 001:   6418 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.9, ups=0.92, wpb=110, bsz=40, num_updates=6410, lr=5.54306e-06, gnorm=1.224, clip=90, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=29853
2022-10-12 01:55:09 - progress_bar.py[line:274] - INFO: epoch 001:   6428 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.2, ups=0.88, wpb=111.1, bsz=40, num_updates=6420, lr=5.55171e-06, gnorm=1.097, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29865
2022-10-12 01:55:20 - progress_bar.py[line:274] - INFO: epoch 001:   6438 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.9, wpb=109.7, bsz=40, num_updates=6430, lr=5.56036e-06, gnorm=1.335, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29876
2022-10-12 01:55:31 - progress_bar.py[line:274] - INFO: epoch 001:   6448 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.8, ups=0.92, wpb=110.9, bsz=40, num_updates=6440, lr=5.56901e-06, gnorm=1.262, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29887
2022-10-12 01:55:42 - progress_bar.py[line:274] - INFO: epoch 001:   6458 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=94.7, ups=0.87, wpb=108.5, bsz=40, num_updates=6450, lr=5.57765e-06, gnorm=1.221, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29898
2022-10-12 01:55:53 - progress_bar.py[line:274] - INFO: epoch 001:   6468 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.9, ups=0.9, wpb=111.8, bsz=40, num_updates=6460, lr=5.5863e-06, gnorm=1.09, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29909
2022-10-12 01:56:05 - progress_bar.py[line:274] - INFO: epoch 001:   6478 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=6470, lr=5.59495e-06, gnorm=1.109, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29921
2022-10-12 01:56:16 - progress_bar.py[line:274] - INFO: epoch 001:   6488 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=6480, lr=5.6036e-06, gnorm=1.178, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29932
2022-10-12 01:56:27 - progress_bar.py[line:274] - INFO: epoch 001:   6498 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=6490, lr=5.61224e-06, gnorm=1.183, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29943
2022-10-12 01:56:38 - progress_bar.py[line:274] - INFO: epoch 001:   6508 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.9, ups=0.92, wpb=110.2, bsz=40, num_updates=6500, lr=5.62089e-06, gnorm=1.107, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29954
2022-10-12 01:56:49 - progress_bar.py[line:274] - INFO: epoch 001:   6518 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100, ups=0.9, wpb=110.9, bsz=40, num_updates=6510, lr=5.62954e-06, gnorm=1.213, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29965
2022-10-12 01:57:00 - progress_bar.py[line:274] - INFO: epoch 001:   6528 / 28910 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=6520, lr=5.63819e-06, gnorm=1.192, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29976
2022-10-12 01:57:11 - progress_bar.py[line:274] - INFO: epoch 001:   6538 / 28910 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.9, ups=0.9, wpb=109.4, bsz=40, num_updates=6530, lr=5.64684e-06, gnorm=1.247, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29987
2022-10-12 01:57:22 - progress_bar.py[line:274] - INFO: epoch 001:   6548 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96, ups=0.88, wpb=109.7, bsz=40, num_updates=6540, lr=5.65548e-06, gnorm=1.241, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29999
2022-10-12 01:57:34 - progress_bar.py[line:274] - INFO: epoch 001:   6558 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=6550, lr=5.66413e-06, gnorm=1.173, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30010
2022-10-12 01:57:45 - progress_bar.py[line:274] - INFO: epoch 001:   6568 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.1, ups=0.92, wpb=109.2, bsz=40, num_updates=6560, lr=5.67278e-06, gnorm=1.291, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30021
2022-10-12 01:57:56 - progress_bar.py[line:274] - INFO: epoch 001:   6578 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96, ups=0.87, wpb=110.3, bsz=40, num_updates=6570, lr=5.68143e-06, gnorm=1.205, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30032
2022-10-12 01:58:08 - progress_bar.py[line:274] - INFO: epoch 001:   6588 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=6580, lr=5.69007e-06, gnorm=1.275, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30044
2022-10-12 01:58:19 - progress_bar.py[line:274] - INFO: epoch 001:   6598 / 28910 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.9, wpb=109.4, bsz=40, num_updates=6590, lr=5.69872e-06, gnorm=1.292, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30055
2022-10-12 01:58:30 - progress_bar.py[line:274] - INFO: epoch 001:   6608 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=6600, lr=5.70737e-06, gnorm=1.357, clip=100, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30066
2022-10-12 01:58:41 - progress_bar.py[line:274] - INFO: epoch 001:   6618 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=6610, lr=5.71602e-06, gnorm=1.151, clip=60, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=30077
2022-10-12 01:58:52 - progress_bar.py[line:274] - INFO: epoch 001:   6628 / 28910 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=6620, lr=5.72466e-06, gnorm=1.32, clip=90, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30088
2022-10-12 01:59:03 - progress_bar.py[line:274] - INFO: epoch 001:   6638 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.89, wpb=109.4, bsz=40, num_updates=6630, lr=5.73331e-06, gnorm=1.299, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30099
2022-10-12 01:59:15 - progress_bar.py[line:274] - INFO: epoch 001:   6648 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=6640, lr=5.74196e-06, gnorm=1.327, clip=90, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30111
2022-10-12 01:59:17 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 01:59:27 - progress_bar.py[line:274] - INFO: epoch 001:   6659 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=91.1, ups=0.83, wpb=109.8, bsz=40, num_updates=6650, lr=5.75061e-06, gnorm=1.289, clip=90, loss_scale=512, train_wall=12, gb_free=11.2, ema_decay=0.9999, wall=30123
2022-10-12 01:59:38 - progress_bar.py[line:274] - INFO: epoch 001:   6669 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.3, ups=0.92, wpb=109.4, bsz=40, num_updates=6660, lr=5.75925e-06, gnorm=1.195, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30134
2022-10-12 01:59:49 - progress_bar.py[line:274] - INFO: epoch 001:   6679 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=6670, lr=5.7679e-06, gnorm=1.359, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30145
2022-10-12 02:00:00 - progress_bar.py[line:274] - INFO: epoch 001:   6689 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=6680, lr=5.77655e-06, gnorm=1.24, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30156
2022-10-12 02:00:11 - progress_bar.py[line:274] - INFO: epoch 001:   6699 / 28910 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.8, ups=0.88, wpb=108.7, bsz=40, num_updates=6690, lr=5.7852e-06, gnorm=1.223, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30167
2022-10-12 02:00:22 - progress_bar.py[line:274] - INFO: epoch 001:   6709 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.8, ups=0.88, wpb=110.8, bsz=40, num_updates=6700, lr=5.79384e-06, gnorm=1.113, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30178
2022-10-12 02:00:34 - progress_bar.py[line:274] - INFO: epoch 001:   6719 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=6710, lr=5.80249e-06, gnorm=1.198, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30190
2022-10-12 02:00:44 - progress_bar.py[line:274] - INFO: epoch 001:   6729 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.8, ups=0.93, wpb=110.9, bsz=40, num_updates=6720, lr=5.81114e-06, gnorm=1.204, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30201
2022-10-12 02:00:56 - progress_bar.py[line:274] - INFO: epoch 001:   6739 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.4, ups=0.9, wpb=109.7, bsz=40, num_updates=6730, lr=5.81979e-06, gnorm=1.209, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30212
2022-10-12 02:01:07 - progress_bar.py[line:274] - INFO: epoch 001:   6749 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=6740, lr=5.82843e-06, gnorm=1.224, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30223
2022-10-12 02:01:18 - progress_bar.py[line:274] - INFO: epoch 001:   6759 / 28910 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.8, ups=0.92, wpb=110.4, bsz=40, num_updates=6750, lr=5.83708e-06, gnorm=1.268, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30234
2022-10-12 02:01:29 - progress_bar.py[line:274] - INFO: epoch 001:   6769 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=6760, lr=5.84573e-06, gnorm=1.046, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30245
2022-10-12 02:01:40 - progress_bar.py[line:274] - INFO: epoch 001:   6779 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.8, ups=0.91, wpb=110.2, bsz=40, num_updates=6770, lr=5.85438e-06, gnorm=1.229, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30256
2022-10-12 02:01:51 - progress_bar.py[line:274] - INFO: epoch 001:   6789 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.6, ups=0.9, wpb=108.5, bsz=40, num_updates=6780, lr=5.86302e-06, gnorm=1.316, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30267
2022-10-12 02:02:02 - progress_bar.py[line:274] - INFO: epoch 001:   6799 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=6790, lr=5.87167e-06, gnorm=1.26, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30278
2022-10-12 02:02:13 - progress_bar.py[line:274] - INFO: epoch 001:   6809 / 28910 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.88, wpb=110.2, bsz=40, num_updates=6800, lr=5.88032e-06, gnorm=1.391, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30290
2022-10-12 02:02:25 - progress_bar.py[line:274] - INFO: epoch 001:   6819 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.7, ups=0.89, wpb=110.2, bsz=40, num_updates=6810, lr=5.88897e-06, gnorm=1.22, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30301
2022-10-12 02:02:36 - progress_bar.py[line:274] - INFO: epoch 001:   6829 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.89, wpb=108.9, bsz=40, num_updates=6820, lr=5.89761e-06, gnorm=1.174, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30312
2022-10-12 02:02:47 - progress_bar.py[line:274] - INFO: epoch 001:   6839 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.5, ups=0.91, wpb=109.8, bsz=40, num_updates=6830, lr=5.90626e-06, gnorm=1.143, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30323
2022-10-12 02:02:58 - progress_bar.py[line:274] - INFO: epoch 001:   6849 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=6840, lr=5.91491e-06, gnorm=1.3, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30334
2022-10-12 02:03:09 - progress_bar.py[line:274] - INFO: epoch 001:   6859 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102, ups=0.92, wpb=110.3, bsz=40, num_updates=6850, lr=5.92356e-06, gnorm=1.1, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30345
2022-10-12 02:03:20 - progress_bar.py[line:274] - INFO: epoch 001:   6869 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.89, wpb=110.8, bsz=40, num_updates=6860, lr=5.9322e-06, gnorm=1.225, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30356
2022-10-12 02:03:31 - progress_bar.py[line:274] - INFO: epoch 001:   6879 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=6870, lr=5.94085e-06, gnorm=1.29, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30367
2022-10-12 02:03:42 - progress_bar.py[line:274] - INFO: epoch 001:   6889 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.8, ups=0.87, wpb=111.3, bsz=40, num_updates=6880, lr=5.9495e-06, gnorm=1.212, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30379
2022-10-12 02:03:54 - progress_bar.py[line:274] - INFO: epoch 001:   6899 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=6890, lr=5.95815e-06, gnorm=1.224, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30390
2022-10-12 02:04:05 - progress_bar.py[line:274] - INFO: epoch 001:   6909 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.4, ups=0.9, wpb=109.9, bsz=40, num_updates=6900, lr=5.96679e-06, gnorm=1.229, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30401
2022-10-12 02:04:16 - progress_bar.py[line:274] - INFO: epoch 001:   6919 / 28910 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=6910, lr=5.97544e-06, gnorm=1.349, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30412
2022-10-12 02:04:27 - progress_bar.py[line:274] - INFO: epoch 001:   6929 / 28910 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=6920, lr=5.98409e-06, gnorm=1.274, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30423
2022-10-12 02:04:38 - progress_bar.py[line:274] - INFO: epoch 001:   6939 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=6930, lr=5.99274e-06, gnorm=1.173, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30434
2022-10-12 02:04:49 - progress_bar.py[line:274] - INFO: epoch 001:   6949 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.91, wpb=110.6, bsz=40, num_updates=6940, lr=6.00138e-06, gnorm=1.204, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30445
2022-10-12 02:05:01 - progress_bar.py[line:274] - INFO: epoch 001:   6959 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96, ups=0.87, wpb=110.4, bsz=40, num_updates=6950, lr=6.01003e-06, gnorm=1.141, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30457
2022-10-12 02:05:12 - progress_bar.py[line:274] - INFO: epoch 001:   6969 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.2, ups=0.9, wpb=111.9, bsz=40, num_updates=6960, lr=6.01868e-06, gnorm=1.126, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30468
2022-10-12 02:05:23 - progress_bar.py[line:274] - INFO: epoch 001:   6979 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.5, ups=0.91, wpb=110.8, bsz=40, num_updates=6970, lr=6.02733e-06, gnorm=1.112, clip=70, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=30479
2022-10-12 02:05:34 - progress_bar.py[line:274] - INFO: epoch 001:   6989 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.89, wpb=111.1, bsz=40, num_updates=6980, lr=6.03597e-06, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30490
2022-10-12 02:05:45 - progress_bar.py[line:274] - INFO: epoch 001:   6999 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102, ups=0.93, wpb=109.9, bsz=40, num_updates=6990, lr=6.04462e-06, gnorm=1.209, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30501
2022-10-12 02:05:56 - progress_bar.py[line:274] - INFO: epoch 001:   7009 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=102, ups=0.92, wpb=111, bsz=40, num_updates=7000, lr=6.05327e-06, gnorm=1.114, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30512
2022-10-12 02:05:56 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 02:05:57 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 02:05:57 - train.py[line:551] - INFO: load:1.26 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 02:05:58 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 8.85 GiB already allocated; 3.89 GiB free; 33.21 GiB reserved in total by PyTorch)
2022-10-12 02:05:58 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-12 02:05:58 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9066 MB |   10291 MB |    4429 TB |    4429 TB |
|       from large pool |    8921 MB |   10145 MB |    4428 TB |    4428 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9066 MB |   10291 MB |    4429 TB |    4429 TB |
|       from large pool |    8921 MB |   10145 MB |    4428 TB |    4428 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   34008 MB |   36162 MB |  187134 MB |  153126 MB |
|       from large pool |   33862 MB |   36010 MB |  186848 MB |  152986 MB |
|       from small pool |     146 MB |     152 MB |     286 MB |     140 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24941 MB |   24941 MB |    4516 TB |    4516 TB |
|       from large pool |   24940 MB |   24940 MB |    4515 TB |    4515 TB |
|       from small pool |       1 MB |       1 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  204382 K  |  204379 K  |
|       from large pool |     563    |     575    |   66097 K  |   66096 K  |
|       from small pool |    3095    |    3114    |  138285 K  |  138282 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  204382 K  |  204379 K  |
|       from large pool |     563    |     575    |   66097 K  |   66096 K  |
|       from small pool |    3095    |    3114    |  138285 K  |  138282 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     163    |     168    |     511    |     348    |
|       from large pool |      90    |      92    |     368    |     278    |
|       from small pool |      73    |      76    |     143    |      70    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     103    |     109    |  146119 K  |  146119 K  |
|       from large pool |      66    |      66    |   24990 K  |   24990 K  |
|       from small pool |      37    |      45    |  121128 K  |  121128 K  |
|===========================================================================|

2022-10-12 02:05:58 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-12 02:08:30 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 02:08:30 - train.py[line:551] - INFO: load:1.28 valid_run:152.52 task_valid:148.30 collect_output:3.12
2022-10-12 02:10:58 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 02:10:58 - train.py[line:551] - INFO: load:1.30 valid_run:300.71 task_valid:291.84 collect_output:6.71
2022-10-12 02:13:30 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 02:13:30 - train.py[line:551] - INFO: load:1.33 valid_run:452.61 task_valid:435.15 collect_output:14.23
2022-10-12 02:15:59 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 02:15:59 - train.py[line:551] - INFO: load:1.35 valid_run:601.71 task_valid:580.36 collect_output:17.03
2022-10-12 02:18:32 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 02:18:32 - train.py[line:551] - INFO: load:1.38 valid_run:753.90 task_valid:728.21 collect_output:20.28
2022-10-12 02:21:03 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 02:21:03 - train.py[line:551] - INFO: load:1.40 valid_run:905.13 task_valid:873.71 collect_output:24.99
2022-10-12 02:23:36 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 02:23:36 - train.py[line:551] - INFO: load:1.43 valid_run:1058.00 task_valid:1019.80 collect_output:30.71
2022-10-12 02:26:07 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 02:26:07 - train.py[line:551] - INFO: load:1.45 valid_run:1208.82 task_valid:1161.13 collect_output:39.16
2022-10-12 02:28:36 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 02:28:36 - train.py[line:551] - INFO: load:1.48 valid_run:1358.18 task_valid:1306.13 collect_output:42.45
2022-10-12 02:31:04 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 02:31:04 - train.py[line:551] - INFO: load:1.50 valid_run:1506.32 task_valid:1449.31 collect_output:46.37
2022-10-12 02:33:34 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 02:33:34 - train.py[line:551] - INFO: load:1.53 valid_run:1655.99 task_valid:1594.66 collect_output:49.64
2022-10-12 02:36:04 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 02:36:04 - train.py[line:551] - INFO: load:1.55 valid_run:1805.88 task_valid:1739.86 collect_output:53.27
2022-10-12 02:38:34 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 02:38:34 - train.py[line:551] - INFO: load:1.58 valid_run:1955.41 task_valid:1881.66 collect_output:59.97
2022-10-12 02:41:04 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 02:41:04 - train.py[line:551] - INFO: load:1.60 valid_run:2105.80 task_valid:2027.10 collect_output:63.91
2022-10-12 02:43:34 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 02:43:34 - train.py[line:551] - INFO: load:1.63 valid_run:2255.62 task_valid:2173.58 collect_output:66.21
2022-10-12 02:46:04 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 02:46:04 - train.py[line:551] - INFO: load:1.65 valid_run:2405.27 task_valid:2317.63 collect_output:70.75
2022-10-12 02:48:35 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 02:48:35 - train.py[line:551] - INFO: load:1.67 valid_run:2556.56 task_valid:2463.21 collect_output:75.42
2022-10-12 02:51:06 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 02:51:06 - train.py[line:551] - INFO: load:1.70 valid_run:2707.15 task_valid:2610.57 collect_output:77.57
2022-10-12 02:53:34 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 02:53:34 - train.py[line:551] - INFO: load:1.72 valid_run:2855.01 task_valid:2752.32 collect_output:82.63
2022-10-12 02:56:04 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 02:56:04 - train.py[line:551] - INFO: load:1.75 valid_run:3004.92 task_valid:2897.40 collect_output:86.44
2022-10-12 02:58:35 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 02:58:35 - train.py[line:551] - INFO: load:1.77 valid_run:3156.41 task_valid:3042.12 collect_output:92.15
2022-10-12 03:01:05 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 03:01:05 - train.py[line:551] - INFO: load:1.80 valid_run:3305.83 task_valid:3187.17 collect_output:95.42
2022-10-12 03:03:35 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 03:03:35 - train.py[line:551] - INFO: load:1.83 valid_run:3456.68 task_valid:3333.61 collect_output:98.77
2022-10-12 03:06:07 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 03:06:07 - train.py[line:551] - INFO: load:1.86 valid_run:3608.10 task_valid:3480.44 collect_output:102.28

====================================================================================================
SGG eval:     R @ 50: 0.6758;     R @ 100: 0.6951;     R @ 500: 0.7159;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4889;    mR @ 100: 0.5009;    mR @ 500: 0.5531;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8902) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7500) (standing on:0.4193) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6758;     R @ 100: 0.6951;     R @ 500: 0.7159;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4889;    mR @ 100: 0.5009;    mR @ 500: 0.5531;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8902) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4032) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7500) (standing on:0.4193) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================

2022-10-12 03:08:38 - train.py[line:487] - INFO: 0.6950770817417876
2022-10-12 03:08:38 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 03:08:38 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.34 | loss_v1 0 | loss_v2 0 | nll_loss 0.182 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.695077 | ppl 1.13 | vqa_score 0.5867 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 7000 | best_R@100 0.70391
2022-10-12 03:08:38 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7000 updates
2022-10-12 03:08:38 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_7000.pt
2022-10-12 03:08:44 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_7000.pt
2022-10-12 03:08:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 0.6950770817417876) (writing took 8.54343185806647 seconds)
2022-10-12 03:08:58 - progress_bar.py[line:274] - INFO: epoch 001:   7019 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=0.3, ups=0, wpb=112, bsz=40, num_updates=7010, lr=6.06192e-06, gnorm=1.204, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34294
2022-10-12 03:09:09 - progress_bar.py[line:274] - INFO: epoch 001:   7029 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=7020, lr=6.07056e-06, gnorm=1.269, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34305
2022-10-12 03:09:20 - progress_bar.py[line:274] - INFO: epoch 001:   7039 / 28910 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.3, ups=0.89, wpb=107.9, bsz=40, num_updates=7030, lr=6.07921e-06, gnorm=1.306, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=34317
2022-10-12 03:09:32 - progress_bar.py[line:274] - INFO: epoch 001:   7049 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=7040, lr=6.08786e-06, gnorm=1.162, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=34328
2022-10-12 03:09:43 - progress_bar.py[line:274] - INFO: epoch 001:   7059 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.6, ups=0.92, wpb=111.3, bsz=40, num_updates=7050, lr=6.09651e-06, gnorm=1.124, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34339
2022-10-12 03:09:54 - progress_bar.py[line:274] - INFO: epoch 001:   7069 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=95.7, ups=0.87, wpb=109.9, bsz=40, num_updates=7060, lr=6.10515e-06, gnorm=1.114, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34350
2022-10-12 03:10:05 - progress_bar.py[line:274] - INFO: epoch 001:   7079 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.9, ups=0.9, wpb=112, bsz=40, num_updates=7070, lr=6.1138e-06, gnorm=1.178, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34361
2022-10-12 03:10:16 - progress_bar.py[line:274] - INFO: epoch 001:   7089 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.9, wpb=111.3, bsz=40, num_updates=7080, lr=6.12245e-06, gnorm=1.289, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34372
2022-10-12 03:10:28 - progress_bar.py[line:274] - INFO: epoch 001:   7099 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97, ups=0.88, wpb=110.9, bsz=40, num_updates=7090, lr=6.1311e-06, gnorm=1.289, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34384
2022-10-12 03:10:39 - progress_bar.py[line:274] - INFO: epoch 001:   7109 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.9, wpb=109.1, bsz=40, num_updates=7100, lr=6.13974e-06, gnorm=1.34, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34395
2022-10-12 03:10:50 - progress_bar.py[line:274] - INFO: epoch 001:   7119 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.2, ups=0.92, wpb=109.2, bsz=40, num_updates=7110, lr=6.14839e-06, gnorm=1.152, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34406
2022-10-12 03:11:01 - progress_bar.py[line:274] - INFO: epoch 001:   7129 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=7120, lr=6.15704e-06, gnorm=1.154, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34417
2022-10-12 03:11:12 - progress_bar.py[line:274] - INFO: epoch 001:   7139 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=7130, lr=6.16569e-06, gnorm=1.23, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34428
2022-10-12 03:11:23 - progress_bar.py[line:274] - INFO: epoch 001:   7149 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.8, ups=0.9, wpb=109.2, bsz=40, num_updates=7140, lr=6.17433e-06, gnorm=1.291, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34440
2022-10-12 03:11:34 - progress_bar.py[line:274] - INFO: epoch 001:   7159 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.4, ups=0.91, wpb=110.9, bsz=40, num_updates=7150, lr=6.18298e-06, gnorm=1.231, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34450
2022-10-12 03:11:46 - progress_bar.py[line:274] - INFO: epoch 001:   7169 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97, ups=0.88, wpb=110.3, bsz=40, num_updates=7160, lr=6.19163e-06, gnorm=1.241, clip=90, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34462
2022-10-12 03:11:57 - progress_bar.py[line:274] - INFO: epoch 001:   7179 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=7170, lr=6.20028e-06, gnorm=1.228, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34473
2022-10-12 03:12:08 - progress_bar.py[line:274] - INFO: epoch 001:   7189 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.1, ups=0.91, wpb=111.5, bsz=40, num_updates=7180, lr=6.20892e-06, gnorm=1.207, clip=70, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34484
2022-10-12 03:12:19 - progress_bar.py[line:274] - INFO: epoch 001:   7199 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=7190, lr=6.21757e-06, gnorm=1.16, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34496
2022-10-12 03:12:31 - progress_bar.py[line:274] - INFO: epoch 001:   7209 / 28910 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.8, ups=0.9, wpb=109.5, bsz=40, num_updates=7200, lr=6.22622e-06, gnorm=1.275, clip=90, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=34507
2022-10-12 03:12:41 - progress_bar.py[line:274] - INFO: epoch 001:   7219 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.9, ups=0.92, wpb=111.1, bsz=40, num_updates=7210, lr=6.23487e-06, gnorm=1.21, clip=80, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=34518
2022-10-12 03:12:43 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 03:12:53 - progress_bar.py[line:274] - INFO: epoch 001:   7230 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=92.5, ups=0.84, wpb=110.7, bsz=40, num_updates=7220, lr=6.24351e-06, gnorm=1.339, clip=100, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=34530
2022-10-12 03:13:05 - progress_bar.py[line:274] - INFO: epoch 001:   7240 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=7230, lr=6.25216e-06, gnorm=1.158, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34541
2022-10-12 03:13:15 - progress_bar.py[line:274] - INFO: epoch 001:   7250 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.9, ups=0.92, wpb=111.7, bsz=40, num_updates=7240, lr=6.26081e-06, gnorm=1.149, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34552
2022-10-12 03:13:27 - progress_bar.py[line:274] - INFO: epoch 001:   7260 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.9, ups=0.89, wpb=111.2, bsz=40, num_updates=7250, lr=6.26946e-06, gnorm=1.05, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34563
2022-10-12 03:13:38 - progress_bar.py[line:274] - INFO: epoch 001:   7270 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=7260, lr=6.2781e-06, gnorm=1.121, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34574
2022-10-12 03:13:49 - progress_bar.py[line:274] - INFO: epoch 001:   7280 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=7270, lr=6.28675e-06, gnorm=1.177, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34585
2022-10-12 03:14:00 - progress_bar.py[line:274] - INFO: epoch 001:   7290 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.91, wpb=109.2, bsz=40, num_updates=7280, lr=6.2954e-06, gnorm=1.157, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34596
2022-10-12 03:14:11 - progress_bar.py[line:274] - INFO: epoch 001:   7300 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.1, ups=0.87, wpb=110.2, bsz=40, num_updates=7290, lr=6.30405e-06, gnorm=1.197, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34608
2022-10-12 03:14:23 - progress_bar.py[line:274] - INFO: epoch 001:   7310 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=7300, lr=6.31269e-06, gnorm=1.184, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34619
2022-10-12 03:14:34 - progress_bar.py[line:274] - INFO: epoch 001:   7320 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.7, ups=0.9, wpb=110.6, bsz=40, num_updates=7310, lr=6.32134e-06, gnorm=1.086, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34630
2022-10-12 03:14:45 - progress_bar.py[line:274] - INFO: epoch 001:   7330 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.89, wpb=111.3, bsz=40, num_updates=7320, lr=6.32999e-06, gnorm=1.202, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34641
2022-10-12 03:14:56 - progress_bar.py[line:274] - INFO: epoch 001:   7340 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.1, ups=0.9, wpb=111.7, bsz=40, num_updates=7330, lr=6.33864e-06, gnorm=1.183, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=34652
2022-10-12 03:15:07 - progress_bar.py[line:274] - INFO: epoch 001:   7350 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=7340, lr=6.34728e-06, gnorm=1.158, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34663
2022-10-12 03:15:19 - progress_bar.py[line:274] - INFO: epoch 001:   7360 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.9, wpb=111.2, bsz=40, num_updates=7350, lr=6.35593e-06, gnorm=1.274, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34675
2022-10-12 03:15:30 - progress_bar.py[line:274] - INFO: epoch 001:   7370 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.9, ups=0.88, wpb=112.3, bsz=40, num_updates=7360, lr=6.36458e-06, gnorm=1.106, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34686
2022-10-12 03:15:41 - progress_bar.py[line:274] - INFO: epoch 001:   7380 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=7370, lr=6.37323e-06, gnorm=1.196, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34697
2022-10-12 03:15:52 - progress_bar.py[line:274] - INFO: epoch 001:   7390 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=7380, lr=6.38187e-06, gnorm=1.082, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34708
2022-10-12 03:16:04 - progress_bar.py[line:274] - INFO: epoch 001:   7400 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=7390, lr=6.39052e-06, gnorm=1.114, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34720
2022-10-12 03:16:15 - progress_bar.py[line:274] - INFO: epoch 001:   7410 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.9, wpb=110.9, bsz=40, num_updates=7400, lr=6.39917e-06, gnorm=1.293, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34731
2022-10-12 03:16:26 - progress_bar.py[line:274] - INFO: epoch 001:   7420 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=7410, lr=6.40782e-06, gnorm=1.082, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=34742
2022-10-12 03:16:37 - progress_bar.py[line:274] - INFO: epoch 001:   7430 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=7420, lr=6.41646e-06, gnorm=1.166, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34753
2022-10-12 03:16:48 - progress_bar.py[line:274] - INFO: epoch 001:   7440 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=7430, lr=6.42511e-06, gnorm=1.165, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34765
2022-10-12 03:17:00 - progress_bar.py[line:274] - INFO: epoch 001:   7450 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97, ups=0.89, wpb=109.3, bsz=40, num_updates=7440, lr=6.43376e-06, gnorm=1.174, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34776
2022-10-12 03:17:11 - progress_bar.py[line:274] - INFO: epoch 001:   7460 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101, ups=0.92, wpb=110.3, bsz=40, num_updates=7450, lr=6.44241e-06, gnorm=1.143, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34787
2022-10-12 03:17:22 - progress_bar.py[line:274] - INFO: epoch 001:   7470 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.9, wpb=110.5, bsz=40, num_updates=7460, lr=6.45105e-06, gnorm=1.2, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34798
2022-10-12 03:17:33 - progress_bar.py[line:274] - INFO: epoch 001:   7480 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=7470, lr=6.4597e-06, gnorm=1.046, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34809
2022-10-12 03:17:44 - progress_bar.py[line:274] - INFO: epoch 001:   7490 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=7480, lr=6.46835e-06, gnorm=1.287, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34820
2022-10-12 03:17:55 - progress_bar.py[line:274] - INFO: epoch 001:   7500 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103.1, ups=0.93, wpb=110.9, bsz=40, num_updates=7490, lr=6.477e-06, gnorm=1.149, clip=70, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=34831
2022-10-12 03:18:07 - progress_bar.py[line:274] - INFO: epoch 001:   7510 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=7500, lr=6.48565e-06, gnorm=1.143, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34843
2022-10-12 03:18:18 - progress_bar.py[line:274] - INFO: epoch 001:   7520 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=7510, lr=6.49429e-06, gnorm=1.168, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34854
2022-10-12 03:18:29 - progress_bar.py[line:274] - INFO: epoch 001:   7530 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=7520, lr=6.50294e-06, gnorm=1.251, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34865
2022-10-12 03:18:40 - progress_bar.py[line:274] - INFO: epoch 001:   7540 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.88, wpb=111.9, bsz=40, num_updates=7530, lr=6.51159e-06, gnorm=1.166, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34876
2022-10-12 03:18:51 - progress_bar.py[line:274] - INFO: epoch 001:   7550 / 28910 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.5, ups=0.93, wpb=109.5, bsz=40, num_updates=7540, lr=6.52024e-06, gnorm=1.143, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34887
2022-10-12 03:19:02 - progress_bar.py[line:274] - INFO: epoch 001:   7560 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97, ups=0.89, wpb=108.7, bsz=40, num_updates=7550, lr=6.52888e-06, gnorm=1.107, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34898
2022-10-12 03:19:13 - progress_bar.py[line:274] - INFO: epoch 001:   7570 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=7560, lr=6.53753e-06, gnorm=1.12, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34909
2022-10-12 03:19:24 - progress_bar.py[line:274] - INFO: epoch 001:   7580 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=7570, lr=6.54618e-06, gnorm=1.158, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34920
2022-10-12 03:19:35 - progress_bar.py[line:274] - INFO: epoch 001:   7590 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=7580, lr=6.55483e-06, gnorm=1.119, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34931
2022-10-12 03:19:47 - progress_bar.py[line:274] - INFO: epoch 001:   7600 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.8, ups=0.87, wpb=109.9, bsz=40, num_updates=7590, lr=6.56347e-06, gnorm=1.183, clip=80, loss_scale=512, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=34943
2022-10-12 03:19:58 - progress_bar.py[line:274] - INFO: epoch 001:   7610 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=7600, lr=6.57212e-06, gnorm=1.171, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=34954
2022-10-12 03:20:09 - progress_bar.py[line:274] - INFO: epoch 001:   7620 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=7610, lr=6.58077e-06, gnorm=1.109, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34965
2022-10-12 03:20:21 - progress_bar.py[line:274] - INFO: epoch 001:   7630 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.8, ups=0.89, wpb=109.9, bsz=40, num_updates=7620, lr=6.58942e-06, gnorm=1.107, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34977
2022-10-12 03:20:32 - progress_bar.py[line:274] - INFO: epoch 001:   7640 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.6, ups=0.89, wpb=111.5, bsz=40, num_updates=7630, lr=6.59806e-06, gnorm=1.082, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34988
2022-10-12 03:20:43 - progress_bar.py[line:274] - INFO: epoch 001:   7650 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101.4, ups=0.91, wpb=111.9, bsz=40, num_updates=7640, lr=6.60671e-06, gnorm=1.09, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34999
2022-10-12 03:20:54 - progress_bar.py[line:274] - INFO: epoch 001:   7660 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.1, ups=0.87, wpb=109.3, bsz=40, num_updates=7650, lr=6.61536e-06, gnorm=1.209, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35010
2022-10-12 03:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   7670 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=7660, lr=6.62401e-06, gnorm=1.037, clip=60, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=35021
2022-10-12 03:21:17 - progress_bar.py[line:274] - INFO: epoch 001:   7680 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=7670, lr=6.63265e-06, gnorm=1.089, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35033
2022-10-12 03:21:28 - progress_bar.py[line:274] - INFO: epoch 001:   7690 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=95.8, ups=0.87, wpb=109.7, bsz=40, num_updates=7680, lr=6.6413e-06, gnorm=1.056, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35044
2022-10-12 03:21:39 - progress_bar.py[line:274] - INFO: epoch 001:   7700 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=7690, lr=6.64995e-06, gnorm=1.154, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35055
2022-10-12 03:21:50 - progress_bar.py[line:274] - INFO: epoch 001:   7710 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.1, ups=0.9, wpb=109.7, bsz=40, num_updates=7700, lr=6.6586e-06, gnorm=1.218, clip=90, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=35066
2022-10-12 03:22:01 - progress_bar.py[line:274] - INFO: epoch 001:   7720 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=7710, lr=6.66724e-06, gnorm=1.184, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35077
2022-10-12 03:22:13 - progress_bar.py[line:274] - INFO: epoch 001:   7730 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.3, ups=0.87, wpb=110.5, bsz=40, num_updates=7720, lr=6.67589e-06, gnorm=1.157, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35089
2022-10-12 03:22:23 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 03:22:25 - progress_bar.py[line:274] - INFO: epoch 001:   7741 / 28910 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=90.9, ups=0.82, wpb=111.1, bsz=40, num_updates=7730, lr=6.68454e-06, gnorm=1.028, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=35101
2022-10-12 03:22:36 - progress_bar.py[line:274] - INFO: epoch 001:   7751 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=7740, lr=6.69319e-06, gnorm=1.083, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35112
2022-10-12 03:22:47 - progress_bar.py[line:274] - INFO: epoch 001:   7761 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.9, ups=0.92, wpb=111.9, bsz=40, num_updates=7750, lr=6.70183e-06, gnorm=1.121, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35123
2022-10-12 03:22:58 - progress_bar.py[line:274] - INFO: epoch 001:   7771 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.2, ups=0.9, wpb=111.8, bsz=40, num_updates=7760, lr=6.71048e-06, gnorm=1.131, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35134
2022-10-12 03:23:10 - progress_bar.py[line:274] - INFO: epoch 001:   7781 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=94.3, ups=0.87, wpb=108.4, bsz=40, num_updates=7770, lr=6.71913e-06, gnorm=1.167, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35146
2022-10-12 03:23:20 - progress_bar.py[line:274] - INFO: epoch 001:   7791 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.8, ups=0.92, wpb=109.8, bsz=40, num_updates=7780, lr=6.72778e-06, gnorm=1.111, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35157
2022-10-12 03:23:32 - progress_bar.py[line:274] - INFO: epoch 001:   7801 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.4, ups=0.89, wpb=111, bsz=40, num_updates=7790, lr=6.73642e-06, gnorm=1.072, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35168
2022-10-12 03:23:43 - progress_bar.py[line:274] - INFO: epoch 001:   7811 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=7800, lr=6.74507e-06, gnorm=1.172, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35179
2022-10-12 03:23:54 - progress_bar.py[line:274] - INFO: epoch 001:   7821 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.8, ups=0.93, wpb=111, bsz=40, num_updates=7810, lr=6.75372e-06, gnorm=1.188, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35190
2022-10-12 03:24:05 - progress_bar.py[line:274] - INFO: epoch 001:   7831 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.7, ups=0.87, wpb=112, bsz=40, num_updates=7820, lr=6.76237e-06, gnorm=1.33, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35201
2022-10-12 03:24:16 - progress_bar.py[line:274] - INFO: epoch 001:   7841 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=7830, lr=6.77101e-06, gnorm=1.232, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35212
2022-10-12 03:24:28 - progress_bar.py[line:274] - INFO: epoch 001:   7851 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=95.5, ups=0.87, wpb=109.8, bsz=40, num_updates=7840, lr=6.77966e-06, gnorm=1.153, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35224
2022-10-12 03:24:39 - progress_bar.py[line:274] - INFO: epoch 001:   7861 / 28910 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=7850, lr=6.78831e-06, gnorm=1.233, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35235
2022-10-12 03:24:51 - progress_bar.py[line:274] - INFO: epoch 001:   7871 / 28910 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95, ups=0.87, wpb=109.1, bsz=40, num_updates=7860, lr=6.79696e-06, gnorm=1.178, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35247
2022-10-12 03:25:02 - progress_bar.py[line:274] - INFO: epoch 001:   7881 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96, ups=0.88, wpb=108.7, bsz=40, num_updates=7870, lr=6.8056e-06, gnorm=1.207, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35258
2022-10-12 03:25:13 - progress_bar.py[line:274] - INFO: epoch 001:   7891 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.1, ups=0.93, wpb=110.1, bsz=40, num_updates=7880, lr=6.81425e-06, gnorm=1.33, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35269
2022-10-12 03:25:24 - progress_bar.py[line:274] - INFO: epoch 001:   7901 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=7890, lr=6.8229e-06, gnorm=1.086, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35280
2022-10-12 03:25:35 - progress_bar.py[line:274] - INFO: epoch 001:   7911 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.5, ups=0.91, wpb=109.7, bsz=40, num_updates=7900, lr=6.83155e-06, gnorm=1.225, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35291
2022-10-12 03:25:46 - progress_bar.py[line:274] - INFO: epoch 001:   7921 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100, ups=0.9, wpb=111.1, bsz=40, num_updates=7910, lr=6.84019e-06, gnorm=1.257, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35302
2022-10-12 03:25:57 - progress_bar.py[line:274] - INFO: epoch 001:   7931 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=7920, lr=6.84884e-06, gnorm=1.093, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35313
2022-10-12 03:26:08 - progress_bar.py[line:274] - INFO: epoch 001:   7941 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=7930, lr=6.85749e-06, gnorm=1.179, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35324
2022-10-12 03:26:20 - progress_bar.py[line:274] - INFO: epoch 001:   7951 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=7940, lr=6.86614e-06, gnorm=1.153, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35336
2022-10-12 03:26:30 - progress_bar.py[line:274] - INFO: epoch 001:   7961 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100, ups=0.92, wpb=109.3, bsz=40, num_updates=7950, lr=6.87478e-06, gnorm=1.177, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35347
2022-10-12 03:26:42 - progress_bar.py[line:274] - INFO: epoch 001:   7971 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=7960, lr=6.88343e-06, gnorm=1.177, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=35358
2022-10-12 03:26:52 - progress_bar.py[line:274] - INFO: epoch 001:   7981 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.9, ups=0.91, wpb=111.5, bsz=40, num_updates=7970, lr=6.89208e-06, gnorm=1.169, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35369
2022-10-12 03:27:04 - progress_bar.py[line:274] - INFO: epoch 001:   7991 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=7980, lr=6.90073e-06, gnorm=1.15, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35380
2022-10-12 03:27:15 - progress_bar.py[line:274] - INFO: epoch 001:   8001 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.9, ups=0.88, wpb=111.2, bsz=40, num_updates=7990, lr=6.90937e-06, gnorm=1.216, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35391
2022-10-12 03:27:26 - progress_bar.py[line:274] - INFO: epoch 001:   8011 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=8000, lr=6.91802e-06, gnorm=1.226, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35402
2022-10-12 03:27:26 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 03:27:27 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 03:27:27 - train.py[line:551] - INFO: load:1.11 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 03:27:28 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 8.84 GiB already allocated; 4.80 GiB free; 32.30 GiB reserved in total by PyTorch)
2022-10-12 03:27:28 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-12 03:27:28 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9051 MB |   10276 MB |    5139 TB |    5139 TB |
|       from large pool |    8906 MB |   10131 MB |    5138 TB |    5138 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9051 MB |   10276 MB |    5139 TB |    5139 TB |
|       from large pool |    8906 MB |   10131 MB |    5138 TB |    5138 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   33076 MB |   33076 MB |  193446 MB |  160370 MB |
|       from large pool |   32930 MB |   32930 MB |  193136 MB |  160206 MB |
|       from small pool |     146 MB |     146 MB |     310 MB |     164 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24024 MB |   28554 MB |    5575 TB |    5575 TB |
|       from large pool |   24023 MB |   28552 MB |    5573 TB |    5573 TB |
|       from small pool |       1 MB |       1 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  237564 K  |  237561 K  |
|       from large pool |     563    |     575    |   76665 K  |   76665 K  |
|       from small pool |    3095    |    3114    |  160899 K  |  160896 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  237564 K  |  237561 K  |
|       from large pool |     563    |     575    |   76665 K  |   76665 K  |
|       from small pool |    3095    |    3114    |  160899 K  |  160896 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     138    |     138    |     524    |     386    |
|       from large pool |      65    |      65    |     369    |     304    |
|       from small pool |      73    |      73    |     155    |      82    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      89    |  171973 K  |  171973 K  |
|       from large pool |      44    |      47    |   31205 K  |   31205 K  |
|       from small pool |      40    |      47    |  140767 K  |  140767 K  |
|===========================================================================|

2022-10-12 03:27:28 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-12 03:30:00 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 03:30:00 - train.py[line:551] - INFO: load:1.14 valid_run:152.41 task_valid:148.18 collect_output:3.15
2022-10-12 03:32:28 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 03:32:28 - train.py[line:551] - INFO: load:1.16 valid_run:300.90 task_valid:291.63 collect_output:6.99
2022-10-12 03:35:01 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 03:35:01 - train.py[line:551] - INFO: load:1.19 valid_run:453.67 task_valid:435.64 collect_output:14.49
2022-10-12 03:37:31 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 03:37:31 - train.py[line:551] - INFO: load:1.21 valid_run:603.03 task_valid:580.81 collect_output:17.59
2022-10-12 03:40:03 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 03:40:03 - train.py[line:551] - INFO: load:1.24 valid_run:755.75 task_valid:728.86 collect_output:21.07
2022-10-12 03:42:35 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 03:42:35 - train.py[line:551] - INFO: load:1.26 valid_run:907.70 task_valid:874.81 collect_output:25.97
2022-10-12 03:45:09 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 03:45:09 - train.py[line:551] - INFO: load:1.29 valid_run:1060.80 task_valid:1021.18 collect_output:31.55
2022-10-12 03:47:40 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 03:47:40 - train.py[line:551] - INFO: load:1.32 valid_run:1212.06 task_valid:1162.98 collect_output:39.90
2022-10-12 03:50:10 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 03:50:10 - train.py[line:551] - INFO: load:1.34 valid_run:1362.32 task_valid:1308.74 collect_output:43.21
2022-10-12 03:52:38 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 03:52:38 - train.py[line:551] - INFO: load:1.37 valid_run:1510.45 task_valid:1451.93 collect_output:47.09
2022-10-12 03:55:08 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 03:55:08 - train.py[line:551] - INFO: load:1.39 valid_run:1660.06 task_valid:1597.04 collect_output:50.54
2022-10-12 03:57:38 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 03:57:38 - train.py[line:551] - INFO: load:1.42 valid_run:1809.54 task_valid:1741.90 collect_output:54.11
2022-10-12 04:00:07 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 04:00:07 - train.py[line:551] - INFO: load:1.44 valid_run:1959.16 task_valid:1884.09 collect_output:60.49
2022-10-12 04:02:38 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 04:02:38 - train.py[line:551] - INFO: load:1.47 valid_run:2109.37 task_valid:2029.61 collect_output:64.18
2022-10-12 04:05:07 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 04:05:07 - train.py[line:551] - INFO: load:1.49 valid_run:2259.29 task_valid:2176.12 collect_output:66.54
2022-10-12 04:07:37 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 04:07:37 - train.py[line:551] - INFO: load:1.52 valid_run:2409.00 task_valid:2320.33 collect_output:71.01
2022-10-12 04:10:08 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 04:10:08 - train.py[line:551] - INFO: load:1.54 valid_run:2560.02 task_valid:2465.74 collect_output:75.61
2022-10-12 04:12:39 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 04:12:39 - train.py[line:551] - INFO: load:1.57 valid_run:2710.32 task_valid:2612.55 collect_output:78.11
2022-10-12 04:15:07 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 04:15:07 - train.py[line:551] - INFO: load:1.59 valid_run:2858.17 task_valid:2754.01 collect_output:83.47
2022-10-12 04:17:37 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 04:17:37 - train.py[line:551] - INFO: load:1.62 valid_run:3008.33 task_valid:2899.37 collect_output:87.21
2022-10-12 04:20:08 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 04:20:08 - train.py[line:551] - INFO: load:1.64 valid_run:3159.84 task_valid:3043.88 collect_output:93.14
2022-10-12 04:22:38 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 04:22:38 - train.py[line:551] - INFO: load:1.67 valid_run:3308.92 task_valid:3188.38 collect_output:96.70
2022-10-12 04:25:09 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 04:25:09 - train.py[line:551] - INFO: load:1.69 valid_run:3459.85 task_valid:3334.82 collect_output:100.13
2022-10-12 04:27:40 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 04:27:40 - train.py[line:551] - INFO: load:1.72 valid_run:3611.05 task_valid:3481.46 collect_output:103.64

====================================================================================================
SGG eval:     R @ 50: 0.6744;     R @ 100: 0.6941;     R @ 500: 0.7139;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4869;    mR @ 100: 0.5000;    mR @ 500: 0.5521;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8659) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7466) (standing on:0.4343) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6744;     R @ 100: 0.6941;     R @ 500: 0.7139;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4869;    mR @ 100: 0.5000;    mR @ 500: 0.5521;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8659) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7466) (standing on:0.4343) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================

2022-10-12 04:30:11 - train.py[line:487] - INFO: 0.6940770817417876
2022-10-12 04:30:11 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 04:30:11 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.338 | loss_v1 0 | loss_v2 0 | nll_loss 0.182 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.694077 | ppl 1.13 | vqa_score 0.5957 | wps 119.2 | wpb 89.9 | bsz 30 | num_updates 8000 | best_R@100 0.70391
2022-10-12 04:30:11 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2022-10-12 04:30:11 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_8000.pt
2022-10-12 04:30:17 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_8000.pt
2022-10-12 04:30:19 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.6940770817417876) (writing took 8.59958447702229 seconds)
2022-10-12 04:30:31 - progress_bar.py[line:274] - INFO: epoch 001:   8021 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=0.3, ups=0, wpb=110.9, bsz=40, num_updates=8010, lr=6.92667e-06, gnorm=1.246, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39187
2022-10-12 04:30:42 - progress_bar.py[line:274] - INFO: epoch 001:   8031 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=94.5, ups=0.87, wpb=108.3, bsz=40, num_updates=8020, lr=6.93532e-06, gnorm=1.267, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39198
2022-10-12 04:30:53 - progress_bar.py[line:274] - INFO: epoch 001:   8041 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.5, ups=0.9, wpb=110.5, bsz=40, num_updates=8030, lr=6.94396e-06, gnorm=1.142, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39209
2022-10-12 04:31:04 - progress_bar.py[line:274] - INFO: epoch 001:   8051 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=8040, lr=6.95261e-06, gnorm=1.085, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39221
2022-10-12 04:31:16 - progress_bar.py[line:274] - INFO: epoch 001:   8061 / 28910 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.88, wpb=110.3, bsz=40, num_updates=8050, lr=6.96126e-06, gnorm=1.26, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39232
2022-10-12 04:31:27 - progress_bar.py[line:274] - INFO: epoch 001:   8071 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=8060, lr=6.96991e-06, gnorm=1.171, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39243
2022-10-12 04:31:38 - progress_bar.py[line:274] - INFO: epoch 001:   8081 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.1, ups=0.91, wpb=110.6, bsz=40, num_updates=8070, lr=6.97855e-06, gnorm=1.217, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39254
2022-10-12 04:31:49 - progress_bar.py[line:274] - INFO: epoch 001:   8091 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=8080, lr=6.9872e-06, gnorm=1.222, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39265
2022-10-12 04:32:01 - progress_bar.py[line:274] - INFO: epoch 001:   8101 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=8090, lr=6.99585e-06, gnorm=1.297, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39277
2022-10-12 04:32:11 - progress_bar.py[line:274] - INFO: epoch 001:   8111 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=104.6, ups=0.94, wpb=110.9, bsz=40, num_updates=8100, lr=7.0045e-06, gnorm=1.227, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39287
2022-10-12 04:32:22 - progress_bar.py[line:274] - INFO: epoch 001:   8121 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.3, ups=0.93, wpb=109.2, bsz=40, num_updates=8110, lr=7.01314e-06, gnorm=1.306, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39298
2022-10-12 04:32:33 - progress_bar.py[line:274] - INFO: epoch 001:   8131 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.7, ups=0.91, wpb=110.8, bsz=40, num_updates=8120, lr=7.02179e-06, gnorm=1.205, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39309
2022-10-12 04:32:44 - progress_bar.py[line:274] - INFO: epoch 001:   8141 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.4, ups=0.87, wpb=110.6, bsz=40, num_updates=8130, lr=7.03044e-06, gnorm=1.236, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39321
2022-10-12 04:32:56 - progress_bar.py[line:274] - INFO: epoch 001:   8151 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=8140, lr=7.03909e-06, gnorm=1.261, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39332
2022-10-12 04:33:07 - progress_bar.py[line:274] - INFO: epoch 001:   8161 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100, ups=0.92, wpb=109.3, bsz=40, num_updates=8150, lr=7.04773e-06, gnorm=1.136, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39343
2022-10-12 04:33:18 - progress_bar.py[line:274] - INFO: epoch 001:   8171 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.1, ups=0.89, wpb=108.8, bsz=40, num_updates=8160, lr=7.05638e-06, gnorm=1.079, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39354
2022-10-12 04:33:29 - progress_bar.py[line:274] - INFO: epoch 001:   8181 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.9, wpb=109.7, bsz=40, num_updates=8170, lr=7.06503e-06, gnorm=1.283, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39365
2022-10-12 04:33:40 - progress_bar.py[line:274] - INFO: epoch 001:   8191 / 28910 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.2, ups=0.92, wpb=110.6, bsz=40, num_updates=8180, lr=7.07368e-06, gnorm=1.111, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39376
2022-10-12 04:33:51 - progress_bar.py[line:274] - INFO: epoch 001:   8201 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.8, ups=0.9, wpb=109.5, bsz=40, num_updates=8190, lr=7.08232e-06, gnorm=1.263, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39387
2022-10-12 04:34:02 - progress_bar.py[line:274] - INFO: epoch 001:   8211 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=8200, lr=7.09097e-06, gnorm=1.123, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39398
2022-10-12 04:34:13 - progress_bar.py[line:274] - INFO: epoch 001:   8221 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.6, ups=0.92, wpb=111.7, bsz=40, num_updates=8210, lr=7.09962e-06, gnorm=1.188, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39409
2022-10-12 04:34:25 - progress_bar.py[line:274] - INFO: epoch 001:   8231 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.87, wpb=111.4, bsz=40, num_updates=8220, lr=7.10827e-06, gnorm=1.264, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39421
2022-10-12 04:34:36 - progress_bar.py[line:274] - INFO: epoch 001:   8241 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=8230, lr=7.11691e-06, gnorm=1.219, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39432
2022-10-12 04:34:47 - progress_bar.py[line:274] - INFO: epoch 001:   8251 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98, ups=0.88, wpb=111.2, bsz=40, num_updates=8240, lr=7.12556e-06, gnorm=1.162, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39443
2022-10-12 04:34:59 - progress_bar.py[line:274] - INFO: epoch 001:   8261 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.8, ups=0.88, wpb=110.7, bsz=40, num_updates=8250, lr=7.13421e-06, gnorm=1.074, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39455
2022-10-12 04:35:10 - progress_bar.py[line:274] - INFO: epoch 001:   8271 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=8260, lr=7.14286e-06, gnorm=1.079, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39466
2022-10-12 04:35:21 - progress_bar.py[line:274] - INFO: epoch 001:   8281 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.4, ups=0.91, wpb=109.7, bsz=40, num_updates=8270, lr=7.1515e-06, gnorm=1.239, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39477
2022-10-12 04:35:32 - progress_bar.py[line:274] - INFO: epoch 001:   8291 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96, ups=0.88, wpb=109.3, bsz=40, num_updates=8280, lr=7.16015e-06, gnorm=1.283, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39488
2022-10-12 04:35:43 - progress_bar.py[line:274] - INFO: epoch 001:   8301 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.8, ups=0.89, wpb=109.9, bsz=40, num_updates=8290, lr=7.1688e-06, gnorm=1.219, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39499
2022-10-12 04:35:44 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 04:35:55 - progress_bar.py[line:274] - INFO: epoch 001:   8312 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=93.2, ups=0.84, wpb=111.3, bsz=40, num_updates=8300, lr=7.17745e-06, gnorm=1.326, clip=90, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39511
2022-10-12 04:36:07 - progress_bar.py[line:274] - INFO: epoch 001:   8322 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.2, ups=0.89, wpb=109.1, bsz=40, num_updates=8310, lr=7.18609e-06, gnorm=1.197, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39523
2022-10-12 04:36:18 - progress_bar.py[line:274] - INFO: epoch 001:   8332 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.4, ups=0.88, wpb=111, bsz=40, num_updates=8320, lr=7.19474e-06, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=39534
2022-10-12 04:36:29 - progress_bar.py[line:274] - INFO: epoch 001:   8342 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=8330, lr=7.20339e-06, gnorm=1.035, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39545
2022-10-12 04:36:40 - progress_bar.py[line:274] - INFO: epoch 001:   8352 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.7, ups=0.91, wpb=111.2, bsz=40, num_updates=8340, lr=7.21204e-06, gnorm=1.1, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39556
2022-10-12 04:36:52 - progress_bar.py[line:274] - INFO: epoch 001:   8362 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=8350, lr=7.22068e-06, gnorm=1.143, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39568
2022-10-12 04:37:03 - progress_bar.py[line:274] - INFO: epoch 001:   8372 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.2, ups=0.88, wpb=109, bsz=40, num_updates=8360, lr=7.22933e-06, gnorm=1.28, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39580
2022-10-12 04:37:14 - progress_bar.py[line:274] - INFO: epoch 001:   8382 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.91, wpb=109.9, bsz=40, num_updates=8370, lr=7.23798e-06, gnorm=1.085, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39591
2022-10-12 04:37:26 - progress_bar.py[line:274] - INFO: epoch 001:   8392 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=8380, lr=7.24663e-06, gnorm=1.152, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39602
2022-10-12 04:37:37 - progress_bar.py[line:274] - INFO: epoch 001:   8402 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.92, wpb=107.8, bsz=40, num_updates=8390, lr=7.25527e-06, gnorm=1.248, clip=80, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39613
2022-10-12 04:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   8412 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.92, wpb=109.8, bsz=40, num_updates=8400, lr=7.26392e-06, gnorm=1.123, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39624
2022-10-12 04:37:59 - progress_bar.py[line:274] - INFO: epoch 001:   8422 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.3, ups=0.87, wpb=110.5, bsz=40, num_updates=8410, lr=7.27257e-06, gnorm=1.212, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39635
2022-10-12 04:38:10 - progress_bar.py[line:274] - INFO: epoch 001:   8432 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.4, ups=0.91, wpb=109.9, bsz=40, num_updates=8420, lr=7.28122e-06, gnorm=1.227, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39646
2022-10-12 04:38:21 - progress_bar.py[line:274] - INFO: epoch 001:   8442 / 28910 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.2, ups=0.92, wpb=109.4, bsz=40, num_updates=8430, lr=7.28987e-06, gnorm=1.316, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39657
2022-10-12 04:38:32 - progress_bar.py[line:274] - INFO: epoch 001:   8452 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.5, ups=0.89, wpb=112, bsz=40, num_updates=8440, lr=7.29851e-06, gnorm=1.091, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39668
2022-10-12 04:38:44 - progress_bar.py[line:274] - INFO: epoch 001:   8462 / 28910 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.5, ups=0.89, wpb=108.1, bsz=40, num_updates=8450, lr=7.30716e-06, gnorm=1.274, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39680
2022-10-12 04:38:55 - progress_bar.py[line:274] - INFO: epoch 001:   8472 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.8, ups=0.88, wpb=110.4, bsz=40, num_updates=8460, lr=7.31581e-06, gnorm=1.255, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39691
2022-10-12 04:39:06 - progress_bar.py[line:274] - INFO: epoch 001:   8482 / 28910 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102, ups=0.91, wpb=111.9, bsz=40, num_updates=8470, lr=7.32446e-06, gnorm=1.207, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39702
2022-10-12 04:39:17 - progress_bar.py[line:274] - INFO: epoch 001:   8492 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.7, ups=0.88, wpb=109.4, bsz=40, num_updates=8480, lr=7.3331e-06, gnorm=1.132, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39713
2022-10-12 04:39:29 - progress_bar.py[line:274] - INFO: epoch 001:   8502 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.8, ups=0.88, wpb=112.2, bsz=40, num_updates=8490, lr=7.34175e-06, gnorm=1.185, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39725
2022-10-12 04:39:40 - progress_bar.py[line:274] - INFO: epoch 001:   8512 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.8, ups=0.87, wpb=111.1, bsz=40, num_updates=8500, lr=7.3504e-06, gnorm=1.143, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39736
2022-10-12 04:39:51 - progress_bar.py[line:274] - INFO: epoch 001:   8522 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=104.3, ups=0.94, wpb=111, bsz=40, num_updates=8510, lr=7.35905e-06, gnorm=1.136, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39747
2022-10-12 04:40:03 - progress_bar.py[line:274] - INFO: epoch 001:   8532 / 28910 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.91, wpb=110, bsz=40, num_updates=8520, lr=7.36769e-06, gnorm=1.241, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39758
2022-10-12 04:40:14 - progress_bar.py[line:274] - INFO: epoch 001:   8542 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.2, ups=0.91, wpb=111.3, bsz=40, num_updates=8530, lr=7.37634e-06, gnorm=1.171, clip=50, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=39770
2022-10-12 04:40:25 - progress_bar.py[line:274] - INFO: epoch 001:   8552 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=8540, lr=7.38499e-06, gnorm=1.122, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39781
2022-10-12 04:40:37 - progress_bar.py[line:274] - INFO: epoch 001:   8562 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=8550, lr=7.39364e-06, gnorm=1.123, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39793
2022-10-12 04:40:48 - progress_bar.py[line:274] - INFO: epoch 001:   8572 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95.2, ups=0.87, wpb=109.3, bsz=40, num_updates=8560, lr=7.40228e-06, gnorm=1.182, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39804
2022-10-12 04:40:59 - progress_bar.py[line:274] - INFO: epoch 001:   8582 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=8570, lr=7.41093e-06, gnorm=1.213, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39816
2022-10-12 04:41:10 - progress_bar.py[line:274] - INFO: epoch 001:   8592 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.2, ups=0.92, wpb=111.4, bsz=40, num_updates=8580, lr=7.41958e-06, gnorm=1.172, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39826
2022-10-12 04:41:22 - progress_bar.py[line:274] - INFO: epoch 001:   8602 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.6, ups=0.89, wpb=110.3, bsz=40, num_updates=8590, lr=7.42823e-06, gnorm=1.093, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39838
2022-10-12 04:41:33 - progress_bar.py[line:274] - INFO: epoch 001:   8612 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.5, ups=0.91, wpb=109.5, bsz=40, num_updates=8600, lr=7.43687e-06, gnorm=1.12, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39849
2022-10-12 04:41:44 - progress_bar.py[line:274] - INFO: epoch 001:   8622 / 28910 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=8610, lr=7.44552e-06, gnorm=1.091, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39860
2022-10-12 04:41:55 - progress_bar.py[line:274] - INFO: epoch 001:   8632 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=8620, lr=7.45417e-06, gnorm=1.092, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39871
2022-10-12 04:42:06 - progress_bar.py[line:274] - INFO: epoch 001:   8642 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.8, ups=0.89, wpb=110.4, bsz=40, num_updates=8630, lr=7.46282e-06, gnorm=1.069, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39882
2022-10-12 04:42:18 - progress_bar.py[line:274] - INFO: epoch 001:   8652 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99, ups=0.89, wpb=111.5, bsz=40, num_updates=8640, lr=7.47146e-06, gnorm=1.118, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39894
2022-10-12 04:42:29 - progress_bar.py[line:274] - INFO: epoch 001:   8662 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=8650, lr=7.48011e-06, gnorm=1.186, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39905
2022-10-12 04:42:40 - progress_bar.py[line:274] - INFO: epoch 001:   8672 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.9, wpb=110.2, bsz=40, num_updates=8660, lr=7.48876e-06, gnorm=1.123, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39916
2022-10-12 04:42:51 - progress_bar.py[line:274] - INFO: epoch 001:   8682 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.1, ups=0.9, wpb=110.1, bsz=40, num_updates=8670, lr=7.49741e-06, gnorm=1.18, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39927
2022-10-12 04:43:02 - progress_bar.py[line:274] - INFO: epoch 001:   8692 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.9, ups=0.93, wpb=111, bsz=40, num_updates=8680, lr=7.50605e-06, gnorm=1.094, clip=60, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=39938
2022-10-12 04:43:13 - progress_bar.py[line:274] - INFO: epoch 001:   8702 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.3, ups=0.89, wpb=109.6, bsz=40, num_updates=8690, lr=7.5147e-06, gnorm=1.144, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39949
2022-10-12 04:43:24 - progress_bar.py[line:274] - INFO: epoch 001:   8712 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=95.9, ups=0.88, wpb=108.5, bsz=40, num_updates=8700, lr=7.52335e-06, gnorm=1.139, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39961
2022-10-12 04:43:36 - progress_bar.py[line:274] - INFO: epoch 001:   8722 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=8710, lr=7.532e-06, gnorm=1.115, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39972
2022-10-12 04:43:46 - progress_bar.py[line:274] - INFO: epoch 001:   8732 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103.1, ups=0.94, wpb=109.3, bsz=40, num_updates=8720, lr=7.54064e-06, gnorm=1.173, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39982
2022-10-12 04:43:57 - progress_bar.py[line:274] - INFO: epoch 001:   8742 / 28910 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=8730, lr=7.54929e-06, gnorm=0.997, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39993
2022-10-12 04:44:09 - progress_bar.py[line:274] - INFO: epoch 001:   8752 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.3, ups=0.89, wpb=109.3, bsz=40, num_updates=8740, lr=7.55794e-06, gnorm=1.245, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40005
2022-10-12 04:44:20 - progress_bar.py[line:274] - INFO: epoch 001:   8762 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.1, ups=0.89, wpb=111.3, bsz=40, num_updates=8750, lr=7.56659e-06, gnorm=1.214, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40016
2022-10-12 04:44:31 - progress_bar.py[line:274] - INFO: epoch 001:   8772 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.8, ups=0.87, wpb=109.8, bsz=40, num_updates=8760, lr=7.57523e-06, gnorm=1.306, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40027
2022-10-12 04:44:42 - progress_bar.py[line:274] - INFO: epoch 001:   8782 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.3, ups=0.89, wpb=109, bsz=40, num_updates=8770, lr=7.58388e-06, gnorm=1.298, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40039
2022-10-12 04:44:54 - progress_bar.py[line:274] - INFO: epoch 001:   8792 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.9, ups=0.89, wpb=110.5, bsz=40, num_updates=8780, lr=7.59253e-06, gnorm=1.295, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40050
2022-10-12 04:45:05 - progress_bar.py[line:274] - INFO: epoch 001:   8802 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.3, ups=0.89, wpb=108.3, bsz=40, num_updates=8790, lr=7.60118e-06, gnorm=1.232, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40061
2022-10-12 04:45:16 - progress_bar.py[line:274] - INFO: epoch 001:   8812 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.88, wpb=111, bsz=40, num_updates=8800, lr=7.60982e-06, gnorm=1.191, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40072
2022-10-12 04:45:24 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 04:45:28 - progress_bar.py[line:274] - INFO: epoch 001:   8823 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=92.5, ups=0.84, wpb=110.4, bsz=40, num_updates=8810, lr=7.61847e-06, gnorm=1.157, clip=90, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=40084
2022-10-12 04:45:39 - progress_bar.py[line:274] - INFO: epoch 001:   8833 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.6, ups=0.93, wpb=110.7, bsz=40, num_updates=8820, lr=7.62712e-06, gnorm=1.087, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40095
2022-10-12 04:45:50 - progress_bar.py[line:274] - INFO: epoch 001:   8843 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95.5, ups=0.87, wpb=109.6, bsz=40, num_updates=8830, lr=7.63577e-06, gnorm=1.243, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40107
2022-10-12 04:46:02 - progress_bar.py[line:274] - INFO: epoch 001:   8853 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.5, ups=0.88, wpb=109.5, bsz=40, num_updates=8840, lr=7.64441e-06, gnorm=1.111, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40118
2022-10-12 04:46:13 - progress_bar.py[line:274] - INFO: epoch 001:   8863 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=8850, lr=7.65306e-06, gnorm=1.244, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40129
2022-10-12 04:46:24 - progress_bar.py[line:274] - INFO: epoch 001:   8873 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.9, ups=0.88, wpb=110.2, bsz=40, num_updates=8860, lr=7.66171e-06, gnorm=1.187, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40140
2022-10-12 04:46:36 - progress_bar.py[line:274] - INFO: epoch 001:   8883 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96, ups=0.88, wpb=109, bsz=40, num_updates=8870, lr=7.67036e-06, gnorm=1.323, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40152
2022-10-12 04:46:47 - progress_bar.py[line:274] - INFO: epoch 001:   8893 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.1, ups=0.89, wpb=111.3, bsz=40, num_updates=8880, lr=7.679e-06, gnorm=1.29, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40163
2022-10-12 04:46:58 - progress_bar.py[line:274] - INFO: epoch 001:   8903 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=8890, lr=7.68765e-06, gnorm=1.229, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40174
2022-10-12 04:47:09 - progress_bar.py[line:274] - INFO: epoch 001:   8913 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=8900, lr=7.6963e-06, gnorm=1.188, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40185
2022-10-12 04:47:20 - progress_bar.py[line:274] - INFO: epoch 001:   8923 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.6, ups=0.91, wpb=109.8, bsz=40, num_updates=8910, lr=7.70495e-06, gnorm=1.105, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40196
2022-10-12 04:47:31 - progress_bar.py[line:274] - INFO: epoch 001:   8933 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.5, ups=0.89, wpb=111.5, bsz=40, num_updates=8920, lr=7.71359e-06, gnorm=1.065, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=40208
2022-10-12 04:47:42 - progress_bar.py[line:274] - INFO: epoch 001:   8943 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.6, ups=0.91, wpb=111, bsz=40, num_updates=8930, lr=7.72224e-06, gnorm=1.211, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40219
2022-10-12 04:47:53 - progress_bar.py[line:274] - INFO: epoch 001:   8953 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.3, ups=0.92, wpb=110.5, bsz=40, num_updates=8940, lr=7.73089e-06, gnorm=1.141, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40229
2022-10-12 04:48:05 - progress_bar.py[line:274] - INFO: epoch 001:   8963 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.2, ups=0.89, wpb=110.9, bsz=40, num_updates=8950, lr=7.73954e-06, gnorm=1.21, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40241
2022-10-12 04:48:16 - progress_bar.py[line:274] - INFO: epoch 001:   8973 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=8960, lr=7.74818e-06, gnorm=1.247, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40252
2022-10-12 04:48:27 - progress_bar.py[line:274] - INFO: epoch 001:   8983 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.3, ups=0.93, wpb=110.5, bsz=40, num_updates=8970, lr=7.75683e-06, gnorm=1.189, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40263
2022-10-12 04:48:38 - progress_bar.py[line:274] - INFO: epoch 001:   8993 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.3, ups=0.9, wpb=109.2, bsz=40, num_updates=8980, lr=7.76548e-06, gnorm=1.168, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40274
2022-10-12 04:48:49 - progress_bar.py[line:274] - INFO: epoch 001:   9003 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=8990, lr=7.77413e-06, gnorm=1.251, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40285
2022-10-12 04:49:00 - progress_bar.py[line:274] - INFO: epoch 001:   9013 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.4, ups=0.89, wpb=110.7, bsz=40, num_updates=9000, lr=7.78277e-06, gnorm=1.12, clip=70, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=40296
2022-10-12 04:49:00 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 04:49:02 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 04:49:02 - train.py[line:551] - INFO: load:1.28 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 04:49:02 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.37 GiB (GPU 0; 39.59 GiB total capacity; 8.40 GiB already allocated; 1.81 GiB free; 35.30 GiB reserved in total by PyTorch)
2022-10-12 04:49:02 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8606 MB |    9739 MB |    5836 TB |    5836 TB |
|       from large pool |    8461 MB |    9594 MB |    5834 TB |    5834 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8606 MB |    9739 MB |    5836 TB |    5836 TB |
|       from large pool |    8461 MB |    9594 MB |    5834 TB |    5834 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   36144 MB |   36150 MB |  203784 MB |  167640 MB |
|       from large pool |   35998 MB |   35998 MB |  203508 MB |  167510 MB |
|       from small pool |     146 MB |     152 MB |     276 MB |     130 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27537 MB |   27537 MB |    5776 TB |    5776 TB |
|       from large pool |   27536 MB |   27536 MB |    5774 TB |    5774 TB |
|       from small pool |       1 MB |       1 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  270754 K  |  270750 K  |
|       from large pool |     563    |     575    |   87233 K  |   87233 K  |
|       from small pool |    3095    |    3114    |  183520 K  |  183517 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  270754 K  |  270750 K  |
|       from large pool |     563    |     575    |   87233 K  |   87233 K  |
|       from small pool |    3095    |    3114    |  183520 K  |  183517 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     179    |     558    |     382    |
|       from large pool |     103    |     103    |     420    |     317    |
|       from small pool |      73    |      76    |     138    |      65    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     112    |     115    |  194080 K  |  194080 K  |
|       from large pool |      73    |      73    |   34439 K  |   34439 K  |
|       from small pool |      39    |      46    |  159641 K  |  159641 K  |
|===========================================================================|

2022-10-12 04:49:02 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-12 04:49:02 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-12 04:51:34 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 04:51:34 - train.py[line:551] - INFO: load:1.31 valid_run:152.82 task_valid:147.90 collect_output:2.51
2022-10-12 04:54:04 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 04:54:04 - train.py[line:551] - INFO: load:1.33 valid_run:301.90 task_valid:291.69 collect_output:6.66
2022-10-12 04:56:37 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 04:56:37 - train.py[line:551] - INFO: load:1.36 valid_run:454.81 task_valid:435.18 collect_output:14.99
2022-10-12 04:59:05 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 04:59:05 - train.py[line:551] - INFO: load:1.38 valid_run:603.63 task_valid:580.14 collect_output:17.83
2022-10-12 05:01:38 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 05:01:38 - train.py[line:551] - INFO: load:1.41 valid_run:755.73 task_valid:727.67 collect_output:21.38
2022-10-12 05:04:09 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 05:04:09 - train.py[line:551] - INFO: load:1.43 valid_run:907.20 task_valid:873.19 collect_output:26.27
2022-10-12 05:06:42 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 05:06:42 - train.py[line:551] - INFO: load:1.46 valid_run:1059.92 task_valid:1019.36 collect_output:31.74
2022-10-12 05:09:13 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 05:09:13 - train.py[line:551] - INFO: load:1.48 valid_run:1210.72 task_valid:1160.49 collect_output:40.37
2022-10-12 05:11:42 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 05:11:42 - train.py[line:551] - INFO: load:1.50 valid_run:1360.12 task_valid:1305.51 collect_output:43.72
2022-10-12 05:14:11 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 05:14:11 - train.py[line:551] - INFO: load:1.53 valid_run:1508.39 task_valid:1448.71 collect_output:47.76
2022-10-12 05:16:40 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 05:16:40 - train.py[line:551] - INFO: load:1.55 valid_run:1657.76 task_valid:1593.58 collect_output:51.21
2022-10-12 05:19:10 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 05:19:10 - train.py[line:551] - INFO: load:1.58 valid_run:1807.44 task_valid:1738.57 collect_output:54.89
2022-10-12 05:21:39 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 05:21:39 - train.py[line:551] - INFO: load:1.60 valid_run:1957.11 task_valid:1880.60 collect_output:61.48
2022-10-12 05:24:10 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 05:24:10 - train.py[line:551] - INFO: load:1.63 valid_run:2107.28 task_valid:2026.12 collect_output:65.08
2022-10-12 05:26:40 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 05:26:40 - train.py[line:551] - INFO: load:1.65 valid_run:2257.33 task_valid:2172.76 collect_output:67.44
2022-10-12 05:29:10 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 05:29:10 - train.py[line:551] - INFO: load:1.68 valid_run:2407.15 task_valid:2316.87 collect_output:72.12
2022-10-12 05:31:41 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 05:31:41 - train.py[line:551] - INFO: load:1.70 valid_run:2558.46 task_valid:2462.29 collect_output:76.98
2022-10-12 05:34:11 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 05:34:11 - train.py[line:551] - INFO: load:1.73 valid_run:2708.57 task_valid:2609.07 collect_output:79.29
2022-10-12 05:36:39 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 05:36:39 - train.py[line:551] - INFO: load:1.75 valid_run:2856.59 task_valid:2750.75 collect_output:84.60
2022-10-12 05:39:09 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 05:39:09 - train.py[line:551] - INFO: load:1.78 valid_run:3006.47 task_valid:2895.77 collect_output:88.44
2022-10-12 05:41:41 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 05:41:41 - train.py[line:551] - INFO: load:1.80 valid_run:3157.80 task_valid:3040.07 collect_output:94.44
2022-10-12 05:44:10 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 05:44:10 - train.py[line:551] - INFO: load:1.82 valid_run:3307.52 task_valid:3185.17 collect_output:97.92
2022-10-12 05:46:42 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 05:46:42 - train.py[line:551] - INFO: load:1.85 valid_run:3459.26 task_valid:3331.96 collect_output:101.64
2022-10-12 05:49:14 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 05:49:14 - train.py[line:551] - INFO: load:1.88 valid_run:3611.02 task_valid:3478.89 collect_output:105.27

====================================================================================================
SGG eval:     R @ 50: 0.6682;     R @ 100: 0.6927;     R @ 500: 0.7122;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4526;    mR @ 100: 0.4705;    mR @ 500: 0.5268;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8659) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7568) (standing on:0.4243) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6682;     R @ 100: 0.6927;     R @ 500: 0.7122;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4526;    mR @ 100: 0.4705;    mR @ 500: 0.5268;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8659) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.3710) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7568) (standing on:0.4243) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================

2022-10-12 05:51:45 - train.py[line:487] - INFO: 0.6927134453781513
2022-10-12 05:51:46 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 05:51:46 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.34 | loss_v1 0 | loss_v2 0 | nll_loss 0.184 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.692713 | ppl 1.14 | vqa_score 0.598 | wps 119.2 | wpb 89.9 | bsz 30 | num_updates 9000 | best_R@100 0.70391
2022-10-12 05:51:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 9000 updates
2022-10-12 05:51:46 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_9000.pt
2022-10-12 05:51:52 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_9000.pt
2022-10-12 05:51:55 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_9000.pt (epoch 1 @ 9000 updates, score 0.6927134453781513) (writing took 9.19668486667797 seconds)
2022-10-12 05:52:06 - progress_bar.py[line:274] - INFO: epoch 001:   9023 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=0.3, ups=0, wpb=110.2, bsz=40, num_updates=9010, lr=7.79142e-06, gnorm=1.142, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44082
2022-10-12 05:52:18 - progress_bar.py[line:274] - INFO: epoch 001:   9033 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=9020, lr=7.80007e-06, gnorm=1.239, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44094
2022-10-12 05:52:28 - progress_bar.py[line:274] - INFO: epoch 001:   9043 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.4, ups=0.91, wpb=111.1, bsz=40, num_updates=9030, lr=7.80872e-06, gnorm=1.241, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44105
2022-10-12 05:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   9053 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=9040, lr=7.81736e-06, gnorm=1.09, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44116
2022-10-12 05:52:51 - progress_bar.py[line:274] - INFO: epoch 001:   9063 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=9050, lr=7.82601e-06, gnorm=1.081, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44127
2022-10-12 05:53:03 - progress_bar.py[line:274] - INFO: epoch 001:   9073 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95.6, ups=0.88, wpb=109, bsz=40, num_updates=9060, lr=7.83466e-06, gnorm=1.17, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44139
2022-10-12 05:53:14 - progress_bar.py[line:274] - INFO: epoch 001:   9083 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97, ups=0.89, wpb=109.3, bsz=40, num_updates=9070, lr=7.84331e-06, gnorm=1.16, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44150
2022-10-12 05:53:25 - progress_bar.py[line:274] - INFO: epoch 001:   9093 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=9080, lr=7.85195e-06, gnorm=1.129, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44161
2022-10-12 05:53:36 - progress_bar.py[line:274] - INFO: epoch 001:   9103 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.8, ups=0.89, wpb=108.9, bsz=40, num_updates=9090, lr=7.8606e-06, gnorm=1.132, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44172
2022-10-12 05:53:48 - progress_bar.py[line:274] - INFO: epoch 001:   9113 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=94.7, ups=0.87, wpb=108.8, bsz=40, num_updates=9100, lr=7.86925e-06, gnorm=1.29, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44184
2022-10-12 05:53:59 - progress_bar.py[line:274] - INFO: epoch 001:   9123 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.8, ups=0.88, wpb=109.9, bsz=40, num_updates=9110, lr=7.8779e-06, gnorm=1.206, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44195
2022-10-12 05:54:11 - progress_bar.py[line:274] - INFO: epoch 001:   9133 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.7, ups=0.89, wpb=111.3, bsz=40, num_updates=9120, lr=7.88654e-06, gnorm=1.233, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44207
2022-10-12 05:54:22 - progress_bar.py[line:274] - INFO: epoch 001:   9143 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.1, ups=0.9, wpb=112.2, bsz=40, num_updates=9130, lr=7.89519e-06, gnorm=1.297, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44218
2022-10-12 05:54:33 - progress_bar.py[line:274] - INFO: epoch 001:   9153 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.6, ups=0.91, wpb=110.7, bsz=40, num_updates=9140, lr=7.90384e-06, gnorm=1.188, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44229
2022-10-12 05:54:44 - progress_bar.py[line:274] - INFO: epoch 001:   9163 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.6, ups=0.92, wpb=110.2, bsz=40, num_updates=9150, lr=7.91249e-06, gnorm=1.172, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44240
2022-10-12 05:54:55 - progress_bar.py[line:274] - INFO: epoch 001:   9173 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.2, ups=0.87, wpb=111, bsz=40, num_updates=9160, lr=7.92113e-06, gnorm=1.217, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44251
2022-10-12 05:55:07 - progress_bar.py[line:274] - INFO: epoch 001:   9183 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=95, ups=0.87, wpb=109.2, bsz=40, num_updates=9170, lr=7.92978e-06, gnorm=1.245, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44263
2022-10-12 05:55:18 - progress_bar.py[line:274] - INFO: epoch 001:   9193 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.4, ups=0.9, wpb=110.3, bsz=40, num_updates=9180, lr=7.93843e-06, gnorm=1.267, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44274
2022-10-12 05:55:29 - progress_bar.py[line:274] - INFO: epoch 001:   9203 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=9190, lr=7.94708e-06, gnorm=1.061, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44285
2022-10-12 05:55:40 - progress_bar.py[line:274] - INFO: epoch 001:   9213 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=9200, lr=7.95572e-06, gnorm=1.139, clip=80, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=44296
2022-10-12 05:55:51 - progress_bar.py[line:274] - INFO: epoch 001:   9223 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.9, wpb=109.1, bsz=40, num_updates=9210, lr=7.96437e-06, gnorm=1.181, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44307
2022-10-12 05:56:02 - progress_bar.py[line:274] - INFO: epoch 001:   9233 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=102.2, ups=0.93, wpb=110.1, bsz=40, num_updates=9220, lr=7.97302e-06, gnorm=1.087, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44318
2022-10-12 05:56:13 - progress_bar.py[line:274] - INFO: epoch 001:   9243 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=104, ups=0.93, wpb=112, bsz=40, num_updates=9230, lr=7.98167e-06, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44329
2022-10-12 05:56:24 - progress_bar.py[line:274] - INFO: epoch 001:   9253 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.2, ups=0.87, wpb=111, bsz=40, num_updates=9240, lr=7.99031e-06, gnorm=1.244, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44340
2022-10-12 05:56:36 - progress_bar.py[line:274] - INFO: epoch 001:   9263 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.1, ups=0.87, wpb=109.9, bsz=40, num_updates=9250, lr=7.99896e-06, gnorm=1.143, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44352
2022-10-12 05:56:47 - progress_bar.py[line:274] - INFO: epoch 001:   9273 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.4, ups=0.91, wpb=110.8, bsz=40, num_updates=9260, lr=8.00761e-06, gnorm=1.175, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44363
2022-10-12 05:56:58 - progress_bar.py[line:274] - INFO: epoch 001:   9283 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=9270, lr=8.01626e-06, gnorm=1.053, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44374
2022-10-12 05:57:09 - progress_bar.py[line:274] - INFO: epoch 001:   9293 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.5, ups=0.9, wpb=109.2, bsz=40, num_updates=9280, lr=8.0249e-06, gnorm=1.175, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44385
2022-10-12 05:57:20 - progress_bar.py[line:274] - INFO: epoch 001:   9303 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.5, ups=0.89, wpb=110.1, bsz=40, num_updates=9290, lr=8.03355e-06, gnorm=1.175, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44396
2022-10-12 05:57:32 - progress_bar.py[line:274] - INFO: epoch 001:   9313 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.7, ups=0.88, wpb=110.6, bsz=40, num_updates=9300, lr=8.0422e-06, gnorm=1.093, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44408
2022-10-12 05:57:43 - progress_bar.py[line:274] - INFO: epoch 001:   9323 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.2, ups=0.89, wpb=110.4, bsz=40, num_updates=9310, lr=8.05085e-06, gnorm=1.166, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44419
2022-10-12 05:57:54 - progress_bar.py[line:274] - INFO: epoch 001:   9333 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.7, ups=0.87, wpb=111.7, bsz=40, num_updates=9320, lr=8.05949e-06, gnorm=1.105, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44430
2022-10-12 05:58:05 - progress_bar.py[line:274] - INFO: epoch 001:   9343 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.1, ups=0.91, wpb=110.6, bsz=40, num_updates=9330, lr=8.06814e-06, gnorm=1.132, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44441
2022-10-12 05:58:17 - progress_bar.py[line:274] - INFO: epoch 001:   9353 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.7, ups=0.88, wpb=111, bsz=40, num_updates=9340, lr=8.07679e-06, gnorm=1.21, clip=70, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=44453
2022-10-12 05:58:28 - progress_bar.py[line:274] - INFO: epoch 001:   9363 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.9, wpb=110.2, bsz=40, num_updates=9350, lr=8.08544e-06, gnorm=1.335, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44464
2022-10-12 05:58:39 - progress_bar.py[line:274] - INFO: epoch 001:   9373 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=9360, lr=8.09409e-06, gnorm=1.096, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44475
2022-10-12 05:58:50 - progress_bar.py[line:274] - INFO: epoch 001:   9383 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.4, ups=0.91, wpb=110.7, bsz=40, num_updates=9370, lr=8.10273e-06, gnorm=1.1, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44486
2022-10-12 05:59:01 - progress_bar.py[line:274] - INFO: epoch 001:   9393 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.2, ups=0.88, wpb=111.1, bsz=40, num_updates=9380, lr=8.11138e-06, gnorm=1.153, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44497
2022-10-12 05:59:13 - progress_bar.py[line:274] - INFO: epoch 001:   9403 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.3, ups=0.89, wpb=109.5, bsz=40, num_updates=9390, lr=8.12003e-06, gnorm=1.236, clip=100, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44509
2022-10-12 05:59:24 - progress_bar.py[line:274] - INFO: epoch 001:   9413 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=9400, lr=8.12868e-06, gnorm=1.196, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44520
2022-10-12 05:59:35 - progress_bar.py[line:274] - INFO: epoch 001:   9423 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.9, ups=0.91, wpb=109.2, bsz=40, num_updates=9410, lr=8.13732e-06, gnorm=1.16, clip=80, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44531
2022-10-12 05:59:46 - progress_bar.py[line:274] - INFO: epoch 001:   9433 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.1, ups=0.88, wpb=109.4, bsz=40, num_updates=9420, lr=8.14597e-06, gnorm=1.225, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44542
2022-10-12 05:59:52 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 05:59:59 - progress_bar.py[line:274] - INFO: epoch 001:   9444 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=92.3, ups=0.82, wpb=112.5, bsz=40, num_updates=9430, lr=8.15462e-06, gnorm=1.193, clip=70, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=44555
2022-10-12 06:00:10 - progress_bar.py[line:274] - INFO: epoch 001:   9454 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.1, ups=0.88, wpb=109.5, bsz=40, num_updates=9440, lr=8.16327e-06, gnorm=1.136, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44566
2022-10-12 06:00:21 - progress_bar.py[line:274] - INFO: epoch 001:   9464 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.89, wpb=110.6, bsz=40, num_updates=9450, lr=8.17191e-06, gnorm=1.158, clip=100, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=44577
2022-10-12 06:00:32 - progress_bar.py[line:274] - INFO: epoch 001:   9474 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=9460, lr=8.18056e-06, gnorm=1.155, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44589
2022-10-12 06:00:44 - progress_bar.py[line:274] - INFO: epoch 001:   9484 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=9470, lr=8.18921e-06, gnorm=1.3, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44600
2022-10-12 06:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   9494 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95.5, ups=0.87, wpb=109.7, bsz=40, num_updates=9480, lr=8.19786e-06, gnorm=1.201, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44611
2022-10-12 06:01:06 - progress_bar.py[line:274] - INFO: epoch 001:   9504 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.7, ups=0.89, wpb=111, bsz=40, num_updates=9490, lr=8.2065e-06, gnorm=1.02, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44622
2022-10-12 06:01:17 - progress_bar.py[line:274] - INFO: epoch 001:   9514 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103, ups=0.93, wpb=111.2, bsz=40, num_updates=9500, lr=8.21515e-06, gnorm=1.146, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44633
2022-10-12 06:01:28 - progress_bar.py[line:274] - INFO: epoch 001:   9524 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.8, ups=0.92, wpb=109, bsz=40, num_updates=9510, lr=8.2238e-06, gnorm=1.252, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44644
2022-10-12 06:01:39 - progress_bar.py[line:274] - INFO: epoch 001:   9534 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.6, ups=0.88, wpb=110.6, bsz=40, num_updates=9520, lr=8.23245e-06, gnorm=1.101, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44656
2022-10-12 06:01:51 - progress_bar.py[line:274] - INFO: epoch 001:   9544 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=9530, lr=8.24109e-06, gnorm=1.145, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44667
2022-10-12 06:02:02 - progress_bar.py[line:274] - INFO: epoch 001:   9554 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.88, wpb=111.2, bsz=40, num_updates=9540, lr=8.24974e-06, gnorm=1.294, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44678
2022-10-12 06:02:13 - progress_bar.py[line:274] - INFO: epoch 001:   9564 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=104, ups=0.94, wpb=110.4, bsz=40, num_updates=9550, lr=8.25839e-06, gnorm=1.082, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44689
2022-10-12 06:02:24 - progress_bar.py[line:274] - INFO: epoch 001:   9574 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.2, ups=0.88, wpb=110.1, bsz=40, num_updates=9560, lr=8.26704e-06, gnorm=1.189, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44700
2022-10-12 06:02:35 - progress_bar.py[line:274] - INFO: epoch 001:   9584 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=9570, lr=8.27568e-06, gnorm=1.16, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44711
2022-10-12 06:02:47 - progress_bar.py[line:274] - INFO: epoch 001:   9594 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.1, ups=0.87, wpb=109.2, bsz=40, num_updates=9580, lr=8.28433e-06, gnorm=1.266, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44723
2022-10-12 06:02:58 - progress_bar.py[line:274] - INFO: epoch 001:   9604 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.1, ups=0.86, wpb=110.5, bsz=40, num_updates=9590, lr=8.29298e-06, gnorm=1.137, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44734
2022-10-12 06:03:09 - progress_bar.py[line:274] - INFO: epoch 001:   9614 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.3, ups=0.92, wpb=109.6, bsz=40, num_updates=9600, lr=8.30163e-06, gnorm=1.264, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44745
2022-10-12 06:03:20 - progress_bar.py[line:274] - INFO: epoch 001:   9624 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=9610, lr=8.31027e-06, gnorm=1.218, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44757
2022-10-12 06:03:31 - progress_bar.py[line:274] - INFO: epoch 001:   9634 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=103.2, ups=0.94, wpb=110, bsz=40, num_updates=9620, lr=8.31892e-06, gnorm=1.12, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44767
2022-10-12 06:03:42 - progress_bar.py[line:274] - INFO: epoch 001:   9644 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=9630, lr=8.32757e-06, gnorm=1.241, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44779
2022-10-12 06:03:54 - progress_bar.py[line:274] - INFO: epoch 001:   9654 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98, ups=0.89, wpb=110, bsz=40, num_updates=9640, lr=8.33622e-06, gnorm=1.124, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44790
2022-10-12 06:04:05 - progress_bar.py[line:274] - INFO: epoch 001:   9664 / 28910 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.4, ups=0.88, wpb=108.6, bsz=40, num_updates=9650, lr=8.34486e-06, gnorm=1.279, clip=80, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44801
2022-10-12 06:04:16 - progress_bar.py[line:274] - INFO: epoch 001:   9674 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=9660, lr=8.35351e-06, gnorm=1.025, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44812
2022-10-12 06:04:28 - progress_bar.py[line:274] - INFO: epoch 001:   9684 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.7, ups=0.88, wpb=110.6, bsz=40, num_updates=9670, lr=8.36216e-06, gnorm=1.117, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44824
2022-10-12 06:04:39 - progress_bar.py[line:274] - INFO: epoch 001:   9694 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.4, ups=0.92, wpb=108.2, bsz=40, num_updates=9680, lr=8.37081e-06, gnorm=1.141, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44835
2022-10-12 06:04:49 - progress_bar.py[line:274] - INFO: epoch 001:   9704 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.3, ups=0.92, wpb=110.4, bsz=40, num_updates=9690, lr=8.37945e-06, gnorm=1.156, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44846
2022-10-12 06:05:00 - progress_bar.py[line:274] - INFO: epoch 001:   9714 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.7, ups=0.93, wpb=110.4, bsz=40, num_updates=9700, lr=8.3881e-06, gnorm=1.006, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44856
2022-10-12 06:05:11 - progress_bar.py[line:274] - INFO: epoch 001:   9724 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.7, ups=0.9, wpb=111.2, bsz=40, num_updates=9710, lr=8.39675e-06, gnorm=1.231, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44867
2022-10-12 06:05:22 - progress_bar.py[line:274] - INFO: epoch 001:   9734 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.92, wpb=108.7, bsz=40, num_updates=9720, lr=8.4054e-06, gnorm=1.299, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44878
2022-10-12 06:05:34 - progress_bar.py[line:274] - INFO: epoch 001:   9744 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95.4, ups=0.88, wpb=108.4, bsz=40, num_updates=9730, lr=8.41404e-06, gnorm=1.244, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44890
2022-10-12 06:05:45 - progress_bar.py[line:274] - INFO: epoch 001:   9754 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=9740, lr=8.42269e-06, gnorm=1.275, clip=80, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=44901
2022-10-12 06:05:56 - progress_bar.py[line:274] - INFO: epoch 001:   9764 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=9750, lr=8.43134e-06, gnorm=1.122, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44912
2022-10-12 06:06:07 - progress_bar.py[line:274] - INFO: epoch 001:   9774 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.9, ups=0.91, wpb=110.3, bsz=40, num_updates=9760, lr=8.43999e-06, gnorm=1.067, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44923
2022-10-12 06:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   9784 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.6, ups=0.88, wpb=109.5, bsz=40, num_updates=9770, lr=8.44863e-06, gnorm=1.183, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44934
2022-10-12 06:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   9794 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.3, ups=0.91, wpb=110.6, bsz=40, num_updates=9780, lr=8.45728e-06, gnorm=1.19, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44946
2022-10-12 06:06:40 - progress_bar.py[line:274] - INFO: epoch 001:   9804 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103.6, ups=0.93, wpb=111.9, bsz=40, num_updates=9790, lr=8.46593e-06, gnorm=1.039, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44956
2022-10-12 06:06:51 - progress_bar.py[line:274] - INFO: epoch 001:   9814 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=9800, lr=8.47458e-06, gnorm=1.092, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44968
2022-10-12 06:07:03 - progress_bar.py[line:274] - INFO: epoch 001:   9824 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=9810, lr=8.48322e-06, gnorm=1.075, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44979
2022-10-12 06:07:14 - progress_bar.py[line:274] - INFO: epoch 001:   9834 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=9820, lr=8.49187e-06, gnorm=1.139, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44990
2022-10-12 06:07:25 - progress_bar.py[line:274] - INFO: epoch 001:   9844 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.1, ups=0.89, wpb=109.7, bsz=40, num_updates=9830, lr=8.50052e-06, gnorm=1.046, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45001
2022-10-12 06:07:36 - progress_bar.py[line:274] - INFO: epoch 001:   9854 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.1, ups=0.88, wpb=108.7, bsz=40, num_updates=9840, lr=8.50917e-06, gnorm=1.19, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45012
2022-10-12 06:07:47 - progress_bar.py[line:274] - INFO: epoch 001:   9864 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.3, ups=0.91, wpb=111.8, bsz=40, num_updates=9850, lr=8.51781e-06, gnorm=1.013, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=45023
2022-10-12 06:07:58 - progress_bar.py[line:274] - INFO: epoch 001:   9874 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101.1, ups=0.91, wpb=111.5, bsz=40, num_updates=9860, lr=8.52646e-06, gnorm=1.058, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45034
2022-10-12 06:08:10 - progress_bar.py[line:274] - INFO: epoch 001:   9884 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.5, ups=0.89, wpb=108.7, bsz=40, num_updates=9870, lr=8.53511e-06, gnorm=1.1, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45046
2022-10-12 06:08:21 - progress_bar.py[line:274] - INFO: epoch 001:   9894 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.6, ups=0.92, wpb=111.9, bsz=40, num_updates=9880, lr=8.54376e-06, gnorm=1.032, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45057
2022-10-12 06:08:32 - progress_bar.py[line:274] - INFO: epoch 001:   9904 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=9890, lr=8.5524e-06, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=45068
2022-10-12 06:08:43 - progress_bar.py[line:274] - INFO: epoch 001:   9914 / 28910 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.4, ups=0.91, wpb=110.7, bsz=40, num_updates=9900, lr=8.56105e-06, gnorm=1.161, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45079
2022-10-12 06:08:54 - progress_bar.py[line:274] - INFO: epoch 001:   9924 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.6, ups=0.9, wpb=109, bsz=40, num_updates=9910, lr=8.5697e-06, gnorm=1.233, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45090
2022-10-12 06:09:05 - progress_bar.py[line:274] - INFO: epoch 001:   9934 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=9920, lr=8.57835e-06, gnorm=1.131, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45101
2022-10-12 06:09:17 - progress_bar.py[line:274] - INFO: epoch 001:   9944 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.9, ups=0.87, wpb=111.2, bsz=40, num_updates=9930, lr=8.58699e-06, gnorm=1.14, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45113
2022-10-12 06:09:28 - progress_bar.py[line:274] - INFO: epoch 001:   9954 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95.2, ups=0.87, wpb=109.5, bsz=40, num_updates=9940, lr=8.59564e-06, gnorm=1.135, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45124
2022-10-12 06:09:40 - progress_bar.py[line:274] - INFO: epoch 001:   9964 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.5, ups=0.87, wpb=110.5, bsz=40, num_updates=9950, lr=8.60429e-06, gnorm=1.108, clip=90, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45136
2022-10-12 06:09:51 - progress_bar.py[line:274] - INFO: epoch 001:   9974 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=9960, lr=8.61294e-06, gnorm=1.125, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45147
2022-10-12 06:10:00 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 06:10:03 - progress_bar.py[line:274] - INFO: epoch 001:   9985 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=93.8, ups=0.85, wpb=110.5, bsz=40, num_updates=9970, lr=8.62158e-06, gnorm=1.067, clip=70, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=45159
2022-10-12 06:10:14 - progress_bar.py[line:274] - INFO: epoch 001:   9995 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.4, ups=0.9, wpb=110.3, bsz=40, num_updates=9980, lr=8.63023e-06, gnorm=1.128, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45170
2022-10-12 06:10:25 - progress_bar.py[line:274] - INFO: epoch 001:  10005 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99, ups=0.9, wpb=110.1, bsz=40, num_updates=9990, lr=8.63888e-06, gnorm=1.013, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45181
2022-10-12 06:10:37 - progress_bar.py[line:274] - INFO: epoch 001:  10015 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.9, ups=0.89, wpb=108.6, bsz=40, num_updates=10000, lr=8.64753e-06, gnorm=1.101, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45193
2022-10-12 06:10:37 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 06:10:38 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 06:10:38 - train.py[line:551] - INFO: load:1.19 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 06:10:39 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 8.84 GiB already allocated; 304.19 MiB free; 36.81 GiB reserved in total by PyTorch)
2022-10-12 06:10:39 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-12 06:10:39 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9050 MB |   10275 MB |    6559 TB |    6559 TB |
|       from large pool |    8905 MB |   10130 MB |    6557 TB |    6557 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9050 MB |   10275 MB |    6559 TB |    6559 TB |
|       from large pool |    8905 MB |   10130 MB |    6557 TB |    6557 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   37692 MB |   37698 MB |  238568 MB |  200876 MB |
|       from large pool |   37546 MB |   37546 MB |  238226 MB |  200680 MB |
|       from small pool |     146 MB |     152 MB |     342 MB |     196 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   28641 MB |   28641 MB |    7070 TB |    7070 TB |
|       from large pool |   28640 MB |   28640 MB |    7068 TB |    7068 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  303925 K  |  303921 K  |
|       from large pool |     563    |     575    |   97800 K  |   97800 K  |
|       from small pool |    3095    |    3114    |  206124 K  |  206121 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  303925 K  |  303921 K  |
|       from large pool |     563    |     575    |   97800 K  |   97800 K  |
|       from small pool |    3095    |    3114    |  206124 K  |  206121 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     133    |     136    |     547    |     414    |
|       from large pool |      60    |      60    |     376    |     316    |
|       from small pool |      73    |      76    |     171    |      98    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     109    |  220826 K  |  220826 K  |
|       from large pool |      64    |      65    |   40522 K  |   40522 K  |
|       from small pool |      42    |      49    |  180304 K  |  180304 K  |
|===========================================================================|

2022-10-12 06:10:39 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-12 06:13:11 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 06:13:11 - train.py[line:551] - INFO: load:1.21 valid_run:153.35 task_valid:149.23 collect_output:3.03
2022-10-12 06:15:40 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 06:15:40 - train.py[line:551] - INFO: load:1.24 valid_run:301.76 task_valid:292.64 collect_output:6.95
2022-10-12 06:18:12 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 06:18:12 - train.py[line:551] - INFO: load:1.26 valid_run:453.54 task_valid:435.98 collect_output:14.36
2022-10-12 06:20:40 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 06:20:40 - train.py[line:551] - INFO: load:1.29 valid_run:602.18 task_valid:580.85 collect_output:17.12
2022-10-12 06:23:12 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 06:23:12 - train.py[line:551] - INFO: load:1.31 valid_run:754.15 task_valid:728.50 collect_output:20.41
2022-10-12 06:25:44 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 06:25:44 - train.py[line:551] - INFO: load:1.34 valid_run:905.38 task_valid:874.00 collect_output:25.10
2022-10-12 06:28:16 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 06:28:16 - train.py[line:551] - INFO: load:1.36 valid_run:1058.00 task_valid:1020.00 collect_output:30.65
2022-10-12 06:30:47 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 06:30:47 - train.py[line:551] - INFO: load:1.38 valid_run:1208.70 task_valid:1161.13 collect_output:39.22
2022-10-12 06:33:17 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 06:33:17 - train.py[line:551] - INFO: load:1.41 valid_run:1358.01 task_valid:1305.90 collect_output:42.72
2022-10-12 06:35:45 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 06:35:45 - train.py[line:551] - INFO: load:1.43 valid_run:1506.05 task_valid:1448.90 collect_output:46.72
2022-10-12 06:38:14 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 06:38:14 - train.py[line:551] - INFO: load:1.46 valid_run:1655.36 task_valid:1593.67 collect_output:50.25
2022-10-12 06:40:44 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 06:40:44 - train.py[line:551] - INFO: load:1.48 valid_run:1805.30 task_valid:1738.96 collect_output:53.84
2022-10-12 06:43:13 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 06:43:13 - train.py[line:551] - INFO: load:1.50 valid_run:1954.73 task_valid:1880.76 collect_output:60.42
2022-10-12 06:45:44 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 06:45:44 - train.py[line:551] - INFO: load:1.53 valid_run:2105.07 task_valid:2026.26 collect_output:64.23
2022-10-12 06:48:14 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 06:48:14 - train.py[line:551] - INFO: load:1.55 valid_run:2255.39 task_valid:2173.19 collect_output:66.60
2022-10-12 06:50:44 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 06:50:44 - train.py[line:551] - INFO: load:1.58 valid_run:2405.56 task_valid:2317.85 collect_output:70.98
2022-10-12 06:53:16 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 06:53:16 - train.py[line:551] - INFO: load:1.60 valid_run:2557.48 task_valid:2463.85 collect_output:75.66
2022-10-12 06:55:47 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 06:55:47 - train.py[line:551] - INFO: load:1.63 valid_run:2708.33 task_valid:2611.17 collect_output:78.06
2022-10-12 06:58:16 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 06:58:16 - train.py[line:551] - INFO: load:1.66 valid_run:2856.92 task_valid:2753.29 collect_output:83.35
2022-10-12 07:00:47 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 07:00:47 - train.py[line:551] - INFO: load:1.68 valid_run:3007.54 task_valid:2899.08 collect_output:86.96
2022-10-12 07:03:19 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 07:03:19 - train.py[line:551] - INFO: load:1.71 valid_run:3159.53 task_valid:3044.22 collect_output:92.51
2022-10-12 07:05:49 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 07:05:49 - train.py[line:551] - INFO: load:1.73 valid_run:3309.59 task_valid:3189.64 collect_output:95.93
2022-10-12 07:08:20 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 07:08:20 - train.py[line:551] - INFO: load:1.76 valid_run:3461.06 task_valid:3336.28 collect_output:99.62
2022-10-12 07:10:52 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 07:10:52 - train.py[line:551] - INFO: load:1.78 valid_run:3612.28 task_valid:3482.94 collect_output:103.10

====================================================================================================
SGG eval:     R @ 50: 0.6547;     R @ 100: 0.6844;     R @ 500: 0.7037;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4419;    mR @ 100: 0.4635;    mR @ 500: 0.5182;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7432) (standing on:0.4143) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6547;     R @ 100: 0.6844;     R @ 500: 0.7037;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4419;    mR @ 100: 0.4635;    mR @ 500: 0.5182;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7432) (standing on:0.4143) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================

2022-10-12 07:13:22 - train.py[line:487] - INFO: 0.684429131652661
2022-10-12 07:13:23 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 07:13:23 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.335 | loss_v1 0 | loss_v2 0 | nll_loss 0.178 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.684429 | ppl 1.13 | vqa_score 0.589 | wps 119.2 | wpb 89.9 | bsz 30 | num_updates 10000 | best_R@100 0.70391
2022-10-12 07:13:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2022-10-12 07:13:23 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_10000.pt
2022-10-12 07:13:28 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_10000.pt
2022-10-12 07:13:31 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.684429131652661) (writing took 8.309695930220187 seconds)
2022-10-12 07:13:42 - progress_bar.py[line:274] - INFO: epoch 001:  10025 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=0.3, ups=0, wpb=109.7, bsz=40, num_updates=10010, lr=8.65617e-06, gnorm=1.151, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48978
2022-10-12 07:13:53 - progress_bar.py[line:274] - INFO: epoch 001:  10035 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=10020, lr=8.66482e-06, gnorm=1.295, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=48989
2022-10-12 07:14:05 - progress_bar.py[line:274] - INFO: epoch 001:  10045 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=10030, lr=8.67347e-06, gnorm=1.179, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49001
2022-10-12 07:14:16 - progress_bar.py[line:274] - INFO: epoch 001:  10055 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=10040, lr=8.68212e-06, gnorm=1.225, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49012
2022-10-12 07:14:27 - progress_bar.py[line:274] - INFO: epoch 001:  10065 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.5, ups=0.88, wpb=110.9, bsz=40, num_updates=10050, lr=8.69076e-06, gnorm=1.014, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49023
2022-10-12 07:14:38 - progress_bar.py[line:274] - INFO: epoch 001:  10075 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.4, ups=0.91, wpb=109.7, bsz=40, num_updates=10060, lr=8.69941e-06, gnorm=1.226, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49034
2022-10-12 07:14:50 - progress_bar.py[line:274] - INFO: epoch 001:  10085 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=10070, lr=8.70806e-06, gnorm=1.195, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49046
2022-10-12 07:15:01 - progress_bar.py[line:274] - INFO: epoch 001:  10095 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=10080, lr=8.71671e-06, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49057
2022-10-12 07:15:12 - progress_bar.py[line:274] - INFO: epoch 001:  10105 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102, ups=0.93, wpb=109.7, bsz=40, num_updates=10090, lr=8.72535e-06, gnorm=1.346, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49068
2022-10-12 07:15:23 - progress_bar.py[line:274] - INFO: epoch 001:  10115 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.5, ups=0.91, wpb=108.9, bsz=40, num_updates=10100, lr=8.734e-06, gnorm=1.245, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49079
2022-10-12 07:15:34 - progress_bar.py[line:274] - INFO: epoch 001:  10125 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=10110, lr=8.74265e-06, gnorm=1.147, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49090
2022-10-12 07:15:45 - progress_bar.py[line:274] - INFO: epoch 001:  10135 / 28910 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=10120, lr=8.7513e-06, gnorm=1.056, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49101
2022-10-12 07:15:56 - progress_bar.py[line:274] - INFO: epoch 001:  10145 / 28910 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.8, ups=0.92, wpb=111, bsz=40, num_updates=10130, lr=8.75994e-06, gnorm=1.094, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49112
2022-10-12 07:16:07 - progress_bar.py[line:274] - INFO: epoch 001:  10155 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=10140, lr=8.76859e-06, gnorm=1.241, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49123
2022-10-12 07:16:18 - progress_bar.py[line:274] - INFO: epoch 001:  10165 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.1, ups=0.91, wpb=111.6, bsz=40, num_updates=10150, lr=8.77724e-06, gnorm=1.113, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49134
2022-10-12 07:16:29 - progress_bar.py[line:274] - INFO: epoch 001:  10175 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=10160, lr=8.78589e-06, gnorm=1.197, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49145
2022-10-12 07:16:40 - progress_bar.py[line:274] - INFO: epoch 001:  10185 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.2, ups=0.91, wpb=110.1, bsz=40, num_updates=10170, lr=8.79453e-06, gnorm=1.032, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49156
2022-10-12 07:16:52 - progress_bar.py[line:274] - INFO: epoch 001:  10195 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97, ups=0.87, wpb=111.5, bsz=40, num_updates=10180, lr=8.80318e-06, gnorm=1.067, clip=70, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=49168
2022-10-12 07:17:03 - progress_bar.py[line:274] - INFO: epoch 001:  10205 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.1, ups=0.89, wpb=111.5, bsz=40, num_updates=10190, lr=8.81183e-06, gnorm=1.126, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49179
2022-10-12 07:17:15 - progress_bar.py[line:274] - INFO: epoch 001:  10215 / 28910 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=10200, lr=8.82048e-06, gnorm=1.189, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49190
2022-10-12 07:17:27 - progress_bar.py[line:274] - INFO: epoch 001:  10225 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=10210, lr=8.82912e-06, gnorm=1.307, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49203
2022-10-12 07:17:38 - progress_bar.py[line:274] - INFO: epoch 001:  10235 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98, ups=0.89, wpb=110.3, bsz=40, num_updates=10220, lr=8.83777e-06, gnorm=1.171, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49214
2022-10-12 07:17:49 - progress_bar.py[line:274] - INFO: epoch 001:  10245 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=10230, lr=8.84642e-06, gnorm=1.085, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49225
2022-10-12 07:18:01 - progress_bar.py[line:274] - INFO: epoch 001:  10255 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=10240, lr=8.85507e-06, gnorm=1.078, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49237
2022-10-12 07:18:12 - progress_bar.py[line:274] - INFO: epoch 001:  10265 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.4, ups=0.92, wpb=110.4, bsz=40, num_updates=10250, lr=8.86371e-06, gnorm=1.148, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49248
2022-10-12 07:18:22 - progress_bar.py[line:274] - INFO: epoch 001:  10275 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.6, ups=0.93, wpb=109, bsz=40, num_updates=10260, lr=8.87236e-06, gnorm=1.063, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49258
2022-10-12 07:18:33 - progress_bar.py[line:274] - INFO: epoch 001:  10285 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=10270, lr=8.88101e-06, gnorm=1.1, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49269
2022-10-12 07:18:44 - progress_bar.py[line:274] - INFO: epoch 001:  10295 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.8, ups=0.93, wpb=110.6, bsz=40, num_updates=10280, lr=8.88966e-06, gnorm=1.13, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49280
2022-10-12 07:18:55 - progress_bar.py[line:274] - INFO: epoch 001:  10305 / 28910 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=10290, lr=8.89831e-06, gnorm=1.188, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49291
2022-10-12 07:19:06 - progress_bar.py[line:274] - INFO: epoch 001:  10315 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.91, wpb=110.3, bsz=40, num_updates=10300, lr=8.90695e-06, gnorm=1.21, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49302
2022-10-12 07:19:17 - progress_bar.py[line:274] - INFO: epoch 001:  10325 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=10310, lr=8.9156e-06, gnorm=1.144, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49313
2022-10-12 07:19:29 - progress_bar.py[line:274] - INFO: epoch 001:  10335 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.2, ups=0.89, wpb=112, bsz=40, num_updates=10320, lr=8.92425e-06, gnorm=1.194, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49325
2022-10-12 07:19:40 - progress_bar.py[line:274] - INFO: epoch 001:  10345 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99, ups=0.89, wpb=110.6, bsz=40, num_updates=10330, lr=8.9329e-06, gnorm=1.136, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49336
2022-10-12 07:19:51 - progress_bar.py[line:274] - INFO: epoch 001:  10355 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.3, ups=0.87, wpb=110.1, bsz=40, num_updates=10340, lr=8.94154e-06, gnorm=1.165, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49347
2022-10-12 07:20:02 - progress_bar.py[line:274] - INFO: epoch 001:  10365 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.2, ups=0.91, wpb=109.6, bsz=40, num_updates=10350, lr=8.95019e-06, gnorm=1.059, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49358
2022-10-12 07:20:13 - progress_bar.py[line:274] - INFO: epoch 001:  10375 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.4, ups=0.92, wpb=108.6, bsz=40, num_updates=10360, lr=8.95884e-06, gnorm=1.063, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49369
2022-10-12 07:20:25 - progress_bar.py[line:274] - INFO: epoch 001:  10385 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.8, ups=0.87, wpb=111.2, bsz=40, num_updates=10370, lr=8.96749e-06, gnorm=1.206, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49381
2022-10-12 07:20:36 - progress_bar.py[line:274] - INFO: epoch 001:  10395 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.9, ups=0.93, wpb=110.7, bsz=40, num_updates=10380, lr=8.97613e-06, gnorm=1.162, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49392
2022-10-12 07:20:47 - progress_bar.py[line:274] - INFO: epoch 001:  10405 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.1, ups=0.88, wpb=109.1, bsz=40, num_updates=10390, lr=8.98478e-06, gnorm=1.357, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49403
2022-10-12 07:20:58 - progress_bar.py[line:274] - INFO: epoch 001:  10415 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=10400, lr=8.99343e-06, gnorm=1.276, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49414
2022-10-12 07:21:09 - progress_bar.py[line:274] - INFO: epoch 001:  10425 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=10410, lr=9.00208e-06, gnorm=1.115, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49426
2022-10-12 07:21:20 - progress_bar.py[line:274] - INFO: epoch 001:  10435 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=10420, lr=9.01072e-06, gnorm=1.138, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49437
2022-10-12 07:21:32 - progress_bar.py[line:274] - INFO: epoch 001:  10445 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.9, ups=0.9, wpb=111.6, bsz=40, num_updates=10430, lr=9.01937e-06, gnorm=0.967, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49448
2022-10-12 07:21:43 - progress_bar.py[line:274] - INFO: epoch 001:  10455 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.1, ups=0.89, wpb=109.7, bsz=40, num_updates=10440, lr=9.02802e-06, gnorm=1.198, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49459
2022-10-12 07:21:54 - progress_bar.py[line:274] - INFO: epoch 001:  10465 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.3, ups=0.9, wpb=110.8, bsz=40, num_updates=10450, lr=9.03667e-06, gnorm=1.185, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49470
2022-10-12 07:22:05 - progress_bar.py[line:274] - INFO: epoch 001:  10475 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=10460, lr=9.04531e-06, gnorm=1.171, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49481
2022-10-12 07:22:16 - progress_bar.py[line:274] - INFO: epoch 001:  10485 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=10470, lr=9.05396e-06, gnorm=1.062, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49492
2022-10-12 07:22:27 - progress_bar.py[line:274] - INFO: epoch 001:  10495 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=10480, lr=9.06261e-06, gnorm=1.108, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49503
2022-10-12 07:22:38 - progress_bar.py[line:274] - INFO: epoch 001:  10505 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=10490, lr=9.07126e-06, gnorm=1.122, clip=80, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=49514
2022-10-12 07:22:49 - progress_bar.py[line:274] - INFO: epoch 001:  10515 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.2, ups=0.93, wpb=108.8, bsz=40, num_updates=10500, lr=9.0799e-06, gnorm=1.205, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49525
2022-10-12 07:23:00 - progress_bar.py[line:274] - INFO: epoch 001:  10525 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=10510, lr=9.08855e-06, gnorm=1.098, clip=60, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=49536
2022-10-12 07:23:06 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 07:23:12 - progress_bar.py[line:274] - INFO: epoch 001:  10536 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=90.3, ups=0.82, wpb=110.3, bsz=40, num_updates=10520, lr=9.0972e-06, gnorm=1.161, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=49549
2022-10-12 07:23:24 - progress_bar.py[line:274] - INFO: epoch 001:  10546 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.1, ups=0.9, wpb=108.4, bsz=40, num_updates=10530, lr=9.10585e-06, gnorm=1.368, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49560
2022-10-12 07:23:35 - progress_bar.py[line:274] - INFO: epoch 001:  10556 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=10540, lr=9.11449e-06, gnorm=1.167, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49571
2022-10-12 07:23:46 - progress_bar.py[line:274] - INFO: epoch 001:  10566 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=10550, lr=9.12314e-06, gnorm=1.201, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49582
2022-10-12 07:23:57 - progress_bar.py[line:274] - INFO: epoch 001:  10576 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=10560, lr=9.13179e-06, gnorm=1.067, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49593
2022-10-12 07:24:08 - progress_bar.py[line:274] - INFO: epoch 001:  10586 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.5, ups=0.91, wpb=109.7, bsz=40, num_updates=10570, lr=9.14044e-06, gnorm=1.177, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49604
2022-10-12 07:24:19 - progress_bar.py[line:274] - INFO: epoch 001:  10596 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=10580, lr=9.14908e-06, gnorm=1.073, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49615
2022-10-12 07:24:30 - progress_bar.py[line:274] - INFO: epoch 001:  10606 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102, ups=0.92, wpb=110.7, bsz=40, num_updates=10590, lr=9.15773e-06, gnorm=1.089, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49626
2022-10-12 07:24:41 - progress_bar.py[line:274] - INFO: epoch 001:  10616 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=105.4, ups=0.95, wpb=110.6, bsz=40, num_updates=10600, lr=9.16638e-06, gnorm=1.139, clip=50, loss_scale=512, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=49637
2022-10-12 07:24:52 - progress_bar.py[line:274] - INFO: epoch 001:  10626 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=10610, lr=9.17503e-06, gnorm=1.088, clip=80, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49648
2022-10-12 07:25:03 - progress_bar.py[line:274] - INFO: epoch 001:  10636 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.9, ups=0.88, wpb=109.5, bsz=40, num_updates=10620, lr=9.18367e-06, gnorm=1.094, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49659
2022-10-12 07:25:14 - progress_bar.py[line:274] - INFO: epoch 001:  10646 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=10630, lr=9.19232e-06, gnorm=1.119, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49670
2022-10-12 07:25:25 - progress_bar.py[line:274] - INFO: epoch 001:  10656 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=103.3, ups=0.92, wpb=112.3, bsz=40, num_updates=10640, lr=9.20097e-06, gnorm=1.015, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49681
2022-10-12 07:25:36 - progress_bar.py[line:274] - INFO: epoch 001:  10666 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=10650, lr=9.20962e-06, gnorm=1.25, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49693
2022-10-12 07:25:47 - progress_bar.py[line:274] - INFO: epoch 001:  10676 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=10660, lr=9.21826e-06, gnorm=1.304, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49704
2022-10-12 07:25:59 - progress_bar.py[line:274] - INFO: epoch 001:  10686 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.7, ups=0.9, wpb=110.2, bsz=40, num_updates=10670, lr=9.22691e-06, gnorm=1.169, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49715
2022-10-12 07:26:10 - progress_bar.py[line:274] - INFO: epoch 001:  10696 / 28910 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.9, wpb=110.6, bsz=40, num_updates=10680, lr=9.23556e-06, gnorm=1.147, clip=60, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=49726
2022-10-12 07:26:21 - progress_bar.py[line:274] - INFO: epoch 001:  10706 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.7, ups=0.89, wpb=110.4, bsz=40, num_updates=10690, lr=9.24421e-06, gnorm=1.027, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49737
2022-10-12 07:26:33 - progress_bar.py[line:274] - INFO: epoch 001:  10716 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.7, ups=0.88, wpb=111.8, bsz=40, num_updates=10700, lr=9.25285e-06, gnorm=1.123, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49749
2022-10-12 07:26:44 - progress_bar.py[line:274] - INFO: epoch 001:  10726 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=10710, lr=9.2615e-06, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49760
2022-10-12 07:26:55 - progress_bar.py[line:274] - INFO: epoch 001:  10736 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.89, wpb=110.2, bsz=40, num_updates=10720, lr=9.27015e-06, gnorm=1.196, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49771
2022-10-12 07:27:07 - progress_bar.py[line:274] - INFO: epoch 001:  10746 / 28910 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=10730, lr=9.2788e-06, gnorm=1.107, clip=80, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=49783
2022-10-12 07:27:18 - progress_bar.py[line:274] - INFO: epoch 001:  10756 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.6, ups=0.91, wpb=112.2, bsz=40, num_updates=10740, lr=9.28744e-06, gnorm=1.105, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49794
2022-10-12 07:27:29 - progress_bar.py[line:274] - INFO: epoch 001:  10766 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.2, ups=0.9, wpb=109.8, bsz=40, num_updates=10750, lr=9.29609e-06, gnorm=1.168, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49805
2022-10-12 07:27:40 - progress_bar.py[line:274] - INFO: epoch 001:  10776 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=10760, lr=9.30474e-06, gnorm=1.216, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49816
2022-10-12 07:27:51 - progress_bar.py[line:274] - INFO: epoch 001:  10786 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.4, ups=0.9, wpb=109.9, bsz=40, num_updates=10770, lr=9.31339e-06, gnorm=1.127, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49828
2022-10-12 07:28:03 - progress_bar.py[line:274] - INFO: epoch 001:  10796 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.9, ups=0.88, wpb=109.5, bsz=40, num_updates=10780, lr=9.32203e-06, gnorm=1.094, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49839
2022-10-12 07:28:14 - progress_bar.py[line:274] - INFO: epoch 001:  10806 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99, ups=0.89, wpb=110.8, bsz=40, num_updates=10790, lr=9.33068e-06, gnorm=1.137, clip=80, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49850
2022-10-12 07:28:25 - progress_bar.py[line:274] - INFO: epoch 001:  10816 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=10800, lr=9.33933e-06, gnorm=1.25, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49861
2022-10-12 07:28:36 - progress_bar.py[line:274] - INFO: epoch 001:  10826 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.3, ups=0.9, wpb=110.3, bsz=40, num_updates=10810, lr=9.34798e-06, gnorm=1.042, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49872
2022-10-12 07:28:48 - progress_bar.py[line:274] - INFO: epoch 001:  10836 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.8, ups=0.89, wpb=110.4, bsz=40, num_updates=10820, lr=9.35662e-06, gnorm=1.204, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49884
2022-10-12 07:28:58 - progress_bar.py[line:274] - INFO: epoch 001:  10846 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101.4, ups=0.91, wpb=110.8, bsz=40, num_updates=10830, lr=9.36527e-06, gnorm=1.135, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49895
2022-10-12 07:29:10 - progress_bar.py[line:274] - INFO: epoch 001:  10856 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.6, ups=0.9, wpb=111.7, bsz=40, num_updates=10840, lr=9.37392e-06, gnorm=1.06, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49906
2022-10-12 07:29:21 - progress_bar.py[line:274] - INFO: epoch 001:  10866 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.6, ups=0.9, wpb=111.5, bsz=40, num_updates=10850, lr=9.38257e-06, gnorm=1.178, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49917
2022-10-12 07:29:32 - progress_bar.py[line:274] - INFO: epoch 001:  10876 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100, ups=0.9, wpb=110.5, bsz=40, num_updates=10860, lr=9.39121e-06, gnorm=1.091, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49928
2022-10-12 07:29:43 - progress_bar.py[line:274] - INFO: epoch 001:  10886 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.6, ups=0.9, wpb=111.2, bsz=40, num_updates=10870, lr=9.39986e-06, gnorm=1.078, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49939
2022-10-12 07:29:54 - progress_bar.py[line:274] - INFO: epoch 001:  10896 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=10880, lr=9.40851e-06, gnorm=1.216, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49950
2022-10-12 07:30:05 - progress_bar.py[line:274] - INFO: epoch 001:  10906 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.5, ups=0.87, wpb=110.5, bsz=40, num_updates=10890, lr=9.41716e-06, gnorm=1.09, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49962
2022-10-12 07:30:17 - progress_bar.py[line:274] - INFO: epoch 001:  10916 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.6, ups=0.88, wpb=109.9, bsz=40, num_updates=10900, lr=9.4258e-06, gnorm=1.144, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49973
2022-10-12 07:30:28 - progress_bar.py[line:274] - INFO: epoch 001:  10926 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.2, ups=0.87, wpb=111.1, bsz=40, num_updates=10910, lr=9.43445e-06, gnorm=1.034, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=49984
2022-10-12 07:30:40 - progress_bar.py[line:274] - INFO: epoch 001:  10936 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=10920, lr=9.4431e-06, gnorm=1.121, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49996
2022-10-12 07:30:51 - progress_bar.py[line:274] - INFO: epoch 001:  10946 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.9, wpb=112, bsz=40, num_updates=10930, lr=9.45175e-06, gnorm=1.186, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50007
2022-10-12 07:31:02 - progress_bar.py[line:274] - INFO: epoch 001:  10956 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=10940, lr=9.46039e-06, gnorm=1.241, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50018
2022-10-12 07:31:13 - progress_bar.py[line:274] - INFO: epoch 001:  10966 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=10950, lr=9.46904e-06, gnorm=1.103, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50029
2022-10-12 07:31:24 - progress_bar.py[line:274] - INFO: epoch 001:  10976 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=10960, lr=9.47769e-06, gnorm=1.169, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50041
2022-10-12 07:31:36 - progress_bar.py[line:274] - INFO: epoch 001:  10986 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=10970, lr=9.48634e-06, gnorm=1.075, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50052
2022-10-12 07:31:47 - progress_bar.py[line:274] - INFO: epoch 001:  10996 / 28910 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.6, ups=0.92, wpb=110.7, bsz=40, num_updates=10980, lr=9.49498e-06, gnorm=0.991, clip=50, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=50063
2022-10-12 07:31:58 - progress_bar.py[line:274] - INFO: epoch 001:  11006 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.6, ups=0.92, wpb=108.4, bsz=40, num_updates=10990, lr=9.50363e-06, gnorm=0.989, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50074
2022-10-12 07:32:09 - progress_bar.py[line:274] - INFO: epoch 001:  11016 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.2, ups=0.91, wpb=110.8, bsz=40, num_updates=11000, lr=9.51228e-06, gnorm=1.04, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50085
2022-10-12 07:32:09 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 07:32:10 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 07:32:10 - train.py[line:551] - INFO: load:1.17 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 07:32:11 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 8.84 GiB already allocated; 6.02 GiB free; 31.08 GiB reserved in total by PyTorch)
2022-10-12 07:32:11 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-12 07:32:11 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9048 MB |   10273 MB |    7269 TB |    7269 TB |
|       from large pool |    8903 MB |   10127 MB |    7267 TB |    7267 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9048 MB |   10273 MB |    7269 TB |    7269 TB |
|       from large pool |    8903 MB |   10127 MB |    7267 TB |    7267 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   31828 MB |   31828 MB |  245800 MB |  213972 MB |
|       from large pool |   31682 MB |   31682 MB |  245434 MB |  213752 MB |
|       from small pool |     146 MB |     146 MB |     366 MB |     220 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   22779 MB |   27309 MB |    7884 TB |    7884 TB |
|       from large pool |   22778 MB |   27307 MB |    7882 TB |    7882 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  337102 K  |  337098 K  |
|       from large pool |     563    |     575    |  108366 K  |  108365 K  |
|       from small pool |    3095    |    3114    |  228736 K  |  228733 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  337102 K  |  337098 K  |
|       from large pool |     563    |     575    |  108366 K  |  108365 K  |
|       from small pool |    3095    |    3114    |  228736 K  |  228733 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     130    |     130    |     561    |     431    |
|       from large pool |      57    |      57    |     378    |     321    |
|       from small pool |      73    |      73    |     183    |     110    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      78    |      83    |  245806 K  |  245806 K  |
|       from large pool |      38    |      38    |   45768 K  |   45768 K  |
|       from small pool |      40    |      47    |  200037 K  |  200037 K  |
|===========================================================================|

2022-10-12 07:32:11 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-12 07:34:42 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 07:34:42 - train.py[line:551] - INFO: load:1.20 valid_run:151.91 task_valid:147.91 collect_output:2.95
2022-10-12 07:37:10 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 07:37:10 - train.py[line:551] - INFO: load:1.22 valid_run:300.14 task_valid:291.48 collect_output:6.55
2022-10-12 07:39:42 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 07:39:42 - train.py[line:551] - INFO: load:1.24 valid_run:451.94 task_valid:434.71 collect_output:14.05
2022-10-12 07:42:11 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 07:42:11 - train.py[line:551] - INFO: load:1.27 valid_run:600.95 task_valid:579.98 collect_output:16.76
2022-10-12 07:44:44 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 07:44:44 - train.py[line:551] - INFO: load:1.29 valid_run:753.33 task_valid:727.94 collect_output:20.06
2022-10-12 07:47:15 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 07:47:15 - train.py[line:551] - INFO: load:1.32 valid_run:904.71 task_valid:873.57 collect_output:24.77
2022-10-12 07:49:48 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 07:49:48 - train.py[line:551] - INFO: load:1.34 valid_run:1057.48 task_valid:1019.77 collect_output:30.29
2022-10-12 07:52:19 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 07:52:19 - train.py[line:551] - INFO: load:1.37 valid_run:1208.49 task_valid:1161.35 collect_output:38.69
2022-10-12 07:54:49 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 07:54:49 - train.py[line:551] - INFO: load:1.39 valid_run:1358.13 task_valid:1306.42 collect_output:42.14
2022-10-12 07:57:18 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 07:57:18 - train.py[line:551] - INFO: load:1.42 valid_run:1507.01 task_valid:1450.17 collect_output:46.10
2022-10-12 07:59:48 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 07:59:48 - train.py[line:551] - INFO: load:1.44 valid_run:1657.41 task_valid:1595.60 collect_output:49.90
2022-10-12 08:02:18 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 08:02:18 - train.py[line:551] - INFO: load:1.47 valid_run:1807.70 task_valid:1741.11 collect_output:53.52
2022-10-12 08:04:48 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 08:04:48 - train.py[line:551] - INFO: load:1.49 valid_run:1957.61 task_valid:1883.40 collect_output:59.97
2022-10-12 08:07:19 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 08:07:19 - train.py[line:551] - INFO: load:1.52 valid_run:2108.57 task_valid:2029.50 collect_output:63.70
2022-10-12 08:09:50 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 08:09:50 - train.py[line:551] - INFO: load:1.54 valid_run:2259.18 task_valid:2176.56 collect_output:66.14
2022-10-12 08:12:21 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 08:12:21 - train.py[line:551] - INFO: load:1.57 valid_run:2409.66 task_valid:2321.31 collect_output:70.72
2022-10-12 08:14:52 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 08:14:52 - train.py[line:551] - INFO: load:1.59 valid_run:2561.09 task_valid:2467.15 collect_output:75.16
2022-10-12 08:17:23 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 08:17:23 - train.py[line:551] - INFO: load:1.62 valid_run:2711.68 task_valid:2614.28 collect_output:77.55
2022-10-12 08:19:51 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 08:19:51 - train.py[line:551] - INFO: load:1.64 valid_run:2859.70 task_valid:2756.21 collect_output:82.57
2022-10-12 08:22:21 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 08:22:21 - train.py[line:551] - INFO: load:1.66 valid_run:3009.73 task_valid:2901.50 collect_output:86.25
2022-10-12 08:24:53 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 08:24:53 - train.py[line:551] - INFO: load:1.69 valid_run:3161.32 task_valid:3046.20 collect_output:92.10
2022-10-12 08:27:22 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 08:27:22 - train.py[line:551] - INFO: load:1.71 valid_run:3310.59 task_valid:3190.99 collect_output:95.53
2022-10-12 08:29:53 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 08:29:53 - train.py[line:551] - INFO: load:1.74 valid_run:3461.49 task_valid:3337.21 collect_output:99.16
2022-10-12 08:32:24 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 08:32:24 - train.py[line:551] - INFO: load:1.76 valid_run:3612.85 task_valid:3483.91 collect_output:102.73

====================================================================================================
SGG eval:     R @ 50: 0.6480;     R @ 100: 0.6798;     R @ 500: 0.7004;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4425;    mR @ 100: 0.4621;    mR @ 500: 0.5211;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7409) (standing on:0.3893) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6480;     R @ 100: 0.6798;     R @ 500: 0.7004;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4425;    mR @ 100: 0.4621;    mR @ 500: 0.5211;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.9375) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7409) (standing on:0.3893) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5972) 
--------------------------------------------------------
====================================================================================================

2022-10-12 08:34:55 - train.py[line:487] - INFO: 0.6797624649859944
2022-10-12 08:34:56 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 08:34:56 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.338 | loss_v1 0 | loss_v2 0 | nll_loss 0.178 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.679762 | ppl 1.13 | vqa_score 0.5901 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 11000 | best_R@100 0.70391
2022-10-12 08:34:56 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 11000 updates
2022-10-12 08:34:56 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_11000.pt
2022-10-12 08:35:02 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_11000.pt
2022-10-12 08:35:04 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_11000.pt (epoch 1 @ 11000 updates, score 0.6797624649859944) (writing took 8.76213417807594 seconds)
2022-10-12 08:35:15 - progress_bar.py[line:274] - INFO: epoch 001:  11026 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=0.3, ups=0, wpb=109.4, bsz=40, num_updates=11010, lr=9.52093e-06, gnorm=1.133, clip=60, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=53871
2022-10-12 08:35:27 - progress_bar.py[line:274] - INFO: epoch 001:  11036 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=11020, lr=9.52957e-06, gnorm=1.199, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=53883
2022-10-12 08:35:38 - progress_bar.py[line:274] - INFO: epoch 001:  11046 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.1, ups=0.88, wpb=109.3, bsz=40, num_updates=11030, lr=9.53822e-06, gnorm=1.161, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53894
2022-10-12 08:35:48 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 08:35:50 - progress_bar.py[line:274] - INFO: epoch 001:  11057 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=90.6, ups=0.83, wpb=109.6, bsz=40, num_updates=11040, lr=9.54687e-06, gnorm=1.039, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=53906
2022-10-12 08:36:01 - progress_bar.py[line:274] - INFO: epoch 001:  11067 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.9, ups=0.9, wpb=110.4, bsz=40, num_updates=11050, lr=9.55552e-06, gnorm=1.168, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=53917
2022-10-12 08:36:13 - progress_bar.py[line:274] - INFO: epoch 001:  11077 / 28910 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.9, ups=0.87, wpb=110.4, bsz=40, num_updates=11060, lr=9.56416e-06, gnorm=1.067, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=53929
2022-10-12 08:36:24 - progress_bar.py[line:274] - INFO: epoch 001:  11087 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=11070, lr=9.57281e-06, gnorm=1.124, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=53940
2022-10-12 08:36:35 - progress_bar.py[line:274] - INFO: epoch 001:  11097 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=11080, lr=9.58146e-06, gnorm=1.032, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=53951
2022-10-12 08:36:46 - progress_bar.py[line:274] - INFO: epoch 001:  11107 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.8, ups=0.89, wpb=109.4, bsz=40, num_updates=11090, lr=9.59011e-06, gnorm=1.121, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53962
2022-10-12 08:36:57 - progress_bar.py[line:274] - INFO: epoch 001:  11117 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.4, ups=0.92, wpb=110.3, bsz=40, num_updates=11100, lr=9.59875e-06, gnorm=1.21, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=53973
2022-10-12 08:37:09 - progress_bar.py[line:274] - INFO: epoch 001:  11127 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.3, ups=0.88, wpb=109.7, bsz=40, num_updates=11110, lr=9.6074e-06, gnorm=1.104, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=53985
2022-10-12 08:37:20 - progress_bar.py[line:274] - INFO: epoch 001:  11137 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=11120, lr=9.61605e-06, gnorm=1.227, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=53996
2022-10-12 08:37:32 - progress_bar.py[line:274] - INFO: epoch 001:  11147 / 28910 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=11130, lr=9.6247e-06, gnorm=1.029, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54008
2022-10-12 08:37:43 - progress_bar.py[line:274] - INFO: epoch 001:  11157 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.4, ups=0.91, wpb=110.2, bsz=40, num_updates=11140, lr=9.63334e-06, gnorm=1.163, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=54019
2022-10-12 08:37:54 - progress_bar.py[line:274] - INFO: epoch 001:  11167 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.7, ups=0.91, wpb=109.7, bsz=40, num_updates=11150, lr=9.64199e-06, gnorm=1.195, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54030
2022-10-12 08:38:04 - progress_bar.py[line:274] - INFO: epoch 001:  11177 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=11160, lr=9.65064e-06, gnorm=1.029, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54041
2022-10-12 08:38:16 - progress_bar.py[line:274] - INFO: epoch 001:  11187 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.6, ups=0.88, wpb=108.7, bsz=40, num_updates=11170, lr=9.65929e-06, gnorm=1.157, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54052
2022-10-12 08:38:27 - progress_bar.py[line:274] - INFO: epoch 001:  11197 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.1, ups=0.91, wpb=109.3, bsz=40, num_updates=11180, lr=9.66793e-06, gnorm=1.048, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54063
2022-10-12 08:38:38 - progress_bar.py[line:274] - INFO: epoch 001:  11207 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.2, ups=0.9, wpb=110.5, bsz=40, num_updates=11190, lr=9.67658e-06, gnorm=1.099, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54074
2022-10-12 08:38:49 - progress_bar.py[line:274] - INFO: epoch 001:  11217 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=11200, lr=9.68523e-06, gnorm=1.092, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54085
2022-10-12 08:39:01 - progress_bar.py[line:274] - INFO: epoch 001:  11227 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.4, ups=0.89, wpb=110, bsz=40, num_updates=11210, lr=9.69388e-06, gnorm=1.208, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54097
2022-10-12 08:39:12 - progress_bar.py[line:274] - INFO: epoch 001:  11237 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=11220, lr=9.70253e-06, gnorm=1.141, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54108
2022-10-12 08:39:23 - progress_bar.py[line:274] - INFO: epoch 001:  11247 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.5, ups=0.88, wpb=109.2, bsz=40, num_updates=11230, lr=9.71117e-06, gnorm=1.103, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54119
2022-10-12 08:39:34 - progress_bar.py[line:274] - INFO: epoch 001:  11257 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=11240, lr=9.71982e-06, gnorm=1.126, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54130
2022-10-12 08:39:45 - progress_bar.py[line:274] - INFO: epoch 001:  11267 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.5, ups=0.91, wpb=110.3, bsz=40, num_updates=11250, lr=9.72847e-06, gnorm=1.221, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54141
2022-10-12 08:39:57 - progress_bar.py[line:274] - INFO: epoch 001:  11277 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=95.5, ups=0.86, wpb=110.8, bsz=40, num_updates=11260, lr=9.73712e-06, gnorm=1.167, clip=70, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=54153
2022-10-12 08:40:08 - progress_bar.py[line:274] - INFO: epoch 001:  11287 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.1, ups=0.9, wpb=110.6, bsz=40, num_updates=11270, lr=9.74576e-06, gnorm=1.155, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54164
2022-10-12 08:40:19 - progress_bar.py[line:274] - INFO: epoch 001:  11297 / 28910 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=11280, lr=9.75441e-06, gnorm=1.057, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54175
2022-10-12 08:40:30 - progress_bar.py[line:274] - INFO: epoch 001:  11307 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.1, ups=0.91, wpb=111.5, bsz=40, num_updates=11290, lr=9.76306e-06, gnorm=1.096, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54186
2022-10-12 08:40:42 - progress_bar.py[line:274] - INFO: epoch 001:  11317 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=11300, lr=9.77171e-06, gnorm=1.162, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54198
2022-10-12 08:40:55 - progress_bar.py[line:274] - INFO: epoch 001:  11327 / 28910 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.5, ups=0.9, wpb=109.4, bsz=40, num_updates=11310, lr=9.78035e-06, gnorm=1.145, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54209
2022-10-12 08:41:07 - progress_bar.py[line:274] - INFO: epoch 001:  11337 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.5, ups=0.9, wpb=109.1, bsz=40, num_updates=11320, lr=9.789e-06, gnorm=1.055, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54223
2022-10-12 08:41:18 - progress_bar.py[line:274] - INFO: epoch 001:  11347 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=11330, lr=9.79765e-06, gnorm=1.17, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54234
2022-10-12 08:41:29 - progress_bar.py[line:274] - INFO: epoch 001:  11357 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=11340, lr=9.8063e-06, gnorm=1.069, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54245
2022-10-12 08:41:40 - progress_bar.py[line:274] - INFO: epoch 001:  11367 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=11350, lr=9.81494e-06, gnorm=1.127, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54256
2022-10-12 08:41:51 - progress_bar.py[line:274] - INFO: epoch 001:  11377 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.6, ups=0.92, wpb=109.5, bsz=40, num_updates=11360, lr=9.82359e-06, gnorm=1.166, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54267
2022-10-12 08:42:03 - progress_bar.py[line:274] - INFO: epoch 001:  11387 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=11370, lr=9.83224e-06, gnorm=1.099, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54279
2022-10-12 08:42:13 - progress_bar.py[line:274] - INFO: epoch 001:  11397 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103.4, ups=0.93, wpb=111.2, bsz=40, num_updates=11380, lr=9.84089e-06, gnorm=1.1, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54289
2022-10-12 08:42:24 - progress_bar.py[line:274] - INFO: epoch 001:  11407 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.1, ups=0.91, wpb=109.5, bsz=40, num_updates=11390, lr=9.84953e-06, gnorm=1.172, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54300
2022-10-12 08:42:35 - progress_bar.py[line:274] - INFO: epoch 001:  11417 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=11400, lr=9.85818e-06, gnorm=1.129, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54311
2022-10-12 08:42:46 - progress_bar.py[line:274] - INFO: epoch 001:  11427 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.9, ups=0.92, wpb=109, bsz=40, num_updates=11410, lr=9.86683e-06, gnorm=1.215, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54322
2022-10-12 08:42:58 - progress_bar.py[line:274] - INFO: epoch 001:  11437 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.3, ups=0.89, wpb=109.1, bsz=40, num_updates=11420, lr=9.87548e-06, gnorm=1.236, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54334
2022-10-12 08:43:09 - progress_bar.py[line:274] - INFO: epoch 001:  11447 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.91, wpb=109.8, bsz=40, num_updates=11430, lr=9.88412e-06, gnorm=1.172, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54345
2022-10-12 08:43:20 - progress_bar.py[line:274] - INFO: epoch 001:  11457 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=11440, lr=9.89277e-06, gnorm=1.248, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54356
2022-10-12 08:43:31 - progress_bar.py[line:274] - INFO: epoch 001:  11467 / 28910 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103.2, ups=0.93, wpb=110.8, bsz=40, num_updates=11450, lr=9.90142e-06, gnorm=1.003, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54367
2022-10-12 08:43:42 - progress_bar.py[line:274] - INFO: epoch 001:  11477 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.4, ups=0.92, wpb=111.2, bsz=40, num_updates=11460, lr=9.91007e-06, gnorm=1.056, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54378
2022-10-12 08:43:53 - progress_bar.py[line:274] - INFO: epoch 001:  11487 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98, ups=0.89, wpb=110.1, bsz=40, num_updates=11470, lr=9.91871e-06, gnorm=1.135, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54389
2022-10-12 08:44:04 - progress_bar.py[line:274] - INFO: epoch 001:  11497 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=11480, lr=9.92736e-06, gnorm=1.221, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54400
2022-10-12 08:44:15 - progress_bar.py[line:274] - INFO: epoch 001:  11507 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=11490, lr=9.93601e-06, gnorm=1.025, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54411
2022-10-12 08:44:26 - progress_bar.py[line:274] - INFO: epoch 001:  11517 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.8, ups=0.88, wpb=110.3, bsz=40, num_updates=11500, lr=9.94466e-06, gnorm=1.29, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54423
2022-10-12 08:44:37 - progress_bar.py[line:274] - INFO: epoch 001:  11527 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=11510, lr=9.9533e-06, gnorm=1.126, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54434
2022-10-12 08:44:49 - progress_bar.py[line:274] - INFO: epoch 001:  11537 / 28910 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=11520, lr=9.96195e-06, gnorm=1.058, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54445
2022-10-12 08:45:00 - progress_bar.py[line:274] - INFO: epoch 001:  11547 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.6, ups=0.9, wpb=111.2, bsz=40, num_updates=11530, lr=9.9706e-06, gnorm=1.34, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54456
2022-10-12 08:45:11 - progress_bar.py[line:274] - INFO: epoch 001:  11557 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=103.4, ups=0.93, wpb=111.3, bsz=40, num_updates=11540, lr=9.97925e-06, gnorm=1.214, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54467
2022-10-12 08:45:22 - progress_bar.py[line:274] - INFO: epoch 001:  11567 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.3, ups=0.89, wpb=109.1, bsz=40, num_updates=11550, lr=9.98789e-06, gnorm=1.122, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54478
2022-10-12 08:45:32 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 08:45:34 - progress_bar.py[line:274] - INFO: epoch 001:  11578 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=90.6, ups=0.82, wpb=110.8, bsz=40, num_updates=11560, lr=9.99654e-06, gnorm=1.157, clip=90, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54490
2022-10-12 08:45:45 - progress_bar.py[line:274] - INFO: epoch 001:  11588 / 28910 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=11570, lr=1.00052e-05, gnorm=1.229, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54501
2022-10-12 08:45:56 - progress_bar.py[line:274] - INFO: epoch 001:  11598 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.6, ups=0.9, wpb=110.1, bsz=40, num_updates=11580, lr=1.00138e-05, gnorm=1.084, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54513
2022-10-12 08:46:08 - progress_bar.py[line:274] - INFO: epoch 001:  11608 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=11590, lr=1.00225e-05, gnorm=1.193, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54524
2022-10-12 08:46:19 - progress_bar.py[line:274] - INFO: epoch 001:  11618 / 28910 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96, ups=0.87, wpb=110, bsz=40, num_updates=11600, lr=1.00311e-05, gnorm=1.287, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54535
2022-10-12 08:46:30 - progress_bar.py[line:274] - INFO: epoch 001:  11628 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.2, ups=0.92, wpb=110.2, bsz=40, num_updates=11610, lr=1.00398e-05, gnorm=1.153, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54546
2022-10-12 08:46:41 - progress_bar.py[line:274] - INFO: epoch 001:  11638 / 28910 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.1, ups=0.88, wpb=111, bsz=40, num_updates=11620, lr=1.00484e-05, gnorm=1.03, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54557
2022-10-12 08:46:52 - progress_bar.py[line:274] - INFO: epoch 001:  11648 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=11630, lr=1.00571e-05, gnorm=1.098, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54569
2022-10-12 08:47:03 - progress_bar.py[line:274] - INFO: epoch 001:  11658 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.3, ups=0.91, wpb=110.7, bsz=40, num_updates=11640, lr=1.00657e-05, gnorm=1.19, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54579
2022-10-12 08:47:14 - progress_bar.py[line:274] - INFO: epoch 001:  11668 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=11650, lr=1.00744e-05, gnorm=1.2, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54591
2022-10-12 08:47:26 - progress_bar.py[line:274] - INFO: epoch 001:  11678 / 28910 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=95.7, ups=0.87, wpb=109.9, bsz=40, num_updates=11660, lr=1.0083e-05, gnorm=1.06, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54602
2022-10-12 08:47:37 - progress_bar.py[line:274] - INFO: epoch 001:  11688 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=11670, lr=1.00917e-05, gnorm=1.212, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54613
2022-10-12 08:47:48 - progress_bar.py[line:274] - INFO: epoch 001:  11698 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=11680, lr=1.01003e-05, gnorm=1.135, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54624
2022-10-12 08:47:59 - progress_bar.py[line:274] - INFO: epoch 001:  11708 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.4, ups=0.94, wpb=109, bsz=40, num_updates=11690, lr=1.0109e-05, gnorm=1.208, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54635
2022-10-12 08:48:10 - progress_bar.py[line:274] - INFO: epoch 001:  11718 / 28910 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=11700, lr=1.01176e-05, gnorm=1.053, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54646
2022-10-12 08:48:22 - progress_bar.py[line:274] - INFO: epoch 001:  11728 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=11710, lr=1.01263e-05, gnorm=1.106, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54658
2022-10-12 08:48:33 - progress_bar.py[line:274] - INFO: epoch 001:  11738 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=11720, lr=1.01349e-05, gnorm=1.104, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54669
2022-10-12 08:48:44 - progress_bar.py[line:274] - INFO: epoch 001:  11748 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.5, ups=0.89, wpb=111.1, bsz=40, num_updates=11730, lr=1.01435e-05, gnorm=1.133, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54680
2022-10-12 08:48:55 - progress_bar.py[line:274] - INFO: epoch 001:  11758 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99, ups=0.9, wpb=110.2, bsz=40, num_updates=11740, lr=1.01522e-05, gnorm=1.059, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54691
2022-10-12 08:49:06 - progress_bar.py[line:274] - INFO: epoch 001:  11768 / 28910 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=11750, lr=1.01608e-05, gnorm=1.021, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54703
2022-10-12 08:49:18 - progress_bar.py[line:274] - INFO: epoch 001:  11778 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.9, ups=0.9, wpb=110.7, bsz=40, num_updates=11760, lr=1.01695e-05, gnorm=1.077, clip=50, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=54714
2022-10-12 08:49:29 - progress_bar.py[line:274] - INFO: epoch 001:  11788 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=11770, lr=1.01781e-05, gnorm=1.155, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54725
2022-10-12 08:49:40 - progress_bar.py[line:274] - INFO: epoch 001:  11798 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.8, ups=0.91, wpb=109.1, bsz=40, num_updates=11780, lr=1.01868e-05, gnorm=1.142, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54736
2022-10-12 08:49:51 - progress_bar.py[line:274] - INFO: epoch 001:  11808 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=11790, lr=1.01954e-05, gnorm=1.098, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54747
2022-10-12 08:50:02 - progress_bar.py[line:274] - INFO: epoch 001:  11818 / 28910 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=11800, lr=1.02041e-05, gnorm=1.217, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54758
2022-10-12 08:50:13 - progress_bar.py[line:274] - INFO: epoch 001:  11828 / 28910 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.9, ups=0.9, wpb=109.6, bsz=40, num_updates=11810, lr=1.02127e-05, gnorm=1.005, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54769
2022-10-12 08:50:24 - progress_bar.py[line:274] - INFO: epoch 001:  11838 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=11820, lr=1.02214e-05, gnorm=1.232, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54780
2022-10-12 08:50:36 - progress_bar.py[line:274] - INFO: epoch 001:  11848 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.2, ups=0.87, wpb=109.7, bsz=40, num_updates=11830, lr=1.023e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54792
2022-10-12 08:50:48 - progress_bar.py[line:274] - INFO: epoch 001:  11858 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=11840, lr=1.02387e-05, gnorm=1.039, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54804
2022-10-12 08:50:59 - progress_bar.py[line:274] - INFO: epoch 001:  11868 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.8, ups=0.9, wpb=110.6, bsz=40, num_updates=11850, lr=1.02473e-05, gnorm=1.05, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54815
2022-10-12 08:51:10 - progress_bar.py[line:274] - INFO: epoch 001:  11878 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=11860, lr=1.0256e-05, gnorm=1.089, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54826
2022-10-12 08:51:21 - progress_bar.py[line:274] - INFO: epoch 001:  11888 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=94.9, ups=0.87, wpb=109.2, bsz=40, num_updates=11870, lr=1.02646e-05, gnorm=1.196, clip=70, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=54837
2022-10-12 08:51:33 - progress_bar.py[line:274] - INFO: epoch 001:  11898 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.89, wpb=109.7, bsz=40, num_updates=11880, lr=1.02733e-05, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54849
2022-10-12 08:51:44 - progress_bar.py[line:274] - INFO: epoch 001:  11908 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97, ups=0.87, wpb=111.3, bsz=40, num_updates=11890, lr=1.02819e-05, gnorm=1.093, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54860
2022-10-12 08:51:55 - progress_bar.py[line:274] - INFO: epoch 001:  11918 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.8, ups=0.91, wpb=109.1, bsz=40, num_updates=11900, lr=1.02906e-05, gnorm=1.077, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54871
2022-10-12 08:52:06 - progress_bar.py[line:274] - INFO: epoch 001:  11928 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.4, ups=0.92, wpb=110.2, bsz=40, num_updates=11910, lr=1.02992e-05, gnorm=1.126, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54882
2022-10-12 08:52:17 - progress_bar.py[line:274] - INFO: epoch 001:  11938 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.9, ups=0.87, wpb=110.9, bsz=40, num_updates=11920, lr=1.03079e-05, gnorm=1.065, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54894
2022-10-12 08:52:28 - progress_bar.py[line:274] - INFO: epoch 001:  11948 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=11930, lr=1.03165e-05, gnorm=1.03, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54905
2022-10-12 08:52:39 - progress_bar.py[line:274] - INFO: epoch 001:  11958 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=102.9, ups=0.93, wpb=110.9, bsz=40, num_updates=11940, lr=1.03251e-05, gnorm=1.127, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54915
2022-10-12 08:52:50 - progress_bar.py[line:274] - INFO: epoch 001:  11968 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=11950, lr=1.03338e-05, gnorm=1.209, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54926
2022-10-12 08:53:02 - progress_bar.py[line:274] - INFO: epoch 001:  11978 / 28910 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=11960, lr=1.03424e-05, gnorm=1.098, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54938
2022-10-12 08:53:13 - progress_bar.py[line:274] - INFO: epoch 001:  11988 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.7, ups=0.88, wpb=109.4, bsz=40, num_updates=11970, lr=1.03511e-05, gnorm=1.029, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54949
2022-10-12 08:53:24 - progress_bar.py[line:274] - INFO: epoch 001:  11998 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.6, ups=0.89, wpb=110.2, bsz=40, num_updates=11980, lr=1.03597e-05, gnorm=1.143, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54960
2022-10-12 08:53:35 - progress_bar.py[line:274] - INFO: epoch 001:  12008 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=11990, lr=1.03684e-05, gnorm=1.167, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54971
2022-10-12 08:53:47 - progress_bar.py[line:274] - INFO: epoch 001:  12018 / 28910 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=12000, lr=1.0377e-05, gnorm=0.999, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54983
2022-10-12 08:53:47 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 08:53:48 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 08:53:48 - train.py[line:551] - INFO: load:1.12 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 08:54:10 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.35 GiB (GPU 0; 39.59 GiB total capacity; 8.97 GiB already allocated; 5.65 GiB free; 31.45 GiB reserved in total by PyTorch)
2022-10-12 08:54:10 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9182 MB |   15571 MB |    7964 TB |    7964 TB |
|       from large pool |    9037 MB |   15426 MB |    7961 TB |    7961 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9182 MB |   15571 MB |    7964 TB |    7964 TB |
|       from large pool |    9037 MB |   15426 MB |    7961 TB |    7961 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   32208 MB |   35246 MB |  252682 MB |  220474 MB |
|       from large pool |   32062 MB |   35094 MB |  252372 MB |  220310 MB |
|       from small pool |     146 MB |     152 MB |     310 MB |     164 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   23025 MB |   23025 MB |    7946 TB |    7946 TB |
|       from large pool |   23024 MB |   23024 MB |    7944 TB |    7944 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |  370444 K  |  370440 K  |
|       from large pool |     563    |     575    |  118976 K  |  118976 K  |
|       from small pool |    3106    |    3116    |  251467 K  |  251464 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |  370444 K  |  370440 K  |
|       from large pool |     563    |     575    |  118976 K  |  118976 K  |
|       from small pool |    3106    |    3116    |  251467 K  |  251464 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     162    |     172    |     620    |     458    |
|       from large pool |      89    |      96    |     465    |     376    |
|       from small pool |      73    |      76    |     155    |      82    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |     105    |  266887 K  |  266887 K  |
|       from large pool |      62    |      65    |   47573 K  |   47572 K  |
|       from small pool |      35    |      47    |  219314 K  |  219314 K  |
|===========================================================================|

2022-10-12 08:54:10 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-12 08:54:10 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-12 08:56:21 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 08:56:21 - train.py[line:551] - INFO: load:1.15 valid_run:152.59 task_valid:148.39 collect_output:2.27
2022-10-12 08:58:50 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 08:58:50 - train.py[line:551] - INFO: load:1.17 valid_run:301.04 task_valid:291.49 collect_output:6.58
2022-10-12 08:59:37 - trainer.py[line:1334] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.49 GiB (GPU 1; 39.59 GiB total capacity; 9.03 GiB already allocated; 6.43 GiB free; 30.68 GiB reserved in total by PyTorch)
2022-10-12 08:59:37 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2022-10-12 08:59:37 - trainer.py[line:1337] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9250 MB |   15782 MB |    8028 TB |    8028 TB |
|       from large pool |    9105 MB |   15637 MB |    8026 TB |    8026 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9250 MB |   15782 MB |    8028 TB |    8028 TB |
|       from large pool |    9105 MB |   15637 MB |    8026 TB |    8026 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   31416 MB |   37990 MB |  285180 MB |  253764 MB |
|       from large pool |   31270 MB |   37844 MB |  284790 MB |  253520 MB |
|       from small pool |     146 MB |     146 MB |     390 MB |     244 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   22165 MB |   28543 MB |    8753 TB |    8753 TB |
|       from large pool |   22164 MB |   28541 MB |    8751 TB |    8751 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |  372853 K  |  372849 K  |
|       from large pool |     563    |     575    |  119659 K  |  119658 K  |
|       from small pool |    3106    |    3116    |  253193 K  |  253190 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |  372853 K  |  372849 K  |
|       from large pool |     563    |     575    |  119659 K  |  119658 K  |
|       from small pool |    3106    |    3116    |  253193 K  |  253190 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     129    |     130    |     580    |     451    |
|       from large pool |      56    |      57    |     385    |     329    |
|       from small pool |      73    |      73    |     195    |     122    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      68    |      73    |  272307 K  |  272307 K  |
|       from large pool |      35    |      37    |   50700 K  |   50700 K  |
|       from small pool |      33    |      43    |  221606 K  |  221606 K  |
|===========================================================================|

2022-10-12 08:59:37 - trainer.py[line:1083] - WARNING: ran out of memory in validation step, retrying batch
2022-10-12 09:01:23 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 09:01:23 - train.py[line:551] - INFO: load:1.20 valid_run:454.74 task_valid:434.86 collect_output:15.75
2022-10-12 09:03:53 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 09:03:53 - train.py[line:551] - INFO: load:1.22 valid_run:604.21 task_valid:580.35 collect_output:18.59
2022-10-12 09:06:25 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 09:06:25 - train.py[line:551] - INFO: load:1.25 valid_run:756.76 task_valid:728.00 collect_output:22.21
2022-10-12 09:08:58 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 09:08:58 - train.py[line:551] - INFO: load:1.27 valid_run:908.91 task_valid:873.98 collect_output:27.15
2022-10-12 09:11:31 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 09:11:31 - train.py[line:551] - INFO: load:1.31 valid_run:1062.22 task_valid:1020.52 collect_output:32.74
2022-10-12 09:14:02 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 09:14:02 - train.py[line:551] - INFO: load:1.34 valid_run:1213.51 task_valid:1162.29 collect_output:41.08
2022-10-12 09:16:32 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 09:16:32 - train.py[line:551] - INFO: load:1.37 valid_run:1363.35 task_valid:1307.41 collect_output:44.66
2022-10-12 09:19:01 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 09:19:01 - train.py[line:551] - INFO: load:1.39 valid_run:1512.05 task_valid:1450.95 collect_output:48.59
2022-10-12 09:21:31 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 09:21:31 - train.py[line:551] - INFO: load:1.42 valid_run:1661.75 task_valid:1596.15 collect_output:52.00
2022-10-12 09:24:00 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 09:24:00 - train.py[line:551] - INFO: load:1.44 valid_run:1811.38 task_valid:1741.23 collect_output:55.48
2022-10-12 09:26:30 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 09:26:30 - train.py[line:551] - INFO: load:1.47 valid_run:1960.96 task_valid:1883.13 collect_output:62.10
2022-10-12 09:29:00 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 09:29:00 - train.py[line:551] - INFO: load:1.49 valid_run:2111.15 task_valid:2028.55 collect_output:65.81
2022-10-12 09:31:30 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 09:31:30 - train.py[line:551] - INFO: load:1.52 valid_run:2261.17 task_valid:2175.17 collect_output:68.11
2022-10-12 09:34:00 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 09:34:00 - train.py[line:551] - INFO: load:1.54 valid_run:2410.99 task_valid:2319.34 collect_output:72.69
2022-10-12 09:36:32 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 09:36:32 - train.py[line:551] - INFO: load:1.57 valid_run:2562.22 task_valid:2464.97 collect_output:77.21
2022-10-12 09:39:02 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 09:39:02 - train.py[line:551] - INFO: load:1.59 valid_run:2712.66 task_valid:2612.01 collect_output:79.56
2022-10-12 09:41:30 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 09:41:30 - train.py[line:551] - INFO: load:1.62 valid_run:2860.68 task_valid:2753.60 collect_output:84.93
2022-10-12 09:44:00 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 09:44:00 - train.py[line:551] - INFO: load:1.64 valid_run:3010.69 task_valid:2898.69 collect_output:88.77
2022-10-12 09:46:32 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 09:46:32 - train.py[line:551] - INFO: load:1.67 valid_run:3162.36 task_valid:3043.41 collect_output:94.64
2022-10-12 09:49:01 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 09:49:01 - train.py[line:551] - INFO: load:1.69 valid_run:3311.54 task_valid:3188.13 collect_output:98.00
2022-10-12 09:51:32 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 09:51:32 - train.py[line:551] - INFO: load:1.72 valid_run:3462.49 task_valid:3334.35 collect_output:101.65
2022-10-12 09:54:04 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 09:54:04 - train.py[line:551] - INFO: load:1.74 valid_run:3613.71 task_valid:3481.04 collect_output:105.10

====================================================================================================
SGG eval:     R @ 50: 0.6404;     R @ 100: 0.6778;     R @ 500: 0.6982;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4377;    mR @ 100: 0.4644;    mR @ 500: 0.5203;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:1.0000) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7341) (standing on:0.3943) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.5556) 
--------------------------------------------------------
====================================================================================================

2022-10-12 09:56:34 - train.py[line:487] - INFO: 0.6778481792717087

====================================================================================================
SGG eval:     R @ 50: 0.6404;     R @ 100: 0.6778;     R @ 500: 0.6982;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4377;    mR @ 100: 0.4644;    mR @ 500: 0.5203;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:1.0000) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.7341) (standing on:0.3943) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.5556) 
--------------------------------------------------------
====================================================================================================

2022-10-12 09:56:35 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2022-10-12 09:56:35 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.337 | loss_v1 0 | loss_v2 0 | nll_loss 0.178 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.677848 | ppl 1.13 | vqa_score 0.5901 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 12000 | best_R@100 0.70391
2022-10-12 09:56:35 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 12000 updates
2022-10-12 09:56:35 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_12000.pt
2022-10-12 09:56:40 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_12000.pt
2022-10-12 09:56:43 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 0.6778481792717087) (writing took 8.427709489129484 seconds)
2022-10-12 09:56:55 - progress_bar.py[line:274] - INFO: epoch 001:  12028 / 28910 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=0.3, ups=0, wpb=111.2, bsz=40, num_updates=12010, lr=1.03857e-05, gnorm=1.072, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58771
2022-10-12 09:57:06 - progress_bar.py[line:274] - INFO: epoch 001:  12038 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=95.5, ups=0.87, wpb=109.5, bsz=40, num_updates=12020, lr=1.03943e-05, gnorm=1.129, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58782
2022-10-12 09:57:17 - progress_bar.py[line:274] - INFO: epoch 001:  12048 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.4, ups=0.91, wpb=109.1, bsz=40, num_updates=12030, lr=1.0403e-05, gnorm=1.258, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58793
2022-10-12 09:57:28 - progress_bar.py[line:274] - INFO: epoch 001:  12058 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=12040, lr=1.04116e-05, gnorm=1.052, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58804
2022-10-12 09:57:39 - progress_bar.py[line:274] - INFO: epoch 001:  12068 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=12050, lr=1.04203e-05, gnorm=1.254, clip=100, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=58816
2022-10-12 09:57:51 - progress_bar.py[line:274] - INFO: epoch 001:  12078 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=12060, lr=1.04289e-05, gnorm=1.125, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58827
2022-10-12 09:58:02 - progress_bar.py[line:274] - INFO: epoch 001:  12088 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.2, ups=0.87, wpb=110.4, bsz=40, num_updates=12070, lr=1.04376e-05, gnorm=1.061, clip=60, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=58838
2022-10-12 09:58:13 - progress_bar.py[line:274] - INFO: epoch 001:  12098 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.5, ups=0.91, wpb=109.7, bsz=40, num_updates=12080, lr=1.04462e-05, gnorm=1.116, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58849
2022-10-12 09:58:24 - progress_bar.py[line:274] - INFO: epoch 001:  12108 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.9, wpb=108.8, bsz=40, num_updates=12090, lr=1.04549e-05, gnorm=1.118, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58861
2022-10-12 09:58:35 - progress_bar.py[line:274] - INFO: epoch 001:  12118 / 28910 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=12100, lr=1.04635e-05, gnorm=1.065, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58871
2022-10-12 09:58:46 - progress_bar.py[line:274] - INFO: epoch 001:  12128 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.1, ups=0.9, wpb=109.9, bsz=40, num_updates=12110, lr=1.04722e-05, gnorm=1.218, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58883
2022-10-12 09:58:58 - progress_bar.py[line:274] - INFO: epoch 001:  12138 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=12120, lr=1.04808e-05, gnorm=1.149, clip=90, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=58894
2022-10-12 09:59:09 - progress_bar.py[line:274] - INFO: epoch 001:  12148 / 28910 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=12130, lr=1.04895e-05, gnorm=1.014, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58905
2022-10-12 09:59:20 - progress_bar.py[line:274] - INFO: epoch 001:  12158 / 28910 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=12140, lr=1.04981e-05, gnorm=1.015, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=58916
2022-10-12 09:59:31 - progress_bar.py[line:274] - INFO: epoch 001:  12168 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=12150, lr=1.05067e-05, gnorm=1.162, clip=90, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58927
2022-10-12 09:59:42 - progress_bar.py[line:274] - INFO: epoch 001:  12178 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=107.3, ups=0.97, wpb=110.9, bsz=40, num_updates=12160, lr=1.05154e-05, gnorm=1.079, clip=60, loss_scale=1024, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=58938
2022-10-12 09:59:51 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 09:59:54 - progress_bar.py[line:274] - INFO: epoch 001:  12189 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=89.5, ups=0.83, wpb=108.4, bsz=40, num_updates=12170, lr=1.0524e-05, gnorm=1.111, clip=80, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=58950
2022-10-12 10:00:05 - progress_bar.py[line:274] - INFO: epoch 001:  12199 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=12180, lr=1.05327e-05, gnorm=1.104, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58961
2022-10-12 10:00:16 - progress_bar.py[line:274] - INFO: epoch 001:  12209 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.2, ups=0.89, wpb=109.7, bsz=40, num_updates=12190, lr=1.05413e-05, gnorm=1.18, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=58972
2022-10-12 10:00:28 - progress_bar.py[line:274] - INFO: epoch 001:  12219 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=12200, lr=1.055e-05, gnorm=1.023, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=58984
2022-10-12 10:00:39 - progress_bar.py[line:274] - INFO: epoch 001:  12229 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.6, ups=0.89, wpb=110.8, bsz=40, num_updates=12210, lr=1.05586e-05, gnorm=1.071, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58995
2022-10-12 10:00:50 - progress_bar.py[line:274] - INFO: epoch 001:  12239 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=12220, lr=1.05673e-05, gnorm=1.194, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59006
2022-10-12 10:01:01 - progress_bar.py[line:274] - INFO: epoch 001:  12249 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.4, ups=0.92, wpb=110.5, bsz=40, num_updates=12230, lr=1.05759e-05, gnorm=1.051, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59017
2022-10-12 10:01:12 - progress_bar.py[line:274] - INFO: epoch 001:  12259 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=12240, lr=1.05846e-05, gnorm=1.135, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59028
2022-10-12 10:01:23 - progress_bar.py[line:274] - INFO: epoch 001:  12269 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.7, ups=0.89, wpb=109.1, bsz=40, num_updates=12250, lr=1.05932e-05, gnorm=1.099, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59039
2022-10-12 10:01:34 - progress_bar.py[line:274] - INFO: epoch 001:  12279 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.7, ups=0.88, wpb=110.6, bsz=40, num_updates=12260, lr=1.06019e-05, gnorm=1.189, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59050
2022-10-12 10:01:46 - progress_bar.py[line:274] - INFO: epoch 001:  12289 / 28910 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=12270, lr=1.06105e-05, gnorm=1.184, clip=80, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=59062
2022-10-12 10:01:57 - progress_bar.py[line:274] - INFO: epoch 001:  12299 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=12280, lr=1.06192e-05, gnorm=1.116, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59073
2022-10-12 10:02:08 - progress_bar.py[line:274] - INFO: epoch 001:  12309 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=12290, lr=1.06278e-05, gnorm=1.015, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59084
2022-10-12 10:02:19 - progress_bar.py[line:274] - INFO: epoch 001:  12319 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=12300, lr=1.06365e-05, gnorm=1.115, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59095
2022-10-12 10:02:30 - progress_bar.py[line:274] - INFO: epoch 001:  12329 / 28910 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100, ups=0.9, wpb=110.9, bsz=40, num_updates=12310, lr=1.06451e-05, gnorm=1.132, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59106
2022-10-12 10:02:41 - progress_bar.py[line:274] - INFO: epoch 001:  12339 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.2, ups=0.88, wpb=109, bsz=40, num_updates=12320, lr=1.06538e-05, gnorm=1.138, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59118
2022-10-12 10:02:53 - progress_bar.py[line:274] - INFO: epoch 001:  12349 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.5, ups=0.89, wpb=108.8, bsz=40, num_updates=12330, lr=1.06624e-05, gnorm=1.176, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=59129
2022-10-12 10:03:03 - progress_bar.py[line:274] - INFO: epoch 001:  12359 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=104.1, ups=0.94, wpb=111.1, bsz=40, num_updates=12340, lr=1.0671e-05, gnorm=1.196, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59139
2022-10-12 10:03:15 - progress_bar.py[line:274] - INFO: epoch 001:  12369 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.9, ups=0.89, wpb=110.1, bsz=40, num_updates=12350, lr=1.06797e-05, gnorm=1.161, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59151
2022-10-12 10:03:26 - progress_bar.py[line:274] - INFO: epoch 001:  12379 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.2, ups=0.89, wpb=107.8, bsz=40, num_updates=12360, lr=1.06883e-05, gnorm=1.233, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59162
2022-10-12 10:03:37 - progress_bar.py[line:274] - INFO: epoch 001:  12389 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.5, ups=0.9, wpb=109.9, bsz=40, num_updates=12370, lr=1.0697e-05, gnorm=1.296, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59173
2022-10-12 10:03:48 - progress_bar.py[line:274] - INFO: epoch 001:  12399 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.2, ups=0.9, wpb=111.9, bsz=40, num_updates=12380, lr=1.07056e-05, gnorm=1.094, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59184
2022-10-12 10:03:59 - progress_bar.py[line:274] - INFO: epoch 001:  12409 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.4, ups=0.9, wpb=110.4, bsz=40, num_updates=12390, lr=1.07143e-05, gnorm=1.034, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59195
2022-10-12 10:04:10 - progress_bar.py[line:274] - INFO: epoch 001:  12419 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101.1, ups=0.91, wpb=111.1, bsz=40, num_updates=12400, lr=1.07229e-05, gnorm=1.097, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59206
2022-10-12 10:04:22 - progress_bar.py[line:274] - INFO: epoch 001:  12429 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=12410, lr=1.07316e-05, gnorm=1.155, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59218
2022-10-12 10:04:33 - progress_bar.py[line:274] - INFO: epoch 001:  12439 / 28910 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=12420, lr=1.07402e-05, gnorm=1.088, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59229
2022-10-12 10:04:43 - progress_bar.py[line:274] - INFO: epoch 001:  12449 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=12430, lr=1.07489e-05, gnorm=1.063, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59240
2022-10-12 10:04:54 - progress_bar.py[line:274] - INFO: epoch 001:  12459 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=12440, lr=1.07575e-05, gnorm=1.152, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59251
2022-10-12 10:05:06 - progress_bar.py[line:274] - INFO: epoch 001:  12469 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.9, ups=0.9, wpb=110.4, bsz=40, num_updates=12450, lr=1.07662e-05, gnorm=1.095, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59262
2022-10-12 10:05:17 - progress_bar.py[line:274] - INFO: epoch 001:  12479 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=94.5, ups=0.87, wpb=108.3, bsz=40, num_updates=12460, lr=1.07748e-05, gnorm=1.104, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59273
2022-10-12 10:05:28 - progress_bar.py[line:274] - INFO: epoch 001:  12489 / 28910 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=12470, lr=1.07835e-05, gnorm=1.017, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59284
2022-10-12 10:05:39 - progress_bar.py[line:274] - INFO: epoch 001:  12499 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.2, ups=0.91, wpb=110.3, bsz=40, num_updates=12480, lr=1.07921e-05, gnorm=1.079, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59295
2022-10-12 10:05:50 - progress_bar.py[line:274] - INFO: epoch 001:  12509 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.4, ups=0.92, wpb=111.8, bsz=40, num_updates=12490, lr=1.08008e-05, gnorm=1.159, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59306
2022-10-12 10:06:02 - progress_bar.py[line:274] - INFO: epoch 001:  12519 / 28910 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95.4, ups=0.86, wpb=110.4, bsz=40, num_updates=12500, lr=1.08094e-05, gnorm=1.193, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59318
2022-10-12 10:06:13 - progress_bar.py[line:274] - INFO: epoch 001:  12529 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.4, ups=0.89, wpb=109.2, bsz=40, num_updates=12510, lr=1.08181e-05, gnorm=1.108, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59329
2022-10-12 10:06:24 - progress_bar.py[line:274] - INFO: epoch 001:  12539 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=95, ups=0.87, wpb=109.7, bsz=40, num_updates=12520, lr=1.08267e-05, gnorm=1.144, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59341
2022-10-12 10:06:36 - progress_bar.py[line:274] - INFO: epoch 001:  12549 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=12530, lr=1.08354e-05, gnorm=1.118, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59352
2022-10-12 10:06:47 - progress_bar.py[line:274] - INFO: epoch 001:  12559 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=12540, lr=1.0844e-05, gnorm=1.051, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59363
2022-10-12 10:06:58 - progress_bar.py[line:274] - INFO: epoch 001:  12569 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=12550, lr=1.08526e-05, gnorm=1.145, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59374
2022-10-12 10:07:09 - progress_bar.py[line:274] - INFO: epoch 001:  12579 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.7, ups=0.88, wpb=110.2, bsz=40, num_updates=12560, lr=1.08613e-05, gnorm=1.093, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59385
2022-10-12 10:07:20 - progress_bar.py[line:274] - INFO: epoch 001:  12589 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.6, ups=0.89, wpb=110.8, bsz=40, num_updates=12570, lr=1.08699e-05, gnorm=1.082, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59397
2022-10-12 10:07:32 - progress_bar.py[line:274] - INFO: epoch 001:  12599 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=12580, lr=1.08786e-05, gnorm=1.204, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59408
2022-10-12 10:07:43 - progress_bar.py[line:274] - INFO: epoch 001:  12609 / 28910 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=12590, lr=1.08872e-05, gnorm=1.171, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59419
2022-10-12 10:07:54 - progress_bar.py[line:274] - INFO: epoch 001:  12619 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.2, ups=0.89, wpb=108.9, bsz=40, num_updates=12600, lr=1.08959e-05, gnorm=1.222, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59430
2022-10-12 10:08:06 - progress_bar.py[line:274] - INFO: epoch 001:  12629 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98, ups=0.88, wpb=111.4, bsz=40, num_updates=12610, lr=1.09045e-05, gnorm=0.959, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59442
2022-10-12 10:08:17 - progress_bar.py[line:274] - INFO: epoch 001:  12639 / 28910 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=12620, lr=1.09132e-05, gnorm=0.999, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59453
2022-10-12 10:08:28 - progress_bar.py[line:274] - INFO: epoch 001:  12649 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=12630, lr=1.09218e-05, gnorm=1.162, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59464
2022-10-12 10:08:39 - progress_bar.py[line:274] - INFO: epoch 001:  12659 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=12640, lr=1.09305e-05, gnorm=1.123, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59475
2022-10-12 10:08:50 - progress_bar.py[line:274] - INFO: epoch 001:  12669 / 28910 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.4, ups=0.92, wpb=108.9, bsz=40, num_updates=12650, lr=1.09391e-05, gnorm=1.02, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59486
2022-10-12 10:09:01 - progress_bar.py[line:274] - INFO: epoch 001:  12679 / 28910 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=12660, lr=1.09478e-05, gnorm=1.212, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59497
2022-10-12 10:09:13 - progress_bar.py[line:274] - INFO: epoch 001:  12689 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=12670, lr=1.09564e-05, gnorm=1.045, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59509
2022-10-12 10:09:24 - progress_bar.py[line:274] - INFO: epoch 001:  12699 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.6, ups=0.9, wpb=110.1, bsz=40, num_updates=12680, lr=1.09651e-05, gnorm=1.08, clip=60, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59520
2022-10-12 10:09:35 - progress_bar.py[line:274] - INFO: epoch 001:  12709 / 28910 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.5, ups=0.91, wpb=110.8, bsz=40, num_updates=12690, lr=1.09737e-05, gnorm=1.161, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59531
2022-10-12 10:09:46 - progress_bar.py[line:274] - INFO: epoch 001:  12719 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=12700, lr=1.09824e-05, gnorm=1.103, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59542
2022-10-12 10:09:57 - progress_bar.py[line:274] - INFO: epoch 001:  12729 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=95.6, ups=0.88, wpb=108, bsz=40, num_updates=12710, lr=1.0991e-05, gnorm=1.141, clip=90, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=59553
2022-10-12 10:09:58 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-12 10:10:09 - progress_bar.py[line:274] - INFO: epoch 001:  12740 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=91.4, ups=0.83, wpb=110.5, bsz=40, num_updates=12720, lr=1.09997e-05, gnorm=1.179, clip=80, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=59566
2022-10-12 10:10:20 - progress_bar.py[line:274] - INFO: epoch 001:  12750 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.8, ups=0.91, wpb=110.8, bsz=40, num_updates=12730, lr=1.10083e-05, gnorm=1.151, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59577
2022-10-12 10:10:32 - progress_bar.py[line:274] - INFO: epoch 001:  12760 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.2, ups=0.88, wpb=110, bsz=40, num_updates=12740, lr=1.10169e-05, gnorm=1.121, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59588
2022-10-12 10:10:43 - progress_bar.py[line:274] - INFO: epoch 001:  12770 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.9, ups=0.91, wpb=110.4, bsz=40, num_updates=12750, lr=1.10256e-05, gnorm=1.045, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59599
2022-10-12 10:10:54 - progress_bar.py[line:274] - INFO: epoch 001:  12780 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103.1, ups=0.93, wpb=110.9, bsz=40, num_updates=12760, lr=1.10342e-05, gnorm=1.189, clip=80, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59610
2022-10-12 10:11:05 - progress_bar.py[line:274] - INFO: epoch 001:  12790 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.5, ups=0.89, wpb=108.9, bsz=40, num_updates=12770, lr=1.10429e-05, gnorm=1.179, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59621
2022-10-12 10:11:16 - progress_bar.py[line:274] - INFO: epoch 001:  12800 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.3, ups=0.91, wpb=108.8, bsz=40, num_updates=12780, lr=1.10515e-05, gnorm=1.056, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59632
2022-10-12 10:11:27 - progress_bar.py[line:274] - INFO: epoch 001:  12810 / 28910 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.6, ups=0.9, wpb=110.7, bsz=40, num_updates=12790, lr=1.10602e-05, gnorm=0.978, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59643
2022-10-12 10:11:38 - progress_bar.py[line:274] - INFO: epoch 001:  12820 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=12800, lr=1.10688e-05, gnorm=0.958, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59654
2022-10-12 10:11:50 - progress_bar.py[line:274] - INFO: epoch 001:  12830 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=12810, lr=1.10775e-05, gnorm=1.028, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59666
2022-10-12 10:12:01 - progress_bar.py[line:274] - INFO: epoch 001:  12840 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=12820, lr=1.10861e-05, gnorm=1.208, clip=80, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59677
2022-10-12 10:12:12 - progress_bar.py[line:274] - INFO: epoch 001:  12850 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.7, ups=0.87, wpb=109.8, bsz=40, num_updates=12830, lr=1.10948e-05, gnorm=1.008, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59689
2022-10-12 10:12:24 - progress_bar.py[line:274] - INFO: epoch 001:  12860 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=12840, lr=1.11034e-05, gnorm=1.029, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59700
2022-10-12 10:12:35 - progress_bar.py[line:274] - INFO: epoch 001:  12870 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=12850, lr=1.11121e-05, gnorm=1.186, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59711
2022-10-12 10:12:46 - progress_bar.py[line:274] - INFO: epoch 001:  12880 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.9, ups=0.93, wpb=110.1, bsz=40, num_updates=12860, lr=1.11207e-05, gnorm=1.081, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=59722
2022-10-12 10:12:57 - progress_bar.py[line:274] - INFO: epoch 001:  12890 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.9, ups=0.91, wpb=109.8, bsz=40, num_updates=12870, lr=1.11294e-05, gnorm=1.068, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59733
2022-10-12 10:13:08 - progress_bar.py[line:274] - INFO: epoch 001:  12900 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.3, ups=0.91, wpb=111, bsz=40, num_updates=12880, lr=1.1138e-05, gnorm=1.186, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59744
2022-10-12 10:13:19 - progress_bar.py[line:274] - INFO: epoch 001:  12910 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.1, ups=0.88, wpb=109.3, bsz=40, num_updates=12890, lr=1.11467e-05, gnorm=1.086, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59755
2022-10-12 10:13:30 - progress_bar.py[line:274] - INFO: epoch 001:  12920 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.1, ups=0.92, wpb=111.2, bsz=40, num_updates=12900, lr=1.11553e-05, gnorm=1.066, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59766
2022-10-12 10:13:41 - progress_bar.py[line:274] - INFO: epoch 001:  12930 / 28910 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.3, ups=0.88, wpb=110.5, bsz=40, num_updates=12910, lr=1.1164e-05, gnorm=1.097, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59778
2022-10-12 10:13:52 - progress_bar.py[line:274] - INFO: epoch 001:  12940 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=12920, lr=1.11726e-05, gnorm=0.918, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59789
2022-10-12 10:14:04 - progress_bar.py[line:274] - INFO: epoch 001:  12950 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=12930, lr=1.11813e-05, gnorm=1.168, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59800
2022-10-12 10:14:15 - progress_bar.py[line:274] - INFO: epoch 001:  12960 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=12940, lr=1.11899e-05, gnorm=1.132, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59811
2022-10-12 10:14:26 - progress_bar.py[line:274] - INFO: epoch 001:  12970 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.7, ups=0.92, wpb=112.1, bsz=40, num_updates=12950, lr=1.11985e-05, gnorm=1.108, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59822
2022-10-12 10:14:37 - progress_bar.py[line:274] - INFO: epoch 001:  12980 / 28910 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.1, ups=0.89, wpb=111.5, bsz=40, num_updates=12960, lr=1.12072e-05, gnorm=1.072, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59833
2022-10-12 10:14:48 - progress_bar.py[line:274] - INFO: epoch 001:  12990 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.3, ups=0.9, wpb=110.6, bsz=40, num_updates=12970, lr=1.12158e-05, gnorm=1.024, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=59844
2022-10-12 10:14:59 - progress_bar.py[line:274] - INFO: epoch 001:  13000 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.3, ups=0.9, wpb=109.6, bsz=40, num_updates=12980, lr=1.12245e-05, gnorm=1.052, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59856
2022-10-12 10:15:10 - progress_bar.py[line:274] - INFO: epoch 001:  13010 / 28910 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.2, ups=0.9, wpb=110.1, bsz=40, num_updates=12990, lr=1.12331e-05, gnorm=1.066, clip=50, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=59867
2022-10-12 10:15:22 - progress_bar.py[line:274] - INFO: epoch 001:  13020 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=13000, lr=1.12418e-05, gnorm=1.079, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59878
2022-10-12 10:15:22 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 10:15:23 - train.py[line:549] - INFO: 0 / 4988
2022-10-12 10:15:23 - train.py[line:551] - INFO: load:1.28 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 10:17:56 - train.py[line:549] - INFO: 200 / 4988
2022-10-12 10:17:56 - train.py[line:551] - INFO: load:1.30 valid_run:152.16 task_valid:148.63 collect_output:2.35
2022-10-12 10:20:25 - train.py[line:549] - INFO: 400 / 4988
2022-10-12 10:20:25 - train.py[line:551] - INFO: load:1.33 valid_run:301.28 task_valid:292.40 collect_output:6.48
2022-10-12 10:22:58 - train.py[line:549] - INFO: 600 / 4988
2022-10-12 10:22:58 - train.py[line:551] - INFO: load:1.35 valid_run:454.31 task_valid:435.95 collect_output:14.87
2022-10-12 10:25:27 - train.py[line:549] - INFO: 800 / 4988
2022-10-12 10:25:27 - train.py[line:551] - INFO: load:1.37 valid_run:603.58 task_valid:581.21 collect_output:17.73
2022-10-12 10:27:59 - train.py[line:549] - INFO: 1000 / 4988
2022-10-12 10:27:59 - train.py[line:551] - INFO: load:1.40 valid_run:755.74 task_valid:728.86 collect_output:21.18
2022-10-12 10:30:31 - train.py[line:549] - INFO: 1200 / 4988
2022-10-12 10:30:31 - train.py[line:551] - INFO: load:1.42 valid_run:907.04 task_valid:874.32 collect_output:25.98
2022-10-12 10:33:03 - train.py[line:549] - INFO: 1400 / 4988
2022-10-12 10:33:03 - train.py[line:551] - INFO: load:1.45 valid_run:1059.69 task_valid:1020.38 collect_output:31.54
2022-10-12 10:35:34 - train.py[line:549] - INFO: 1600 / 4988
2022-10-12 10:35:34 - train.py[line:551] - INFO: load:1.47 valid_run:1210.46 task_valid:1161.77 collect_output:39.88
2022-10-12 10:38:04 - train.py[line:549] - INFO: 1800 / 4988
2022-10-12 10:38:04 - train.py[line:551] - INFO: load:1.49 valid_run:1359.88 task_valid:1306.63 collect_output:43.39
2022-10-12 10:40:32 - train.py[line:549] - INFO: 2000 / 4988
2022-10-12 10:40:32 - train.py[line:551] - INFO: load:1.52 valid_run:1508.01 task_valid:1449.90 collect_output:47.23
2022-10-12 10:43:02 - train.py[line:549] - INFO: 2200 / 4988
2022-10-12 10:43:02 - train.py[line:551] - INFO: load:1.54 valid_run:1657.60 task_valid:1595.12 collect_output:50.57
2022-10-12 10:45:31 - train.py[line:549] - INFO: 2400 / 4988
2022-10-12 10:45:31 - train.py[line:551] - INFO: load:1.57 valid_run:1807.33 task_valid:1740.10 collect_output:54.26
2022-10-12 10:48:01 - train.py[line:549] - INFO: 2600 / 4988
2022-10-12 10:48:01 - train.py[line:551] - INFO: load:1.59 valid_run:1956.92 task_valid:1882.07 collect_output:60.83
2022-10-12 10:50:31 - train.py[line:549] - INFO: 2800 / 4988
2022-10-12 10:50:31 - train.py[line:551] - INFO: load:1.62 valid_run:2107.31 task_valid:2027.56 collect_output:64.66
2022-10-12 10:53:01 - train.py[line:549] - INFO: 3000 / 4988
2022-10-12 10:53:01 - train.py[line:551] - INFO: load:1.64 valid_run:2257.31 task_valid:2174.23 collect_output:66.95
2022-10-12 10:55:31 - train.py[line:549] - INFO: 3200 / 4988
2022-10-12 10:55:31 - train.py[line:551] - INFO: load:1.66 valid_run:2407.18 task_valid:2318.39 collect_output:71.60
2022-10-12 10:58:03 - train.py[line:549] - INFO: 3400 / 4988
2022-10-12 10:58:03 - train.py[line:551] - INFO: load:1.69 valid_run:2558.46 task_valid:2463.81 collect_output:76.39
2022-10-12 11:00:33 - train.py[line:549] - INFO: 3600 / 4988
2022-10-12 11:00:33 - train.py[line:551] - INFO: load:1.71 valid_run:2709.07 task_valid:2611.02 collect_output:78.73
2022-10-12 11:03:01 - train.py[line:549] - INFO: 3800 / 4988
2022-10-12 11:03:01 - train.py[line:551] - INFO: load:1.74 valid_run:2857.02 task_valid:2752.47 collect_output:84.19
2022-10-12 11:05:31 - train.py[line:549] - INFO: 4000 / 4988
2022-10-12 11:05:31 - train.py[line:551] - INFO: load:1.76 valid_run:3006.92 task_valid:2897.58 collect_output:87.92
2022-10-12 11:08:03 - train.py[line:549] - INFO: 4200 / 4988
2022-10-12 11:08:03 - train.py[line:551] - INFO: load:1.78 valid_run:3158.49 task_valid:3042.21 collect_output:93.82
2022-10-12 11:10:33 - train.py[line:549] - INFO: 4400 / 4988
2022-10-12 11:10:33 - train.py[line:551] - INFO: load:1.81 valid_run:3308.05 task_valid:3187.09 collect_output:97.40
2022-10-12 11:13:04 - train.py[line:549] - INFO: 4600 / 4988
2022-10-12 11:13:04 - train.py[line:551] - INFO: load:1.83 valid_run:3459.30 task_valid:3333.60 collect_output:101.00
2022-10-12 11:15:36 - train.py[line:549] - INFO: 4800 / 4988
2022-10-12 11:15:36 - train.py[line:551] - INFO: load:1.86 valid_run:3611.20 task_valid:3480.70 collect_output:104.57

====================================================================================================
SGG eval:     R @ 50: 0.6348;     R @ 100: 0.6778;     R @ 500: 0.6968;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4303;    mR @ 100: 0.4593;    mR @ 500: 0.5145;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:1.0000) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.3750) (hanging from:0.4355) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9598) (says:0.0000) (sitting on:0.7285) (standing on:0.4077) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5556) 
--------------------------------------------------------
====================================================================================================

2022-10-12 11:18:08 - train.py[line:487] - INFO: 0.6778481792717087
2022-10-12 11:18:08 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6348;     R @ 100: 0.6778;     R @ 500: 0.6968;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4303;    mR @ 100: 0.4593;    mR @ 500: 0.5145;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:1.0000) (covering:0.3000) (eating:0.8235) (flying in:0.0000) (growing on:0.3750) (hanging from:0.4355) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9598) (says:0.0000) (sitting on:0.7285) (standing on:0.4077) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.5556) 
--------------------------------------------------------
====================================================================================================

2022-10-12 11:18:08 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.341 | loss_v1 0 | loss_v2 0 | nll_loss 0.185 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.677848 | ppl 1.14 | vqa_score 0.589 | wps 119.2 | wpb 89.9 | bsz 30 | num_updates 13000 | best_R@100 0.70391
2022-10-12 11:18:08 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 13000 updates
2022-10-12 11:18:08 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_13000.pt
2022-10-12 11:18:14 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_13000.pt
2022-10-12 11:18:16 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.2/1_B20_A1_E50_0.04_5e-5_480/checkpoint_1_13000.pt (epoch 1 @ 13000 updates, score 0.6778481792717087) (writing took 8.621388752944767 seconds)
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1660868
Killing subprocess 1660869
Main process received SIGINT, exiting
