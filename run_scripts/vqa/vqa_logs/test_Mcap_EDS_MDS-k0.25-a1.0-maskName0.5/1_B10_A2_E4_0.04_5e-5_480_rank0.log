2022-10-12 15:34:25 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-10-12 15:34:25 - utils.py[line:261] - INFO: Start init
2022-10-12 15:34:25 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-10-12 15:34:25 - utils.py[line:261] - INFO: Start init
2022-10-12 15:34:26 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-10-12 15:34:26 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-10-12 15:34:26 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2022-10-12 15:34:26 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2022-10-12 15:34:32 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 5, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 10, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.5/1_B10_A2_E4_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=10, batch_size_valid='8', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=4, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=5, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.5/1_B10_A2_E4_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_Mcap_EDS_MDS-k0.25-a1.0-maskName0.5', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[2], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2022-10-12 15:34:33 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2022-10-12 15:34:33 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-10-12 15:34:37 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-10-12 15:34:37 - train.py[line:118] - INFO: task: VqaGenTask
2022-10-12 15:34:37 - train.py[line:119] - INFO: model: OFAModel
2022-10-12 15:34:37 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2022-10-12 15:34:37 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2022-10-12 15:34:37 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-10-12 15:34:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-10-12 15:34:37 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-10-12 15:34:38 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-12 15:34:38 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-12 15:34:38 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-12 15:34:38 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-12 15:34:38 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2022-10-12 15:34:38 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 10
2022-10-12 15:34:38 - trainer.py[line:458] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt
2022-10-12 15:35:00 - trainer.py[line:605] - INFO: Loading EMA from checkpoint
2022-10-12 15:35:00 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2022-10-12 15:35:01 - trainer.py[line:273] - INFO: Exponential Moving Average Shadow Model is initialized.
2022-10-12 15:35:01 - trainer.py[line:612] - INFO: Loading EMA fp32 params from checkpoint
2022-10-12 15:35:02 - trainer.py[line:623] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B3_A1_E50_0.04_5e-5_480/checkpoint_best.pt (epoch 8 @ 0 updates)
2022-10-12 15:35:02 - trainer.py[line:643] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 1 row count 578200 total row count 1156400
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 0 row count 578200 total row count 1156400
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
2022-10-12 15:35:03 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
2022-10-12 15:35:04 - trainer.py[line:707] - INFO: begin training epoch 1
2022-10-12 15:35:04 - train.py[line:312] - INFO: Start iterating over samples
2022-10-12 15:35:25 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 28910 loss=0.763, loss_v1=0, loss_v2=0, nll_loss=0.666, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=87.7, ups=0.79, wpb=110.8, bsz=40, num_updates=10, lr=1.08108e-07, gnorm=2.365, clip=100, loss_scale=128, train_wall=18, gb_free=22.7, ema_decay=0.9999, wall=47
2022-10-12 15:35:37 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 28910 loss=0.765, loss_v1=0, loss_v2=0, nll_loss=0.664, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=91.9, ups=0.84, wpb=109.6, bsz=40, num_updates=20, lr=2.16216e-07, gnorm=2.319, clip=100, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=59
2022-10-12 15:35:49 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 28910 loss=0.804, loss_v1=0, loss_v2=0, nll_loss=0.716, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=92.7, ups=0.83, wpb=111.1, bsz=40, num_updates=30, lr=3.24324e-07, gnorm=2.501, clip=100, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=71
2022-10-12 15:36:01 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 28910 loss=0.751, loss_v1=0, loss_v2=0, nll_loss=0.65, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=95.9, ups=0.86, wpb=111.6, bsz=40, num_updates=40, lr=4.32432e-07, gnorm=2.35, clip=100, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=83
2022-10-12 15:36:13 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 28910 loss=0.755, loss_v1=0, loss_v2=0, nll_loss=0.656, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=89.7, ups=0.82, wpb=109.4, bsz=40, num_updates=50, lr=5.40541e-07, gnorm=2.714, clip=100, loss_scale=128, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=95
2022-10-12 15:36:25 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 28910 loss=0.722, loss_v1=0, loss_v2=0, nll_loss=0.625, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=93.8, ups=0.84, wpb=111.1, bsz=40, num_updates=60, lr=6.48649e-07, gnorm=2.228, clip=100, loss_scale=128, train_wall=12, gb_free=22.5, ema_decay=0.9999, wall=107
2022-10-12 15:36:37 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 28910 loss=0.715, loss_v1=0, loss_v2=0, nll_loss=0.62, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=93, ups=0.85, wpb=109.8, bsz=40, num_updates=70, lr=7.56757e-07, gnorm=2.195, clip=100, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=119
2022-10-12 15:36:49 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 28910 loss=0.715, loss_v1=0, loss_v2=0, nll_loss=0.625, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=91.6, ups=0.83, wpb=110, bsz=40, num_updates=80, lr=8.64865e-07, gnorm=2.081, clip=100, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=131
2022-10-12 15:37:01 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 28910 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.602, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=93.5, ups=0.84, wpb=111, bsz=40, num_updates=90, lr=9.72973e-07, gnorm=2.011, clip=100, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=143
2022-10-12 15:37:14 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 28910 loss=0.71, loss_v1=0, loss_v2=0, nll_loss=0.627, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=90.5, ups=0.82, wpb=110.1, bsz=40, num_updates=100, lr=1.08108e-06, gnorm=2.001, clip=100, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=155
2022-10-12 15:37:25 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 28910 loss=0.72, loss_v1=0, loss_v2=0, nll_loss=0.636, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=94.4, ups=0.86, wpb=110, bsz=40, num_updates=110, lr=1.18919e-06, gnorm=2.143, clip=100, loss_scale=128, train_wall=12, gb_free=23, ema_decay=0.9999, wall=167
2022-10-12 15:37:37 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 28910 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.574, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=92.7, ups=0.83, wpb=111.2, bsz=40, num_updates=120, lr=1.2973e-06, gnorm=1.714, clip=90, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=179
2022-10-12 15:37:49 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 28910 loss=0.697, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=93.3, ups=0.84, wpb=110.8, bsz=40, num_updates=130, lr=1.40541e-06, gnorm=1.885, clip=100, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=191
2022-10-12 15:38:01 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 28910 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=93.7, ups=0.85, wpb=110.9, bsz=40, num_updates=140, lr=1.51351e-06, gnorm=1.727, clip=90, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=203
2022-10-12 15:38:13 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 28910 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.571, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=93.1, ups=0.84, wpb=110.6, bsz=40, num_updates=150, lr=1.62162e-06, gnorm=1.332, clip=70, loss_scale=128, train_wall=12, gb_free=23, ema_decay=0.9999, wall=215
2022-10-12 15:38:25 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 28910 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=94.5, ups=0.86, wpb=110.5, bsz=40, num_updates=160, lr=1.72973e-06, gnorm=1.621, clip=90, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=227
2022-10-12 15:38:37 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 28910 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.608, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=93.4, ups=0.84, wpb=111, bsz=40, num_updates=170, lr=1.83784e-06, gnorm=1.448, clip=80, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=239
2022-10-12 15:38:49 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 28910 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.527, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=92.8, ups=0.83, wpb=111.4, bsz=40, num_updates=180, lr=1.94595e-06, gnorm=1.429, clip=70, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=251
2022-10-12 15:39:00 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 28910 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.514, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=95, ups=0.85, wpb=111.8, bsz=40, num_updates=190, lr=2.05405e-06, gnorm=1.345, clip=80, loss_scale=128, train_wall=12, gb_free=22.5, ema_decay=0.9999, wall=262
2022-10-12 15:39:12 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 28910 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=93.3, ups=0.85, wpb=109.9, bsz=40, num_updates=200, lr=2.16216e-06, gnorm=1.262, clip=90, loss_scale=128, train_wall=12, gb_free=23, ema_decay=0.9999, wall=274
2022-10-12 15:39:24 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 28910 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.518, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=92.6, ups=0.84, wpb=110.7, bsz=40, num_updates=210, lr=2.27027e-06, gnorm=1.158, clip=50, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=286
2022-10-12 15:39:36 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 28910 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.504, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=91.4, ups=0.83, wpb=109.8, bsz=40, num_updates=220, lr=2.37838e-06, gnorm=1.332, clip=70, loss_scale=128, train_wall=12, gb_free=23, ema_decay=0.9999, wall=298
2022-10-12 15:39:48 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 28910 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=94.1, ups=0.85, wpb=110.9, bsz=40, num_updates=230, lr=2.48649e-06, gnorm=1.429, clip=90, loss_scale=128, train_wall=12, gb_free=22.6, ema_decay=0.9999, wall=310
2022-10-12 15:40:00 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 28910 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=93.1, ups=0.84, wpb=111.4, bsz=40, num_updates=240, lr=2.59459e-06, gnorm=1.135, clip=70, loss_scale=128, train_wall=12, gb_free=23.1, ema_decay=0.9999, wall=322
2022-10-12 15:40:12 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 28910 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=93.3, ups=0.85, wpb=109.5, bsz=40, num_updates=250, lr=2.7027e-06, gnorm=1.202, clip=70, loss_scale=128, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=334
2022-10-12 15:40:23 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 28910 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.516, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=95.5, ups=0.86, wpb=110.5, bsz=40, num_updates=260, lr=2.81081e-06, gnorm=1.242, clip=80, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=345
2022-10-12 15:40:35 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 28910 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.569, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=91.7, ups=0.84, wpb=108.9, bsz=40, num_updates=270, lr=2.91892e-06, gnorm=1.468, clip=90, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=357
2022-10-12 15:40:47 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 28910 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.535, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=91.8, ups=0.84, wpb=109.6, bsz=40, num_updates=280, lr=3.02703e-06, gnorm=1.316, clip=90, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=369
2022-10-12 15:40:59 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 28910 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=91.8, ups=0.83, wpb=110, bsz=40, num_updates=290, lr=3.13514e-06, gnorm=1.19, clip=70, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=381
2022-10-12 15:41:11 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 28910 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=91.8, ups=0.84, wpb=109.8, bsz=40, num_updates=300, lr=3.24324e-06, gnorm=1.305, clip=80, loss_scale=128, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=393
2022-10-12 15:41:23 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 28910 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=92.1, ups=0.84, wpb=110.2, bsz=40, num_updates=310, lr=3.35135e-06, gnorm=1.163, clip=70, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=405
2022-10-12 15:41:35 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 28910 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=94.4, ups=0.85, wpb=111.3, bsz=40, num_updates=320, lr=3.45946e-06, gnorm=1.288, clip=70, loss_scale=128, train_wall=12, gb_free=23, ema_decay=0.9999, wall=417
2022-10-12 15:41:47 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 28910 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.537, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=93.7, ups=0.85, wpb=110.5, bsz=40, num_updates=330, lr=3.56757e-06, gnorm=1.209, clip=90, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=429
2022-10-12 15:41:59 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 28910 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.55, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=94.6, ups=0.85, wpb=111.2, bsz=40, num_updates=340, lr=3.67568e-06, gnorm=1.181, clip=60, loss_scale=128, train_wall=12, gb_free=23, ema_decay=0.9999, wall=440
2022-10-12 15:42:11 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 28910 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=92.4, ups=0.83, wpb=111.8, bsz=40, num_updates=350, lr=3.78378e-06, gnorm=1.175, clip=70, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=453
2022-10-12 15:42:22 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 28910 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=93.6, ups=0.85, wpb=110.2, bsz=40, num_updates=360, lr=3.89189e-06, gnorm=1.258, clip=80, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=464
2022-10-12 15:42:34 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 28910 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.503, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=91.9, ups=0.83, wpb=110.4, bsz=40, num_updates=370, lr=4e-06, gnorm=1.193, clip=90, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=476
2022-10-12 15:42:47 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 28910 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=91.3, ups=0.83, wpb=109.9, bsz=40, num_updates=380, lr=4.10811e-06, gnorm=1.089, clip=50, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=488
2022-10-12 15:42:58 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 28910 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=93.3, ups=0.84, wpb=111, bsz=40, num_updates=390, lr=4.21622e-06, gnorm=1.197, clip=40, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=500
2022-10-12 15:43:10 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 28910 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=93.3, ups=0.84, wpb=110.5, bsz=40, num_updates=400, lr=4.32432e-06, gnorm=1.037, clip=60, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=512
2022-10-12 15:43:22 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 28910 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=91.8, ups=0.84, wpb=109.5, bsz=40, num_updates=410, lr=4.43243e-06, gnorm=1.17, clip=70, loss_scale=128, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=524
2022-10-12 15:43:34 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 28910 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=92.9, ups=0.84, wpb=110.4, bsz=40, num_updates=420, lr=4.54054e-06, gnorm=1.22, clip=90, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=536
2022-10-12 15:43:46 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 28910 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=94, ups=0.84, wpb=111.3, bsz=40, num_updates=430, lr=4.64865e-06, gnorm=1.006, clip=50, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=548
2022-10-12 15:43:58 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 28910 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=92.2, ups=0.84, wpb=109.6, bsz=40, num_updates=440, lr=4.75676e-06, gnorm=1.176, clip=70, loss_scale=128, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=560
2022-10-12 15:44:10 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 28910 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=92.4, ups=0.83, wpb=110.8, bsz=40, num_updates=450, lr=4.86486e-06, gnorm=1.028, clip=40, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=572
2022-10-12 15:44:22 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 28910 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=92.4, ups=0.84, wpb=109.6, bsz=40, num_updates=460, lr=4.97297e-06, gnorm=1.111, clip=80, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=584
2022-10-12 15:44:34 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 28910 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=92.9, ups=0.84, wpb=110.3, bsz=40, num_updates=470, lr=5.08108e-06, gnorm=0.987, clip=30, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=596
2022-10-12 15:44:46 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 28910 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=91, ups=0.83, wpb=109.2, bsz=40, num_updates=480, lr=5.18919e-06, gnorm=1.025, clip=40, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=608
2022-10-12 15:44:57 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 28910 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=93.4, ups=0.85, wpb=110.4, bsz=40, num_updates=490, lr=5.2973e-06, gnorm=1.04, clip=50, loss_scale=128, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=619
2022-10-12 15:45:10 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 28910 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=91.7, ups=0.83, wpb=110, bsz=40, num_updates=500, lr=5.40541e-06, gnorm=1.028, clip=50, loss_scale=128, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=631
2022-10-12 15:45:22 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 28910 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=89.7, ups=0.82, wpb=108.7, bsz=40, num_updates=510, lr=5.51351e-06, gnorm=1.137, clip=70, loss_scale=128, train_wall=12, gb_free=23, ema_decay=0.9999, wall=644
2022-10-12 15:45:34 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 28910 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=94, ups=0.84, wpb=111.5, bsz=40, num_updates=520, lr=5.62162e-06, gnorm=0.968, clip=30, loss_scale=256, train_wall=12, gb_free=22.6, ema_decay=0.9999, wall=655
2022-10-12 15:45:45 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 28910 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=94.6, ups=0.85, wpb=111, bsz=40, num_updates=530, lr=5.72973e-06, gnorm=0.946, clip=40, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=667
2022-10-12 15:45:57 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 28910 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=93.8, ups=0.85, wpb=110.1, bsz=40, num_updates=540, lr=5.83784e-06, gnorm=0.975, clip=40, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=679
2022-10-12 15:46:09 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 28910 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=94.9, ups=0.86, wpb=110.9, bsz=40, num_updates=550, lr=5.94595e-06, gnorm=1.116, clip=70, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=691
2022-10-12 15:46:20 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 28910 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=95.2, ups=0.87, wpb=110.1, bsz=40, num_updates=560, lr=6.05405e-06, gnorm=1.089, clip=50, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=702
2022-10-12 15:46:32 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 28910 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=93.5, ups=0.85, wpb=110.1, bsz=40, num_updates=570, lr=6.16216e-06, gnorm=1.084, clip=40, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=714
2022-10-12 15:46:44 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 28910 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=93.8, ups=0.84, wpb=111.1, bsz=40, num_updates=580, lr=6.27027e-06, gnorm=1.123, clip=70, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=726
2022-10-12 15:46:56 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 28910 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96, ups=0.86, wpb=112.1, bsz=40, num_updates=590, lr=6.37838e-06, gnorm=1.155, clip=60, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=738
2022-10-12 15:47:08 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 28910 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=90.6, ups=0.82, wpb=110.4, bsz=40, num_updates=600, lr=6.48649e-06, gnorm=1.208, clip=100, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=750
2022-10-12 15:47:20 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 28910 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=93.8, ups=0.86, wpb=109.4, bsz=40, num_updates=610, lr=6.59459e-06, gnorm=1.098, clip=90, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=761
2022-10-12 15:47:32 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 28910 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=91.7, ups=0.84, wpb=109.6, bsz=40, num_updates=620, lr=6.7027e-06, gnorm=0.944, clip=30, loss_scale=256, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=773
2022-10-12 15:47:44 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 28910 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=92, ups=0.82, wpb=111.7, bsz=40, num_updates=630, lr=6.81081e-06, gnorm=1.006, clip=60, loss_scale=256, train_wall=12, gb_free=22.6, ema_decay=0.9999, wall=786
2022-10-12 15:47:56 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 28910 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=92.3, ups=0.84, wpb=109.8, bsz=40, num_updates=640, lr=6.91892e-06, gnorm=1.131, clip=80, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=797
2022-10-12 15:48:07 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 28910 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=93, ups=0.84, wpb=110.1, bsz=40, num_updates=650, lr=7.02703e-06, gnorm=1.063, clip=60, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=809
2022-10-12 15:48:19 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 28910 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=92.4, ups=0.84, wpb=110.6, bsz=40, num_updates=660, lr=7.13514e-06, gnorm=0.867, clip=30, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=821
2022-10-12 15:48:31 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 28910 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=93.6, ups=0.85, wpb=109.8, bsz=40, num_updates=670, lr=7.24324e-06, gnorm=1.065, clip=50, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=833
2022-10-12 15:48:43 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 28910 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=91.2, ups=0.83, wpb=109.6, bsz=40, num_updates=680, lr=7.35135e-06, gnorm=1.07, clip=40, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=845
2022-10-12 15:48:55 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 28910 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=92.6, ups=0.84, wpb=109.9, bsz=40, num_updates=690, lr=7.45946e-06, gnorm=1.079, clip=60, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=857
2022-10-12 15:49:07 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 28910 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.4, ups=0.87, wpb=112.4, bsz=40, num_updates=700, lr=7.56757e-06, gnorm=1.051, clip=50, loss_scale=256, train_wall=11, gb_free=22.8, ema_decay=0.9999, wall=869
2022-10-12 15:49:18 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 28910 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.3, ups=0.88, wpb=110, bsz=40, num_updates=710, lr=7.67568e-06, gnorm=1.114, clip=60, loss_scale=256, train_wall=11, gb_free=22.8, ema_decay=0.9999, wall=880
2022-10-12 15:49:30 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 28910 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=92, ups=0.83, wpb=110.5, bsz=40, num_updates=720, lr=7.78378e-06, gnorm=1.084, clip=70, loss_scale=256, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=892
2022-10-12 15:49:42 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 28910 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=92.4, ups=0.84, wpb=110.5, bsz=40, num_updates=730, lr=7.89189e-06, gnorm=1.13, clip=80, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=904
2022-10-12 15:49:54 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 28910 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=93.8, ups=0.85, wpb=109.9, bsz=40, num_updates=740, lr=8e-06, gnorm=1.21, clip=70, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=916
2022-10-12 15:50:06 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 28910 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.2, ups=0.85, wpb=111.6, bsz=40, num_updates=750, lr=8.10811e-06, gnorm=1.069, clip=80, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=927
2022-10-12 15:50:17 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 28910 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=91.8, ups=0.84, wpb=109.4, bsz=40, num_updates=760, lr=8.21622e-06, gnorm=1.105, clip=70, loss_scale=256, train_wall=12, gb_free=22.6, ema_decay=0.9999, wall=939
2022-10-12 15:50:29 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 28910 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=94.7, ups=0.85, wpb=111.8, bsz=40, num_updates=770, lr=8.32432e-06, gnorm=1.158, clip=80, loss_scale=256, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=951
2022-10-12 15:50:41 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 28910 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=93.8, ups=0.84, wpb=111.6, bsz=40, num_updates=780, lr=8.43243e-06, gnorm=1.174, clip=70, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=963
2022-10-12 15:50:53 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 28910 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=93.6, ups=0.85, wpb=110.5, bsz=40, num_updates=790, lr=8.54054e-06, gnorm=1.151, clip=80, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=975
2022-10-12 15:51:05 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 28910 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=91.8, ups=0.84, wpb=108.7, bsz=40, num_updates=800, lr=8.64865e-06, gnorm=1.164, clip=70, loss_scale=256, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=987
2022-10-12 15:51:17 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 28910 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=94.3, ups=0.86, wpb=110.1, bsz=40, num_updates=810, lr=8.75676e-06, gnorm=1.007, clip=40, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=998
2022-10-12 15:51:28 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 28910 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.1, ups=0.87, wpb=110.3, bsz=40, num_updates=820, lr=8.86486e-06, gnorm=0.998, clip=30, loss_scale=256, train_wall=11, gb_free=22.7, ema_decay=0.9999, wall=1010
2022-10-12 15:51:40 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 28910 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=94.3, ups=0.85, wpb=111.5, bsz=40, num_updates=830, lr=8.97297e-06, gnorm=1.155, clip=80, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=1022
2022-10-12 15:51:51 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 28910 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.3, ups=0.86, wpb=110.4, bsz=40, num_updates=840, lr=9.08108e-06, gnorm=1.14, clip=60, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=1033
2022-10-12 15:52:04 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 28910 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=91.1, ups=0.83, wpb=109.9, bsz=40, num_updates=850, lr=9.18919e-06, gnorm=1.075, clip=60, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=1045
2022-10-12 15:52:15 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 28910 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95, ups=0.86, wpb=111, bsz=40, num_updates=860, lr=9.2973e-06, gnorm=1.26, clip=90, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=1057
2022-10-12 15:52:27 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 28910 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=92.1, ups=0.84, wpb=109.8, bsz=40, num_updates=870, lr=9.40541e-06, gnorm=1.107, clip=80, loss_scale=256, train_wall=12, gb_free=22.6, ema_decay=0.9999, wall=1069
2022-10-12 15:52:39 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 28910 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=93.3, ups=0.85, wpb=109.8, bsz=40, num_updates=880, lr=9.51351e-06, gnorm=1.07, clip=50, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=1081
2022-10-12 15:52:51 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 28910 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=93.1, ups=0.83, wpb=112.1, bsz=40, num_updates=890, lr=9.62162e-06, gnorm=1.001, clip=40, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=1093
2022-10-12 15:53:03 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 28910 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=92.1, ups=0.84, wpb=110.2, bsz=40, num_updates=900, lr=9.72973e-06, gnorm=1.04, clip=80, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=1105
2022-10-12 15:53:15 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 28910 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=94, ups=0.85, wpb=110.6, bsz=40, num_updates=910, lr=9.83784e-06, gnorm=1.123, clip=60, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=1117
2022-10-12 15:53:27 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 28910 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=91.7, ups=0.82, wpb=111.7, bsz=40, num_updates=920, lr=9.94595e-06, gnorm=1.101, clip=70, loss_scale=256, train_wall=12, gb_free=22.9, ema_decay=0.9999, wall=1129
2022-10-12 15:53:39 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 28910 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=94, ups=0.84, wpb=111.3, bsz=40, num_updates=930, lr=1.00541e-05, gnorm=1.061, clip=60, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=1141
2022-10-12 15:53:50 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 28910 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=94.9, ups=0.86, wpb=110.7, bsz=40, num_updates=940, lr=1.01622e-05, gnorm=1.047, clip=50, loss_scale=256, train_wall=12, gb_free=22.4, ema_decay=0.9999, wall=1152
2022-10-12 15:54:02 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 28910 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=91.6, ups=0.83, wpb=109.8, bsz=40, num_updates=950, lr=1.02703e-05, gnorm=1.118, clip=70, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=1164
2022-10-12 15:54:14 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 28910 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=94.5, ups=0.87, wpb=109, bsz=40, num_updates=960, lr=1.03784e-05, gnorm=1.134, clip=80, loss_scale=256, train_wall=11, gb_free=22.3, ema_decay=0.9999, wall=1176
2022-10-12 15:54:26 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 28910 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=92.2, ups=0.83, wpb=110.6, bsz=40, num_updates=970, lr=1.04865e-05, gnorm=1.159, clip=70, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=1188
2022-10-12 15:54:38 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 28910 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=89.6, ups=0.82, wpb=109.6, bsz=40, num_updates=980, lr=1.05946e-05, gnorm=1.063, clip=50, loss_scale=256, train_wall=12, gb_free=22.7, ema_decay=0.9999, wall=1200
2022-10-12 15:54:50 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 28910 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.9, ups=0.86, wpb=111.3, bsz=40, num_updates=990, lr=1.07027e-05, gnorm=1.06, clip=70, loss_scale=256, train_wall=12, gb_free=22.8, ema_decay=0.9999, wall=1212
2022-10-12 15:55:02 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 28910 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.2, ups=0.86, wpb=112.1, bsz=40, num_updates=1000, lr=1.08108e-05, gnorm=0.997, clip=30, loss_scale=256, train_wall=12, gb_free=23, ema_decay=0.9999, wall=1223
2022-10-12 15:55:02 - train.py[line:506] - INFO: begin validation on "valid" subset
2022-10-12 15:55:02 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2022-10-12 15:55:03 - train.py[line:549] - INFO: 0 / 9351
2022-10-12 15:55:03 - train.py[line:551] - INFO: load:1.59 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-12 15:56:29 - train.py[line:549] - INFO: 200 / 9351
2022-10-12 15:56:29 - train.py[line:551] - INFO: load:1.61 valid_run:86.11 task_valid:82.78 collect_output:2.27
2022-10-12 15:57:53 - train.py[line:549] - INFO: 400 / 9351
2022-10-12 15:57:53 - train.py[line:551] - INFO: load:1.63 valid_run:169.93 task_valid:164.23 collect_output:3.63
2022-10-12 15:59:16 - train.py[line:549] - INFO: 600 / 9351
2022-10-12 15:59:16 - train.py[line:551] - INFO: load:1.65 valid_run:252.05 task_valid:243.14 collect_output:5.84
2022-10-12 16:00:40 - train.py[line:549] - INFO: 800 / 9351
2022-10-12 16:00:40 - train.py[line:551] - INFO: load:1.67 valid_run:336.90 task_valid:321.98 collect_output:10.84
2022-10-12 16:02:05 - train.py[line:549] - INFO: 1000 / 9351
2022-10-12 16:02:05 - train.py[line:551] - INFO: load:1.69 valid_run:421.11 task_valid:401.12 collect_output:14.93
2022-10-12 16:03:28 - train.py[line:549] - INFO: 1200 / 9351
2022-10-12 16:03:28 - train.py[line:551] - INFO: load:1.71 valid_run:504.58 task_valid:479.06 collect_output:19.49
2022-10-12 16:04:51 - train.py[line:549] - INFO: 1400 / 9351
2022-10-12 16:04:51 - train.py[line:551] - INFO: load:1.73 valid_run:587.56 task_valid:559.29 collect_output:21.25
2022-10-12 16:06:15 - train.py[line:549] - INFO: 1600 / 9351
2022-10-12 16:06:15 - train.py[line:551] - INFO: load:1.75 valid_run:671.28 task_valid:641.18 collect_output:22.08
2022-10-12 16:07:40 - train.py[line:549] - INFO: 1800 / 9351
2022-10-12 16:07:40 - train.py[line:551] - INFO: load:1.77 valid_run:755.74 task_valid:721.97 collect_output:24.72
2022-10-12 16:09:03 - train.py[line:549] - INFO: 2000 / 9351
2022-10-12 16:09:03 - train.py[line:551] - INFO: load:1.80 valid_run:839.57 task_valid:802.20 collect_output:27.35
2022-10-12 16:10:28 - train.py[line:549] - INFO: 2200 / 9351
2022-10-12 16:10:28 - train.py[line:551] - INFO: load:1.82 valid_run:923.71 task_valid:882.27 collect_output:30.40
2022-10-12 16:11:52 - train.py[line:549] - INFO: 2400 / 9351
2022-10-12 16:11:52 - train.py[line:551] - INFO: load:1.84 valid_run:1007.75 task_valid:963.11 collect_output:32.58
2022-10-12 16:13:17 - train.py[line:549] - INFO: 2600 / 9351
2022-10-12 16:13:17 - train.py[line:551] - INFO: load:1.86 valid_run:1092.49 task_valid:1043.17 collect_output:36.25
2022-10-12 16:14:41 - train.py[line:549] - INFO: 2800 / 9351
2022-10-12 16:14:41 - train.py[line:551] - INFO: load:1.88 valid_run:1176.58 task_valid:1122.07 collect_output:40.44
2022-10-12 16:16:04 - train.py[line:549] - INFO: 3000 / 9351
2022-10-12 16:16:04 - train.py[line:551] - INFO: load:1.90 valid_run:1259.71 task_valid:1199.92 collect_output:44.71
2022-10-12 16:17:26 - train.py[line:549] - INFO: 3200 / 9351
2022-10-12 16:17:26 - train.py[line:551] - INFO: load:1.92 valid_run:1341.94 task_valid:1278.96 collect_output:46.90
2022-10-12 16:18:50 - train.py[line:549] - INFO: 3400 / 9351
2022-10-12 16:18:50 - train.py[line:551] - INFO: load:1.95 valid_run:1425.82 task_valid:1358.58 collect_output:50.19
2022-10-12 16:20:14 - train.py[line:549] - INFO: 3600 / 9351
2022-10-12 16:20:14 - train.py[line:551] - INFO: load:1.97 valid_run:1509.34 task_valid:1439.55 collect_output:51.70
2022-10-12 16:21:36 - train.py[line:549] - INFO: 3800 / 9351
2022-10-12 16:21:36 - train.py[line:551] - INFO: load:1.99 valid_run:1591.45 task_valid:1517.46 collect_output:54.89
2022-10-12 16:22:58 - train.py[line:549] - INFO: 4000 / 9351
2022-10-12 16:22:58 - train.py[line:551] - INFO: load:2.01 valid_run:1673.48 task_valid:1596.26 collect_output:57.13
2022-10-12 16:24:22 - train.py[line:549] - INFO: 4200 / 9351
2022-10-12 16:24:22 - train.py[line:551] - INFO: load:2.03 valid_run:1757.06 task_valid:1677.85 collect_output:58.12
2022-10-12 16:25:45 - train.py[line:549] - INFO: 4400 / 9351
2022-10-12 16:25:45 - train.py[line:551] - INFO: load:2.05 valid_run:1840.45 task_valid:1757.79 collect_output:60.56
2022-10-12 16:27:08 - train.py[line:549] - INFO: 4600 / 9351
2022-10-12 16:27:08 - train.py[line:551] - INFO: load:2.07 valid_run:1923.03 task_valid:1837.23 collect_output:62.69
2022-10-12 16:28:32 - train.py[line:549] - INFO: 4800 / 9351
2022-10-12 16:28:32 - train.py[line:551] - INFO: load:2.10 valid_run:2006.86 task_valid:1915.87 collect_output:66.81
2022-10-12 16:29:56 - train.py[line:549] - INFO: 5000 / 9351
2022-10-12 16:29:56 - train.py[line:551] - INFO: load:2.12 valid_run:2090.95 task_valid:1996.77 collect_output:68.99
2022-10-12 16:31:19 - train.py[line:549] - INFO: 5200 / 9351
2022-10-12 16:31:19 - train.py[line:551] - INFO: load:2.14 valid_run:2174.50 task_valid:2077.31 collect_output:70.97
2022-10-12 16:32:42 - train.py[line:549] - INFO: 5400 / 9351
2022-10-12 16:32:42 - train.py[line:551] - INFO: load:2.16 valid_run:2257.27 task_valid:2157.32 collect_output:72.73
2022-10-12 16:34:06 - train.py[line:549] - INFO: 5600 / 9351
2022-10-12 16:34:06 - train.py[line:551] - INFO: load:2.18 valid_run:2341.38 task_valid:2238.66 collect_output:74.48
2022-10-12 16:35:29 - train.py[line:549] - INFO: 5800 / 9351
2022-10-12 16:35:29 - train.py[line:551] - INFO: load:2.21 valid_run:2424.42 task_valid:2317.78 collect_output:77.40
2022-10-12 16:36:52 - train.py[line:549] - INFO: 6000 / 9351
2022-10-12 16:36:52 - train.py[line:551] - INFO: load:2.23 valid_run:2507.48 task_valid:2397.93 collect_output:79.31
2022-10-12 16:38:16 - train.py[line:549] - INFO: 6200 / 9351
2022-10-12 16:38:16 - train.py[line:551] - INFO: load:2.25 valid_run:2590.56 task_valid:2477.58 collect_output:81.72
2022-10-12 16:39:41 - train.py[line:549] - INFO: 6400 / 9351
2022-10-12 16:39:41 - train.py[line:551] - INFO: load:2.27 valid_run:2676.18 task_valid:2559.38 collect_output:84.45
2022-10-12 16:41:06 - train.py[line:549] - INFO: 6600 / 9351
2022-10-12 16:41:06 - train.py[line:551] - INFO: load:2.30 valid_run:2761.25 task_valid:2642.16 collect_output:85.53
2022-10-12 16:42:30 - train.py[line:549] - INFO: 6800 / 9351
2022-10-12 16:42:30 - train.py[line:551] - INFO: load:2.32 valid_run:2845.09 task_valid:2722.64 collect_output:87.79
2022-10-12 16:43:54 - train.py[line:549] - INFO: 7000 / 9351
2022-10-12 16:43:54 - train.py[line:551] - INFO: load:2.34 valid_run:2929.09 task_valid:2802.79 collect_output:90.46
2022-10-12 16:45:18 - train.py[line:549] - INFO: 7200 / 9351
2022-10-12 16:45:18 - train.py[line:551] - INFO: load:2.37 valid_run:3012.93 task_valid:2883.99 collect_output:91.89
2022-10-12 16:46:43 - train.py[line:549] - INFO: 7400 / 9351
2022-10-12 16:46:43 - train.py[line:551] - INFO: load:2.39 valid_run:3097.13 task_valid:2964.25 collect_output:94.70
2022-10-12 16:48:08 - train.py[line:549] - INFO: 7600 / 9351
2022-10-12 16:48:08 - train.py[line:551] - INFO: load:2.42 valid_run:3182.81 task_valid:3045.62 collect_output:97.91
2022-10-12 16:49:34 - train.py[line:549] - INFO: 7800 / 9351
2022-10-12 16:49:34 - train.py[line:551] - INFO: load:2.45 valid_run:3268.13 task_valid:3126.30 collect_output:101.45
2022-10-12 16:50:57 - train.py[line:549] - INFO: 8000 / 9351
2022-10-12 16:50:57 - train.py[line:551] - INFO: load:2.47 valid_run:3351.40 task_valid:3205.50 collect_output:104.36
2022-10-12 16:52:22 - train.py[line:549] - INFO: 8200 / 9351
2022-10-12 16:52:22 - train.py[line:551] - INFO: load:2.49 valid_run:3436.15 task_valid:3287.82 collect_output:105.64
2022-10-12 16:53:46 - train.py[line:549] - INFO: 8400 / 9351
2022-10-12 16:53:46 - train.py[line:551] - INFO: load:2.51 valid_run:3520.71 task_valid:3369.94 collect_output:106.93
2022-10-12 16:55:11 - train.py[line:549] - INFO: 8600 / 9351
2022-10-12 16:55:11 - train.py[line:551] - INFO: load:2.54 valid_run:3605.02 task_valid:3450.64 collect_output:109.49
2022-10-12 16:56:35 - train.py[line:549] - INFO: 8800 / 9351
2022-10-12 16:56:35 - train.py[line:551] - INFO: load:2.56 valid_run:3689.02 task_valid:3530.60 collect_output:112.41
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 2602666
Killing subprocess 2602701
Main process received SIGINT, exiting
