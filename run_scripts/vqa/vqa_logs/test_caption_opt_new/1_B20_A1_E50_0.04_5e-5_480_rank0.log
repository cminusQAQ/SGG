2023-02-21 20:15:49 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-21 20:15:49 - utils.py[line:261] - INFO: Start init
2023-02-21 20:15:49 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-21 20:15:49 - utils.py[line:261] - INFO: Start init
2023-02-21 20:15:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-21 20:15:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-21 20:15:49 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-21 20:15:49 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-21 20:15:53 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_caption_opt_new', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=50, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_caption_opt_new', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-21 20:15:53 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-21 20:15:53 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-21 20:15:57 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-21 20:15:57 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-21 20:15:57 - train.py[line:119] - INFO: model: OFAModel
2023-02-21 20:15:57 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-21 20:15:57 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-21 20:15:57 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-21 20:15:57 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-21 20:15:58 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-21 20:15:58 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-21 20:15:58 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 20:15:58 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-21 20:15:58 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-21 20:16:00 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-21 20:16:00 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-21 20:16:00 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-02-21 20:16:08 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-21 20:16:09 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-21 20:16:09 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-21 20:16:09 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-21 20:16:09 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-21 20:16:09 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-21 20:16:09 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 20:16:09 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E0.tsv slice_id 1 row count 8240 total row count 16480
Total steps 20600, warmup steps 824, warmup_factor 0.0012135922330097086
Total steps 20600, warmup steps 824, warmup_factor 0.0012135922330097086
2023-02-21 20:16:10 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-21 20:16:10 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 20:16:32 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 412 loss=1.105, loss_v1=0, loss_v2=0, nll_loss=0.948, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.93, wps=69.2, ups=0.63, wpb=109.8, bsz=40, num_updates=10, lr=6.06796e-07, gnorm=15.722, clip=100, loss_scale=128, train_wall=19, gb_free=10.8, ema_decay=0.9999, wall=33
2023-02-21 20:16:43 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 412 loss=0.988, loss_v1=0, loss_v2=0, nll_loss=0.828, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=98, ups=0.88, wpb=111.9, bsz=40, num_updates=20, lr=1.21359e-06, gnorm=14.251, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45
2023-02-21 20:16:55 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 412 loss=0.983, loss_v1=0, loss_v2=0, nll_loss=0.838, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=95.7, ups=0.86, wpb=111.5, bsz=40, num_updates=30, lr=1.82039e-06, gnorm=11.52, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=56
2023-02-21 20:17:06 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 412 loss=0.854, loss_v1=0, loss_v2=0, nll_loss=0.716, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=96.2, ups=0.87, wpb=110.8, bsz=40, num_updates=40, lr=2.42718e-06, gnorm=9.145, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=68
2023-02-21 20:17:18 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 412 loss=0.719, loss_v1=0, loss_v2=0, nll_loss=0.577, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=95.5, ups=0.86, wpb=111.4, bsz=40, num_updates=50, lr=3.03398e-06, gnorm=5.867, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=79
2023-02-21 20:17:29 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 412 loss=0.732, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=97.8, ups=0.88, wpb=110.7, bsz=40, num_updates=60, lr=3.64078e-06, gnorm=5.263, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=91
2023-02-21 20:17:41 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 412 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=97.3, ups=0.87, wpb=111.3, bsz=40, num_updates=70, lr=4.24757e-06, gnorm=4.12, clip=100, loss_scale=128, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=102
2023-02-21 20:17:52 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 412 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=80, lr=4.85437e-06, gnorm=3.734, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=113
2023-02-21 20:18:03 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 412 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.4, ups=0.88, wpb=111.5, bsz=40, num_updates=90, lr=5.46117e-06, gnorm=3.62, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=125
2023-02-21 20:18:14 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 412 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=100, lr=6.06796e-06, gnorm=3.459, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=136
2023-02-21 20:18:25 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 412 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=110, lr=6.67476e-06, gnorm=3.285, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=147
2023-02-21 20:18:37 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 412 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.9, ups=0.88, wpb=110, bsz=40, num_updates=120, lr=7.28155e-06, gnorm=3.415, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=158
2023-02-21 20:18:48 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 412 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.6, ups=0.9, wpb=111.5, bsz=40, num_updates=130, lr=7.88835e-06, gnorm=3.248, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=170
2023-02-21 20:18:59 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 412 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.8, ups=0.87, wpb=112.2, bsz=40, num_updates=140, lr=8.49515e-06, gnorm=3.623, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=181
2023-02-21 20:19:11 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 412 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.9, ups=0.87, wpb=111.2, bsz=40, num_updates=150, lr=9.10194e-06, gnorm=3.22, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=192
2023-02-21 20:19:22 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 412 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.4, ups=0.87, wpb=112.2, bsz=40, num_updates=160, lr=9.70874e-06, gnorm=2.94, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=204
2023-02-21 20:19:34 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 412 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=170, lr=1.03155e-05, gnorm=2.612, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=215
2023-02-21 20:19:45 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 412 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=180, lr=1.09223e-05, gnorm=2.576, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=226
2023-02-21 20:19:56 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 412 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.4, ups=0.92, wpb=112.8, bsz=40, num_updates=190, lr=1.15291e-05, gnorm=2.51, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=237
2023-02-21 20:20:07 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 412 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.3, ups=0.88, wpb=111.5, bsz=40, num_updates=200, lr=1.21359e-05, gnorm=2.412, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=249
2023-02-21 20:20:18 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 412 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=210, lr=1.27427e-05, gnorm=2.724, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=260
2023-02-21 20:20:30 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 412 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=220, lr=1.33495e-05, gnorm=2.454, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=271
2023-02-21 20:20:40 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 412 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=104, ups=0.94, wpb=110.8, bsz=40, num_updates=230, lr=1.39563e-05, gnorm=2.882, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=282
2023-02-21 20:20:51 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 412 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101.4, ups=0.91, wpb=111.8, bsz=40, num_updates=240, lr=1.45631e-05, gnorm=2.532, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=293
2023-02-21 20:21:02 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 412 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=250, lr=1.51699e-05, gnorm=2.246, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=304
2023-02-21 20:21:13 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 412 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.1, ups=0.9, wpb=111.4, bsz=40, num_updates=260, lr=1.57767e-05, gnorm=2.582, clip=100, loss_scale=128, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=315
2023-02-21 20:21:25 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 412 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100, ups=0.9, wpb=111.1, bsz=40, num_updates=270, lr=1.63835e-05, gnorm=2.387, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=326
2023-02-21 20:21:36 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 412 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=280, lr=1.69903e-05, gnorm=2.265, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=338
2023-02-21 20:21:48 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 412 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.6, ups=0.89, wpb=109.8, bsz=40, num_updates=290, lr=1.75971e-05, gnorm=2.546, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=349
2023-02-21 20:21:59 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 412 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=300, lr=1.82039e-05, gnorm=2.438, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=361
2023-02-21 20:22:10 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 412 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=310, lr=1.88107e-05, gnorm=2.143, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=372
2023-02-21 20:22:22 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 412 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=320, lr=1.94175e-05, gnorm=2.026, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=383
2023-02-21 20:22:33 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 412 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=103.5, ups=0.92, wpb=112.8, bsz=40, num_updates=330, lr=2.00243e-05, gnorm=2.187, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=394
2023-02-21 20:22:43 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 412 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.7, ups=0.91, wpb=110.1, bsz=40, num_updates=340, lr=2.06311e-05, gnorm=1.992, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=405
2023-02-21 20:22:55 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 412 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=350, lr=2.12379e-05, gnorm=2.099, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=416
2023-02-21 20:23:06 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 412 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.1, ups=0.87, wpb=110.3, bsz=40, num_updates=360, lr=2.18447e-05, gnorm=2.25, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=428
2023-02-21 20:23:17 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 412 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=370, lr=2.24515e-05, gnorm=2.005, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=439
2023-02-21 20:23:28 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 412 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=380, lr=2.30583e-05, gnorm=2.118, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=450
2023-02-21 20:23:40 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 412 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=390, lr=2.3665e-05, gnorm=1.883, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=461
2023-02-21 20:23:51 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 412 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=400, lr=2.42718e-05, gnorm=1.808, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=473
2023-02-21 20:24:02 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 412 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.6, ups=0.92, wpb=111.6, bsz=40, num_updates=410, lr=2.48786e-05, gnorm=1.946, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=484
2023-02-21 20:24:04 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-02-21 20:24:04 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.497 | loss_v1 0 | loss_v2 0 | nll_loss 0.332 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.26 | wps 97.8 | ups 0.88 | wpb 111.1 | bsz 40 | num_updates 412 | lr 2.5e-05 | gnorm 3.749 | clip 99.8 | loss_scale 128 | train_wall 469 | gb_free 10.7 | ema_decay 0.9999 | wall 486
2023-02-21 20:24:04 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E1.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 20:24:04 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 20:24:05 - trainer.py[line:758] - INFO: begin training epoch 2
2023-02-21 20:24:05 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 20:24:16 - progress_bar.py[line:274] - INFO: epoch 002:      8 / 412 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=82.7, ups=0.75, wpb=110, bsz=40, num_updates=420, lr=2.54854e-05, gnorm=1.898, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=497
2023-02-21 20:24:27 - progress_bar.py[line:274] - INFO: epoch 002:     18 / 412 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.5, ups=0.91, wpb=111.2, bsz=40, num_updates=430, lr=2.60922e-05, gnorm=1.772, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=509
2023-02-21 20:24:38 - progress_bar.py[line:274] - INFO: epoch 002:     28 / 412 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=110.3, bsz=40, num_updates=440, lr=2.6699e-05, gnorm=1.65, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=520
2023-02-21 20:24:50 - progress_bar.py[line:274] - INFO: epoch 002:     38 / 412 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.2, ups=0.88, wpb=111.7, bsz=40, num_updates=450, lr=2.73058e-05, gnorm=1.857, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=531
2023-02-21 20:25:01 - progress_bar.py[line:274] - INFO: epoch 002:     48 / 412 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.2, ups=0.92, wpb=109.4, bsz=40, num_updates=460, lr=2.79126e-05, gnorm=1.886, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=542
2023-02-21 20:25:12 - progress_bar.py[line:274] - INFO: epoch 002:     58 / 412 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100, ups=0.9, wpb=111.1, bsz=40, num_updates=470, lr=2.85194e-05, gnorm=1.999, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=553
2023-02-21 20:25:23 - progress_bar.py[line:274] - INFO: epoch 002:     68 / 412 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.5, ups=0.87, wpb=111.3, bsz=40, num_updates=480, lr=2.91262e-05, gnorm=1.936, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=565
2023-02-21 20:25:34 - progress_bar.py[line:274] - INFO: epoch 002:     78 / 412 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=490, lr=2.9733e-05, gnorm=2.087, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=576
2023-02-21 20:25:46 - progress_bar.py[line:274] - INFO: epoch 002:     88 / 412 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=500, lr=3.03398e-05, gnorm=1.95, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=587
2023-02-21 20:25:57 - progress_bar.py[line:274] - INFO: epoch 002:     98 / 412 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=95.3, ups=0.87, wpb=109.8, bsz=40, num_updates=510, lr=3.09466e-05, gnorm=1.847, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=599
2023-02-21 20:26:08 - progress_bar.py[line:274] - INFO: epoch 002:    108 / 412 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=520, lr=3.15534e-05, gnorm=2.02, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=610
2023-02-21 20:26:19 - progress_bar.py[line:274] - INFO: epoch 002:    118 / 412 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100, ups=0.9, wpb=111, bsz=40, num_updates=530, lr=3.21602e-05, gnorm=2.071, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=621
2023-02-21 20:26:30 - progress_bar.py[line:274] - INFO: epoch 002:    128 / 412 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.9, ups=0.91, wpb=111.5, bsz=40, num_updates=540, lr=3.2767e-05, gnorm=1.574, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=632
2023-02-21 20:26:41 - progress_bar.py[line:274] - INFO: epoch 002:    138 / 412 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=550, lr=3.33738e-05, gnorm=2.253, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=643
2023-02-21 20:26:52 - progress_bar.py[line:274] - INFO: epoch 002:    148 / 412 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.9, ups=0.92, wpb=112.4, bsz=40, num_updates=560, lr=3.39806e-05, gnorm=1.591, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=654
2023-02-21 20:27:03 - progress_bar.py[line:274] - INFO: epoch 002:    158 / 412 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=570, lr=3.45874e-05, gnorm=1.656, clip=100, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=665
2023-02-21 20:27:15 - progress_bar.py[line:274] - INFO: epoch 002:    168 / 412 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.88, wpb=112.5, bsz=40, num_updates=580, lr=3.51942e-05, gnorm=1.801, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=676
2023-02-21 20:27:27 - progress_bar.py[line:274] - INFO: epoch 002:    178 / 412 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=590, lr=3.5801e-05, gnorm=1.83, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=688
2023-02-21 20:27:38 - progress_bar.py[line:274] - INFO: epoch 002:    188 / 412 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=600, lr=3.64078e-05, gnorm=1.862, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=699
2023-02-21 20:27:49 - progress_bar.py[line:274] - INFO: epoch 002:    198 / 412 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=610, lr=3.70146e-05, gnorm=1.783, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=711
2023-02-21 20:28:00 - progress_bar.py[line:274] - INFO: epoch 002:    208 / 412 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.89, wpb=111.5, bsz=40, num_updates=620, lr=3.76214e-05, gnorm=1.8, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=722
2023-02-21 20:28:12 - progress_bar.py[line:274] - INFO: epoch 002:    218 / 412 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=630, lr=3.82282e-05, gnorm=1.976, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=733
2023-02-21 20:28:23 - progress_bar.py[line:274] - INFO: epoch 002:    228 / 412 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98, ups=0.89, wpb=110.1, bsz=40, num_updates=640, lr=3.8835e-05, gnorm=1.575, clip=90, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=744
2023-02-21 20:28:34 - progress_bar.py[line:274] - INFO: epoch 002:    238 / 412 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.6, ups=0.87, wpb=111.8, bsz=40, num_updates=650, lr=3.94417e-05, gnorm=1.637, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=756
2023-02-21 20:28:46 - progress_bar.py[line:274] - INFO: epoch 002:    248 / 412 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=660, lr=4.00485e-05, gnorm=1.737, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=767
2023-02-21 20:28:57 - progress_bar.py[line:274] - INFO: epoch 002:    258 / 412 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=670, lr=4.06553e-05, gnorm=1.708, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=778
2023-02-21 20:29:08 - progress_bar.py[line:274] - INFO: epoch 002:    268 / 412 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.89, wpb=110.6, bsz=40, num_updates=680, lr=4.12621e-05, gnorm=1.804, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=790
2023-02-21 20:29:19 - progress_bar.py[line:274] - INFO: epoch 002:    278 / 412 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=690, lr=4.18689e-05, gnorm=1.577, clip=70, loss_scale=256, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=801
2023-02-21 20:29:31 - progress_bar.py[line:274] - INFO: epoch 002:    288 / 412 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.4, ups=0.88, wpb=112, bsz=40, num_updates=700, lr=4.24757e-05, gnorm=1.51, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=812
2023-02-21 20:29:42 - progress_bar.py[line:274] - INFO: epoch 002:    298 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96, ups=0.87, wpb=110.1, bsz=40, num_updates=710, lr=4.30825e-05, gnorm=1.628, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=824
2023-02-21 20:29:53 - progress_bar.py[line:274] - INFO: epoch 002:    308 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.89, wpb=111.7, bsz=40, num_updates=720, lr=4.36893e-05, gnorm=1.494, clip=100, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=835
2023-02-21 20:30:05 - progress_bar.py[line:274] - INFO: epoch 002:    318 / 412 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.1, ups=0.89, wpb=111.3, bsz=40, num_updates=730, lr=4.42961e-05, gnorm=1.536, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=846
2023-02-21 20:30:16 - progress_bar.py[line:274] - INFO: epoch 002:    328 / 412 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.9, ups=0.88, wpb=110.5, bsz=40, num_updates=740, lr=4.49029e-05, gnorm=1.571, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=858
2023-02-21 20:30:27 - progress_bar.py[line:274] - INFO: epoch 002:    338 / 412 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.4, ups=0.91, wpb=111.4, bsz=40, num_updates=750, lr=4.55097e-05, gnorm=1.472, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=869
2023-02-21 20:30:38 - progress_bar.py[line:274] - INFO: epoch 002:    348 / 412 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=760, lr=4.61165e-05, gnorm=1.62, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=880
2023-02-21 20:30:49 - progress_bar.py[line:274] - INFO: epoch 002:    358 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.7, ups=0.88, wpb=110.6, bsz=40, num_updates=770, lr=4.67233e-05, gnorm=1.36, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=891
2023-02-21 20:31:01 - progress_bar.py[line:274] - INFO: epoch 002:    368 / 412 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=780, lr=4.73301e-05, gnorm=1.674, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=902
2023-02-21 20:31:12 - progress_bar.py[line:274] - INFO: epoch 002:    378 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.7, ups=0.87, wpb=111.3, bsz=40, num_updates=790, lr=4.79369e-05, gnorm=1.582, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=914
2023-02-21 20:31:23 - progress_bar.py[line:274] - INFO: epoch 002:    388 / 412 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=800, lr=4.85437e-05, gnorm=1.601, clip=80, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=925
2023-02-21 20:31:34 - progress_bar.py[line:274] - INFO: epoch 002:    398 / 412 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=810, lr=4.91505e-05, gnorm=1.515, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=936
2023-02-21 20:31:45 - progress_bar.py[line:274] - INFO: epoch 002:    408 / 412 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.2, ups=0.91, wpb=111.4, bsz=40, num_updates=820, lr=4.97573e-05, gnorm=2.007, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=947
2023-02-21 20:31:50 - train.py[line:339] - INFO: end of epoch 2 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 20:31:50 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.327 | loss_v1 0 | loss_v2 0 | nll_loss 0.149 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.11 | wps 98.3 | ups 0.88 | wpb 111.1 | bsz 40 | num_updates 824 | lr 5e-05 | gnorm 1.745 | clip 94.2 | loss_scale 256 | train_wall 461 | gb_free 10.6 | ema_decay 0.9999 | wall 952
2023-02-21 20:31:50 - trainer.py[line:694] - INFO: loading train data for epoch 3
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E2.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 20:31:50 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 20:31:51 - trainer.py[line:758] - INFO: begin training epoch 3
2023-02-21 20:31:51 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 20:31:59 - progress_bar.py[line:274] - INFO: epoch 003:      6 / 412 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=82.5, ups=0.73, wpb=112.5, bsz=40, num_updates=830, lr=4.99848e-05, gnorm=1.272, clip=80, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=961
2023-02-21 20:32:11 - progress_bar.py[line:274] - INFO: epoch 003:     16 / 412 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96, ups=0.87, wpb=110.5, bsz=40, num_updates=840, lr=4.99595e-05, gnorm=1.647, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=972
2023-02-21 20:32:22 - progress_bar.py[line:274] - INFO: epoch 003:     26 / 412 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=850, lr=4.99343e-05, gnorm=1.617, clip=90, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=983
2023-02-21 20:32:33 - progress_bar.py[line:274] - INFO: epoch 003:     36 / 412 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102, ups=0.91, wpb=111.6, bsz=40, num_updates=860, lr=4.9909e-05, gnorm=1.796, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=994
2023-02-21 20:32:44 - progress_bar.py[line:274] - INFO: epoch 003:     46 / 412 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=870, lr=4.98837e-05, gnorm=1.277, clip=70, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1006
2023-02-21 20:32:55 - progress_bar.py[line:274] - INFO: epoch 003:     56 / 412 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102, ups=0.92, wpb=111.3, bsz=40, num_updates=880, lr=4.98584e-05, gnorm=1.542, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1017
2023-02-21 20:33:06 - progress_bar.py[line:274] - INFO: epoch 003:     66 / 412 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.8, ups=0.92, wpb=111.2, bsz=40, num_updates=890, lr=4.98331e-05, gnorm=1.719, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1027
2023-02-21 20:33:17 - progress_bar.py[line:274] - INFO: epoch 003:     76 / 412 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.5, ups=0.92, wpb=110.8, bsz=40, num_updates=900, lr=4.98078e-05, gnorm=1.476, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1038
2023-02-21 20:33:28 - progress_bar.py[line:274] - INFO: epoch 003:     86 / 412 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=910, lr=4.97826e-05, gnorm=1.298, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1049
2023-02-21 20:33:39 - progress_bar.py[line:274] - INFO: epoch 003:     96 / 412 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.87, wpb=112.1, bsz=40, num_updates=920, lr=4.97573e-05, gnorm=1.215, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1061
2023-02-21 20:33:50 - progress_bar.py[line:274] - INFO: epoch 003:    106 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.91, wpb=111.5, bsz=40, num_updates=930, lr=4.9732e-05, gnorm=1.183, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1072
2023-02-21 20:34:01 - progress_bar.py[line:274] - INFO: epoch 003:    116 / 412 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.5, ups=0.9, wpb=112.3, bsz=40, num_updates=940, lr=4.97067e-05, gnorm=1.578, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1083
2023-02-21 20:34:13 - progress_bar.py[line:274] - INFO: epoch 003:    126 / 412 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=950, lr=4.96814e-05, gnorm=1.697, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1094
2023-02-21 20:34:24 - progress_bar.py[line:274] - INFO: epoch 003:    136 / 412 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.4, ups=0.88, wpb=110.1, bsz=40, num_updates=960, lr=4.96561e-05, gnorm=1.486, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1106
2023-02-21 20:34:35 - progress_bar.py[line:274] - INFO: epoch 003:    146 / 412 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.89, wpb=110.8, bsz=40, num_updates=970, lr=4.96309e-05, gnorm=1.189, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1117
2023-02-21 20:34:47 - progress_bar.py[line:274] - INFO: epoch 003:    156 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=980, lr=4.96056e-05, gnorm=1.543, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1128
2023-02-21 20:34:58 - progress_bar.py[line:274] - INFO: epoch 003:    166 / 412 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=990, lr=4.95803e-05, gnorm=1.726, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1139
2023-02-21 20:35:09 - progress_bar.py[line:274] - INFO: epoch 003:    176 / 412 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=1000, lr=4.9555e-05, gnorm=1.312, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1150
2023-02-21 20:35:09 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 20:35:09 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 20:35:10 - train.py[line:549] - INFO: 0 / 6234
2023-02-21 20:35:10 - train.py[line:551] - INFO: load:0.98 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 20:37:15 - train.py[line:549] - INFO: 200 / 6234
2023-02-21 20:37:15 - train.py[line:551] - INFO: load:1.00 valid_run:124.73 task_valid:121.58 collect_output:2.09
2023-02-21 20:39:15 - train.py[line:549] - INFO: 400 / 6234
2023-02-21 20:39:15 - train.py[line:551] - INFO: load:1.02 valid_run:245.02 task_valid:237.63 collect_output:5.31
2023-02-21 20:41:19 - train.py[line:549] - INFO: 600 / 6234
2023-02-21 20:41:19 - train.py[line:551] - INFO: load:1.05 valid_run:369.03 task_valid:354.48 collect_output:11.46
2023-02-21 20:43:22 - train.py[line:549] - INFO: 800 / 6234
2023-02-21 20:43:22 - train.py[line:551] - INFO: load:1.07 valid_run:491.27 task_valid:468.44 collect_output:18.72
2023-02-21 20:45:23 - train.py[line:549] - INFO: 1000 / 6234
2023-02-21 20:45:23 - train.py[line:551] - INFO: load:1.10 valid_run:612.15 task_valid:586.00 collect_output:21.05
2023-02-21 20:47:26 - train.py[line:549] - INFO: 1200 / 6234
2023-02-21 20:47:26 - train.py[line:551] - INFO: load:1.12 valid_run:735.50 task_valid:705.01 collect_output:24.40
2023-02-21 20:49:29 - train.py[line:549] - INFO: 1400 / 6234
2023-02-21 20:49:29 - train.py[line:551] - INFO: load:1.15 valid_run:858.83 task_valid:823.34 collect_output:28.38
2023-02-21 20:51:32 - train.py[line:549] - INFO: 1600 / 6234
2023-02-21 20:51:32 - train.py[line:551] - INFO: load:1.17 valid_run:980.98 task_valid:940.09 collect_output:32.78
2023-02-21 20:53:36 - train.py[line:549] - INFO: 1800 / 6234
2023-02-21 20:53:36 - train.py[line:551] - INFO: load:1.19 valid_run:1105.11 task_valid:1057.56 collect_output:38.44
2023-02-21 20:55:38 - train.py[line:549] - INFO: 2000 / 6234
2023-02-21 20:55:38 - train.py[line:551] - INFO: load:1.22 valid_run:1227.19 task_valid:1170.41 collect_output:46.68
2023-02-21 20:57:39 - train.py[line:549] - INFO: 2200 / 6234
2023-02-21 20:57:39 - train.py[line:551] - INFO: load:1.24 valid_run:1347.88 task_valid:1286.47 collect_output:50.26
2023-02-21 20:59:41 - train.py[line:549] - INFO: 2400 / 6234
2023-02-21 20:59:41 - train.py[line:551] - INFO: load:1.27 valid_run:1470.41 task_valid:1404.22 collect_output:53.97
2023-02-21 21:01:41 - train.py[line:549] - INFO: 2600 / 6234
2023-02-21 21:01:41 - train.py[line:551] - INFO: load:1.30 valid_run:1590.09 task_valid:1518.59 collect_output:58.25
2023-02-21 21:03:43 - train.py[line:549] - INFO: 2800 / 6234
2023-02-21 21:03:43 - train.py[line:551] - INFO: load:1.33 valid_run:1712.25 task_valid:1637.34 collect_output:60.53
2023-02-21 21:05:45 - train.py[line:549] - INFO: 3000 / 6234
2023-02-21 21:05:45 - train.py[line:551] - INFO: load:1.36 valid_run:1833.84 task_valid:1754.11 collect_output:64.29
2023-02-21 21:07:47 - train.py[line:549] - INFO: 3200 / 6234
2023-02-21 21:07:47 - train.py[line:551] - INFO: load:1.39 valid_run:1955.54 task_valid:1868.51 collect_output:70.52
2023-02-21 21:09:49 - train.py[line:549] - INFO: 3400 / 6234
2023-02-21 21:09:49 - train.py[line:551] - INFO: load:1.42 valid_run:2077.39 task_valid:1985.13 collect_output:74.67
2023-02-21 21:11:50 - train.py[line:549] - INFO: 3600 / 6234
2023-02-21 21:11:50 - train.py[line:551] - INFO: load:1.45 valid_run:2198.46 task_valid:2103.33 collect_output:76.47
2023-02-21 21:13:52 - train.py[line:549] - INFO: 3800 / 6234
2023-02-21 21:13:52 - train.py[line:551] - INFO: load:1.47 valid_run:2320.30 task_valid:2220.69 collect_output:79.90
2023-02-21 21:15:53 - train.py[line:549] - INFO: 4000 / 6234
2023-02-21 21:15:53 - train.py[line:551] - INFO: load:1.50 valid_run:2441.40 task_valid:2337.72 collect_output:82.91
2023-02-21 21:17:55 - train.py[line:549] - INFO: 4200 / 6234
2023-02-21 21:17:55 - train.py[line:551] - INFO: load:1.53 valid_run:2563.85 task_valid:2454.74 collect_output:87.29
2023-02-21 21:19:58 - train.py[line:549] - INFO: 4400 / 6234
2023-02-21 21:19:58 - train.py[line:551] - INFO: load:1.56 valid_run:2686.45 task_valid:2574.04 collect_output:89.52
2023-02-21 21:21:59 - train.py[line:549] - INFO: 4600 / 6234
2023-02-21 21:21:59 - train.py[line:551] - INFO: load:1.59 valid_run:2807.73 task_valid:2688.95 collect_output:94.83
2023-02-21 21:24:00 - train.py[line:549] - INFO: 4800 / 6234
2023-02-21 21:24:00 - train.py[line:551] - INFO: load:1.61 valid_run:2928.04 task_valid:2805.42 collect_output:97.60
2023-02-21 21:26:02 - train.py[line:549] - INFO: 5000 / 6234
2023-02-21 21:26:02 - train.py[line:551] - INFO: load:1.64 valid_run:3050.38 task_valid:2922.01 collect_output:102.27
2023-02-21 21:28:06 - train.py[line:549] - INFO: 5200 / 6234
2023-02-21 21:28:06 - train.py[line:551] - INFO: load:1.67 valid_run:3173.88 task_valid:3038.36 collect_output:108.33
2023-02-21 21:30:06 - train.py[line:549] - INFO: 5400 / 6234
2023-02-21 21:30:06 - train.py[line:551] - INFO: load:1.70 valid_run:3294.26 task_valid:3153.11 collect_output:112.88
2023-02-21 21:32:09 - train.py[line:549] - INFO: 5600 / 6234
2023-02-21 21:32:09 - train.py[line:551] - INFO: load:1.73 valid_run:3416.83 task_valid:3273.05 collect_output:114.42
2023-02-21 21:34:11 - train.py[line:549] - INFO: 5800 / 6234
2023-02-21 21:34:11 - train.py[line:551] - INFO: load:1.76 valid_run:3539.47 task_valid:3389.35 collect_output:119.68
2023-02-21 21:36:14 - train.py[line:549] - INFO: 6000 / 6234
2023-02-21 21:36:14 - train.py[line:551] - INFO: load:1.79 valid_run:3662.43 task_valid:3508.48 collect_output:122.42
2023-02-21 21:38:16 - train.py[line:549] - INFO: 6200 / 6234
2023-02-21 21:38:16 - train.py[line:551] - INFO: load:1.81 valid_run:3784.15 task_valid:3627.28 collect_output:124.25

====================================================================================================
SGG eval:     R @ 50: 0.2462;     R @ 100: 0.3179;     R @ 500: 0.3922;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1213;    mR @ 100: 0.1688;    mR @ 500: 0.2105;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0634) (covered in:0.0000) (covering:0.0000) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.4516) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2083) (playing:0.0000) (riding:0.3363) (says:0.0000) (sitting on:0.3498) (standing on:0.5317) (using:0.1500) (walking in:0.0000) (walking on:0.1351) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-21 21:38:47 - train.py[line:487] - INFO: 0.31789999999999996

====================================================================================================
SGG eval:     R @ 50: 0.2462;     R @ 100: 0.3179;     R @ 500: 0.3922;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1213;    mR @ 100: 0.1688;    mR @ 500: 0.2105;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0634) (covered in:0.0000) (covering:0.0000) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.4516) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2083) (playing:0.0000) (riding:0.3363) (says:0.0000) (sitting on:0.3498) (standing on:0.5317) (using:0.1500) (walking in:0.0000) (walking on:0.1351) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-21 21:38:47 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-21 21:38:47 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.24 | loss_v1 0 | loss_v2 0 | nll_loss 0.064 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.3179 | ppl 1.05 | vqa_score 0.1757 | wps 117.6 | wpb 72 | bsz 24 | num_updates 1000
2023-02-21 21:38:47 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 1000 updates
2023-02-21 21:38:47 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_3_1000.pt
2023-02-21 21:38:53 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_3_1000.pt
2023-02-21 21:38:59 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_3_1000.pt (epoch 3 @ 1000 updates, score 0.31789999999999996) (writing took 12.034866655245423 seconds)
2023-02-21 21:39:10 - progress_bar.py[line:274] - INFO: epoch 003:    186 / 412 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=0.3, ups=0, wpb=112.3, bsz=40, num_updates=1010, lr=4.95297e-05, gnorm=1.395, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4992
2023-02-21 21:39:21 - progress_bar.py[line:274] - INFO: epoch 003:    196 / 412 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=1020, lr=4.95044e-05, gnorm=1.116, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5003
2023-02-21 21:39:33 - progress_bar.py[line:274] - INFO: epoch 003:    206 / 412 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.9, wpb=109.8, bsz=40, num_updates=1030, lr=4.94792e-05, gnorm=1.358, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5014
2023-02-21 21:39:44 - progress_bar.py[line:274] - INFO: epoch 003:    216 / 412 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=1040, lr=4.94539e-05, gnorm=1.812, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5025
2023-02-21 21:39:55 - progress_bar.py[line:274] - INFO: epoch 003:    226 / 412 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.7, ups=0.91, wpb=111.4, bsz=40, num_updates=1050, lr=4.94286e-05, gnorm=1.325, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5036
2023-02-21 21:40:06 - progress_bar.py[line:274] - INFO: epoch 003:    236 / 412 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.9, wpb=110, bsz=40, num_updates=1060, lr=4.94033e-05, gnorm=1.214, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5047
2023-02-21 21:40:17 - progress_bar.py[line:274] - INFO: epoch 003:    246 / 412 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.9, wpb=110.9, bsz=40, num_updates=1070, lr=4.9378e-05, gnorm=1.269, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5058
2023-02-21 21:40:28 - progress_bar.py[line:274] - INFO: epoch 003:    256 / 412 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=1080, lr=4.93528e-05, gnorm=1.677, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5070
2023-02-21 21:40:39 - progress_bar.py[line:274] - INFO: epoch 003:    266 / 412 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.5, bsz=40, num_updates=1090, lr=4.93275e-05, gnorm=1.13, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5081
2023-02-21 21:40:51 - progress_bar.py[line:274] - INFO: epoch 003:    276 / 412 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.88, wpb=112.1, bsz=40, num_updates=1100, lr=4.93022e-05, gnorm=1.294, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5092
2023-02-21 21:41:02 - progress_bar.py[line:274] - INFO: epoch 003:    286 / 412 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.3, ups=0.9, wpb=111.1, bsz=40, num_updates=1110, lr=4.92769e-05, gnorm=1.554, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5103
2023-02-21 21:41:13 - progress_bar.py[line:274] - INFO: epoch 003:    296 / 412 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.9, wpb=110.1, bsz=40, num_updates=1120, lr=4.92516e-05, gnorm=1.323, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5114
2023-02-21 21:41:24 - progress_bar.py[line:274] - INFO: epoch 003:    306 / 412 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.6, ups=0.9, wpb=110.2, bsz=40, num_updates=1130, lr=4.92263e-05, gnorm=1.618, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5126
2023-02-21 21:41:35 - progress_bar.py[line:274] - INFO: epoch 003:    316 / 412 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.2, ups=0.93, wpb=111.4, bsz=40, num_updates=1140, lr=4.92011e-05, gnorm=1.456, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5136
2023-02-21 21:41:46 - progress_bar.py[line:274] - INFO: epoch 003:    326 / 412 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.9, wpb=110.5, bsz=40, num_updates=1150, lr=4.91758e-05, gnorm=1.536, clip=100, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=5148
2023-02-21 21:41:57 - progress_bar.py[line:274] - INFO: epoch 003:    336 / 412 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.8, ups=0.92, wpb=111.3, bsz=40, num_updates=1160, lr=4.91505e-05, gnorm=1.374, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5158
2023-02-21 21:42:08 - progress_bar.py[line:274] - INFO: epoch 003:    346 / 412 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=1170, lr=4.91252e-05, gnorm=1.072, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5170
2023-02-21 21:42:19 - progress_bar.py[line:274] - INFO: epoch 003:    356 / 412 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=1180, lr=4.90999e-05, gnorm=1.885, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5181
2023-02-21 21:42:30 - progress_bar.py[line:274] - INFO: epoch 003:    366 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=1190, lr=4.90746e-05, gnorm=1.873, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5192
2023-02-21 21:42:41 - progress_bar.py[line:274] - INFO: epoch 003:    376 / 412 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.3, ups=0.9, wpb=110.3, bsz=40, num_updates=1200, lr=4.90494e-05, gnorm=1.965, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5203
2023-02-21 21:42:53 - progress_bar.py[line:274] - INFO: epoch 003:    386 / 412 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=1210, lr=4.90241e-05, gnorm=1.216, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5214
2023-02-21 21:43:03 - progress_bar.py[line:274] - INFO: epoch 003:    396 / 412 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.7, ups=0.93, wpb=112.1, bsz=40, num_updates=1220, lr=4.89988e-05, gnorm=2.176, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5225
2023-02-21 21:43:15 - progress_bar.py[line:274] - INFO: epoch 003:    406 / 412 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.89, wpb=109.6, bsz=40, num_updates=1230, lr=4.89735e-05, gnorm=1.252, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5236
2023-02-21 21:43:21 - train.py[line:339] - INFO: end of epoch 3 (average epoch stats below)
2023-02-21 21:43:21 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.301 | loss_v1 0 | loss_v2 0 | nll_loss 0.122 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.09 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 1236 | lr 4.89583e-05 | gnorm 1.472 | clip 76.9 | loss_scale 512 | train_wall 457 | gb_free 10.7 | ema_decay 0.9999 | wall 5243
2023-02-21 21:43:21 - trainer.py[line:694] - INFO: loading train data for epoch 4
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E3.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 21:43:21 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 21:43:22 - trainer.py[line:758] - INFO: begin training epoch 4
2023-02-21 21:43:22 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 21:43:28 - progress_bar.py[line:274] - INFO: epoch 004:      4 / 412 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=80.9, ups=0.74, wpb=110, bsz=40, num_updates=1240, lr=4.89482e-05, gnorm=1.147, clip=50, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5250
2023-02-21 21:43:39 - progress_bar.py[line:274] - INFO: epoch 004:     14 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=1250, lr=4.89229e-05, gnorm=1.218, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5261
2023-02-21 21:43:51 - progress_bar.py[line:274] - INFO: epoch 004:     24 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.9, wpb=110.5, bsz=40, num_updates=1260, lr=4.88977e-05, gnorm=1.141, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5272
2023-02-21 21:44:02 - progress_bar.py[line:274] - INFO: epoch 004:     34 / 412 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=1270, lr=4.88724e-05, gnorm=1.202, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5283
2023-02-21 21:44:13 - progress_bar.py[line:274] - INFO: epoch 004:     44 / 412 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.5, ups=0.87, wpb=109.9, bsz=40, num_updates=1280, lr=4.88471e-05, gnorm=0.952, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5295
2023-02-21 21:44:24 - progress_bar.py[line:274] - INFO: epoch 004:     54 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.91, wpb=111.1, bsz=40, num_updates=1290, lr=4.88218e-05, gnorm=1.198, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5306
2023-02-21 21:44:36 - progress_bar.py[line:274] - INFO: epoch 004:     64 / 412 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.87, wpb=111.9, bsz=40, num_updates=1300, lr=4.87965e-05, gnorm=1.482, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5317
2023-02-21 21:44:47 - progress_bar.py[line:274] - INFO: epoch 004:     74 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=111.6, bsz=40, num_updates=1310, lr=4.87712e-05, gnorm=1.558, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5329
2023-02-21 21:44:58 - progress_bar.py[line:274] - INFO: epoch 004:     84 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.9, wpb=109.6, bsz=40, num_updates=1320, lr=4.8746e-05, gnorm=1.102, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5340
2023-02-21 21:45:09 - progress_bar.py[line:274] - INFO: epoch 004:     94 / 412 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.5, ups=0.89, wpb=111.1, bsz=40, num_updates=1330, lr=4.87207e-05, gnorm=1.455, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5351
2023-02-21 21:45:20 - progress_bar.py[line:274] - INFO: epoch 004:    104 / 412 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.5, ups=0.94, wpb=110.7, bsz=40, num_updates=1340, lr=4.86954e-05, gnorm=1.137, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5362
2023-02-21 21:45:31 - progress_bar.py[line:274] - INFO: epoch 004:    114 / 412 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=111.6, bsz=40, num_updates=1350, lr=4.86701e-05, gnorm=1.574, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5373
2023-02-21 21:45:43 - progress_bar.py[line:274] - INFO: epoch 004:    124 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=112, bsz=40, num_updates=1360, lr=4.86448e-05, gnorm=1.218, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5384
2023-02-21 21:45:54 - progress_bar.py[line:274] - INFO: epoch 004:    134 / 412 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=1370, lr=4.86195e-05, gnorm=1.121, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5396
2023-02-21 21:46:05 - progress_bar.py[line:274] - INFO: epoch 004:    144 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.92, wpb=110.7, bsz=40, num_updates=1380, lr=4.85943e-05, gnorm=1.225, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5407
2023-02-21 21:46:16 - progress_bar.py[line:274] - INFO: epoch 004:    154 / 412 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=1390, lr=4.8569e-05, gnorm=1.452, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5418
2023-02-21 21:46:27 - progress_bar.py[line:274] - INFO: epoch 004:    164 / 412 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.9, wpb=112, bsz=40, num_updates=1400, lr=4.85437e-05, gnorm=1.298, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5429
2023-02-21 21:46:39 - progress_bar.py[line:274] - INFO: epoch 004:    174 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=112.1, bsz=40, num_updates=1410, lr=4.85184e-05, gnorm=1.223, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5440
2023-02-21 21:46:50 - progress_bar.py[line:274] - INFO: epoch 004:    184 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.88, wpb=111.5, bsz=40, num_updates=1420, lr=4.84931e-05, gnorm=1.117, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5452
2023-02-21 21:47:01 - progress_bar.py[line:274] - INFO: epoch 004:    194 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=112.3, bsz=40, num_updates=1430, lr=4.84678e-05, gnorm=1.341, clip=50, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=5463
2023-02-21 21:47:12 - progress_bar.py[line:274] - INFO: epoch 004:    204 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=111, bsz=40, num_updates=1440, lr=4.84426e-05, gnorm=1.399, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5474
2023-02-21 21:47:23 - progress_bar.py[line:274] - INFO: epoch 004:    214 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112.5, bsz=40, num_updates=1450, lr=4.84173e-05, gnorm=1.426, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5485
2023-02-21 21:47:34 - progress_bar.py[line:274] - INFO: epoch 004:    224 / 412 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.5, ups=0.91, wpb=112.6, bsz=40, num_updates=1460, lr=4.8392e-05, gnorm=1.274, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5496
2023-02-21 21:47:46 - progress_bar.py[line:274] - INFO: epoch 004:    234 / 412 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.5, ups=0.87, wpb=110.1, bsz=40, num_updates=1470, lr=4.83667e-05, gnorm=1.226, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5508
2023-02-21 21:47:57 - progress_bar.py[line:274] - INFO: epoch 004:    244 / 412 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=1480, lr=4.83414e-05, gnorm=1.249, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5519
2023-02-21 21:48:08 - progress_bar.py[line:274] - INFO: epoch 004:    254 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=1490, lr=4.83161e-05, gnorm=0.943, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5530
2023-02-21 21:48:20 - progress_bar.py[line:274] - INFO: epoch 004:    264 / 412 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=111.5, bsz=40, num_updates=1500, lr=4.82909e-05, gnorm=1.244, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5541
2023-02-21 21:48:31 - progress_bar.py[line:274] - INFO: epoch 004:    274 / 412 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.88, wpb=110.4, bsz=40, num_updates=1510, lr=4.82656e-05, gnorm=1.134, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5553
2023-02-21 21:48:42 - progress_bar.py[line:274] - INFO: epoch 004:    284 / 412 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=110.7, bsz=40, num_updates=1520, lr=4.82403e-05, gnorm=1.08, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5564
2023-02-21 21:48:53 - progress_bar.py[line:274] - INFO: epoch 004:    294 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.6, ups=0.93, wpb=111.9, bsz=40, num_updates=1530, lr=4.8215e-05, gnorm=1.207, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5575
2023-02-21 21:49:04 - progress_bar.py[line:274] - INFO: epoch 004:    304 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=1540, lr=4.81897e-05, gnorm=1.185, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5586
2023-02-21 21:49:16 - progress_bar.py[line:274] - INFO: epoch 004:    314 / 412 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.89, wpb=111.1, bsz=40, num_updates=1550, lr=4.81644e-05, gnorm=1.051, clip=60, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5597
2023-02-21 21:49:27 - progress_bar.py[line:274] - INFO: epoch 004:    324 / 412 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.9, bsz=40, num_updates=1560, lr=4.81392e-05, gnorm=1.082, clip=50, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5609
2023-02-21 21:49:38 - progress_bar.py[line:274] - INFO: epoch 004:    334 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=1570, lr=4.81139e-05, gnorm=1.029, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5620
2023-02-21 21:49:49 - progress_bar.py[line:274] - INFO: epoch 004:    344 / 412 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.91, wpb=111.2, bsz=40, num_updates=1580, lr=4.80886e-05, gnorm=1.227, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5631
2023-02-21 21:49:57 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-21 21:50:02 - progress_bar.py[line:274] - INFO: epoch 004:    355 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=89.4, ups=0.8, wpb=111.7, bsz=40, num_updates=1590, lr=4.80633e-05, gnorm=1.052, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5643
2023-02-21 21:50:13 - progress_bar.py[line:274] - INFO: epoch 004:    365 / 412 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.88, wpb=111, bsz=40, num_updates=1600, lr=4.8038e-05, gnorm=1.093, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5655
2023-02-21 21:50:24 - progress_bar.py[line:274] - INFO: epoch 004:    375 / 412 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=110.2, bsz=40, num_updates=1610, lr=4.80127e-05, gnorm=1.632, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5666
2023-02-21 21:50:35 - progress_bar.py[line:274] - INFO: epoch 004:    385 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.9, wpb=109.5, bsz=40, num_updates=1620, lr=4.79875e-05, gnorm=1.129, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5677
2023-02-21 21:50:46 - progress_bar.py[line:274] - INFO: epoch 004:    395 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=1630, lr=4.79622e-05, gnorm=1.138, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5688
2023-02-21 21:50:58 - progress_bar.py[line:274] - INFO: epoch 004:    405 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.89, wpb=109.6, bsz=40, num_updates=1640, lr=4.79369e-05, gnorm=1.674, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5699
2023-02-21 21:51:06 - train.py[line:339] - INFO: end of epoch 4 (average epoch stats below)
2023-02-21 21:51:06 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.28 | loss_v1 0 | loss_v2 0 | nll_loss 0.098 | ntokens 111.129 | nsentences 40 | sample_size 111.129 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.07 | wps 98.4 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 1647 | lr 4.79192e-05 | gnorm 1.235 | clip 59.6 | loss_scale 512 | train_wall 460 | gb_free 10.9 | ema_decay 0.9999 | wall 5707
2023-02-21 21:51:06 - trainer.py[line:694] - INFO: loading train data for epoch 5
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E4.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 21:51:06 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 21:51:06 - trainer.py[line:758] - INFO: begin training epoch 5
2023-02-21 21:51:06 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 21:51:11 - progress_bar.py[line:274] - INFO: epoch 005:      3 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=81.3, ups=0.73, wpb=111, bsz=40, num_updates=1650, lr=4.79116e-05, gnorm=1.027, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5713
2023-02-21 21:51:23 - progress_bar.py[line:274] - INFO: epoch 005:     13 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=112.3, bsz=40, num_updates=1660, lr=4.78863e-05, gnorm=1.46, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5724
2023-02-21 21:51:34 - progress_bar.py[line:274] - INFO: epoch 005:     23 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=110.8, bsz=40, num_updates=1670, lr=4.7861e-05, gnorm=1.017, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5735
2023-02-21 21:51:45 - progress_bar.py[line:274] - INFO: epoch 005:     33 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.87, wpb=111.1, bsz=40, num_updates=1680, lr=4.78358e-05, gnorm=1.031, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5747
2023-02-21 21:51:56 - progress_bar.py[line:274] - INFO: epoch 005:     43 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.92, wpb=110.8, bsz=40, num_updates=1690, lr=4.78105e-05, gnorm=1.169, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5758
2023-02-21 21:52:07 - progress_bar.py[line:274] - INFO: epoch 005:     53 / 412 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.92, wpb=110.5, bsz=40, num_updates=1700, lr=4.77852e-05, gnorm=1.037, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5769
2023-02-21 21:52:18 - progress_bar.py[line:274] - INFO: epoch 005:     63 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.88, wpb=110.9, bsz=40, num_updates=1710, lr=4.77599e-05, gnorm=0.766, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5780
2023-02-21 21:52:30 - progress_bar.py[line:274] - INFO: epoch 005:     73 / 412 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.89, wpb=110.6, bsz=40, num_updates=1720, lr=4.77346e-05, gnorm=1.167, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5791
2023-02-21 21:52:41 - progress_bar.py[line:274] - INFO: epoch 005:     83 / 412 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=111, bsz=40, num_updates=1730, lr=4.77093e-05, gnorm=1.05, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5802
2023-02-21 21:52:52 - progress_bar.py[line:274] - INFO: epoch 005:     93 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=111.7, bsz=40, num_updates=1740, lr=4.76841e-05, gnorm=0.897, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5813
2023-02-21 21:53:03 - progress_bar.py[line:274] - INFO: epoch 005:    103 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=1750, lr=4.76588e-05, gnorm=0.735, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5825
2023-02-21 21:53:14 - progress_bar.py[line:274] - INFO: epoch 005:    113 / 412 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=110.8, bsz=40, num_updates=1760, lr=4.76335e-05, gnorm=1.317, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5836
2023-02-21 21:53:26 - progress_bar.py[line:274] - INFO: epoch 005:    123 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97, ups=0.87, wpb=112, bsz=40, num_updates=1770, lr=4.76082e-05, gnorm=0.923, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5847
2023-02-21 21:53:37 - progress_bar.py[line:274] - INFO: epoch 005:    133 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.86, wpb=111.7, bsz=40, num_updates=1780, lr=4.75829e-05, gnorm=1.169, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5859
2023-02-21 21:53:48 - progress_bar.py[line:274] - INFO: epoch 005:    143 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.2, bsz=40, num_updates=1790, lr=4.75576e-05, gnorm=1.118, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5870
2023-02-21 21:53:59 - progress_bar.py[line:274] - INFO: epoch 005:    153 / 412 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=1800, lr=4.75324e-05, gnorm=0.943, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5881
2023-02-21 21:54:11 - progress_bar.py[line:274] - INFO: epoch 005:    163 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=111.8, bsz=40, num_updates=1810, lr=4.75071e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5892
2023-02-21 21:54:22 - progress_bar.py[line:274] - INFO: epoch 005:    173 / 412 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=1820, lr=4.74818e-05, gnorm=1.162, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5903
2023-02-21 21:54:33 - progress_bar.py[line:274] - INFO: epoch 005:    183 / 412 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.1, bsz=40, num_updates=1830, lr=4.74565e-05, gnorm=1.106, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5915
2023-02-21 21:54:44 - progress_bar.py[line:274] - INFO: epoch 005:    193 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=1840, lr=4.74312e-05, gnorm=1.273, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5926
2023-02-21 21:54:55 - progress_bar.py[line:274] - INFO: epoch 005:    203 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=1850, lr=4.74059e-05, gnorm=0.673, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5937
2023-02-21 21:55:06 - progress_bar.py[line:274] - INFO: epoch 005:    213 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=112.3, bsz=40, num_updates=1860, lr=4.73807e-05, gnorm=0.705, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5948
2023-02-21 21:55:18 - progress_bar.py[line:274] - INFO: epoch 005:    223 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=1870, lr=4.73554e-05, gnorm=0.987, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5959
2023-02-21 21:55:29 - progress_bar.py[line:274] - INFO: epoch 005:    233 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=1880, lr=4.73301e-05, gnorm=0.916, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5970
2023-02-21 21:55:40 - progress_bar.py[line:274] - INFO: epoch 005:    243 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=1890, lr=4.73048e-05, gnorm=1.517, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5981
2023-02-21 21:55:51 - progress_bar.py[line:274] - INFO: epoch 005:    253 / 412 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.89, wpb=110.4, bsz=40, num_updates=1900, lr=4.72795e-05, gnorm=1.364, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5993
2023-02-21 21:56:03 - progress_bar.py[line:274] - INFO: epoch 005:    263 / 412 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.3, ups=0.87, wpb=110.5, bsz=40, num_updates=1910, lr=4.72542e-05, gnorm=1.172, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6004
2023-02-21 21:56:14 - progress_bar.py[line:274] - INFO: epoch 005:    273 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=1920, lr=4.7229e-05, gnorm=0.934, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6015
2023-02-21 21:56:25 - progress_bar.py[line:274] - INFO: epoch 005:    283 / 412 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=1930, lr=4.72037e-05, gnorm=1.373, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6027
2023-02-21 21:56:37 - progress_bar.py[line:274] - INFO: epoch 005:    293 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=1940, lr=4.71784e-05, gnorm=0.899, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6038
2023-02-21 21:56:48 - progress_bar.py[line:274] - INFO: epoch 005:    303 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=112.8, bsz=40, num_updates=1950, lr=4.71531e-05, gnorm=0.947, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6049
2023-02-21 21:56:59 - progress_bar.py[line:274] - INFO: epoch 005:    313 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=1960, lr=4.71278e-05, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6061
2023-02-21 21:57:10 - progress_bar.py[line:274] - INFO: epoch 005:    323 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=110.6, bsz=40, num_updates=1970, lr=4.71025e-05, gnorm=0.765, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6072
2023-02-21 21:57:21 - progress_bar.py[line:274] - INFO: epoch 005:    333 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=1980, lr=4.70773e-05, gnorm=0.987, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6083
2023-02-21 21:57:33 - progress_bar.py[line:274] - INFO: epoch 005:    343 / 412 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=110.5, bsz=40, num_updates=1990, lr=4.7052e-05, gnorm=1.241, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6094
2023-02-21 21:57:44 - progress_bar.py[line:274] - INFO: epoch 005:    353 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.2, ups=0.89, wpb=109, bsz=40, num_updates=2000, lr=4.70267e-05, gnorm=1.14, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6106
2023-02-21 21:57:44 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 21:57:46 - train.py[line:549] - INFO: 0 / 6234
2023-02-21 21:57:46 - train.py[line:551] - INFO: load:1.12 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 21:59:50 - train.py[line:549] - INFO: 200 / 6234
2023-02-21 21:59:50 - train.py[line:551] - INFO: load:1.15 valid_run:123.88 task_valid:120.61 collect_output:2.09
2023-02-21 22:01:50 - train.py[line:549] - INFO: 400 / 6234
2023-02-21 22:01:50 - train.py[line:551] - INFO: load:1.18 valid_run:244.76 task_valid:237.43 collect_output:5.06
2023-02-21 22:03:54 - train.py[line:549] - INFO: 600 / 6234
2023-02-21 22:03:54 - train.py[line:551] - INFO: load:1.21 valid_run:367.85 task_valid:354.58 collect_output:9.93
2023-02-21 22:05:56 - train.py[line:549] - INFO: 800 / 6234
2023-02-21 22:05:56 - train.py[line:551] - INFO: load:1.23 valid_run:490.22 task_valid:468.71 collect_output:17.12
2023-02-21 22:07:57 - train.py[line:549] - INFO: 1000 / 6234
2023-02-21 22:07:57 - train.py[line:551] - INFO: load:1.26 valid_run:611.31 task_valid:586.63 collect_output:19.26
2023-02-21 22:10:01 - train.py[line:549] - INFO: 1200 / 6234
2023-02-21 22:10:01 - train.py[line:551] - INFO: load:1.28 valid_run:734.89 task_valid:706.03 collect_output:22.40
2023-02-21 22:12:04 - train.py[line:549] - INFO: 1400 / 6234
2023-02-21 22:12:04 - train.py[line:551] - INFO: load:1.31 valid_run:858.38 task_valid:824.62 collect_output:26.30
2023-02-21 22:14:07 - train.py[line:549] - INFO: 1600 / 6234
2023-02-21 22:14:07 - train.py[line:551] - INFO: load:1.33 valid_run:980.65 task_valid:941.58 collect_output:30.60
2023-02-21 22:16:11 - train.py[line:549] - INFO: 1800 / 6234
2023-02-21 22:16:11 - train.py[line:551] - INFO: load:1.36 valid_run:1104.85 task_valid:1059.29 collect_output:36.07
2023-02-21 22:18:13 - train.py[line:549] - INFO: 2000 / 6234
2023-02-21 22:18:13 - train.py[line:551] - INFO: load:1.38 valid_run:1227.04 task_valid:1172.66 collect_output:43.85
2023-02-21 22:20:14 - train.py[line:549] - INFO: 2200 / 6234
2023-02-21 22:20:14 - train.py[line:551] - INFO: load:1.41 valid_run:1347.81 task_valid:1288.87 collect_output:47.40
2023-02-21 22:22:16 - train.py[line:549] - INFO: 2400 / 6234
2023-02-21 22:22:16 - train.py[line:551] - INFO: load:1.43 valid_run:1469.89 task_valid:1406.41 collect_output:50.90
2023-02-21 22:24:16 - train.py[line:549] - INFO: 2600 / 6234
2023-02-21 22:24:16 - train.py[line:551] - INFO: load:1.46 valid_run:1589.29 task_valid:1520.68 collect_output:55.01
2023-02-21 22:26:17 - train.py[line:549] - INFO: 2800 / 6234
2023-02-21 22:26:17 - train.py[line:551] - INFO: load:1.48 valid_run:1711.09 task_valid:1639.33 collect_output:57.10
2023-02-21 22:28:19 - train.py[line:549] - INFO: 3000 / 6234
2023-02-21 22:28:19 - train.py[line:551] - INFO: load:1.51 valid_run:1832.60 task_valid:1756.17 collect_output:60.73
2023-02-21 22:30:21 - train.py[line:549] - INFO: 3200 / 6234
2023-02-21 22:30:21 - train.py[line:551] - INFO: load:1.53 valid_run:1954.33 task_valid:1870.74 collect_output:66.87
2023-02-21 22:32:23 - train.py[line:549] - INFO: 3400 / 6234
2023-02-21 22:32:23 - train.py[line:551] - INFO: load:1.56 valid_run:2076.36 task_valid:1987.58 collect_output:71.03
2023-02-21 22:34:24 - train.py[line:549] - INFO: 3600 / 6234
2023-02-21 22:34:24 - train.py[line:551] - INFO: load:1.59 valid_run:2197.64 task_valid:2106.11 collect_output:72.75
2023-02-21 22:36:26 - train.py[line:549] - INFO: 3800 / 6234
2023-02-21 22:36:26 - train.py[line:551] - INFO: load:1.61 valid_run:2319.45 task_valid:2223.63 collect_output:76.01
2023-02-21 22:38:27 - train.py[line:549] - INFO: 4000 / 6234
2023-02-21 22:38:27 - train.py[line:551] - INFO: load:1.64 valid_run:2440.61 task_valid:2341.11 collect_output:78.66
2023-02-21 22:40:30 - train.py[line:549] - INFO: 4200 / 6234
2023-02-21 22:40:30 - train.py[line:551] - INFO: load:1.67 valid_run:2563.02 task_valid:2458.52 collect_output:82.60
2023-02-21 22:42:33 - train.py[line:549] - INFO: 4400 / 6234
2023-02-21 22:42:33 - train.py[line:551] - INFO: load:1.70 valid_run:2685.68 task_valid:2578.25 collect_output:84.46
2023-02-21 22:44:34 - train.py[line:549] - INFO: 4600 / 6234
2023-02-21 22:44:34 - train.py[line:551] - INFO: load:1.72 valid_run:2806.59 task_valid:2693.15 collect_output:89.44
2023-02-21 22:46:34 - train.py[line:549] - INFO: 4800 / 6234
2023-02-21 22:46:34 - train.py[line:551] - INFO: load:1.75 valid_run:2926.86 task_valid:2809.92 collect_output:91.93
2023-02-21 22:48:36 - train.py[line:549] - INFO: 5000 / 6234
2023-02-21 22:48:36 - train.py[line:551] - INFO: load:1.77 valid_run:3048.88 task_valid:2926.57 collect_output:96.26
2023-02-21 22:50:39 - train.py[line:549] - INFO: 5200 / 6234
2023-02-21 22:50:39 - train.py[line:551] - INFO: load:1.80 valid_run:3172.15 task_valid:3043.07 collect_output:101.99
2023-02-21 22:52:39 - train.py[line:549] - INFO: 5400 / 6234
2023-02-21 22:52:39 - train.py[line:551] - INFO: load:1.82 valid_run:3292.12 task_valid:3157.72 collect_output:106.31
2023-02-21 22:54:42 - train.py[line:549] - INFO: 5600 / 6234
2023-02-21 22:54:42 - train.py[line:551] - INFO: load:1.85 valid_run:3414.48 task_valid:3277.61 collect_output:107.76
2023-02-21 22:56:44 - train.py[line:549] - INFO: 5800 / 6234
2023-02-21 22:56:44 - train.py[line:551] - INFO: load:1.87 valid_run:3536.73 task_valid:3393.73 collect_output:112.87
2023-02-21 22:58:46 - train.py[line:549] - INFO: 6000 / 6234
2023-02-21 22:58:46 - train.py[line:551] - INFO: load:1.90 valid_run:3659.04 task_valid:3512.66 collect_output:115.23
2023-02-21 23:00:48 - train.py[line:549] - INFO: 6200 / 6234
2023-02-21 23:00:48 - train.py[line:551] - INFO: load:1.92 valid_run:3780.67 task_valid:3631.63 collect_output:116.89

====================================================================================================
SGG eval:     R @ 50: 0.4290;     R @ 100: 0.4754;     R @ 500: 0.5362;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2205;    mR @ 100: 0.2671;    mR @ 500: 0.3015;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3073) (covered in:0.0000) (covering:0.2857) (eating:0.5882) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5226) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.6716) (says:0.0000) (sitting on:0.5346) (standing on:0.5467) (using:0.2500) (walking in:0.0000) (walking on:0.2432) (watching:0.0417) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4290;     R @ 100: 0.4754;     R @ 500: 0.5362;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2205;    mR @ 100: 0.2671;    mR @ 500: 0.3015;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3073) (covered in:0.0000) (covering:0.2857) (eating:0.5882) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5226) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.6716) (says:0.0000) (sitting on:0.5346) (standing on:0.5467) (using:0.2500) (walking in:0.0000) (walking on:0.2432) (watching:0.0417) 
--------------------------------------------------------
====================================================================================================

2023-02-21 23:01:19 - train.py[line:487] - INFO: 0.47543333333333326
2023-02-21 23:01:19 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-21 23:01:19 - progress_bar.py[line:282] - INFO: epoch 005 | valid on 'valid' subset | loss 0.24 | loss_v1 0 | loss_v2 0 | nll_loss 0.064 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.475433 | ppl 1.05 | vqa_score 0.2117 | wps 117.6 | wpb 72 | bsz 24 | num_updates 2000 | best_R@100 0.475433
2023-02-21 23:01:19 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 5 @ 2000 updates
2023-02-21 23:01:19 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_5_2000.pt
2023-02-21 23:01:25 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_5_2000.pt
2023-02-21 23:01:30 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_5_2000.pt (epoch 5 @ 2000 updates, score 0.47543333333333326) (writing took 11.554528886452317 seconds)
2023-02-21 23:01:42 - progress_bar.py[line:274] - INFO: epoch 005:    363 / 412 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=111.4, bsz=40, num_updates=2010, lr=4.70014e-05, gnorm=1.151, clip=60, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=9943
2023-02-21 23:02:03 - progress_bar.py[line:274] - INFO: epoch 005:    373 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112, bsz=40, num_updates=2020, lr=4.69761e-05, gnorm=1.183, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=9954
2023-02-21 23:02:14 - progress_bar.py[line:274] - INFO: epoch 005:    383 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=2030, lr=4.69508e-05, gnorm=1.116, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9976
2023-02-21 23:02:26 - progress_bar.py[line:274] - INFO: epoch 005:    393 / 412 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=110.9, bsz=40, num_updates=2040, lr=4.69256e-05, gnorm=1.125, clip=60, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=9987
2023-02-21 23:02:37 - progress_bar.py[line:274] - INFO: epoch 005:    403 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97, ups=0.87, wpb=111.7, bsz=40, num_updates=2050, lr=4.69003e-05, gnorm=0.992, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9999
2023-02-21 23:02:47 - train.py[line:339] - INFO: end of epoch 5 (average epoch stats below)
2023-02-21 23:02:47 - progress_bar.py[line:282] - INFO: epoch 005 | loss 0.269 | loss_v1 0 | loss_v2 0 | nll_loss 0.086 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.06 | wps 10.6 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 2059 | lr 4.68775e-05 | gnorm 1.055 | clip 48.8 | loss_scale 512 | train_wall 460 | gb_free 10.8 | ema_decay 0.9999 | wall 10009
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 23:02:47 - trainer.py[line:694] - INFO: loading train data for epoch 6
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E5.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 23:02:47 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 23:02:48 - trainer.py[line:758] - INFO: begin training epoch 6
2023-02-21 23:02:48 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 23:02:51 - progress_bar.py[line:274] - INFO: epoch 006:      1 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=82, ups=0.75, wpb=110, bsz=40, num_updates=2060, lr=4.6875e-05, gnorm=0.866, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10012
2023-02-21 23:03:01 - progress_bar.py[line:274] - INFO: epoch 006:     11 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.92, wpb=111.8, bsz=40, num_updates=2070, lr=4.68497e-05, gnorm=0.84, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10023
2023-02-21 23:03:13 - progress_bar.py[line:274] - INFO: epoch 006:     21 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=110.7, bsz=40, num_updates=2080, lr=4.68244e-05, gnorm=0.736, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10034
2023-02-21 23:03:24 - progress_bar.py[line:274] - INFO: epoch 006:     31 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=2090, lr=4.67992e-05, gnorm=0.872, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10046
2023-02-21 23:03:35 - progress_bar.py[line:274] - INFO: epoch 006:     41 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.1, bsz=40, num_updates=2100, lr=4.67739e-05, gnorm=0.808, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10057
2023-02-21 23:03:46 - progress_bar.py[line:274] - INFO: epoch 006:     51 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=2110, lr=4.67486e-05, gnorm=1.081, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10068
2023-02-21 23:03:58 - progress_bar.py[line:274] - INFO: epoch 006:     61 / 412 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.9, bsz=40, num_updates=2120, lr=4.67233e-05, gnorm=1.093, clip=60, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10079
2023-02-21 23:04:09 - progress_bar.py[line:274] - INFO: epoch 006:     71 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=110.5, bsz=40, num_updates=2130, lr=4.6698e-05, gnorm=1.098, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10090
2023-02-21 23:04:20 - progress_bar.py[line:274] - INFO: epoch 006:     81 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.4, ups=0.88, wpb=109.5, bsz=40, num_updates=2140, lr=4.66727e-05, gnorm=0.754, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10102
2023-02-21 23:04:31 - progress_bar.py[line:274] - INFO: epoch 006:     91 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=2150, lr=4.66475e-05, gnorm=0.948, clip=40, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10113
2023-02-21 23:04:42 - progress_bar.py[line:274] - INFO: epoch 006:    101 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=2160, lr=4.66222e-05, gnorm=0.861, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10124
2023-02-21 23:04:53 - progress_bar.py[line:274] - INFO: epoch 006:    111 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=2170, lr=4.65969e-05, gnorm=0.911, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10135
2023-02-21 23:05:04 - progress_bar.py[line:274] - INFO: epoch 006:    121 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=112.8, bsz=40, num_updates=2180, lr=4.65716e-05, gnorm=1.009, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10146
2023-02-21 23:05:07 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-21 23:05:17 - progress_bar.py[line:274] - INFO: epoch 006:    132 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=91.2, ups=0.82, wpb=111.6, bsz=40, num_updates=2190, lr=4.65463e-05, gnorm=0.782, clip=30, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=10158
2023-02-21 23:05:28 - progress_bar.py[line:274] - INFO: epoch 006:    142 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=2200, lr=4.6521e-05, gnorm=1.028, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10170
2023-02-21 23:05:39 - progress_bar.py[line:274] - INFO: epoch 006:    152 / 412 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=2210, lr=4.64958e-05, gnorm=0.943, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10181
2023-02-21 23:05:50 - progress_bar.py[line:274] - INFO: epoch 006:    162 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=2220, lr=4.64705e-05, gnorm=1.148, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10192
2023-02-21 23:06:01 - progress_bar.py[line:274] - INFO: epoch 006:    172 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=2230, lr=4.64452e-05, gnorm=0.915, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10203
2023-02-21 23:06:12 - progress_bar.py[line:274] - INFO: epoch 006:    182 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.91, wpb=111.3, bsz=40, num_updates=2240, lr=4.64199e-05, gnorm=0.817, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10214
2023-02-21 23:06:24 - progress_bar.py[line:274] - INFO: epoch 006:    192 / 412 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.89, wpb=109.2, bsz=40, num_updates=2250, lr=4.63946e-05, gnorm=0.977, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10225
2023-02-21 23:06:35 - progress_bar.py[line:274] - INFO: epoch 006:    202 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.89, wpb=110.8, bsz=40, num_updates=2260, lr=4.63693e-05, gnorm=1.095, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10237
2023-02-21 23:06:47 - progress_bar.py[line:274] - INFO: epoch 006:    212 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.88, wpb=111.1, bsz=40, num_updates=2270, lr=4.63441e-05, gnorm=0.757, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10248
2023-02-21 23:06:58 - progress_bar.py[line:274] - INFO: epoch 006:    222 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.89, wpb=113, bsz=40, num_updates=2280, lr=4.63188e-05, gnorm=0.963, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10259
2023-02-21 23:07:09 - progress_bar.py[line:274] - INFO: epoch 006:    232 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.92, wpb=112.7, bsz=40, num_updates=2290, lr=4.62935e-05, gnorm=1.14, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10270
2023-02-21 23:07:20 - progress_bar.py[line:274] - INFO: epoch 006:    242 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.6, bsz=40, num_updates=2300, lr=4.62682e-05, gnorm=0.872, clip=20, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=10282
2023-02-21 23:07:31 - progress_bar.py[line:274] - INFO: epoch 006:    252 / 412 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=2310, lr=4.62429e-05, gnorm=1.21, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10293
2023-02-21 23:07:42 - progress_bar.py[line:274] - INFO: epoch 006:    262 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.92, wpb=110.9, bsz=40, num_updates=2320, lr=4.62176e-05, gnorm=1.278, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10304
2023-02-21 23:07:54 - progress_bar.py[line:274] - INFO: epoch 006:    272 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=2330, lr=4.61924e-05, gnorm=0.746, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10315
2023-02-21 23:08:05 - progress_bar.py[line:274] - INFO: epoch 006:    282 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=2340, lr=4.61671e-05, gnorm=0.986, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10326
2023-02-21 23:08:16 - progress_bar.py[line:274] - INFO: epoch 006:    292 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=2350, lr=4.61418e-05, gnorm=1.087, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10338
2023-02-21 23:08:27 - progress_bar.py[line:274] - INFO: epoch 006:    302 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=2360, lr=4.61165e-05, gnorm=0.898, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10349
2023-02-21 23:08:38 - progress_bar.py[line:274] - INFO: epoch 006:    312 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=112.4, bsz=40, num_updates=2370, lr=4.60912e-05, gnorm=0.982, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10360
2023-02-21 23:08:49 - progress_bar.py[line:274] - INFO: epoch 006:    322 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.2, bsz=40, num_updates=2380, lr=4.60659e-05, gnorm=1.122, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10371
2023-02-21 23:09:00 - progress_bar.py[line:274] - INFO: epoch 006:    332 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=2390, lr=4.60407e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10382
2023-02-21 23:09:12 - progress_bar.py[line:274] - INFO: epoch 006:    342 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=2400, lr=4.60154e-05, gnorm=0.998, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10393
2023-02-21 23:09:23 - progress_bar.py[line:274] - INFO: epoch 006:    352 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96, ups=0.87, wpb=109.9, bsz=40, num_updates=2410, lr=4.59901e-05, gnorm=1.226, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10405
2023-02-21 23:09:34 - progress_bar.py[line:274] - INFO: epoch 006:    362 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=2420, lr=4.59648e-05, gnorm=0.915, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10416
2023-02-21 23:09:45 - progress_bar.py[line:274] - INFO: epoch 006:    372 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.93, wpb=111.3, bsz=40, num_updates=2430, lr=4.59395e-05, gnorm=0.855, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10427
2023-02-21 23:09:56 - progress_bar.py[line:274] - INFO: epoch 006:    382 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.7, bsz=40, num_updates=2440, lr=4.59142e-05, gnorm=0.849, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10438
2023-02-21 23:10:08 - progress_bar.py[line:274] - INFO: epoch 006:    392 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=2450, lr=4.5889e-05, gnorm=0.945, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10449
2023-02-21 23:10:19 - progress_bar.py[line:274] - INFO: epoch 006:    402 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=110.5, bsz=40, num_updates=2460, lr=4.58637e-05, gnorm=0.873, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10461
2023-02-21 23:10:30 - progress_bar.py[line:274] - INFO: epoch 006:    412 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=111.5, bsz=40, num_updates=2470, lr=4.58384e-05, gnorm=1.159, clip=70, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=10471
2023-02-21 23:10:30 - train.py[line:339] - INFO: end of epoch 6 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 23:10:30 - progress_bar.py[line:282] - INFO: epoch 006 | loss 0.261 | loss_v1 0 | loss_v2 0 | nll_loss 0.077 | ntokens 111.131 | nsentences 40 | sample_size 111.131 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.06 | wps 98.7 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 2470 | lr 4.58384e-05 | gnorm 0.955 | clip 41.1 | loss_scale 512 | train_wall 458 | gb_free 10 | ema_decay 0.9999 | wall 10472
2023-02-21 23:10:30 - trainer.py[line:694] - INFO: loading train data for epoch 7
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E6.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 23:10:30 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 23:10:30 - trainer.py[line:758] - INFO: begin training epoch 7
2023-02-21 23:10:30 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 23:10:43 - progress_bar.py[line:274] - INFO: epoch 007:     10 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=84.4, ups=0.75, wpb=112.5, bsz=40, num_updates=2480, lr=4.58131e-05, gnorm=0.673, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10485
2023-02-21 23:10:54 - progress_bar.py[line:274] - INFO: epoch 007:     20 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=2490, lr=4.57878e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10496
2023-02-21 23:11:06 - progress_bar.py[line:274] - INFO: epoch 007:     30 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=2500, lr=4.57625e-05, gnorm=0.77, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10507
2023-02-21 23:11:17 - progress_bar.py[line:274] - INFO: epoch 007:     40 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=2510, lr=4.57373e-05, gnorm=0.892, clip=40, loss_scale=512, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=10518
2023-02-21 23:11:28 - progress_bar.py[line:274] - INFO: epoch 007:     50 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111.1, bsz=40, num_updates=2520, lr=4.5712e-05, gnorm=0.68, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10530
2023-02-21 23:11:40 - progress_bar.py[line:274] - INFO: epoch 007:     60 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=2530, lr=4.56867e-05, gnorm=0.779, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=10541
2023-02-21 23:11:51 - progress_bar.py[line:274] - INFO: epoch 007:     70 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=2540, lr=4.56614e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10552
2023-02-21 23:12:02 - progress_bar.py[line:274] - INFO: epoch 007:     80 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=2550, lr=4.56361e-05, gnorm=0.837, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=10563
2023-02-21 23:12:13 - progress_bar.py[line:274] - INFO: epoch 007:     90 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111, bsz=40, num_updates=2560, lr=4.56108e-05, gnorm=0.614, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10575
2023-02-21 23:12:24 - progress_bar.py[line:274] - INFO: epoch 007:    100 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97, ups=0.88, wpb=110.2, bsz=40, num_updates=2570, lr=4.55856e-05, gnorm=0.569, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10586
2023-02-21 23:12:36 - progress_bar.py[line:274] - INFO: epoch 007:    110 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.4, bsz=40, num_updates=2580, lr=4.55603e-05, gnorm=0.738, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10597
2023-02-21 23:12:47 - progress_bar.py[line:274] - INFO: epoch 007:    120 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=111.8, bsz=40, num_updates=2590, lr=4.5535e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10608
2023-02-21 23:12:57 - progress_bar.py[line:274] - INFO: epoch 007:    130 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.92, wpb=110.6, bsz=40, num_updates=2600, lr=4.55097e-05, gnorm=1.16, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10619
2023-02-21 23:13:09 - progress_bar.py[line:274] - INFO: epoch 007:    140 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.87, wpb=111.6, bsz=40, num_updates=2610, lr=4.54844e-05, gnorm=0.928, clip=30, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10630
2023-02-21 23:13:20 - progress_bar.py[line:274] - INFO: epoch 007:    150 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=2620, lr=4.54591e-05, gnorm=0.946, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10642
2023-02-21 23:13:31 - progress_bar.py[line:274] - INFO: epoch 007:    160 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=2630, lr=4.54339e-05, gnorm=0.71, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10653
2023-02-21 23:13:42 - progress_bar.py[line:274] - INFO: epoch 007:    170 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112.2, bsz=40, num_updates=2640, lr=4.54086e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10664
2023-02-21 23:13:53 - progress_bar.py[line:274] - INFO: epoch 007:    180 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=2650, lr=4.53833e-05, gnorm=0.829, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10675
2023-02-21 23:14:04 - progress_bar.py[line:274] - INFO: epoch 007:    190 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=2660, lr=4.5358e-05, gnorm=0.921, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10686
2023-02-21 23:14:15 - progress_bar.py[line:274] - INFO: epoch 007:    200 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.91, wpb=111.5, bsz=40, num_updates=2670, lr=4.53327e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10697
2023-02-21 23:14:26 - progress_bar.py[line:274] - INFO: epoch 007:    210 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.91, wpb=110.6, bsz=40, num_updates=2680, lr=4.53074e-05, gnorm=0.812, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10708
2023-02-21 23:14:37 - progress_bar.py[line:274] - INFO: epoch 007:    220 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.92, wpb=110.9, bsz=40, num_updates=2690, lr=4.52822e-05, gnorm=0.872, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10719
2023-02-21 23:14:49 - progress_bar.py[line:274] - INFO: epoch 007:    230 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.6, ups=0.87, wpb=109.9, bsz=40, num_updates=2700, lr=4.52569e-05, gnorm=0.761, clip=30, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10730
2023-02-21 23:15:00 - progress_bar.py[line:274] - INFO: epoch 007:    240 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=2710, lr=4.52316e-05, gnorm=0.736, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10742
2023-02-21 23:15:11 - progress_bar.py[line:274] - INFO: epoch 007:    250 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.2, ups=0.89, wpb=108.7, bsz=40, num_updates=2720, lr=4.52063e-05, gnorm=0.704, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10753
2023-02-21 23:15:22 - progress_bar.py[line:274] - INFO: epoch 007:    260 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.92, wpb=110.6, bsz=40, num_updates=2730, lr=4.5181e-05, gnorm=0.755, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10764
2023-02-21 23:15:34 - progress_bar.py[line:274] - INFO: epoch 007:    270 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=2740, lr=4.51557e-05, gnorm=0.671, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10775
2023-02-21 23:15:45 - progress_bar.py[line:274] - INFO: epoch 007:    280 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=2750, lr=4.51305e-05, gnorm=0.831, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10786
2023-02-21 23:15:56 - progress_bar.py[line:274] - INFO: epoch 007:    290 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=2760, lr=4.51052e-05, gnorm=0.897, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10798
2023-02-21 23:16:07 - progress_bar.py[line:274] - INFO: epoch 007:    300 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=111.6, bsz=40, num_updates=2770, lr=4.50799e-05, gnorm=0.728, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10809
2023-02-21 23:16:18 - progress_bar.py[line:274] - INFO: epoch 007:    310 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.6, ups=0.94, wpb=112.2, bsz=40, num_updates=2780, lr=4.50546e-05, gnorm=0.945, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10820
2023-02-21 23:16:30 - progress_bar.py[line:274] - INFO: epoch 007:    320 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.87, wpb=111.4, bsz=40, num_updates=2790, lr=4.50293e-05, gnorm=1.129, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10831
2023-02-21 23:16:41 - progress_bar.py[line:274] - INFO: epoch 007:    330 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=2800, lr=4.5004e-05, gnorm=0.881, clip=30, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10842
2023-02-21 23:16:52 - progress_bar.py[line:274] - INFO: epoch 007:    340 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=2810, lr=4.49788e-05, gnorm=0.962, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10854
2023-02-21 23:17:03 - progress_bar.py[line:274] - INFO: epoch 007:    350 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=2820, lr=4.49535e-05, gnorm=0.93, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10865
2023-02-21 23:17:14 - progress_bar.py[line:274] - INFO: epoch 007:    360 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=112, bsz=40, num_updates=2830, lr=4.49282e-05, gnorm=0.919, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10876
2023-02-21 23:17:26 - progress_bar.py[line:274] - INFO: epoch 007:    370 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.2, ups=0.87, wpb=111.4, bsz=40, num_updates=2840, lr=4.49029e-05, gnorm=0.767, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10887
2023-02-21 23:17:37 - progress_bar.py[line:274] - INFO: epoch 007:    380 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=2850, lr=4.48776e-05, gnorm=1.356, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10899
2023-02-21 23:17:45 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-21 23:17:49 - progress_bar.py[line:274] - INFO: epoch 007:    391 / 412 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=91, ups=0.81, wpb=112.5, bsz=40, num_updates=2860, lr=4.48523e-05, gnorm=1.179, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=10911
2023-02-21 23:18:00 - progress_bar.py[line:274] - INFO: epoch 007:    401 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.4, ups=0.94, wpb=111.7, bsz=40, num_updates=2870, lr=4.48271e-05, gnorm=0.819, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10922
2023-02-21 23:18:11 - progress_bar.py[line:274] - INFO: epoch 007:    411 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=2880, lr=4.48018e-05, gnorm=0.812, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10933
2023-02-21 23:18:12 - train.py[line:339] - INFO: end of epoch 7 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv slice_id 1 row count 8240 total row count 16480
2023-02-21 23:18:12 - progress_bar.py[line:282] - INFO: epoch 007 | loss 0.256 | loss_v1 0 | loss_v2 0 | nll_loss 0.072 | ntokens 111.112 | nsentences 40 | sample_size 111.112 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 98.7 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 2881 | lr 4.47993e-05 | gnorm 0.834 | clip 29.9 | loss_scale 512 | train_wall 458 | gb_free 11.1 | ema_decay 0.9999 | wall 10934
2023-02-21 23:18:12 - trainer.py[line:694] - INFO: loading train data for epoch 8
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E7.tsv slice_id 0 row count 8240 total row count 16480
2023-02-21 23:18:12 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-21 23:18:13 - trainer.py[line:758] - INFO: begin training epoch 8
2023-02-21 23:18:13 - train.py[line:312] - INFO: Start iterating over samples
2023-02-21 23:18:25 - progress_bar.py[line:274] - INFO: epoch 008:      9 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=80.5, ups=0.72, wpb=111.3, bsz=40, num_updates=2890, lr=4.47765e-05, gnorm=0.664, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=10947
2023-02-21 23:18:36 - progress_bar.py[line:274] - INFO: epoch 008:     19 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.3, bsz=40, num_updates=2900, lr=4.47512e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=10958
2023-02-21 23:18:47 - progress_bar.py[line:274] - INFO: epoch 008:     29 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=2910, lr=4.47259e-05, gnorm=0.679, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10969
2023-02-21 23:18:59 - progress_bar.py[line:274] - INFO: epoch 008:     39 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.6, ups=0.88, wpb=109.4, bsz=40, num_updates=2920, lr=4.47006e-05, gnorm=0.685, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10980
2023-02-21 23:19:10 - progress_bar.py[line:274] - INFO: epoch 008:     49 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.91, wpb=108.9, bsz=40, num_updates=2930, lr=4.46754e-05, gnorm=0.624, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10991
2023-02-21 23:19:21 - progress_bar.py[line:274] - INFO: epoch 008:     59 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=2940, lr=4.46501e-05, gnorm=0.578, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11002
2023-02-21 23:19:32 - progress_bar.py[line:274] - INFO: epoch 008:     69 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=111.7, bsz=40, num_updates=2950, lr=4.46248e-05, gnorm=0.865, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=11013
2023-02-21 23:19:43 - progress_bar.py[line:274] - INFO: epoch 008:     79 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=2960, lr=4.45995e-05, gnorm=0.544, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=11024
2023-02-21 23:19:54 - progress_bar.py[line:274] - INFO: epoch 008:     89 / 412 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=110.8, bsz=40, num_updates=2970, lr=4.45742e-05, gnorm=0.764, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=11036
2023-02-21 23:20:05 - progress_bar.py[line:274] - INFO: epoch 008:     99 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=2980, lr=4.45489e-05, gnorm=0.454, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=11047
2023-02-21 23:20:16 - progress_bar.py[line:274] - INFO: epoch 008:    109 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.87, wpb=111.3, bsz=40, num_updates=2990, lr=4.45237e-05, gnorm=0.395, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=11058
2023-02-21 23:20:27 - progress_bar.py[line:274] - INFO: epoch 008:    119 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.1, bsz=40, num_updates=3000, lr=4.44984e-05, gnorm=0.847, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=11069
2023-02-21 23:20:27 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-21 23:20:29 - train.py[line:549] - INFO: 0 / 6234
2023-02-21 23:20:29 - train.py[line:551] - INFO: load:1.35 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-21 23:22:32 - train.py[line:549] - INFO: 200 / 6234
2023-02-21 23:22:32 - train.py[line:551] - INFO: load:1.38 valid_run:122.37 task_valid:119.48 collect_output:1.76
2023-02-21 23:24:32 - train.py[line:549] - INFO: 400 / 6234
2023-02-21 23:24:32 - train.py[line:551] - INFO: load:1.41 valid_run:242.60 task_valid:235.55 collect_output:4.90
2023-02-21 23:26:35 - train.py[line:549] - INFO: 600 / 6234
2023-02-21 23:26:35 - train.py[line:551] - INFO: load:1.43 valid_run:365.68 task_valid:352.40 collect_output:10.11
2023-02-21 23:28:37 - train.py[line:549] - INFO: 800 / 6234
2023-02-21 23:28:37 - train.py[line:551] - INFO: load:1.46 valid_run:487.91 task_valid:466.31 collect_output:17.40
2023-02-21 23:30:38 - train.py[line:549] - INFO: 1000 / 6234
2023-02-21 23:30:38 - train.py[line:551] - INFO: load:1.48 valid_run:608.84 task_valid:584.08 collect_output:19.51
2023-02-21 23:32:42 - train.py[line:549] - INFO: 1200 / 6234
2023-02-21 23:32:42 - train.py[line:551] - INFO: load:1.51 valid_run:732.07 task_valid:703.02 collect_output:22.78
2023-02-21 23:34:45 - train.py[line:549] - INFO: 1400 / 6234
2023-02-21 23:34:45 - train.py[line:551] - INFO: load:1.53 valid_run:855.64 task_valid:821.55 collect_output:26.79
2023-02-21 23:36:48 - train.py[line:549] - INFO: 1600 / 6234
2023-02-21 23:36:48 - train.py[line:551] - INFO: load:1.56 valid_run:978.00 task_valid:938.43 collect_output:31.25
2023-02-21 23:38:52 - train.py[line:549] - INFO: 1800 / 6234
2023-02-21 23:38:52 - train.py[line:551] - INFO: load:1.58 valid_run:1102.23 task_valid:1055.90 collect_output:37.00
2023-02-21 23:40:55 - train.py[line:549] - INFO: 2000 / 6234
2023-02-21 23:40:55 - train.py[line:551] - INFO: load:1.60 valid_run:1225.18 task_valid:1169.14 collect_output:45.66
2023-02-21 23:42:56 - train.py[line:549] - INFO: 2200 / 6234
2023-02-21 23:42:56 - train.py[line:551] - INFO: load:1.63 valid_run:1345.99 task_valid:1285.45 collect_output:49.14
2023-02-21 23:44:58 - train.py[line:549] - INFO: 2400 / 6234
2023-02-21 23:44:58 - train.py[line:551] - INFO: load:1.65 valid_run:1468.02 task_valid:1402.80 collect_output:52.79
2023-02-21 23:46:57 - train.py[line:549] - INFO: 2600 / 6234
2023-02-21 23:46:57 - train.py[line:551] - INFO: load:1.68 valid_run:1587.42 task_valid:1517.17 collect_output:56.81
2023-02-21 23:48:59 - train.py[line:549] - INFO: 2800 / 6234
2023-02-21 23:48:59 - train.py[line:551] - INFO: load:1.70 valid_run:1708.95 task_valid:1635.44 collect_output:59.04
2023-02-21 23:51:00 - train.py[line:549] - INFO: 3000 / 6234
2023-02-21 23:51:00 - train.py[line:551] - INFO: load:1.73 valid_run:1830.34 task_valid:1751.93 collect_output:62.89
2023-02-21 23:53:02 - train.py[line:549] - INFO: 3200 / 6234
2023-02-21 23:53:02 - train.py[line:551] - INFO: load:1.75 valid_run:1951.93 task_valid:1866.31 collect_output:69.08
2023-02-21 23:55:04 - train.py[line:549] - INFO: 3400 / 6234
2023-02-21 23:55:04 - train.py[line:551] - INFO: load:1.78 valid_run:2073.83 task_valid:1982.87 collect_output:73.39
2023-02-21 23:57:05 - train.py[line:549] - INFO: 3600 / 6234
2023-02-21 23:57:05 - train.py[line:551] - INFO: load:1.80 valid_run:2194.89 task_valid:2101.07 collect_output:75.23
2023-02-21 23:59:07 - train.py[line:549] - INFO: 3800 / 6234
2023-02-21 23:59:07 - train.py[line:551] - INFO: load:1.83 valid_run:2316.61 task_valid:2218.52 collect_output:78.48
2023-02-22 00:01:08 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 00:01:08 - train.py[line:551] - INFO: load:1.86 valid_run:2437.35 task_valid:2335.47 collect_output:81.26
2023-02-22 00:03:10 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 00:03:10 - train.py[line:551] - INFO: load:1.88 valid_run:2559.55 task_valid:2452.47 collect_output:85.41
2023-02-22 00:05:13 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 00:05:13 - train.py[line:551] - INFO: load:1.91 valid_run:2682.09 task_valid:2571.93 collect_output:87.47
2023-02-22 00:07:13 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 00:07:13 - train.py[line:551] - INFO: load:1.93 valid_run:2803.00 task_valid:2686.76 collect_output:92.49
2023-02-22 00:09:14 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 00:09:14 - train.py[line:551] - INFO: load:1.96 valid_run:2923.23 task_valid:2803.28 collect_output:95.17
2023-02-22 00:11:16 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 00:11:16 - train.py[line:551] - INFO: load:1.98 valid_run:3045.33 task_valid:2919.88 collect_output:99.66
2023-02-22 00:13:19 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 00:13:19 - train.py[line:551] - INFO: load:2.01 valid_run:3168.77 task_valid:3036.27 collect_output:105.69
2023-02-22 00:15:20 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 00:15:20 - train.py[line:551] - INFO: load:2.03 valid_run:3288.87 task_valid:3150.93 collect_output:110.10
2023-02-22 00:17:22 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 00:17:22 - train.py[line:551] - INFO: load:2.06 valid_run:3411.16 task_valid:3270.76 collect_output:111.53
2023-02-22 00:19:24 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 00:19:24 - train.py[line:551] - INFO: load:2.08 valid_run:3533.41 task_valid:3386.83 collect_output:116.68
2023-02-22 00:21:27 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 00:21:27 - train.py[line:551] - INFO: load:2.11 valid_run:3655.69 task_valid:3505.62 collect_output:119.14
2023-02-22 00:23:28 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 00:23:28 - train.py[line:551] - INFO: load:2.13 valid_run:3777.23 task_valid:3624.56 collect_output:120.71

====================================================================================================
SGG eval:     R @ 50: 0.5537;     R @ 100: 0.6130;     R @ 500: 0.6605;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3322;    mR @ 100: 0.3970;    mR @ 500: 0.4496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.4756) (covered in:0.1250) (covering:0.2857) (eating:0.6765) (flying in:0.8182) (growing on:0.2500) (hanging from:0.5935) (lying on:0.5000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8333) (playing:0.0000) (riding:0.8948) (says:0.0000) (sitting on:0.6417) (standing on:0.5439) (using:0.3000) (walking in:0.0000) (walking on:0.4595) (watching:0.2917) 
--------------------------------------------------------
====================================================================================================

2023-02-22 00:23:59 - train.py[line:487] - INFO: 0.6130494779730074
2023-02-22 00:23:59 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.5537;     R @ 100: 0.6130;     R @ 500: 0.6605;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3322;    mR @ 100: 0.3970;    mR @ 500: 0.4496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.4756) (covered in:0.1250) (covering:0.2857) (eating:0.6765) (flying in:0.8182) (growing on:0.2500) (hanging from:0.5935) (lying on:0.5000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8333) (playing:0.0000) (riding:0.8948) (says:0.0000) (sitting on:0.6417) (standing on:0.5439) (using:0.3000) (walking in:0.0000) (walking on:0.4595) (watching:0.2917) 
--------------------------------------------------------
====================================================================================================

2023-02-22 00:23:59 - progress_bar.py[line:282] - INFO: epoch 008 | valid on 'valid' subset | loss 0.235 | loss_v1 0 | loss_v2 0 | nll_loss 0.069 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.613049 | ppl 1.05 | vqa_score 0.3164 | wps 117.7 | wpb 72 | bsz 24 | num_updates 3000 | best_R@100 0.613049
2023-02-22 00:23:59 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 8 @ 3000 updates
2023-02-22 00:23:59 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_8_3000.pt
2023-02-22 00:24:05 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_8_3000.pt
2023-02-22 00:24:10 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_8_3000.pt (epoch 8 @ 3000 updates, score 0.6130494779730074) (writing took 10.967424245551229 seconds)
2023-02-22 00:24:21 - progress_bar.py[line:274] - INFO: epoch 008:    129 / 412 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=112.3, bsz=40, num_updates=3010, lr=4.44731e-05, gnorm=0.877, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14903
2023-02-22 00:24:33 - progress_bar.py[line:274] - INFO: epoch 008:    139 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111, bsz=40, num_updates=3020, lr=4.44478e-05, gnorm=0.98, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14914
2023-02-22 00:24:44 - progress_bar.py[line:274] - INFO: epoch 008:    149 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=3030, lr=4.44225e-05, gnorm=0.648, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14925
2023-02-22 00:24:55 - progress_bar.py[line:274] - INFO: epoch 008:    159 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=3040, lr=4.43972e-05, gnorm=0.781, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14936
2023-02-22 00:25:06 - progress_bar.py[line:274] - INFO: epoch 008:    169 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.93, wpb=110.8, bsz=40, num_updates=3050, lr=4.4372e-05, gnorm=0.708, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14947
2023-02-22 00:25:17 - progress_bar.py[line:274] - INFO: epoch 008:    179 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.9, wpb=111.1, bsz=40, num_updates=3060, lr=4.43467e-05, gnorm=0.582, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14958
2023-02-22 00:25:31 - progress_bar.py[line:274] - INFO: epoch 008:    189 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.9, wpb=110.8, bsz=40, num_updates=3070, lr=4.43214e-05, gnorm=1.057, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14970
2023-02-22 00:25:42 - progress_bar.py[line:274] - INFO: epoch 008:    199 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=3080, lr=4.42961e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14984
2023-02-22 00:25:53 - progress_bar.py[line:274] - INFO: epoch 008:    209 / 412 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=3090, lr=4.42708e-05, gnorm=0.754, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14995
2023-02-22 00:26:09 - progress_bar.py[line:274] - INFO: epoch 008:    219 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=110.5, bsz=40, num_updates=3100, lr=4.42456e-05, gnorm=0.687, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15006
2023-02-22 00:26:20 - progress_bar.py[line:274] - INFO: epoch 008:    229 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=3110, lr=4.42203e-05, gnorm=0.711, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15022
2023-02-22 00:26:31 - progress_bar.py[line:274] - INFO: epoch 008:    239 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=111.9, bsz=40, num_updates=3120, lr=4.4195e-05, gnorm=0.671, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15033
2023-02-22 00:26:42 - progress_bar.py[line:274] - INFO: epoch 008:    249 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.93, wpb=111.6, bsz=40, num_updates=3130, lr=4.41697e-05, gnorm=0.819, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15043
2023-02-22 00:26:53 - progress_bar.py[line:274] - INFO: epoch 008:    259 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.5, bsz=40, num_updates=3140, lr=4.41444e-05, gnorm=0.77, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=15055
2023-02-22 00:27:04 - progress_bar.py[line:274] - INFO: epoch 008:    269 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.92, wpb=110.6, bsz=40, num_updates=3150, lr=4.41191e-05, gnorm=0.93, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=15065
2023-02-22 00:27:15 - progress_bar.py[line:274] - INFO: epoch 008:    279 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.7, bsz=40, num_updates=3160, lr=4.40939e-05, gnorm=0.938, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15077
2023-02-22 00:27:26 - progress_bar.py[line:274] - INFO: epoch 008:    289 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.92, wpb=110.7, bsz=40, num_updates=3170, lr=4.40686e-05, gnorm=0.949, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15088
2023-02-22 00:27:37 - progress_bar.py[line:274] - INFO: epoch 008:    299 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.92, wpb=112.2, bsz=40, num_updates=3180, lr=4.40433e-05, gnorm=0.906, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15099
2023-02-22 00:27:48 - progress_bar.py[line:274] - INFO: epoch 008:    309 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.93, wpb=110.9, bsz=40, num_updates=3190, lr=4.4018e-05, gnorm=0.787, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15110
2023-02-22 00:28:00 - progress_bar.py[line:274] - INFO: epoch 008:    319 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.4, ups=0.87, wpb=110.8, bsz=40, num_updates=3200, lr=4.39927e-05, gnorm=0.746, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15121
2023-02-22 00:28:11 - progress_bar.py[line:274] - INFO: epoch 008:    329 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=112.8, bsz=40, num_updates=3210, lr=4.39674e-05, gnorm=0.729, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15132
2023-02-22 00:28:22 - progress_bar.py[line:274] - INFO: epoch 008:    339 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=3220, lr=4.39422e-05, gnorm=0.758, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15143
2023-02-22 00:28:33 - progress_bar.py[line:274] - INFO: epoch 008:    349 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=3230, lr=4.39169e-05, gnorm=0.727, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15155
2023-02-22 00:28:44 - progress_bar.py[line:274] - INFO: epoch 008:    359 / 412 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=3240, lr=4.38916e-05, gnorm=1.031, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15166
2023-02-22 00:28:56 - progress_bar.py[line:274] - INFO: epoch 008:    369 / 412 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.8, ups=0.87, wpb=110.1, bsz=40, num_updates=3250, lr=4.38663e-05, gnorm=1.067, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15177
2023-02-22 00:29:07 - progress_bar.py[line:274] - INFO: epoch 008:    379 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=3260, lr=4.3841e-05, gnorm=0.703, clip=30, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=15189
2023-02-22 00:29:18 - progress_bar.py[line:274] - INFO: epoch 008:    389 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=3270, lr=4.38157e-05, gnorm=1.149, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15200
2023-02-22 00:29:30 - progress_bar.py[line:274] - INFO: epoch 008:    399 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.87, wpb=112, bsz=40, num_updates=3280, lr=4.37905e-05, gnorm=0.727, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15211
2023-02-22 00:29:41 - progress_bar.py[line:274] - INFO: epoch 008:    409 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=3290, lr=4.37652e-05, gnorm=1.065, clip=40, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15222
2023-02-22 00:29:44 - train.py[line:339] - INFO: end of epoch 8 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 00:29:44 - progress_bar.py[line:282] - INFO: epoch 008 | loss 0.255 | loss_v1 0 | loss_v2 0 | nll_loss 0.071 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 3293 | lr 4.37576e-05 | gnorm 0.77 | clip 26.2 | loss_scale 512 | train_wall 456 | gb_free 10.6 | ema_decay 0.9999 | wall 15226
2023-02-22 00:29:44 - trainer.py[line:694] - INFO: loading train data for epoch 9
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E8.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 00:29:44 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 00:29:44 - trainer.py[line:758] - INFO: begin training epoch 9
2023-02-22 00:29:44 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 00:29:54 - progress_bar.py[line:274] - INFO: epoch 009:      7 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=84.3, ups=0.75, wpb=112.6, bsz=40, num_updates=3300, lr=4.37399e-05, gnorm=0.788, clip=40, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=15236
2023-02-22 00:30:05 - progress_bar.py[line:274] - INFO: epoch 009:     17 / 412 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=110.8, bsz=40, num_updates=3310, lr=4.37146e-05, gnorm=1.17, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15247
2023-02-22 00:30:16 - progress_bar.py[line:274] - INFO: epoch 009:     27 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=111.8, bsz=40, num_updates=3320, lr=4.36893e-05, gnorm=0.876, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15258
2023-02-22 00:30:28 - progress_bar.py[line:274] - INFO: epoch 009:     37 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=3330, lr=4.3664e-05, gnorm=0.759, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15269
2023-02-22 00:30:39 - progress_bar.py[line:274] - INFO: epoch 009:     47 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=3340, lr=4.36388e-05, gnorm=0.728, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15280
2023-02-22 00:30:50 - progress_bar.py[line:274] - INFO: epoch 009:     57 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=3350, lr=4.36135e-05, gnorm=0.754, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15292
2023-02-22 00:31:01 - progress_bar.py[line:274] - INFO: epoch 009:     67 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.4, bsz=40, num_updates=3360, lr=4.35882e-05, gnorm=0.629, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=15303
2023-02-22 00:31:12 - progress_bar.py[line:274] - INFO: epoch 009:     77 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.92, wpb=110.9, bsz=40, num_updates=3370, lr=4.35629e-05, gnorm=0.696, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15314
2023-02-22 00:31:24 - progress_bar.py[line:274] - INFO: epoch 009:     87 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=3380, lr=4.35376e-05, gnorm=0.63, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15325
2023-02-22 00:31:35 - progress_bar.py[line:274] - INFO: epoch 009:     97 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=111.9, bsz=40, num_updates=3390, lr=4.35123e-05, gnorm=0.718, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15337
2023-02-22 00:31:46 - progress_bar.py[line:274] - INFO: epoch 009:    107 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.92, wpb=112.3, bsz=40, num_updates=3400, lr=4.34871e-05, gnorm=0.543, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15347
2023-02-22 00:31:57 - progress_bar.py[line:274] - INFO: epoch 009:    117 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=3410, lr=4.34618e-05, gnorm=0.67, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15359
2023-02-22 00:32:08 - progress_bar.py[line:274] - INFO: epoch 009:    127 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=3420, lr=4.34365e-05, gnorm=0.603, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15370
2023-02-22 00:32:20 - progress_bar.py[line:274] - INFO: epoch 009:    137 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112.2, bsz=40, num_updates=3430, lr=4.34112e-05, gnorm=0.844, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15381
2023-02-22 00:32:30 - progress_bar.py[line:274] - INFO: epoch 009:    147 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.4, ups=0.93, wpb=112.3, bsz=40, num_updates=3440, lr=4.33859e-05, gnorm=0.907, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15392
2023-02-22 00:32:42 - progress_bar.py[line:274] - INFO: epoch 009:    157 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=3450, lr=4.33606e-05, gnorm=0.734, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15403
2023-02-22 00:32:53 - progress_bar.py[line:274] - INFO: epoch 009:    167 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=3460, lr=4.33354e-05, gnorm=0.562, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15414
2023-02-22 00:33:04 - progress_bar.py[line:274] - INFO: epoch 009:    177 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=111.7, bsz=40, num_updates=3470, lr=4.33101e-05, gnorm=0.71, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15426
2023-02-22 00:33:07 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 00:33:16 - progress_bar.py[line:274] - INFO: epoch 009:    188 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.4, ups=0.85, wpb=112.5, bsz=40, num_updates=3480, lr=4.32848e-05, gnorm=0.511, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15437
2023-02-22 00:33:27 - progress_bar.py[line:274] - INFO: epoch 009:    198 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.89, wpb=110.9, bsz=40, num_updates=3490, lr=4.32595e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15449
2023-02-22 00:33:38 - progress_bar.py[line:274] - INFO: epoch 009:    208 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.92, wpb=111.2, bsz=40, num_updates=3500, lr=4.32342e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15460
2023-02-22 00:33:50 - progress_bar.py[line:274] - INFO: epoch 009:    218 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.9, ups=0.87, wpb=110.3, bsz=40, num_updates=3510, lr=4.32089e-05, gnorm=0.601, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15471
2023-02-22 00:34:01 - progress_bar.py[line:274] - INFO: epoch 009:    228 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=3520, lr=4.31837e-05, gnorm=0.6, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15482
2023-02-22 00:34:12 - progress_bar.py[line:274] - INFO: epoch 009:    238 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=3530, lr=4.31584e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=15494
2023-02-22 00:34:24 - progress_bar.py[line:274] - INFO: epoch 009:    248 / 412 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=3540, lr=4.31331e-05, gnorm=1.04, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15505
2023-02-22 00:34:35 - progress_bar.py[line:274] - INFO: epoch 009:    258 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.89, wpb=110, bsz=40, num_updates=3550, lr=4.31078e-05, gnorm=0.417, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15516
2023-02-22 00:34:46 - progress_bar.py[line:274] - INFO: epoch 009:    268 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=3560, lr=4.30825e-05, gnorm=0.755, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15528
2023-02-22 00:34:57 - progress_bar.py[line:274] - INFO: epoch 009:    278 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.92, wpb=110.4, bsz=40, num_updates=3570, lr=4.30572e-05, gnorm=0.955, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15539
2023-02-22 00:35:08 - progress_bar.py[line:274] - INFO: epoch 009:    288 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.9, bsz=40, num_updates=3580, lr=4.3032e-05, gnorm=0.635, clip=30, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=15550
2023-02-22 00:35:19 - progress_bar.py[line:274] - INFO: epoch 009:    298 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.93, wpb=110.2, bsz=40, num_updates=3590, lr=4.30067e-05, gnorm=0.777, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15561
2023-02-22 00:35:30 - progress_bar.py[line:274] - INFO: epoch 009:    308 / 412 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=3600, lr=4.29814e-05, gnorm=0.866, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15572
2023-02-22 00:35:42 - progress_bar.py[line:274] - INFO: epoch 009:    318 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.1, ups=0.88, wpb=109.7, bsz=40, num_updates=3610, lr=4.29561e-05, gnorm=0.7, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15583
2023-02-22 00:35:53 - progress_bar.py[line:274] - INFO: epoch 009:    328 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=3620, lr=4.29308e-05, gnorm=0.776, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15595
2023-02-22 00:36:04 - progress_bar.py[line:274] - INFO: epoch 009:    338 / 412 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.91, wpb=111.3, bsz=40, num_updates=3630, lr=4.29055e-05, gnorm=0.984, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15606
2023-02-22 00:36:16 - progress_bar.py[line:274] - INFO: epoch 009:    348 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=3640, lr=4.28803e-05, gnorm=0.752, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15617
2023-02-22 00:36:27 - progress_bar.py[line:274] - INFO: epoch 009:    358 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=110.9, bsz=40, num_updates=3650, lr=4.2855e-05, gnorm=0.799, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15629
2023-02-22 00:36:39 - progress_bar.py[line:274] - INFO: epoch 009:    368 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.87, wpb=111.1, bsz=40, num_updates=3660, lr=4.28297e-05, gnorm=0.783, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15640
2023-02-22 00:36:50 - progress_bar.py[line:274] - INFO: epoch 009:    378 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=111.6, bsz=40, num_updates=3670, lr=4.28044e-05, gnorm=1.286, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15651
2023-02-22 00:37:01 - progress_bar.py[line:274] - INFO: epoch 009:    388 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.2, ups=0.87, wpb=110.7, bsz=40, num_updates=3680, lr=4.27791e-05, gnorm=0.762, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15663
2023-02-22 00:37:13 - progress_bar.py[line:274] - INFO: epoch 009:    398 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=112.6, bsz=40, num_updates=3690, lr=4.27538e-05, gnorm=0.674, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15674
2023-02-22 00:37:24 - progress_bar.py[line:274] - INFO: epoch 009:    408 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=3700, lr=4.27286e-05, gnorm=0.874, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15685
2023-02-22 00:37:28 - train.py[line:339] - INFO: end of epoch 9 (average epoch stats below)
2023-02-22 00:37:28 - progress_bar.py[line:282] - INFO: epoch 009 | loss 0.251 | loss_v1 0 | loss_v2 0 | nll_loss 0.067 | ntokens 111.122 | nsentences 40 | sample_size 111.122 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 98.4 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 3704 | lr 4.27184e-05 | gnorm 0.74 | clip 22.4 | loss_scale 512 | train_wall 458 | gb_free 10.6 | ema_decay 0.9999 | wall 15690
2023-02-22 00:37:28 - trainer.py[line:694] - INFO: loading train data for epoch 10
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E9.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 00:37:28 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 00:37:29 - trainer.py[line:758] - INFO: begin training epoch 10
2023-02-22 00:37:29 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 00:37:37 - progress_bar.py[line:274] - INFO: epoch 010:      6 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=81.6, ups=0.73, wpb=111.3, bsz=40, num_updates=3710, lr=4.27033e-05, gnorm=0.662, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15699
2023-02-22 00:37:49 - progress_bar.py[line:274] - INFO: epoch 010:     16 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=113, bsz=40, num_updates=3720, lr=4.2678e-05, gnorm=0.966, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15710
2023-02-22 00:38:00 - progress_bar.py[line:274] - INFO: epoch 010:     26 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=3730, lr=4.26527e-05, gnorm=0.611, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15722
2023-02-22 00:38:11 - progress_bar.py[line:274] - INFO: epoch 010:     36 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=111.5, bsz=40, num_updates=3740, lr=4.26274e-05, gnorm=0.667, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15733
2023-02-22 00:38:22 - progress_bar.py[line:274] - INFO: epoch 010:     46 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.93, wpb=110.2, bsz=40, num_updates=3750, lr=4.26021e-05, gnorm=0.667, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15744
2023-02-22 00:38:33 - progress_bar.py[line:274] - INFO: epoch 010:     56 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=3760, lr=4.25769e-05, gnorm=0.574, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15755
2023-02-22 00:38:45 - progress_bar.py[line:274] - INFO: epoch 010:     66 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=3770, lr=4.25516e-05, gnorm=0.713, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15766
2023-02-22 00:38:56 - progress_bar.py[line:274] - INFO: epoch 010:     76 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.91, wpb=109.7, bsz=40, num_updates=3780, lr=4.25263e-05, gnorm=0.601, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15777
2023-02-22 00:39:07 - progress_bar.py[line:274] - INFO: epoch 010:     86 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.91, wpb=111.4, bsz=40, num_updates=3790, lr=4.2501e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15788
2023-02-22 00:39:18 - progress_bar.py[line:274] - INFO: epoch 010:     96 / 412 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.3, ups=0.87, wpb=110.1, bsz=40, num_updates=3800, lr=4.24757e-05, gnorm=0.791, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15800
2023-02-22 00:39:29 - progress_bar.py[line:274] - INFO: epoch 010:    106 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.93, wpb=110.8, bsz=40, num_updates=3810, lr=4.24504e-05, gnorm=0.547, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15811
2023-02-22 00:39:40 - progress_bar.py[line:274] - INFO: epoch 010:    116 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.8, ups=0.93, wpb=111.9, bsz=40, num_updates=3820, lr=4.24252e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15821
2023-02-22 00:39:51 - progress_bar.py[line:274] - INFO: epoch 010:    126 / 412 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=3830, lr=4.23999e-05, gnorm=0.922, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15833
2023-02-22 00:40:02 - progress_bar.py[line:274] - INFO: epoch 010:    136 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=3840, lr=4.23746e-05, gnorm=0.768, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15844
2023-02-22 00:40:14 - progress_bar.py[line:274] - INFO: epoch 010:    146 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.6, bsz=40, num_updates=3850, lr=4.23493e-05, gnorm=0.533, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15855
2023-02-22 00:40:25 - progress_bar.py[line:274] - INFO: epoch 010:    156 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=3860, lr=4.2324e-05, gnorm=0.369, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15866
2023-02-22 00:40:36 - progress_bar.py[line:274] - INFO: epoch 010:    166 / 412 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=111.8, bsz=40, num_updates=3870, lr=4.22987e-05, gnorm=0.985, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15878
2023-02-22 00:40:47 - progress_bar.py[line:274] - INFO: epoch 010:    176 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=3880, lr=4.22735e-05, gnorm=1.214, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15889
2023-02-22 00:40:59 - progress_bar.py[line:274] - INFO: epoch 010:    186 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.7, ups=0.87, wpb=111, bsz=40, num_updates=3890, lr=4.22482e-05, gnorm=0.756, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=15900
2023-02-22 00:41:10 - progress_bar.py[line:274] - INFO: epoch 010:    196 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.9, wpb=110.7, bsz=40, num_updates=3900, lr=4.22229e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15912
2023-02-22 00:41:21 - progress_bar.py[line:274] - INFO: epoch 010:    206 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=3910, lr=4.21976e-05, gnorm=0.652, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15923
2023-02-22 00:41:32 - progress_bar.py[line:274] - INFO: epoch 010:    216 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.91, wpb=110.6, bsz=40, num_updates=3920, lr=4.21723e-05, gnorm=0.51, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15934
2023-02-22 00:41:43 - progress_bar.py[line:274] - INFO: epoch 010:    226 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=3930, lr=4.2147e-05, gnorm=0.366, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15945
2023-02-22 00:41:55 - progress_bar.py[line:274] - INFO: epoch 010:    236 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.88, wpb=111.5, bsz=40, num_updates=3940, lr=4.21218e-05, gnorm=0.846, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15956
2023-02-22 00:42:06 - progress_bar.py[line:274] - INFO: epoch 010:    246 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111.4, bsz=40, num_updates=3950, lr=4.20965e-05, gnorm=0.789, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15968
2023-02-22 00:42:17 - progress_bar.py[line:274] - INFO: epoch 010:    256 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.6, bsz=40, num_updates=3960, lr=4.20712e-05, gnorm=0.73, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15979
2023-02-22 00:42:28 - progress_bar.py[line:274] - INFO: epoch 010:    266 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.93, wpb=111.9, bsz=40, num_updates=3970, lr=4.20459e-05, gnorm=0.497, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15990
2023-02-22 00:42:39 - progress_bar.py[line:274] - INFO: epoch 010:    276 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=3980, lr=4.20206e-05, gnorm=0.735, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=16001
2023-02-22 00:42:51 - progress_bar.py[line:274] - INFO: epoch 010:    286 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=3990, lr=4.19953e-05, gnorm=0.567, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=16012
2023-02-22 00:43:02 - progress_bar.py[line:274] - INFO: epoch 010:    296 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=111.4, bsz=40, num_updates=4000, lr=4.19701e-05, gnorm=0.53, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=16023
2023-02-22 00:43:02 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 00:43:04 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 00:43:04 - train.py[line:551] - INFO: load:1.26 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 00:45:06 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 00:45:06 - train.py[line:551] - INFO: load:1.28 valid_run:122.41 task_valid:119.66 collect_output:1.67
2023-02-22 00:47:07 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 00:47:07 - train.py[line:551] - INFO: load:1.31 valid_run:242.91 task_valid:236.04 collect_output:4.72
2023-02-22 00:49:09 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 00:49:09 - train.py[line:551] - INFO: load:1.33 valid_run:365.61 task_valid:352.95 collect_output:9.46
2023-02-22 00:51:12 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 00:51:12 - train.py[line:551] - INFO: load:1.36 valid_run:488.26 task_valid:467.40 collect_output:16.60
2023-02-22 00:53:13 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 00:53:13 - train.py[line:551] - INFO: load:1.38 valid_run:609.49 task_valid:585.41 collect_output:18.77
2023-02-22 00:55:17 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 00:55:17 - train.py[line:551] - INFO: load:1.41 valid_run:733.26 task_valid:704.71 collect_output:22.20
2023-02-22 00:57:21 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 00:57:21 - train.py[line:551] - INFO: load:1.43 valid_run:857.00 task_valid:823.36 collect_output:26.24
2023-02-22 00:59:23 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 00:59:23 - train.py[line:551] - INFO: load:1.46 valid_run:979.49 task_valid:940.19 collect_output:30.88
2023-02-22 01:01:28 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 01:01:28 - train.py[line:551] - INFO: load:1.48 valid_run:1103.95 task_valid:1057.72 collect_output:36.78
2023-02-22 01:03:30 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 01:03:30 - train.py[line:551] - INFO: load:1.50 valid_run:1226.09 task_valid:1170.56 collect_output:45.07
2023-02-22 01:05:31 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 01:05:31 - train.py[line:551] - INFO: load:1.53 valid_run:1346.72 task_valid:1286.42 collect_output:48.82
2023-02-22 01:07:33 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 01:07:33 - train.py[line:551] - INFO: load:1.55 valid_run:1468.76 task_valid:1403.73 collect_output:52.52
2023-02-22 01:09:32 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 01:09:32 - train.py[line:551] - INFO: load:1.58 valid_run:1587.98 task_valid:1517.77 collect_output:56.67
2023-02-22 01:11:34 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 01:11:34 - train.py[line:551] - INFO: load:1.60 valid_run:1709.64 task_valid:1635.99 collect_output:59.06
2023-02-22 01:13:36 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 01:13:36 - train.py[line:551] - INFO: load:1.63 valid_run:1831.15 task_valid:1752.34 collect_output:63.21
2023-02-22 01:15:37 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 01:15:37 - train.py[line:551] - INFO: load:1.65 valid_run:1952.62 task_valid:1866.53 collect_output:69.46
2023-02-22 01:17:39 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 01:17:39 - train.py[line:551] - INFO: load:1.68 valid_run:2074.35 task_valid:1982.90 collect_output:73.81
2023-02-22 01:19:40 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 01:19:40 - train.py[line:551] - INFO: load:1.70 valid_run:2195.53 task_valid:2101.27 collect_output:75.59
2023-02-22 01:21:42 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 01:21:42 - train.py[line:551] - INFO: load:1.73 valid_run:2317.20 task_valid:2218.64 collect_output:78.87
2023-02-22 01:23:43 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 01:23:43 - train.py[line:551] - INFO: load:1.75 valid_run:2438.04 task_valid:2335.56 collect_output:81.76
2023-02-22 01:25:45 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 01:25:45 - train.py[line:551] - INFO: load:1.78 valid_run:2560.08 task_valid:2452.41 collect_output:85.90
2023-02-22 01:27:47 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 01:27:47 - train.py[line:551] - INFO: load:1.80 valid_run:2682.38 task_valid:2571.58 collect_output:88.00
2023-02-22 01:29:48 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 01:29:48 - train.py[line:551] - INFO: load:1.83 valid_run:2803.08 task_valid:2686.19 collect_output:93.07
2023-02-22 01:31:48 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 01:31:48 - train.py[line:551] - INFO: load:1.85 valid_run:2923.31 task_valid:2802.64 collect_output:95.82
2023-02-22 01:33:50 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 01:33:50 - train.py[line:551] - INFO: load:1.88 valid_run:3045.31 task_valid:2919.06 collect_output:100.37
2023-02-22 01:35:54 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 01:35:54 - train.py[line:551] - INFO: load:1.90 valid_run:3168.72 task_valid:3035.32 collect_output:106.49
2023-02-22 01:37:54 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 01:37:54 - train.py[line:551] - INFO: load:1.93 valid_run:3288.57 task_valid:3149.74 collect_output:110.91
2023-02-22 01:39:56 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 01:39:56 - train.py[line:551] - INFO: load:1.95 valid_run:3410.87 task_valid:3269.50 collect_output:112.44
2023-02-22 01:41:58 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 01:41:58 - train.py[line:551] - INFO: load:1.98 valid_run:3532.98 task_valid:3385.28 collect_output:117.74
2023-02-22 01:44:01 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 01:44:01 - train.py[line:551] - INFO: load:2.00 valid_run:3655.25 task_valid:3504.13 collect_output:120.13
2023-02-22 01:46:02 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 01:46:02 - train.py[line:551] - INFO: load:2.03 valid_run:3776.67 task_valid:3622.85 collect_output:121.78

====================================================================================================
SGG eval:     R @ 50: 0.6128;     R @ 100: 0.6588;     R @ 500: 0.6957;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4039;    mR @ 100: 0.4537;    mR @ 500: 0.4998;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.3125) (covering:0.2857) (eating:0.7059) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5387) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.9330) (says:0.0000) (sitting on:0.6689) (standing on:0.5360) (using:0.3500) (walking in:0.0000) (walking on:0.6216) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-02-22 01:46:32 - train.py[line:487] - INFO: 0.6588386554621849
2023-02-22 01:46:33 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 01:46:33 - progress_bar.py[line:282] - INFO: epoch 010 | valid on 'valid' subset | loss 0.234 | loss_v1 0 | loss_v2 0 | nll_loss 0.064 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.658839 | ppl 1.05 | vqa_score 0.4212 | wps 117.8 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.658839
2023-02-22 01:46:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 4000 updates
2023-02-22 01:46:33 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_10_4000.pt

====================================================================================================
SGG eval:     R @ 50: 0.6128;     R @ 100: 0.6588;     R @ 500: 0.6957;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4039;    mR @ 100: 0.4537;    mR @ 500: 0.4998;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.3125) (covering:0.2857) (eating:0.7059) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5387) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.9330) (says:0.0000) (sitting on:0.6689) (standing on:0.5360) (using:0.3500) (walking in:0.0000) (walking on:0.6216) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-02-22 01:46:39 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_10_4000.pt
2023-02-22 01:46:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_10_4000.pt (epoch 10 @ 4000 updates, score 0.6588386554621849) (writing took 11.02540291659534 seconds)
2023-02-22 01:46:55 - progress_bar.py[line:274] - INFO: epoch 010:    306 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=110.7, bsz=40, num_updates=4010, lr=4.19448e-05, gnorm=0.813, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19857
2023-02-22 01:47:06 - progress_bar.py[line:274] - INFO: epoch 010:    316 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.92, wpb=110.8, bsz=40, num_updates=4020, lr=4.19195e-05, gnorm=0.385, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19868
2023-02-22 01:47:17 - progress_bar.py[line:274] - INFO: epoch 010:    326 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111.6, bsz=40, num_updates=4030, lr=4.18942e-05, gnorm=0.579, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19879
2023-02-22 01:47:29 - progress_bar.py[line:274] - INFO: epoch 010:    336 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=4040, lr=4.18689e-05, gnorm=0.59, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19890
2023-02-22 01:47:40 - progress_bar.py[line:274] - INFO: epoch 010:    346 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.91, wpb=111.6, bsz=40, num_updates=4050, lr=4.18436e-05, gnorm=1.002, clip=30, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19901
2023-02-22 01:47:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 01:47:52 - progress_bar.py[line:274] - INFO: epoch 010:    357 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=90.6, ups=0.81, wpb=112.1, bsz=40, num_updates=4060, lr=4.18184e-05, gnorm=0.421, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19914
2023-02-22 01:48:03 - progress_bar.py[line:274] - INFO: epoch 010:    367 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=4070, lr=4.17931e-05, gnorm=0.689, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19925
2023-02-22 01:48:15 - progress_bar.py[line:274] - INFO: epoch 010:    377 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=4080, lr=4.17678e-05, gnorm=0.836, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19936
2023-02-22 01:48:25 - progress_bar.py[line:274] - INFO: epoch 010:    387 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.92, wpb=111.6, bsz=40, num_updates=4090, lr=4.17425e-05, gnorm=0.896, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19947
2023-02-22 01:48:36 - progress_bar.py[line:274] - INFO: epoch 010:    397 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.92, wpb=111.9, bsz=40, num_updates=4100, lr=4.17172e-05, gnorm=0.534, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=19958
2023-02-22 01:48:48 - progress_bar.py[line:274] - INFO: epoch 010:    407 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=110.8, bsz=40, num_updates=4110, lr=4.16919e-05, gnorm=0.633, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19969
2023-02-22 01:48:53 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 01:48:55 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 01:48:55 - train.py[line:551] - INFO: load:1.42 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 01:50:58 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 01:50:58 - train.py[line:551] - INFO: load:1.45 valid_run:122.52 task_valid:119.69 collect_output:1.75
2023-02-22 01:52:58 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 01:52:58 - train.py[line:551] - INFO: load:1.48 valid_run:242.70 task_valid:235.97 collect_output:4.63
2023-02-22 01:55:00 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 01:55:00 - train.py[line:551] - INFO: load:1.50 valid_run:365.03 task_valid:352.92 collect_output:9.02
2023-02-22 01:57:03 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 01:57:03 - train.py[line:551] - INFO: load:1.52 valid_run:487.29 task_valid:467.08 collect_output:16.11
2023-02-22 01:59:03 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 01:59:03 - train.py[line:551] - INFO: load:1.55 valid_run:608.07 task_valid:584.78 collect_output:18.20
2023-02-22 02:01:07 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 02:01:07 - train.py[line:551] - INFO: load:1.57 valid_run:731.46 task_valid:704.00 collect_output:21.37
2023-02-22 02:03:10 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 02:03:10 - train.py[line:551] - INFO: load:1.60 valid_run:854.86 task_valid:822.53 collect_output:25.25
2023-02-22 02:05:13 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 02:05:13 - train.py[line:551] - INFO: load:1.62 valid_run:977.12 task_valid:939.57 collect_output:29.46
2023-02-22 02:07:17 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 02:07:17 - train.py[line:551] - INFO: load:1.65 valid_run:1101.17 task_valid:1057.19 collect_output:34.89
2023-02-22 02:09:19 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 02:09:19 - train.py[line:551] - INFO: load:1.67 valid_run:1223.14 task_valid:1170.53 collect_output:42.50
2023-02-22 02:11:19 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 02:11:19 - train.py[line:551] - INFO: load:1.70 valid_run:1343.75 task_valid:1286.64 collect_output:46.01
2023-02-22 02:13:21 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 02:13:21 - train.py[line:551] - INFO: load:1.72 valid_run:1465.59 task_valid:1404.03 collect_output:49.47
2023-02-22 02:15:21 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 02:15:21 - train.py[line:551] - INFO: load:1.75 valid_run:1584.89 task_valid:1518.29 collect_output:53.50
2023-02-22 02:17:22 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 02:17:22 - train.py[line:551] - INFO: load:1.77 valid_run:1706.37 task_valid:1636.51 collect_output:55.77
2023-02-22 02:19:23 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 02:19:23 - train.py[line:551] - INFO: load:1.80 valid_run:1827.57 task_valid:1753.05 collect_output:59.43
2023-02-22 02:21:25 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 02:21:25 - train.py[line:551] - INFO: load:1.82 valid_run:1949.12 task_valid:1867.47 collect_output:65.55
2023-02-22 02:23:27 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 02:23:27 - train.py[line:551] - INFO: load:1.84 valid_run:2070.72 task_valid:1983.98 collect_output:69.66
2023-02-22 02:25:28 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 02:25:28 - train.py[line:551] - INFO: load:1.87 valid_run:2191.79 task_valid:2102.26 collect_output:71.43
2023-02-22 02:27:30 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 02:27:30 - train.py[line:551] - INFO: load:1.90 valid_run:2313.58 task_valid:2219.86 collect_output:74.59
2023-02-22 02:29:31 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 02:29:31 - train.py[line:551] - INFO: load:1.92 valid_run:2434.34 task_valid:2336.95 collect_output:77.25
2023-02-22 02:31:33 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 02:31:33 - train.py[line:551] - INFO: load:1.95 valid_run:2556.37 task_valid:2453.99 collect_output:81.22
2023-02-22 02:33:35 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 02:33:35 - train.py[line:551] - INFO: load:1.97 valid_run:2678.82 task_valid:2573.42 collect_output:83.23
2023-02-22 02:35:36 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 02:35:36 - train.py[line:551] - INFO: load:2.00 valid_run:2799.63 task_valid:2688.23 collect_output:88.22
2023-02-22 02:37:36 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 02:37:36 - train.py[line:551] - INFO: load:2.02 valid_run:2919.74 task_valid:2804.71 collect_output:90.84
2023-02-22 02:39:38 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 02:39:38 - train.py[line:551] - INFO: load:2.05 valid_run:3041.66 task_valid:2921.36 collect_output:95.10
2023-02-22 02:41:41 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 02:41:41 - train.py[line:551] - INFO: load:2.07 valid_run:3164.81 task_valid:3037.79 collect_output:100.80
2023-02-22 02:43:41 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 02:43:41 - train.py[line:551] - INFO: load:2.10 valid_run:3284.83 task_valid:3152.54 collect_output:105.04
2023-02-22 02:45:44 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 02:45:44 - train.py[line:551] - INFO: load:2.13 valid_run:3407.21 task_valid:3272.61 collect_output:106.27
2023-02-22 02:47:46 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 02:47:46 - train.py[line:551] - INFO: load:2.16 valid_run:3529.24 task_valid:3388.52 collect_output:111.37
2023-02-22 02:49:48 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 02:49:48 - train.py[line:551] - INFO: load:2.18 valid_run:3651.47 task_valid:3507.37 collect_output:113.77
2023-02-22 02:51:50 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 02:51:50 - train.py[line:551] - INFO: load:2.21 valid_run:3773.03 task_valid:3626.30 collect_output:115.38

====================================================================================================
SGG eval:     R @ 50: 0.6163;     R @ 100: 0.6647;     R @ 500: 0.7044;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4046;    mR @ 100: 0.4634;    mR @ 500: 0.5028;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.3125) (covering:0.4286) (eating:0.7059) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5774) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6757) (standing on:0.5313) (using:0.3500) (walking in:0.0000) (walking on:0.6216) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-02-22 02:52:20 - train.py[line:487] - INFO: 0.6646896358543417
2023-02-22 02:52:21 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 02:52:21 - progress_bar.py[line:282] - INFO: epoch 010 | valid on 'valid' subset | loss 0.237 | loss_v1 0 | loss_v2 0 | nll_loss 0.066 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.66469 | ppl 1.05 | vqa_score 0.4268 | wps 117.9 | wpb 72 | bsz 24 | num_updates 4115 | best_R@100 0.66469

====================================================================================================
SGG eval:     R @ 50: 0.6163;     R @ 100: 0.6647;     R @ 500: 0.7044;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4046;    mR @ 100: 0.4634;    mR @ 500: 0.5028;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.3125) (covering:0.4286) (eating:0.7059) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5774) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6757) (standing on:0.5313) (using:0.3500) (walking in:0.0000) (walking on:0.6216) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-02-22 02:52:21 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 10 @ 4115 updates
2023-02-22 02:52:21 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint10.pt
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 02:52:27 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint10.pt
2023-02-22 02:52:32 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint10.pt (epoch 10 @ 4115 updates, score 0.6646896358543417) (writing took 11.445274353027344 seconds)
2023-02-22 02:52:32 - train.py[line:339] - INFO: end of epoch 10 (average epoch stats below)
2023-02-22 02:52:33 - progress_bar.py[line:282] - INFO: epoch 010 | loss 0.249 | loss_v1 0 | loss_v2 0 | nll_loss 0.064 | ntokens 111.112 | nsentences 40 | sample_size 111.112 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 5.6 | ups 0.05 | wpb 111.1 | bsz 40 | num_updates 4115 | lr 4.16793e-05 | gnorm 0.665 | clip 18.2 | loss_scale 512 | train_wall 457 | gb_free 10.7 | ema_decay 0.9999 | wall 23794
2023-02-22 02:52:33 - trainer.py[line:694] - INFO: loading train data for epoch 11
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E10.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 02:52:33 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 02:52:34 - trainer.py[line:758] - INFO: begin training epoch 11
2023-02-22 02:52:34 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 02:52:41 - progress_bar.py[line:274] - INFO: epoch 011:      5 / 412 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=110.7, bsz=40, num_updates=4120, lr=4.16667e-05, gnorm=0.717, clip=20, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=23803
2023-02-22 02:52:53 - progress_bar.py[line:274] - INFO: epoch 011:     15 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.5, bsz=40, num_updates=4130, lr=4.16414e-05, gnorm=0.754, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=23814
2023-02-22 02:53:04 - progress_bar.py[line:274] - INFO: epoch 011:     25 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.9, wpb=110, bsz=40, num_updates=4140, lr=4.16161e-05, gnorm=0.558, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23825
2023-02-22 02:53:15 - progress_bar.py[line:274] - INFO: epoch 011:     35 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=111.4, bsz=40, num_updates=4150, lr=4.15908e-05, gnorm=0.561, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23837
2023-02-22 02:53:26 - progress_bar.py[line:274] - INFO: epoch 011:     45 / 412 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=4160, lr=4.15655e-05, gnorm=0.824, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=23848
2023-02-22 02:53:37 - progress_bar.py[line:274] - INFO: epoch 011:     55 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.92, wpb=110.1, bsz=40, num_updates=4170, lr=4.15403e-05, gnorm=0.703, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=23859
2023-02-22 02:53:48 - progress_bar.py[line:274] - INFO: epoch 011:     65 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.91, wpb=110.4, bsz=40, num_updates=4180, lr=4.1515e-05, gnorm=0.686, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=23870
2023-02-22 02:53:59 - progress_bar.py[line:274] - INFO: epoch 011:     75 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.89, wpb=109.8, bsz=40, num_updates=4190, lr=4.14897e-05, gnorm=0.572, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23881
2023-02-22 02:54:10 - progress_bar.py[line:274] - INFO: epoch 011:     85 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=4200, lr=4.14644e-05, gnorm=0.64, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23892
2023-02-22 02:54:21 - progress_bar.py[line:274] - INFO: epoch 011:     95 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=4210, lr=4.14391e-05, gnorm=0.743, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=23903
2023-02-22 02:54:33 - progress_bar.py[line:274] - INFO: epoch 011:    105 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.1, bsz=40, num_updates=4220, lr=4.14138e-05, gnorm=0.721, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=23914
2023-02-22 02:54:44 - progress_bar.py[line:274] - INFO: epoch 011:    115 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=4230, lr=4.13886e-05, gnorm=0.726, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=23926
2023-02-22 02:54:55 - progress_bar.py[line:274] - INFO: epoch 011:    125 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=4240, lr=4.13633e-05, gnorm=0.517, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=23937
2023-02-22 02:55:07 - progress_bar.py[line:274] - INFO: epoch 011:    135 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=4250, lr=4.1338e-05, gnorm=0.812, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=23948
2023-02-22 02:55:18 - progress_bar.py[line:274] - INFO: epoch 011:    145 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.87, wpb=112.4, bsz=40, num_updates=4260, lr=4.13127e-05, gnorm=0.895, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23960
2023-02-22 02:55:29 - progress_bar.py[line:274] - INFO: epoch 011:    155 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=4270, lr=4.12874e-05, gnorm=0.981, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=23971
2023-02-22 02:55:40 - progress_bar.py[line:274] - INFO: epoch 011:    165 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=111.4, bsz=40, num_updates=4280, lr=4.12621e-05, gnorm=0.801, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=23982
2023-02-22 02:55:51 - progress_bar.py[line:274] - INFO: epoch 011:    175 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112, bsz=40, num_updates=4290, lr=4.12369e-05, gnorm=0.703, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=23993
2023-02-22 02:56:03 - progress_bar.py[line:274] - INFO: epoch 011:    185 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.9, wpb=114, bsz=40, num_updates=4300, lr=4.12116e-05, gnorm=0.835, clip=40, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24004
2023-02-22 02:56:14 - progress_bar.py[line:274] - INFO: epoch 011:    195 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=4310, lr=4.11863e-05, gnorm=0.659, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24015
2023-02-22 02:56:25 - progress_bar.py[line:274] - INFO: epoch 011:    205 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=4320, lr=4.1161e-05, gnorm=0.605, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24027
2023-02-22 02:56:36 - progress_bar.py[line:274] - INFO: epoch 011:    215 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=4330, lr=4.11357e-05, gnorm=0.947, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24038
2023-02-22 02:56:47 - progress_bar.py[line:274] - INFO: epoch 011:    225 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111, bsz=40, num_updates=4340, lr=4.11104e-05, gnorm=1.062, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24049
2023-02-22 02:56:59 - progress_bar.py[line:274] - INFO: epoch 011:    235 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=4350, lr=4.10852e-05, gnorm=0.984, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24060
2023-02-22 02:57:10 - progress_bar.py[line:274] - INFO: epoch 011:    245 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=4360, lr=4.10599e-05, gnorm=0.872, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24071
2023-02-22 02:57:21 - progress_bar.py[line:274] - INFO: epoch 011:    255 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.92, wpb=109.8, bsz=40, num_updates=4370, lr=4.10346e-05, gnorm=1.057, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24082
2023-02-22 02:57:32 - progress_bar.py[line:274] - INFO: epoch 011:    265 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112, bsz=40, num_updates=4380, lr=4.10093e-05, gnorm=0.792, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24094
2023-02-22 02:57:43 - progress_bar.py[line:274] - INFO: epoch 011:    275 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=4390, lr=4.0984e-05, gnorm=0.606, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24105
2023-02-22 02:57:54 - progress_bar.py[line:274] - INFO: epoch 011:    285 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.9, wpb=111.5, bsz=40, num_updates=4400, lr=4.09587e-05, gnorm=0.608, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24116
2023-02-22 02:58:05 - progress_bar.py[line:274] - INFO: epoch 011:    295 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=111.5, bsz=40, num_updates=4410, lr=4.09335e-05, gnorm=0.739, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=24127
2023-02-22 02:58:17 - progress_bar.py[line:274] - INFO: epoch 011:    305 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=4420, lr=4.09082e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24138
2023-02-22 02:58:28 - progress_bar.py[line:274] - INFO: epoch 011:    315 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.89, wpb=110.3, bsz=40, num_updates=4430, lr=4.08829e-05, gnorm=0.6, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24150
2023-02-22 02:58:39 - progress_bar.py[line:274] - INFO: epoch 011:    325 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.8, ups=0.87, wpb=110.2, bsz=40, num_updates=4440, lr=4.08576e-05, gnorm=0.55, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24161
2023-02-22 02:58:51 - progress_bar.py[line:274] - INFO: epoch 011:    335 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=4450, lr=4.08323e-05, gnorm=0.574, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24172
2023-02-22 02:59:01 - progress_bar.py[line:274] - INFO: epoch 011:    345 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.93, wpb=109.9, bsz=40, num_updates=4460, lr=4.0807e-05, gnorm=0.466, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24183
2023-02-22 02:59:12 - progress_bar.py[line:274] - INFO: epoch 011:    355 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.4, bsz=40, num_updates=4470, lr=4.07818e-05, gnorm=0.804, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24194
2023-02-22 02:59:23 - progress_bar.py[line:274] - INFO: epoch 011:    365 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.91, wpb=110.8, bsz=40, num_updates=4480, lr=4.07565e-05, gnorm=0.62, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24205
2023-02-22 02:59:35 - progress_bar.py[line:274] - INFO: epoch 011:    375 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.88, wpb=110.9, bsz=40, num_updates=4490, lr=4.07312e-05, gnorm=0.939, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24216
2023-02-22 02:59:46 - progress_bar.py[line:274] - INFO: epoch 011:    385 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.91, wpb=110.5, bsz=40, num_updates=4500, lr=4.07059e-05, gnorm=0.715, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24227
2023-02-22 02:59:57 - progress_bar.py[line:274] - INFO: epoch 011:    395 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.93, wpb=111.3, bsz=40, num_updates=4510, lr=4.06806e-05, gnorm=0.703, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24238
2023-02-22 03:00:08 - progress_bar.py[line:274] - INFO: epoch 011:    405 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.92, wpb=111.6, bsz=40, num_updates=4520, lr=4.06553e-05, gnorm=0.605, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24249
2023-02-22 03:00:15 - train.py[line:339] - INFO: end of epoch 11 (average epoch stats below)
2023-02-22 03:00:15 - progress_bar.py[line:282] - INFO: epoch 011 | loss 0.249 | loss_v1 0 | loss_v2 0 | nll_loss 0.064 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 99.1 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 4527 | lr 4.06376e-05 | gnorm 0.725 | clip 24.8 | loss_scale 512 | train_wall 458 | gb_free 10.4 | ema_decay 0.9999 | wall 24257
2023-02-22 03:00:15 - trainer.py[line:694] - INFO: loading train data for epoch 12
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E11.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 03:00:16 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 03:00:16 - trainer.py[line:758] - INFO: begin training epoch 12
2023-02-22 03:00:16 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 03:00:21 - progress_bar.py[line:274] - INFO: epoch 012:      3 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=80.5, ups=0.73, wpb=109.9, bsz=40, num_updates=4530, lr=4.06301e-05, gnorm=0.638, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=24263
2023-02-22 03:00:32 - progress_bar.py[line:274] - INFO: epoch 012:     13 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.9, wpb=111.1, bsz=40, num_updates=4540, lr=4.06048e-05, gnorm=1.021, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24274
2023-02-22 03:00:43 - progress_bar.py[line:274] - INFO: epoch 012:     23 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=4550, lr=4.05795e-05, gnorm=0.444, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24285
2023-02-22 03:00:55 - progress_bar.py[line:274] - INFO: epoch 012:     33 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.87, wpb=112.2, bsz=40, num_updates=4560, lr=4.05542e-05, gnorm=0.927, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24296
2023-02-22 03:01:06 - progress_bar.py[line:274] - INFO: epoch 012:     43 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.3, ups=0.87, wpb=110.9, bsz=40, num_updates=4570, lr=4.05289e-05, gnorm=0.53, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24308
2023-02-22 03:01:18 - progress_bar.py[line:274] - INFO: epoch 012:     53 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111.5, bsz=40, num_updates=4580, lr=4.05036e-05, gnorm=0.459, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24319
2023-02-22 03:01:29 - progress_bar.py[line:274] - INFO: epoch 012:     63 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111, bsz=40, num_updates=4590, lr=4.04784e-05, gnorm=0.732, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24331
2023-02-22 03:01:40 - progress_bar.py[line:274] - INFO: epoch 012:     73 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.5, bsz=40, num_updates=4600, lr=4.04531e-05, gnorm=0.82, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24342
2023-02-22 03:01:51 - progress_bar.py[line:274] - INFO: epoch 012:     83 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=112.5, bsz=40, num_updates=4610, lr=4.04278e-05, gnorm=0.66, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24353
2023-02-22 03:02:03 - progress_bar.py[line:274] - INFO: epoch 012:     93 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.87, wpb=111.6, bsz=40, num_updates=4620, lr=4.04025e-05, gnorm=0.596, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24365
2023-02-22 03:02:14 - progress_bar.py[line:274] - INFO: epoch 012:    103 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=4630, lr=4.03772e-05, gnorm=0.62, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24376
2023-02-22 03:02:26 - progress_bar.py[line:274] - INFO: epoch 012:    113 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=110.4, bsz=40, num_updates=4640, lr=4.03519e-05, gnorm=0.498, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24387
2023-02-22 03:02:37 - progress_bar.py[line:274] - INFO: epoch 012:    123 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=110.1, bsz=40, num_updates=4650, lr=4.03267e-05, gnorm=0.583, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24398
2023-02-22 03:02:48 - progress_bar.py[line:274] - INFO: epoch 012:    133 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=4660, lr=4.03014e-05, gnorm=0.455, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24410
2023-02-22 03:02:59 - progress_bar.py[line:274] - INFO: epoch 012:    143 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.89, wpb=111, bsz=40, num_updates=4670, lr=4.02761e-05, gnorm=0.987, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24421
2023-02-22 03:03:11 - progress_bar.py[line:274] - INFO: epoch 012:    153 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.88, wpb=110.9, bsz=40, num_updates=4680, lr=4.02508e-05, gnorm=0.543, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24432
2023-02-22 03:03:22 - progress_bar.py[line:274] - INFO: epoch 012:    163 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=4690, lr=4.02255e-05, gnorm=0.615, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24443
2023-02-22 03:03:33 - progress_bar.py[line:274] - INFO: epoch 012:    173 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=4700, lr=4.02002e-05, gnorm=0.891, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24455
2023-02-22 03:03:44 - progress_bar.py[line:274] - INFO: epoch 012:    183 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.91, wpb=109.8, bsz=40, num_updates=4710, lr=4.0175e-05, gnorm=0.426, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24466
2023-02-22 03:03:56 - progress_bar.py[line:274] - INFO: epoch 012:    193 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=4720, lr=4.01497e-05, gnorm=0.813, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24477
2023-02-22 03:04:07 - progress_bar.py[line:274] - INFO: epoch 012:    203 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=111.4, bsz=40, num_updates=4730, lr=4.01244e-05, gnorm=0.825, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24489
2023-02-22 03:04:18 - progress_bar.py[line:274] - INFO: epoch 012:    213 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.89, wpb=110.8, bsz=40, num_updates=4740, lr=4.00991e-05, gnorm=0.409, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24500
2023-02-22 03:04:29 - progress_bar.py[line:274] - INFO: epoch 012:    223 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=112.4, bsz=40, num_updates=4750, lr=4.00738e-05, gnorm=0.627, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24511
2023-02-22 03:04:40 - progress_bar.py[line:274] - INFO: epoch 012:    233 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.94, wpb=110.2, bsz=40, num_updates=4760, lr=4.00485e-05, gnorm=0.554, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24521
2023-02-22 03:04:51 - progress_bar.py[line:274] - INFO: epoch 012:    243 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=4770, lr=4.00233e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24533
2023-02-22 03:05:02 - progress_bar.py[line:274] - INFO: epoch 012:    253 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.93, wpb=110.9, bsz=40, num_updates=4780, lr=3.9998e-05, gnorm=0.784, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24543
2023-02-22 03:05:13 - progress_bar.py[line:274] - INFO: epoch 012:    263 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=4790, lr=3.99727e-05, gnorm=0.552, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24554
2023-02-22 03:05:24 - progress_bar.py[line:274] - INFO: epoch 012:    273 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=4800, lr=3.99474e-05, gnorm=0.557, clip=20, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24566
2023-02-22 03:05:35 - progress_bar.py[line:274] - INFO: epoch 012:    283 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=4810, lr=3.99221e-05, gnorm=0.779, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24577
2023-02-22 03:05:47 - progress_bar.py[line:274] - INFO: epoch 012:    293 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=4820, lr=3.98968e-05, gnorm=0.717, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24588
2023-02-22 03:05:58 - progress_bar.py[line:274] - INFO: epoch 012:    303 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.9, ups=0.87, wpb=112.4, bsz=40, num_updates=4830, lr=3.98716e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24600
2023-02-22 03:06:09 - progress_bar.py[line:274] - INFO: epoch 012:    313 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=4840, lr=3.98463e-05, gnorm=0.775, clip=30, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24611
2023-02-22 03:06:20 - progress_bar.py[line:274] - INFO: epoch 012:    323 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.92, wpb=111, bsz=40, num_updates=4850, lr=3.9821e-05, gnorm=0.39, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24622
2023-02-22 03:06:32 - progress_bar.py[line:274] - INFO: epoch 012:    333 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=112.2, bsz=40, num_updates=4860, lr=3.97957e-05, gnorm=0.758, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24633
2023-02-22 03:06:43 - progress_bar.py[line:274] - INFO: epoch 012:    343 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.8, ups=0.87, wpb=111.4, bsz=40, num_updates=4870, lr=3.97704e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24645
2023-02-22 03:06:54 - progress_bar.py[line:274] - INFO: epoch 012:    353 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=4880, lr=3.97451e-05, gnorm=0.709, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24656
2023-02-22 03:07:06 - progress_bar.py[line:274] - INFO: epoch 012:    363 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=4890, lr=3.97199e-05, gnorm=0.963, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24667
2023-02-22 03:07:17 - progress_bar.py[line:274] - INFO: epoch 012:    373 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=111.1, bsz=40, num_updates=4900, lr=3.96946e-05, gnorm=0.505, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24679
2023-02-22 03:07:28 - progress_bar.py[line:274] - INFO: epoch 012:    383 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=4910, lr=3.96693e-05, gnorm=0.37, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24690
2023-02-22 03:07:39 - progress_bar.py[line:274] - INFO: epoch 012:    393 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=4920, lr=3.9644e-05, gnorm=0.721, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24701
2023-02-22 03:07:50 - progress_bar.py[line:274] - INFO: epoch 012:    403 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111.9, bsz=40, num_updates=4930, lr=3.96187e-05, gnorm=0.541, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24712
2023-02-22 03:08:00 - train.py[line:339] - INFO: end of epoch 12 (average epoch stats below)
2023-02-22 03:08:00 - progress_bar.py[line:282] - INFO: epoch 012 | loss 0.246 | loss_v1 0 | loss_v2 0 | nll_loss 0.061 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.5 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 4939 | lr 3.9596e-05 | gnorm 0.635 | clip 18.4 | loss_scale 1024 | train_wall 460 | gb_free 10.7 | ema_decay 0.9999 | wall 24722
2023-02-22 03:08:00 - trainer.py[line:694] - INFO: loading train data for epoch 13
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E12.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 03:08:01 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 03:08:01 - trainer.py[line:758] - INFO: begin training epoch 13
2023-02-22 03:08:01 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 03:08:05 - progress_bar.py[line:274] - INFO: epoch 013:      1 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=77.8, ups=0.7, wpb=111.1, bsz=40, num_updates=4940, lr=3.95934e-05, gnorm=0.576, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=24726
2023-02-22 03:08:16 - progress_bar.py[line:274] - INFO: epoch 013:     11 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.89, wpb=111.4, bsz=40, num_updates=4950, lr=3.95682e-05, gnorm=0.476, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24738
2023-02-22 03:08:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 03:08:28 - progress_bar.py[line:274] - INFO: epoch 013:     22 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=93.2, ups=0.83, wpb=111.7, bsz=40, num_updates=4960, lr=3.95429e-05, gnorm=0.623, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=24750
2023-02-22 03:08:39 - progress_bar.py[line:274] - INFO: epoch 013:     32 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.2, bsz=40, num_updates=4970, lr=3.95176e-05, gnorm=0.397, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24761
2023-02-22 03:08:50 - progress_bar.py[line:274] - INFO: epoch 013:     42 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.2, ups=0.89, wpb=109.3, bsz=40, num_updates=4980, lr=3.94923e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24772
2023-02-22 03:09:01 - progress_bar.py[line:274] - INFO: epoch 013:     52 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.91, wpb=110, bsz=40, num_updates=4990, lr=3.9467e-05, gnorm=0.528, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24783
2023-02-22 03:09:12 - progress_bar.py[line:274] - INFO: epoch 013:     62 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=110.7, bsz=40, num_updates=5000, lr=3.94417e-05, gnorm=0.893, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24794
2023-02-22 03:09:12 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 03:09:14 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 03:09:14 - train.py[line:551] - INFO: load:1.21 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 03:11:16 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 03:11:16 - train.py[line:551] - INFO: load:1.24 valid_run:122.48 task_valid:119.53 collect_output:1.86
2023-02-22 03:13:17 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 03:13:17 - train.py[line:551] - INFO: load:1.26 valid_run:242.87 task_valid:235.77 collect_output:5.00
2023-02-22 03:15:19 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 03:15:19 - train.py[line:551] - INFO: load:1.29 valid_run:365.43 task_valid:352.85 collect_output:9.44
2023-02-22 03:17:22 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 03:17:22 - train.py[line:551] - INFO: load:1.31 valid_run:487.83 task_valid:466.94 collect_output:16.75
2023-02-22 03:19:23 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 03:19:23 - train.py[line:551] - INFO: load:1.34 valid_run:608.91 task_valid:584.60 collect_output:19.17
2023-02-22 03:21:26 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 03:21:26 - train.py[line:551] - INFO: load:1.36 valid_run:732.24 task_valid:703.70 collect_output:22.41
2023-02-22 03:23:30 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 03:23:30 - train.py[line:551] - INFO: load:1.39 valid_run:855.79 task_valid:822.20 collect_output:26.44
2023-02-22 03:25:33 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 03:25:33 - train.py[line:551] - INFO: load:1.41 valid_run:978.29 task_valid:939.10 collect_output:31.05
2023-02-22 03:27:37 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 03:27:37 - train.py[line:551] - INFO: load:1.43 valid_run:1102.39 task_valid:1056.62 collect_output:36.64
2023-02-22 03:29:39 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 03:29:39 - train.py[line:551] - INFO: load:1.46 valid_run:1224.43 task_valid:1169.69 collect_output:44.59
2023-02-22 03:31:40 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 03:31:40 - train.py[line:551] - INFO: load:1.48 valid_run:1345.13 task_valid:1285.85 collect_output:48.13
2023-02-22 03:33:42 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 03:33:42 - train.py[line:551] - INFO: load:1.51 valid_run:1467.30 task_valid:1403.18 collect_output:51.95
2023-02-22 03:35:41 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 03:35:41 - train.py[line:551] - INFO: load:1.53 valid_run:1586.87 task_valid:1517.50 collect_output:56.19
2023-02-22 03:37:43 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 03:37:43 - train.py[line:551] - INFO: load:1.55 valid_run:1708.41 task_valid:1635.76 collect_output:58.47
2023-02-22 03:39:45 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 03:39:45 - train.py[line:551] - INFO: load:1.58 valid_run:1830.12 task_valid:1752.42 collect_output:62.50
2023-02-22 03:41:47 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 03:41:47 - train.py[line:551] - INFO: load:1.60 valid_run:1951.87 task_valid:1866.90 collect_output:68.75
2023-02-22 03:43:49 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 03:43:49 - train.py[line:551] - INFO: load:1.63 valid_run:2073.78 task_valid:1983.46 collect_output:73.08
2023-02-22 03:45:50 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 03:45:50 - train.py[line:551] - INFO: load:1.65 valid_run:2194.97 task_valid:2101.82 collect_output:74.89
2023-02-22 03:47:52 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 03:47:52 - train.py[line:551] - INFO: load:1.68 valid_run:2316.84 task_valid:2219.22 collect_output:78.35
2023-02-22 03:49:53 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 03:49:53 - train.py[line:551] - INFO: load:1.70 valid_run:2437.76 task_valid:2336.34 collect_output:81.11
2023-02-22 03:51:55 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 03:51:55 - train.py[line:551] - INFO: load:1.72 valid_run:2559.99 task_valid:2453.33 collect_output:85.35
2023-02-22 03:53:57 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 03:53:57 - train.py[line:551] - INFO: load:1.75 valid_run:2682.42 task_valid:2572.60 collect_output:87.49
2023-02-22 03:55:58 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 03:55:58 - train.py[line:551] - INFO: load:1.77 valid_run:2803.31 task_valid:2687.29 collect_output:92.70
2023-02-22 03:57:59 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 03:57:59 - train.py[line:551] - INFO: load:1.80 valid_run:2923.92 task_valid:2804.27 collect_output:95.26
2023-02-22 04:00:02 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 04:00:02 - train.py[line:551] - INFO: load:1.82 valid_run:3046.29 task_valid:2921.08 collect_output:99.81
2023-02-22 04:02:05 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 04:02:05 - train.py[line:551] - INFO: load:1.85 valid_run:3169.80 task_valid:3037.53 collect_output:105.85
2023-02-22 04:04:05 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 04:04:05 - train.py[line:551] - INFO: load:1.87 valid_run:3289.89 task_valid:3152.04 collect_output:110.42
2023-02-22 04:06:08 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 04:06:08 - train.py[line:551] - INFO: load:1.90 valid_run:3412.33 task_valid:3272.00 collect_output:111.88
2023-02-22 04:08:10 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 04:08:10 - train.py[line:551] - INFO: load:1.92 valid_run:3534.82 task_valid:3387.93 collect_output:117.43
2023-02-22 04:10:13 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 04:10:13 - train.py[line:551] - INFO: load:1.95 valid_run:3657.43 task_valid:3506.90 collect_output:120.06
2023-02-22 04:12:15 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 04:12:15 - train.py[line:551] - INFO: load:1.98 valid_run:3779.16 task_valid:3625.87 collect_output:121.80

====================================================================================================
SGG eval:     R @ 50: 0.6252;     R @ 100: 0.6697;     R @ 500: 0.7052;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4250;    mR @ 100: 0.4765;    mR @ 500: 0.5143;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.3125) (covering:0.4286) (eating:0.7059) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5581) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9167) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6664) (standing on:0.5013) (using:0.4500) (walking in:0.0000) (walking on:0.6532) (watching:0.4583) 
--------------------------------------------------------
====================================================================================================

2023-02-22 04:12:45 - train.py[line:487] - INFO: 0.6696896358543417
2023-02-22 04:12:45 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 04:12:46 - progress_bar.py[line:282] - INFO: epoch 013 | valid on 'valid' subset | loss 0.23 | loss_v1 0 | loss_v2 0 | nll_loss 0.062 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.66969 | ppl 1.04 | vqa_score 0.4516 | wps 117.7 | wpb 72 | bsz 24 | num_updates 5000 | best_R@100 0.66969
2023-02-22 04:12:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 13 @ 5000 updates
2023-02-22 04:12:46 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_13_5000.pt

====================================================================================================
SGG eval:     R @ 50: 0.6252;     R @ 100: 0.6697;     R @ 500: 0.7052;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4250;    mR @ 100: 0.4765;    mR @ 500: 0.5143;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.3125) (covering:0.4286) (eating:0.7059) (flying in:1.0000) (growing on:0.3750) (hanging from:0.5581) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9167) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6664) (standing on:0.5013) (using:0.4500) (walking in:0.0000) (walking on:0.6532) (watching:0.4583) 
--------------------------------------------------------
====================================================================================================

2023-02-22 04:12:52 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_13_5000.pt
2023-02-22 04:12:57 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_13_5000.pt (epoch 13 @ 5000 updates, score 0.6696896358543417) (writing took 11.593231728300452 seconds)
2023-02-22 04:13:09 - progress_bar.py[line:274] - INFO: epoch 013:     72 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=110, bsz=40, num_updates=5010, lr=3.94165e-05, gnorm=0.418, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=28630
2023-02-22 04:13:20 - progress_bar.py[line:274] - INFO: epoch 013:     82 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.3, bsz=40, num_updates=5020, lr=3.93912e-05, gnorm=0.432, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28641
2023-02-22 04:13:31 - progress_bar.py[line:274] - INFO: epoch 013:     92 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=5030, lr=3.93659e-05, gnorm=0.443, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28652
2023-02-22 04:13:42 - progress_bar.py[line:274] - INFO: epoch 013:    102 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=5040, lr=3.93406e-05, gnorm=0.285, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28664
2023-02-22 04:13:53 - progress_bar.py[line:274] - INFO: epoch 013:    112 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.91, wpb=111.1, bsz=40, num_updates=5050, lr=3.93153e-05, gnorm=0.775, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28675
2023-02-22 04:14:04 - progress_bar.py[line:274] - INFO: epoch 013:    122 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.9, wpb=110.5, bsz=40, num_updates=5060, lr=3.929e-05, gnorm=0.805, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28686
2023-02-22 04:14:15 - progress_bar.py[line:274] - INFO: epoch 013:    132 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.88, wpb=111.3, bsz=40, num_updates=5070, lr=3.92648e-05, gnorm=0.39, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=28697
2023-02-22 04:14:26 - progress_bar.py[line:274] - INFO: epoch 013:    142 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.91, wpb=111.3, bsz=40, num_updates=5080, lr=3.92395e-05, gnorm=0.601, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28708
2023-02-22 04:14:38 - progress_bar.py[line:274] - INFO: epoch 013:    152 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.2, bsz=40, num_updates=5090, lr=3.92142e-05, gnorm=0.447, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28719
2023-02-22 04:14:49 - progress_bar.py[line:274] - INFO: epoch 013:    162 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.91, wpb=112, bsz=40, num_updates=5100, lr=3.91889e-05, gnorm=0.841, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28730
2023-02-22 04:15:00 - progress_bar.py[line:274] - INFO: epoch 013:    172 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=110.2, bsz=40, num_updates=5110, lr=3.91636e-05, gnorm=0.367, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28741
2023-02-22 04:15:11 - progress_bar.py[line:274] - INFO: epoch 013:    182 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=5120, lr=3.91383e-05, gnorm=0.681, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28753
2023-02-22 04:15:22 - progress_bar.py[line:274] - INFO: epoch 013:    192 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=5130, lr=3.91131e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28764
2023-02-22 04:15:33 - progress_bar.py[line:274] - INFO: epoch 013:    202 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=110.5, bsz=40, num_updates=5140, lr=3.90878e-05, gnorm=0.613, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28775
2023-02-22 04:15:45 - progress_bar.py[line:274] - INFO: epoch 013:    212 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=5150, lr=3.90625e-05, gnorm=0.728, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28786
2023-02-22 04:15:56 - progress_bar.py[line:274] - INFO: epoch 013:    222 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.87, wpb=111.5, bsz=40, num_updates=5160, lr=3.90372e-05, gnorm=0.764, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28798
2023-02-22 04:16:07 - progress_bar.py[line:274] - INFO: epoch 013:    232 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=110.6, bsz=40, num_updates=5170, lr=3.90119e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28809
2023-02-22 04:16:18 - progress_bar.py[line:274] - INFO: epoch 013:    242 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=112, bsz=40, num_updates=5180, lr=3.89867e-05, gnorm=0.395, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28820
2023-02-22 04:16:30 - progress_bar.py[line:274] - INFO: epoch 013:    252 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=110.9, bsz=40, num_updates=5190, lr=3.89614e-05, gnorm=1.161, clip=20, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=28831
2023-02-22 04:16:41 - progress_bar.py[line:274] - INFO: epoch 013:    262 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=110.8, bsz=40, num_updates=5200, lr=3.89361e-05, gnorm=0.917, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28842
2023-02-22 04:16:52 - progress_bar.py[line:274] - INFO: epoch 013:    272 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.3, ups=0.92, wpb=111.6, bsz=40, num_updates=5210, lr=3.89108e-05, gnorm=0.539, clip=20, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=28853
2023-02-22 04:17:03 - progress_bar.py[line:274] - INFO: epoch 013:    282 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=112.1, bsz=40, num_updates=5220, lr=3.88855e-05, gnorm=0.651, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28865
2023-02-22 04:17:14 - progress_bar.py[line:274] - INFO: epoch 013:    292 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.92, wpb=109.6, bsz=40, num_updates=5230, lr=3.88602e-05, gnorm=0.718, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28876
2023-02-22 04:17:25 - progress_bar.py[line:274] - INFO: epoch 013:    302 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.9, wpb=109.9, bsz=40, num_updates=5240, lr=3.8835e-05, gnorm=0.569, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28887
2023-02-22 04:17:36 - progress_bar.py[line:274] - INFO: epoch 013:    312 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=110.9, bsz=40, num_updates=5250, lr=3.88097e-05, gnorm=0.535, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28898
2023-02-22 04:17:47 - progress_bar.py[line:274] - INFO: epoch 013:    322 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.92, wpb=112.1, bsz=40, num_updates=5260, lr=3.87844e-05, gnorm=0.824, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28909
2023-02-22 04:17:58 - progress_bar.py[line:274] - INFO: epoch 013:    332 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.9, wpb=110.5, bsz=40, num_updates=5270, lr=3.87591e-05, gnorm=0.661, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28920
2023-02-22 04:18:09 - progress_bar.py[line:274] - INFO: epoch 013:    342 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=111.3, bsz=40, num_updates=5280, lr=3.87338e-05, gnorm=0.424, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28931
2023-02-22 04:18:20 - progress_bar.py[line:274] - INFO: epoch 013:    352 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=111, bsz=40, num_updates=5290, lr=3.87085e-05, gnorm=0.646, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=28942
2023-02-22 04:18:32 - progress_bar.py[line:274] - INFO: epoch 013:    362 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=5300, lr=3.86833e-05, gnorm=0.923, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28953
2023-02-22 04:18:43 - progress_bar.py[line:274] - INFO: epoch 013:    372 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.9, wpb=112.4, bsz=40, num_updates=5310, lr=3.8658e-05, gnorm=1.219, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28964
2023-02-22 04:18:54 - progress_bar.py[line:274] - INFO: epoch 013:    382 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=111.9, bsz=40, num_updates=5320, lr=3.86327e-05, gnorm=0.858, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28975
2023-02-22 04:19:05 - progress_bar.py[line:274] - INFO: epoch 013:    392 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.4, bsz=40, num_updates=5330, lr=3.86074e-05, gnorm=0.729, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28986
2023-02-22 04:19:16 - progress_bar.py[line:274] - INFO: epoch 013:    402 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.87, wpb=112.3, bsz=40, num_updates=5340, lr=3.85821e-05, gnorm=1.432, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28998
2023-02-22 04:19:27 - progress_bar.py[line:274] - INFO: epoch 013:    412 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=5350, lr=3.85568e-05, gnorm=0.814, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29009
2023-02-22 04:19:27 - train.py[line:339] - INFO: end of epoch 13 (average epoch stats below)
2023-02-22 04:19:27 - progress_bar.py[line:282] - INFO: epoch 013 | loss 0.243 | loss_v1 0 | loss_v2 0 | nll_loss 0.058 | ntokens 111.119 | nsentences 40 | sample_size 111.119 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 5350 | lr 3.85568e-05 | gnorm 0.666 | clip 21.4 | loss_scale 512 | train_wall 457 | gb_free 10.7 | ema_decay 0.9999 | wall 29009
2023-02-22 04:19:27 - trainer.py[line:694] - INFO: loading train data for epoch 14
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E13.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 04:19:28 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 04:19:28 - trainer.py[line:758] - INFO: begin training epoch 14
2023-02-22 04:19:28 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 04:19:41 - progress_bar.py[line:274] - INFO: epoch 014:     10 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=81.1, ups=0.74, wpb=109.6, bsz=40, num_updates=5360, lr=3.85316e-05, gnorm=0.703, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29022
2023-02-22 04:19:52 - progress_bar.py[line:274] - INFO: epoch 014:     20 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.4, ups=0.87, wpb=109.3, bsz=40, num_updates=5370, lr=3.85063e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29034
2023-02-22 04:20:03 - progress_bar.py[line:274] - INFO: epoch 014:     30 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=5380, lr=3.8481e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29045
2023-02-22 04:20:15 - progress_bar.py[line:274] - INFO: epoch 014:     40 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96, ups=0.86, wpb=111.7, bsz=40, num_updates=5390, lr=3.84557e-05, gnorm=0.832, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29057
2023-02-22 04:20:26 - progress_bar.py[line:274] - INFO: epoch 014:     50 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.89, wpb=110.9, bsz=40, num_updates=5400, lr=3.84304e-05, gnorm=0.745, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29068
2023-02-22 04:20:38 - progress_bar.py[line:274] - INFO: epoch 014:     60 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=5410, lr=3.84051e-05, gnorm=1.036, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29079
2023-02-22 04:20:49 - progress_bar.py[line:274] - INFO: epoch 014:     70 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.89, wpb=109.8, bsz=40, num_updates=5420, lr=3.83799e-05, gnorm=0.663, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29090
2023-02-22 04:21:00 - progress_bar.py[line:274] - INFO: epoch 014:     80 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.93, wpb=109.4, bsz=40, num_updates=5430, lr=3.83546e-05, gnorm=0.521, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29101
2023-02-22 04:21:11 - progress_bar.py[line:274] - INFO: epoch 014:     90 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=112.3, bsz=40, num_updates=5440, lr=3.83293e-05, gnorm=0.681, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29113
2023-02-22 04:21:22 - progress_bar.py[line:274] - INFO: epoch 014:    100 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=5450, lr=3.8304e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29124
2023-02-22 04:21:34 - progress_bar.py[line:274] - INFO: epoch 014:    110 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=113.3, bsz=40, num_updates=5460, lr=3.82787e-05, gnorm=0.521, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29135
2023-02-22 04:21:45 - progress_bar.py[line:274] - INFO: epoch 014:    120 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=5470, lr=3.82534e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29146
2023-02-22 04:21:56 - progress_bar.py[line:274] - INFO: epoch 014:    130 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=5480, lr=3.82282e-05, gnorm=0.494, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29158
2023-02-22 04:22:07 - progress_bar.py[line:274] - INFO: epoch 014:    140 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.89, wpb=111.4, bsz=40, num_updates=5490, lr=3.82029e-05, gnorm=0.742, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29169
2023-02-22 04:22:18 - progress_bar.py[line:274] - INFO: epoch 014:    150 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=5500, lr=3.81776e-05, gnorm=0.758, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29180
2023-02-22 04:22:30 - progress_bar.py[line:274] - INFO: epoch 014:    160 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=5510, lr=3.81523e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29191
2023-02-22 04:22:41 - progress_bar.py[line:274] - INFO: epoch 014:    170 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.8, bsz=40, num_updates=5520, lr=3.8127e-05, gnorm=0.585, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29202
2023-02-22 04:22:52 - progress_bar.py[line:274] - INFO: epoch 014:    180 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.1, ups=0.89, wpb=108.7, bsz=40, num_updates=5530, lr=3.81017e-05, gnorm=0.563, clip=30, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29213
2023-02-22 04:23:03 - progress_bar.py[line:274] - INFO: epoch 014:    190 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=113, bsz=40, num_updates=5540, lr=3.80765e-05, gnorm=0.87, clip=40, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=29224
2023-02-22 04:23:14 - progress_bar.py[line:274] - INFO: epoch 014:    200 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=5550, lr=3.80512e-05, gnorm=0.691, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29236
2023-02-22 04:23:25 - progress_bar.py[line:274] - INFO: epoch 014:    210 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=5560, lr=3.80259e-05, gnorm=0.667, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29247
2023-02-22 04:23:37 - progress_bar.py[line:274] - INFO: epoch 014:    220 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=5570, lr=3.80006e-05, gnorm=0.653, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29258
2023-02-22 04:23:47 - progress_bar.py[line:274] - INFO: epoch 014:    230 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.92, wpb=110.3, bsz=40, num_updates=5580, lr=3.79753e-05, gnorm=0.471, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29269
2023-02-22 04:23:59 - progress_bar.py[line:274] - INFO: epoch 014:    240 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=5590, lr=3.795e-05, gnorm=0.541, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29280
2023-02-22 04:24:10 - progress_bar.py[line:274] - INFO: epoch 014:    250 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.92, wpb=111.2, bsz=40, num_updates=5600, lr=3.79248e-05, gnorm=0.681, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29291
2023-02-22 04:24:20 - progress_bar.py[line:274] - INFO: epoch 014:    260 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.93, wpb=111.9, bsz=40, num_updates=5610, lr=3.78995e-05, gnorm=0.509, clip=20, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=29302
2023-02-22 04:24:31 - progress_bar.py[line:274] - INFO: epoch 014:    270 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.92, wpb=111.1, bsz=40, num_updates=5620, lr=3.78742e-05, gnorm=0.592, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29313
2023-02-22 04:24:34 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 04:24:43 - progress_bar.py[line:274] - INFO: epoch 014:    281 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=94.7, ups=0.84, wpb=113.2, bsz=40, num_updates=5630, lr=3.78489e-05, gnorm=0.862, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29325
2023-02-22 04:24:55 - progress_bar.py[line:274] - INFO: epoch 014:    291 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=5640, lr=3.78236e-05, gnorm=0.528, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=29336
2023-02-22 04:25:06 - progress_bar.py[line:274] - INFO: epoch 014:    301 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.92, wpb=111.7, bsz=40, num_updates=5650, lr=3.77983e-05, gnorm=0.552, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29347
2023-02-22 04:25:17 - progress_bar.py[line:274] - INFO: epoch 014:    311 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=5660, lr=3.77731e-05, gnorm=0.705, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29358
2023-02-22 04:25:28 - progress_bar.py[line:274] - INFO: epoch 014:    321 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=5670, lr=3.77478e-05, gnorm=0.57, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=29370
2023-02-22 04:25:39 - progress_bar.py[line:274] - INFO: epoch 014:    331 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=5680, lr=3.77225e-05, gnorm=0.821, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29381
2023-02-22 04:25:50 - progress_bar.py[line:274] - INFO: epoch 014:    341 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.9, wpb=111.6, bsz=40, num_updates=5690, lr=3.76972e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29392
2023-02-22 04:26:02 - progress_bar.py[line:274] - INFO: epoch 014:    351 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.2, bsz=40, num_updates=5700, lr=3.76719e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=29403
2023-02-22 04:26:13 - progress_bar.py[line:274] - INFO: epoch 014:    361 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=5710, lr=3.76466e-05, gnorm=0.576, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29415
2023-02-22 04:26:24 - progress_bar.py[line:274] - INFO: epoch 014:    371 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.93, wpb=111.1, bsz=40, num_updates=5720, lr=3.76214e-05, gnorm=0.383, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29425
2023-02-22 04:26:35 - progress_bar.py[line:274] - INFO: epoch 014:    381 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.92, wpb=111.8, bsz=40, num_updates=5730, lr=3.75961e-05, gnorm=0.483, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29436
2023-02-22 04:26:46 - progress_bar.py[line:274] - INFO: epoch 014:    391 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.91, wpb=110, bsz=40, num_updates=5740, lr=3.75708e-05, gnorm=0.836, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29447
2023-02-22 04:26:57 - progress_bar.py[line:274] - INFO: epoch 014:    401 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.89, wpb=112.4, bsz=40, num_updates=5750, lr=3.75455e-05, gnorm=0.729, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29458
2023-02-22 04:27:08 - progress_bar.py[line:274] - INFO: epoch 014:    411 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.87, wpb=112.2, bsz=40, num_updates=5760, lr=3.75202e-05, gnorm=0.268, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29470
2023-02-22 04:27:09 - train.py[line:339] - INFO: end of epoch 14 (average epoch stats below)
2023-02-22 04:27:09 - progress_bar.py[line:282] - INFO: epoch 014 | loss 0.245 | loss_v1 0 | loss_v2 0 | nll_loss 0.06 | ntokens 111.131 | nsentences 40 | sample_size 111.131 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.9 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 5761 | lr 3.75177e-05 | gnorm 0.609 | clip 18.5 | loss_scale 512 | train_wall 458 | gb_free 10.8 | ema_decay 0.9999 | wall 29471
2023-02-22 04:27:09 - trainer.py[line:694] - INFO: loading train data for epoch 15
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E14.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 04:27:09 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 04:27:10 - trainer.py[line:758] - INFO: begin training epoch 15
2023-02-22 04:27:10 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 04:27:22 - progress_bar.py[line:274] - INFO: epoch 015:      9 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=81.2, ups=0.73, wpb=112, bsz=40, num_updates=5770, lr=3.74949e-05, gnorm=0.404, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29484
2023-02-22 04:27:33 - progress_bar.py[line:274] - INFO: epoch 015:     19 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=112.3, bsz=40, num_updates=5780, lr=3.74697e-05, gnorm=0.525, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29495
2023-02-22 04:27:45 - progress_bar.py[line:274] - INFO: epoch 015:     29 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.9, wpb=109.9, bsz=40, num_updates=5790, lr=3.74444e-05, gnorm=0.505, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29506
2023-02-22 04:27:56 - progress_bar.py[line:274] - INFO: epoch 015:     39 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=110.6, bsz=40, num_updates=5800, lr=3.74191e-05, gnorm=0.258, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29517
2023-02-22 04:28:07 - progress_bar.py[line:274] - INFO: epoch 015:     49 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=5810, lr=3.73938e-05, gnorm=0.681, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29528
2023-02-22 04:28:18 - progress_bar.py[line:274] - INFO: epoch 015:     59 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.91, wpb=110.6, bsz=40, num_updates=5820, lr=3.73685e-05, gnorm=0.483, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=29539
2023-02-22 04:28:29 - progress_bar.py[line:274] - INFO: epoch 015:     69 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=5830, lr=3.73432e-05, gnorm=0.578, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29550
2023-02-22 04:28:40 - progress_bar.py[line:274] - INFO: epoch 015:     79 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.9, wpb=111.8, bsz=40, num_updates=5840, lr=3.7318e-05, gnorm=0.513, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29561
2023-02-22 04:28:51 - progress_bar.py[line:274] - INFO: epoch 015:     89 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=5850, lr=3.72927e-05, gnorm=0.874, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29573
2023-02-22 04:29:02 - progress_bar.py[line:274] - INFO: epoch 015:     99 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111.2, bsz=40, num_updates=5860, lr=3.72674e-05, gnorm=0.72, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29584
2023-02-22 04:29:14 - progress_bar.py[line:274] - INFO: epoch 015:    109 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.5, ups=0.88, wpb=110.1, bsz=40, num_updates=5870, lr=3.72421e-05, gnorm=0.647, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29595
2023-02-22 04:29:25 - progress_bar.py[line:274] - INFO: epoch 015:    119 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112.2, bsz=40, num_updates=5880, lr=3.72168e-05, gnorm=1.207, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29606
2023-02-22 04:29:36 - progress_bar.py[line:274] - INFO: epoch 015:    129 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=5890, lr=3.71915e-05, gnorm=0.552, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=29618
2023-02-22 04:29:47 - progress_bar.py[line:274] - INFO: epoch 015:    139 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=5900, lr=3.71663e-05, gnorm=0.652, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29629
2023-02-22 04:29:58 - progress_bar.py[line:274] - INFO: epoch 015:    149 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.89, wpb=110.3, bsz=40, num_updates=5910, lr=3.7141e-05, gnorm=0.604, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29640
2023-02-22 04:30:10 - progress_bar.py[line:274] - INFO: epoch 015:    159 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.89, wpb=109.4, bsz=40, num_updates=5920, lr=3.71157e-05, gnorm=0.678, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29651
2023-02-22 04:30:21 - progress_bar.py[line:274] - INFO: epoch 015:    169 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.9, wpb=111.7, bsz=40, num_updates=5930, lr=3.70904e-05, gnorm=0.359, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=29662
2023-02-22 04:30:32 - progress_bar.py[line:274] - INFO: epoch 015:    179 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97, ups=0.88, wpb=110.4, bsz=40, num_updates=5940, lr=3.70651e-05, gnorm=0.554, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29674
2023-02-22 04:30:43 - progress_bar.py[line:274] - INFO: epoch 015:    189 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=5950, lr=3.70398e-05, gnorm=0.461, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29685
2023-02-22 04:30:54 - progress_bar.py[line:274] - INFO: epoch 015:    199 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=111.7, bsz=40, num_updates=5960, lr=3.70146e-05, gnorm=0.908, clip=40, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29696
2023-02-22 04:31:05 - progress_bar.py[line:274] - INFO: epoch 015:    209 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=5970, lr=3.69893e-05, gnorm=0.421, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29707
2023-02-22 04:31:16 - progress_bar.py[line:274] - INFO: epoch 015:    219 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=111.8, bsz=40, num_updates=5980, lr=3.6964e-05, gnorm=0.757, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29718
2023-02-22 04:31:28 - progress_bar.py[line:274] - INFO: epoch 015:    229 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=5990, lr=3.69387e-05, gnorm=0.808, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29729
2023-02-22 04:31:39 - progress_bar.py[line:274] - INFO: epoch 015:    239 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=6000, lr=3.69134e-05, gnorm=0.316, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29740
2023-02-22 04:31:39 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 04:31:40 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 04:31:40 - train.py[line:551] - INFO: load:1.15 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 04:33:43 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 04:33:43 - train.py[line:551] - INFO: load:1.17 valid_run:122.67 task_valid:119.57 collect_output:2.03
2023-02-22 04:35:43 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 04:35:43 - train.py[line:551] - INFO: load:1.20 valid_run:242.93 task_valid:235.57 collect_output:5.25
2023-02-22 04:37:45 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 04:37:45 - train.py[line:551] - INFO: load:1.22 valid_run:365.18 task_valid:352.51 collect_output:9.54
2023-02-22 04:39:48 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 04:39:48 - train.py[line:551] - INFO: load:1.25 valid_run:487.43 task_valid:466.65 collect_output:16.64
2023-02-22 04:41:49 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 04:41:49 - train.py[line:551] - INFO: load:1.27 valid_run:608.22 task_valid:584.30 collect_output:18.77
2023-02-22 04:43:52 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 04:43:52 - train.py[line:551] - INFO: load:1.30 valid_run:731.54 task_valid:703.37 collect_output:21.98
2023-02-22 04:45:55 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 04:45:55 - train.py[line:551] - INFO: load:1.32 valid_run:854.90 task_valid:821.79 collect_output:25.90
2023-02-22 04:47:58 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 04:47:58 - train.py[line:551] - INFO: load:1.35 valid_run:977.15 task_valid:938.73 collect_output:30.21
2023-02-22 04:50:02 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 04:50:02 - train.py[line:551] - INFO: load:1.37 valid_run:1101.19 task_valid:1056.35 collect_output:35.60
2023-02-22 04:52:04 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 04:52:04 - train.py[line:551] - INFO: load:1.40 valid_run:1223.35 task_valid:1169.48 collect_output:43.60
2023-02-22 04:54:05 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 04:54:05 - train.py[line:551] - INFO: load:1.42 valid_run:1343.96 task_valid:1285.57 collect_output:47.07
2023-02-22 04:56:07 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 04:56:07 - train.py[line:551] - INFO: load:1.45 valid_run:1465.98 task_valid:1403.09 collect_output:50.54
2023-02-22 04:58:06 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 04:58:06 - train.py[line:551] - INFO: load:1.47 valid_run:1585.10 task_valid:1517.18 collect_output:54.54
2023-02-22 05:00:07 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 05:00:07 - train.py[line:551] - INFO: load:1.50 valid_run:1706.53 task_valid:1635.33 collect_output:56.80
2023-02-22 05:02:09 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 05:02:09 - train.py[line:551] - INFO: load:1.53 valid_run:1827.87 task_valid:1751.92 collect_output:60.53
2023-02-22 05:04:10 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 05:04:10 - train.py[line:551] - INFO: load:1.55 valid_run:1949.23 task_valid:1866.18 collect_output:66.62
2023-02-22 05:06:12 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 05:06:12 - train.py[line:551] - INFO: load:1.57 valid_run:2070.97 task_valid:1982.72 collect_output:70.77
2023-02-22 05:08:13 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 05:08:13 - train.py[line:551] - INFO: load:1.60 valid_run:2192.15 task_valid:2101.12 collect_output:72.53
2023-02-22 05:10:15 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 05:10:15 - train.py[line:551] - INFO: load:1.63 valid_run:2313.89 task_valid:2218.55 collect_output:75.80
2023-02-22 05:12:16 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 05:12:16 - train.py[line:551] - INFO: load:1.65 valid_run:2434.69 task_valid:2335.42 collect_output:78.71
2023-02-22 05:14:18 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 05:14:18 - train.py[line:551] - INFO: load:1.68 valid_run:2556.85 task_valid:2452.47 collect_output:82.79
2023-02-22 05:16:21 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 05:16:21 - train.py[line:551] - INFO: load:1.70 valid_run:2679.23 task_valid:2571.84 collect_output:84.76
2023-02-22 05:18:22 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 05:18:22 - train.py[line:551] - INFO: load:1.73 valid_run:2800.04 task_valid:2686.58 collect_output:89.82
2023-02-22 05:20:22 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 05:20:22 - train.py[line:551] - INFO: load:1.75 valid_run:2920.28 task_valid:2803.16 collect_output:92.46
2023-02-22 05:22:24 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 05:22:24 - train.py[line:551] - INFO: load:1.78 valid_run:3042.43 task_valid:2919.87 collect_output:96.86
2023-02-22 05:24:27 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 05:24:27 - train.py[line:551] - INFO: load:1.80 valid_run:3165.70 task_valid:3036.22 collect_output:102.77
2023-02-22 05:26:27 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 05:26:27 - train.py[line:551] - INFO: load:1.83 valid_run:3285.73 task_valid:3150.67 collect_output:107.32
2023-02-22 05:28:30 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 05:28:30 - train.py[line:551] - INFO: load:1.86 valid_run:3407.85 task_valid:3270.33 collect_output:108.77
2023-02-22 05:30:32 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 05:30:32 - train.py[line:551] - INFO: load:1.88 valid_run:3529.82 task_valid:3386.10 collect_output:113.96
2023-02-22 05:32:34 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 05:32:34 - train.py[line:551] - INFO: load:1.91 valid_run:3652.17 task_valid:3505.00 collect_output:116.39
2023-02-22 05:34:36 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 05:34:36 - train.py[line:551] - INFO: load:1.93 valid_run:3773.61 task_valid:3623.85 collect_output:117.96

====================================================================================================
SGG eval:     R @ 50: 0.6424;     R @ 100: 0.6838;     R @ 500: 0.7158;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4474;    mR @ 100: 0.4955;    mR @ 500: 0.5406;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.4792) (covering:0.4286) (eating:0.7353) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4839) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9314) (says:0.0000) (sitting on:0.6888) (standing on:0.4980) (using:0.5000) (walking in:0.0000) (walking on:0.6216) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================

2023-02-22 05:35:06 - train.py[line:487] - INFO: 0.6837896358543417

====================================================================================================
SGG eval:     R @ 50: 0.6424;     R @ 100: 0.6838;     R @ 500: 0.7158;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4474;    mR @ 100: 0.4955;    mR @ 500: 0.5406;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.4792) (covering:0.4286) (eating:0.7353) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4839) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9314) (says:0.0000) (sitting on:0.6888) (standing on:0.4980) (using:0.5000) (walking in:0.0000) (walking on:0.6216) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================

2023-02-22 05:35:07 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 05:35:07 - progress_bar.py[line:282] - INFO: epoch 015 | valid on 'valid' subset | loss 0.229 | loss_v1 0 | loss_v2 0 | nll_loss 0.059 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.68379 | ppl 1.04 | vqa_score 0.4572 | wps 117.9 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.68379
2023-02-22 05:35:07 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 15 @ 6000 updates
2023-02-22 05:35:07 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_15_6000.pt
2023-02-22 05:35:13 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_15_6000.pt
2023-02-22 05:35:18 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_15_6000.pt (epoch 15 @ 6000 updates, score 0.6837896358543417) (writing took 11.696908747777343 seconds)
2023-02-22 05:35:29 - progress_bar.py[line:274] - INFO: epoch 015:    249 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=110.6, bsz=40, num_updates=6010, lr=3.68881e-05, gnorm=0.767, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33571
2023-02-22 05:35:41 - progress_bar.py[line:274] - INFO: epoch 015:    259 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=6020, lr=3.68629e-05, gnorm=0.738, clip=40, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=33582
2023-02-22 05:35:52 - progress_bar.py[line:274] - INFO: epoch 015:    269 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.88, wpb=112.2, bsz=40, num_updates=6030, lr=3.68376e-05, gnorm=0.8, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33594
2023-02-22 05:36:03 - progress_bar.py[line:274] - INFO: epoch 015:    279 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=6040, lr=3.68123e-05, gnorm=0.614, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33605
2023-02-22 05:36:14 - progress_bar.py[line:274] - INFO: epoch 015:    289 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.93, wpb=111.5, bsz=40, num_updates=6050, lr=3.6787e-05, gnorm=0.48, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33616
2023-02-22 05:36:26 - progress_bar.py[line:274] - INFO: epoch 015:    299 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.87, wpb=112.2, bsz=40, num_updates=6060, lr=3.67617e-05, gnorm=0.59, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33627
2023-02-22 05:36:37 - progress_bar.py[line:274] - INFO: epoch 015:    309 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=6070, lr=3.67364e-05, gnorm=0.658, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33638
2023-02-22 05:36:48 - progress_bar.py[line:274] - INFO: epoch 015:    319 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=6080, lr=3.67112e-05, gnorm=0.379, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33649
2023-02-22 05:36:59 - progress_bar.py[line:274] - INFO: epoch 015:    329 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.93, wpb=111.2, bsz=40, num_updates=6090, lr=3.66859e-05, gnorm=0.746, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33660
2023-02-22 05:37:10 - progress_bar.py[line:274] - INFO: epoch 015:    339 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.1, bsz=40, num_updates=6100, lr=3.66606e-05, gnorm=0.474, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33671
2023-02-22 05:37:21 - progress_bar.py[line:274] - INFO: epoch 015:    349 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=6110, lr=3.66353e-05, gnorm=0.378, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33682
2023-02-22 05:37:32 - progress_bar.py[line:274] - INFO: epoch 015:    359 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.92, wpb=111.6, bsz=40, num_updates=6120, lr=3.661e-05, gnorm=0.591, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33693
2023-02-22 05:37:43 - progress_bar.py[line:274] - INFO: epoch 015:    369 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.9, wpb=110.9, bsz=40, num_updates=6130, lr=3.65847e-05, gnorm=0.382, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33704
2023-02-22 05:37:54 - progress_bar.py[line:274] - INFO: epoch 015:    379 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.92, wpb=109.6, bsz=40, num_updates=6140, lr=3.65595e-05, gnorm=0.672, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33715
2023-02-22 05:38:05 - progress_bar.py[line:274] - INFO: epoch 015:    389 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.3, bsz=40, num_updates=6150, lr=3.65342e-05, gnorm=0.788, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33726
2023-02-22 05:38:16 - progress_bar.py[line:274] - INFO: epoch 015:    399 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=6160, lr=3.65089e-05, gnorm=0.325, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33738
2023-02-22 05:38:27 - progress_bar.py[line:274] - INFO: epoch 015:    409 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.92, wpb=110.7, bsz=40, num_updates=6170, lr=3.64836e-05, gnorm=0.756, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33749
2023-02-22 05:38:30 - train.py[line:339] - INFO: end of epoch 15 (average epoch stats below)
2023-02-22 05:38:30 - progress_bar.py[line:282] - INFO: epoch 015 | loss 0.244 | loss_v1 0 | loss_v2 0 | nll_loss 0.059 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 6173 | lr 3.6476e-05 | gnorm 0.604 | clip 19.7 | loss_scale 1024 | train_wall 456 | gb_free 10.8 | ema_decay 0.9999 | wall 33752
2023-02-22 05:38:30 - trainer.py[line:694] - INFO: loading train data for epoch 16
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E15.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 05:38:30 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 05:38:31 - trainer.py[line:758] - INFO: begin training epoch 16
2023-02-22 05:38:31 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 05:38:41 - progress_bar.py[line:274] - INFO: epoch 016:      7 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=80, ups=0.72, wpb=111.1, bsz=40, num_updates=6180, lr=3.64583e-05, gnorm=0.574, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33763
2023-02-22 05:38:52 - progress_bar.py[line:274] - INFO: epoch 016:     17 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.9, wpb=111.6, bsz=40, num_updates=6190, lr=3.64331e-05, gnorm=0.35, clip=10, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=33774
2023-02-22 05:39:03 - progress_bar.py[line:274] - INFO: epoch 016:     27 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=110.8, bsz=40, num_updates=6200, lr=3.64078e-05, gnorm=0.417, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33785
2023-02-22 05:39:15 - progress_bar.py[line:274] - INFO: epoch 016:     37 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.89, wpb=111.9, bsz=40, num_updates=6210, lr=3.63825e-05, gnorm=0.553, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33796
2023-02-22 05:39:25 - progress_bar.py[line:274] - INFO: epoch 016:     47 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.91, wpb=110.6, bsz=40, num_updates=6220, lr=3.63572e-05, gnorm=0.375, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33807
2023-02-22 05:39:37 - progress_bar.py[line:274] - INFO: epoch 016:     57 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=6230, lr=3.63319e-05, gnorm=0.376, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33818
2023-02-22 05:39:48 - progress_bar.py[line:274] - INFO: epoch 016:     67 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.88, wpb=111, bsz=40, num_updates=6240, lr=3.63066e-05, gnorm=0.482, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33830
2023-02-22 05:40:00 - progress_bar.py[line:274] - INFO: epoch 016:     77 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.7, bsz=40, num_updates=6250, lr=3.62814e-05, gnorm=0.645, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33841
2023-02-22 05:40:11 - progress_bar.py[line:274] - INFO: epoch 016:     87 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=6260, lr=3.62561e-05, gnorm=0.384, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33852
2023-02-22 05:40:22 - progress_bar.py[line:274] - INFO: epoch 016:     97 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.88, wpb=111.1, bsz=40, num_updates=6270, lr=3.62308e-05, gnorm=1.108, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33864
2023-02-22 05:40:34 - progress_bar.py[line:274] - INFO: epoch 016:    107 / 412 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.87, wpb=111.2, bsz=40, num_updates=6280, lr=3.62055e-05, gnorm=0.567, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33875
2023-02-22 05:40:40 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 05:40:46 - progress_bar.py[line:274] - INFO: epoch 016:    118 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=90.9, ups=0.82, wpb=111.3, bsz=40, num_updates=6290, lr=3.61802e-05, gnorm=0.516, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=33887
2023-02-22 05:40:57 - progress_bar.py[line:274] - INFO: epoch 016:    128 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.89, wpb=111, bsz=40, num_updates=6300, lr=3.61549e-05, gnorm=0.578, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33899
2023-02-22 05:41:08 - progress_bar.py[line:274] - INFO: epoch 016:    138 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=6310, lr=3.61297e-05, gnorm=0.518, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33910
2023-02-22 05:41:19 - progress_bar.py[line:274] - INFO: epoch 016:    148 / 412 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.9, wpb=109.8, bsz=40, num_updates=6320, lr=3.61044e-05, gnorm=0.507, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33921
2023-02-22 05:41:31 - progress_bar.py[line:274] - INFO: epoch 016:    158 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=6330, lr=3.60791e-05, gnorm=0.667, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33932
2023-02-22 05:41:42 - progress_bar.py[line:274] - INFO: epoch 016:    168 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.3, ups=0.87, wpb=111.6, bsz=40, num_updates=6340, lr=3.60538e-05, gnorm=0.549, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33944
2023-02-22 05:41:54 - progress_bar.py[line:274] - INFO: epoch 016:    178 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=6350, lr=3.60285e-05, gnorm=0.235, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33955
2023-02-22 05:42:05 - progress_bar.py[line:274] - INFO: epoch 016:    188 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111, bsz=40, num_updates=6360, lr=3.60032e-05, gnorm=0.921, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33966
2023-02-22 05:42:16 - progress_bar.py[line:274] - INFO: epoch 016:    198 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=6370, lr=3.5978e-05, gnorm=0.409, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33978
2023-02-22 05:42:27 - progress_bar.py[line:274] - INFO: epoch 016:    208 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.92, wpb=111.2, bsz=40, num_updates=6380, lr=3.59527e-05, gnorm=0.474, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33989
2023-02-22 05:42:38 - progress_bar.py[line:274] - INFO: epoch 016:    218 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=6390, lr=3.59274e-05, gnorm=0.438, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34000
2023-02-22 05:42:49 - progress_bar.py[line:274] - INFO: epoch 016:    228 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=6400, lr=3.59021e-05, gnorm=0.892, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34011
2023-02-22 05:43:00 - progress_bar.py[line:274] - INFO: epoch 016:    238 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=6410, lr=3.58768e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=34022
2023-02-22 05:43:11 - progress_bar.py[line:274] - INFO: epoch 016:    248 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.92, wpb=111.4, bsz=40, num_updates=6420, lr=3.58515e-05, gnorm=0.595, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=34033
2023-02-22 05:43:23 - progress_bar.py[line:274] - INFO: epoch 016:    258 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=6430, lr=3.58263e-05, gnorm=0.587, clip=20, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=34044
2023-02-22 05:43:34 - progress_bar.py[line:274] - INFO: epoch 016:    268 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=6440, lr=3.5801e-05, gnorm=0.641, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34055
2023-02-22 05:43:45 - progress_bar.py[line:274] - INFO: epoch 016:    278 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.91, wpb=109.9, bsz=40, num_updates=6450, lr=3.57757e-05, gnorm=0.505, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34067
2023-02-22 05:43:56 - progress_bar.py[line:274] - INFO: epoch 016:    288 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112.3, bsz=40, num_updates=6460, lr=3.57504e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34078
2023-02-22 05:44:07 - progress_bar.py[line:274] - INFO: epoch 016:    298 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.91, wpb=110.3, bsz=40, num_updates=6470, lr=3.57251e-05, gnorm=0.856, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34089
2023-02-22 05:44:18 - progress_bar.py[line:274] - INFO: epoch 016:    308 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.92, wpb=109.7, bsz=40, num_updates=6480, lr=3.56998e-05, gnorm=0.4, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34100
2023-02-22 05:44:30 - progress_bar.py[line:274] - INFO: epoch 016:    318 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.4, ups=0.87, wpb=110.9, bsz=40, num_updates=6490, lr=3.56746e-05, gnorm=0.644, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34111
2023-02-22 05:44:41 - progress_bar.py[line:274] - INFO: epoch 016:    328 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.3, bsz=40, num_updates=6500, lr=3.56493e-05, gnorm=0.738, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=34122
2023-02-22 05:44:52 - progress_bar.py[line:274] - INFO: epoch 016:    338 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.9, wpb=110, bsz=40, num_updates=6510, lr=3.5624e-05, gnorm=0.259, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34134
2023-02-22 05:45:04 - progress_bar.py[line:274] - INFO: epoch 016:    348 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96, ups=0.87, wpb=110, bsz=40, num_updates=6520, lr=3.55987e-05, gnorm=0.616, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34145
2023-02-22 05:45:15 - progress_bar.py[line:274] - INFO: epoch 016:    358 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111.2, bsz=40, num_updates=6530, lr=3.55734e-05, gnorm=0.516, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34156
2023-02-22 05:45:26 - progress_bar.py[line:274] - INFO: epoch 016:    368 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=6540, lr=3.55481e-05, gnorm=0.46, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34168
2023-02-22 05:45:37 - progress_bar.py[line:274] - INFO: epoch 016:    378 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.89, wpb=111.2, bsz=40, num_updates=6550, lr=3.55229e-05, gnorm=0.508, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34179
2023-02-22 05:45:48 - progress_bar.py[line:274] - INFO: epoch 016:    388 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.9, bsz=40, num_updates=6560, lr=3.54976e-05, gnorm=0.524, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34190
2023-02-22 05:46:00 - progress_bar.py[line:274] - INFO: epoch 016:    398 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=6570, lr=3.54723e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34201
2023-02-22 05:46:11 - progress_bar.py[line:274] - INFO: epoch 016:    408 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.1, bsz=40, num_updates=6580, lr=3.5447e-05, gnorm=0.719, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34213
2023-02-22 05:46:16 - train.py[line:339] - INFO: end of epoch 16 (average epoch stats below)
2023-02-22 05:46:16 - progress_bar.py[line:282] - INFO: epoch 016 | loss 0.242 | loss_v1 0 | loss_v2 0 | nll_loss 0.057 | ntokens 111.129 | nsentences 40 | sample_size 111.129 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.2 | ups 0.88 | wpb 111.1 | bsz 40 | num_updates 6584 | lr 3.54369e-05 | gnorm 0.553 | clip 16.3 | loss_scale 512 | train_wall 460 | gb_free 10.5 | ema_decay 0.9999 | wall 34217
2023-02-22 05:46:16 - trainer.py[line:694] - INFO: loading train data for epoch 17
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E16.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 05:46:16 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 05:46:16 - trainer.py[line:758] - INFO: begin training epoch 17
2023-02-22 05:46:16 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 05:46:25 - progress_bar.py[line:274] - INFO: epoch 017:      6 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=83.5, ups=0.74, wpb=112.3, bsz=40, num_updates=6590, lr=3.54217e-05, gnorm=0.307, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=34226
2023-02-22 05:46:36 - progress_bar.py[line:274] - INFO: epoch 017:     16 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.9, ups=0.91, wpb=112.6, bsz=40, num_updates=6600, lr=3.53964e-05, gnorm=0.445, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34237
2023-02-22 05:46:47 - progress_bar.py[line:274] - INFO: epoch 017:     26 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=6610, lr=3.53712e-05, gnorm=0.568, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34248
2023-02-22 05:46:58 - progress_bar.py[line:274] - INFO: epoch 017:     36 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=6620, lr=3.53459e-05, gnorm=0.501, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=34259
2023-02-22 05:47:09 - progress_bar.py[line:274] - INFO: epoch 017:     46 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=6630, lr=3.53206e-05, gnorm=0.361, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34270
2023-02-22 05:47:20 - progress_bar.py[line:274] - INFO: epoch 017:     56 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.9, wpb=111.3, bsz=40, num_updates=6640, lr=3.52953e-05, gnorm=0.5, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34281
2023-02-22 05:47:31 - progress_bar.py[line:274] - INFO: epoch 017:     66 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=94.8, ups=0.86, wpb=110.1, bsz=40, num_updates=6650, lr=3.527e-05, gnorm=0.878, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=34293
2023-02-22 05:47:43 - progress_bar.py[line:274] - INFO: epoch 017:     76 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.9, wpb=111.6, bsz=40, num_updates=6660, lr=3.52447e-05, gnorm=0.353, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=34304
2023-02-22 05:47:54 - progress_bar.py[line:274] - INFO: epoch 017:     86 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=6670, lr=3.52195e-05, gnorm=0.7, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34315
2023-02-22 05:48:04 - progress_bar.py[line:274] - INFO: epoch 017:     96 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.93, wpb=111.9, bsz=40, num_updates=6680, lr=3.51942e-05, gnorm=0.422, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34326
2023-02-22 05:48:15 - progress_bar.py[line:274] - INFO: epoch 017:    106 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.93, wpb=111.8, bsz=40, num_updates=6690, lr=3.51689e-05, gnorm=0.428, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34337
2023-02-22 05:48:26 - progress_bar.py[line:274] - INFO: epoch 017:    116 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.93, wpb=112.3, bsz=40, num_updates=6700, lr=3.51436e-05, gnorm=0.426, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=34348
2023-02-22 05:48:37 - progress_bar.py[line:274] - INFO: epoch 017:    126 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=6710, lr=3.51183e-05, gnorm=0.981, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34359
2023-02-22 05:48:48 - progress_bar.py[line:274] - INFO: epoch 017:    136 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.91, wpb=111.1, bsz=40, num_updates=6720, lr=3.5093e-05, gnorm=0.552, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34370
2023-02-22 05:48:59 - progress_bar.py[line:274] - INFO: epoch 017:    146 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.2, bsz=40, num_updates=6730, lr=3.50678e-05, gnorm=0.488, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=34381
2023-02-22 05:49:11 - progress_bar.py[line:274] - INFO: epoch 017:    156 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=6740, lr=3.50425e-05, gnorm=0.66, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34392
2023-02-22 05:49:22 - progress_bar.py[line:274] - INFO: epoch 017:    166 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.8, ups=0.87, wpb=111.2, bsz=40, num_updates=6750, lr=3.50172e-05, gnorm=0.917, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=34404
2023-02-22 05:49:34 - progress_bar.py[line:274] - INFO: epoch 017:    176 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111.3, bsz=40, num_updates=6760, lr=3.49919e-05, gnorm=0.482, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34415
2023-02-22 05:49:45 - progress_bar.py[line:274] - INFO: epoch 017:    186 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=6770, lr=3.49666e-05, gnorm=0.968, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34426
2023-02-22 05:49:56 - progress_bar.py[line:274] - INFO: epoch 017:    196 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=6780, lr=3.49413e-05, gnorm=0.631, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34437
2023-02-22 05:50:07 - progress_bar.py[line:274] - INFO: epoch 017:    206 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=6790, lr=3.49161e-05, gnorm=0.988, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34449
2023-02-22 05:50:18 - progress_bar.py[line:274] - INFO: epoch 017:    216 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=6800, lr=3.48908e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34460
2023-02-22 05:50:29 - progress_bar.py[line:274] - INFO: epoch 017:    226 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.5, ups=0.92, wpb=111.4, bsz=40, num_updates=6810, lr=3.48655e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34471
2023-02-22 05:50:40 - progress_bar.py[line:274] - INFO: epoch 017:    236 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=6820, lr=3.48402e-05, gnorm=0.354, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34482
2023-02-22 05:50:51 - progress_bar.py[line:274] - INFO: epoch 017:    246 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112.1, bsz=40, num_updates=6830, lr=3.48149e-05, gnorm=0.681, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34493
2023-02-22 05:51:03 - progress_bar.py[line:274] - INFO: epoch 017:    256 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.5, ups=0.87, wpb=111, bsz=40, num_updates=6840, lr=3.47896e-05, gnorm=0.492, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34505
2023-02-22 05:51:14 - progress_bar.py[line:274] - INFO: epoch 017:    266 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=6850, lr=3.47644e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34516
2023-02-22 05:51:25 - progress_bar.py[line:274] - INFO: epoch 017:    276 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102, ups=0.92, wpb=111.3, bsz=40, num_updates=6860, lr=3.47391e-05, gnorm=0.358, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34527
2023-02-22 05:51:37 - progress_bar.py[line:274] - INFO: epoch 017:    286 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=6870, lr=3.47138e-05, gnorm=0.749, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34538
2023-02-22 05:51:48 - progress_bar.py[line:274] - INFO: epoch 017:    296 / 412 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=6880, lr=3.46885e-05, gnorm=0.562, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=34549
2023-02-22 05:51:59 - progress_bar.py[line:274] - INFO: epoch 017:    306 / 412 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=6890, lr=3.46632e-05, gnorm=0.657, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34561
2023-02-22 05:52:10 - progress_bar.py[line:274] - INFO: epoch 017:    316 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=6900, lr=3.46379e-05, gnorm=0.366, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34572
2023-02-22 05:52:21 - progress_bar.py[line:274] - INFO: epoch 017:    326 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=111.9, bsz=40, num_updates=6910, lr=3.46127e-05, gnorm=0.589, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34583
2023-02-22 05:52:33 - progress_bar.py[line:274] - INFO: epoch 017:    336 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.4, bsz=40, num_updates=6920, lr=3.45874e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34594
2023-02-22 05:52:44 - progress_bar.py[line:274] - INFO: epoch 017:    346 / 412 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=6930, lr=3.45621e-05, gnorm=0.552, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34605
2023-02-22 05:52:54 - progress_bar.py[line:274] - INFO: epoch 017:    356 / 412 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.93, wpb=110.1, bsz=40, num_updates=6940, lr=3.45368e-05, gnorm=0.509, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34616
2023-02-22 05:53:06 - progress_bar.py[line:274] - INFO: epoch 017:    366 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=6950, lr=3.45115e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34627
2023-02-22 05:53:17 - progress_bar.py[line:274] - INFO: epoch 017:    376 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=111.4, bsz=40, num_updates=6960, lr=3.44862e-05, gnorm=0.52, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34638
2023-02-22 05:53:28 - progress_bar.py[line:274] - INFO: epoch 017:    386 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=6970, lr=3.4461e-05, gnorm=0.557, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34649
2023-02-22 05:53:39 - progress_bar.py[line:274] - INFO: epoch 017:    396 / 412 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=6980, lr=3.44357e-05, gnorm=0.68, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34661
2023-02-22 05:53:50 - progress_bar.py[line:274] - INFO: epoch 017:    406 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.88, wpb=110.8, bsz=40, num_updates=6990, lr=3.44104e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34672
2023-02-22 05:53:57 - train.py[line:339] - INFO: end of epoch 17 (average epoch stats below)
2023-02-22 05:53:57 - progress_bar.py[line:282] - INFO: epoch 017 | loss 0.242 | loss_v1 0 | loss_v2 0 | nll_loss 0.057 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 99.2 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 6996 | lr 3.43952e-05 | gnorm 0.548 | clip 12.4 | loss_scale 1024 | train_wall 457 | gb_free 10.7 | ema_decay 0.9999 | wall 34679
2023-02-22 05:53:57 - trainer.py[line:694] - INFO: loading train data for epoch 18
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv slice_id 0 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E17.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 05:53:57 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 05:53:58 - trainer.py[line:758] - INFO: begin training epoch 18
2023-02-22 05:53:58 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 05:54:04 - progress_bar.py[line:274] - INFO: epoch 018:      4 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=78.9, ups=0.71, wpb=110.6, bsz=40, num_updates=7000, lr=3.43851e-05, gnorm=0.548, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=34686
2023-02-22 05:54:04 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 05:54:06 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 05:54:06 - train.py[line:551] - INFO: load:1.38 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 05:56:09 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 05:56:09 - train.py[line:551] - INFO: load:1.41 valid_run:122.63 task_valid:119.91 collect_output:1.62
2023-02-22 05:58:09 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 05:58:09 - train.py[line:551] - INFO: load:1.43 valid_run:242.97 task_valid:236.23 collect_output:4.61
2023-02-22 06:00:12 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 06:00:12 - train.py[line:551] - INFO: load:1.45 valid_run:365.23 task_valid:353.07 collect_output:9.01
2023-02-22 06:02:14 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 06:02:14 - train.py[line:551] - INFO: load:1.48 valid_run:487.39 task_valid:467.09 collect_output:16.15
2023-02-22 06:04:15 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 06:04:15 - train.py[line:551] - INFO: load:1.50 valid_run:608.24 task_valid:584.80 collect_output:18.26
2023-02-22 06:06:18 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 06:06:18 - train.py[line:551] - INFO: load:1.53 valid_run:731.48 task_valid:703.87 collect_output:21.42
2023-02-22 06:08:21 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 06:08:21 - train.py[line:551] - INFO: load:1.55 valid_run:854.81 task_valid:822.26 collect_output:25.35
2023-02-22 06:10:24 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 06:10:24 - train.py[line:551] - INFO: load:1.58 valid_run:976.99 task_valid:939.17 collect_output:29.60
2023-02-22 06:12:28 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 06:12:28 - train.py[line:551] - INFO: load:1.60 valid_run:1101.07 task_valid:1056.65 collect_output:35.20
2023-02-22 06:14:30 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 06:14:30 - train.py[line:551] - INFO: load:1.62 valid_run:1223.05 task_valid:1169.61 collect_output:43.23
2023-02-22 06:16:30 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 06:16:30 - train.py[line:551] - INFO: load:1.65 valid_run:1343.52 task_valid:1285.57 collect_output:46.71
2023-02-22 06:18:32 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 06:18:32 - train.py[line:551] - INFO: load:1.67 valid_run:1465.43 task_valid:1402.84 collect_output:50.34
2023-02-22 06:20:32 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 06:20:32 - train.py[line:551] - INFO: load:1.69 valid_run:1584.71 task_valid:1517.19 collect_output:54.26
2023-02-22 06:22:33 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 06:22:33 - train.py[line:551] - INFO: load:1.72 valid_run:1706.16 task_valid:1635.43 collect_output:56.44
2023-02-22 06:24:35 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 06:24:35 - train.py[line:551] - INFO: load:1.74 valid_run:1827.75 task_valid:1752.20 collect_output:60.24
2023-02-22 06:26:36 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 06:26:36 - train.py[line:551] - INFO: load:1.77 valid_run:1949.18 task_valid:1866.76 collect_output:66.07
2023-02-22 06:28:38 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 06:28:38 - train.py[line:551] - INFO: load:1.79 valid_run:2070.86 task_valid:1983.27 collect_output:70.19
2023-02-22 06:30:39 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 06:30:39 - train.py[line:551] - INFO: load:1.82 valid_run:2191.93 task_valid:2101.60 collect_output:71.92
2023-02-22 06:32:41 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 06:32:41 - train.py[line:551] - INFO: load:1.84 valid_run:2313.56 task_valid:2218.92 collect_output:75.24
2023-02-22 06:34:42 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 06:34:42 - train.py[line:551] - INFO: load:1.87 valid_run:2434.31 task_valid:2335.89 collect_output:78.01
2023-02-22 06:36:44 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 06:36:44 - train.py[line:551] - INFO: load:1.89 valid_run:2556.42 task_valid:2452.93 collect_output:82.07
2023-02-22 06:38:46 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 06:38:46 - train.py[line:551] - INFO: load:1.92 valid_run:2678.80 task_valid:2572.34 collect_output:84.02
2023-02-22 06:40:47 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 06:40:47 - train.py[line:551] - INFO: load:1.94 valid_run:2799.56 task_valid:2687.21 collect_output:88.91
2023-02-22 06:42:47 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 06:42:47 - train.py[line:551] - INFO: load:1.97 valid_run:2919.76 task_valid:2803.73 collect_output:91.59
2023-02-22 06:44:49 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 06:44:49 - train.py[line:551] - INFO: load:1.99 valid_run:3041.64 task_valid:2920.28 collect_output:95.91
2023-02-22 06:46:52 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 06:46:52 - train.py[line:551] - INFO: load:2.01 valid_run:3164.76 task_valid:3036.54 collect_output:101.75
2023-02-22 06:48:52 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 06:48:52 - train.py[line:551] - INFO: load:2.04 valid_run:3284.62 task_valid:3151.10 collect_output:106.03
2023-02-22 06:50:55 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 06:50:55 - train.py[line:551] - INFO: load:2.07 valid_run:3407.04 task_valid:3271.13 collect_output:107.38
2023-02-22 06:52:57 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 06:52:57 - train.py[line:551] - INFO: load:2.09 valid_run:3529.22 task_valid:3387.04 collect_output:112.65
2023-02-22 06:54:59 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 06:54:59 - train.py[line:551] - INFO: load:2.11 valid_run:3651.43 task_valid:3505.86 collect_output:115.03
2023-02-22 06:57:01 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 06:57:01 - train.py[line:551] - INFO: load:2.14 valid_run:3772.89 task_valid:3624.68 collect_output:116.66

====================================================================================================
SGG eval:     R @ 50: 0.6345;     R @ 100: 0.6753;     R @ 500: 0.7059;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4376;    mR @ 100: 0.4840;    mR @ 500: 0.5379;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.4792) (covering:0.2286) (eating:0.7647) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4516) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.7001) (standing on:0.4530) (using:0.5500) (walking in:0.0000) (walking on:0.5676) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6345;     R @ 100: 0.6753;     R @ 500: 0.7059;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4376;    mR @ 100: 0.4840;    mR @ 500: 0.5379;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.4792) (covering:0.2286) (eating:0.7647) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4516) (lying on:0.4000) (mounted on:0.0000) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.7001) (standing on:0.4530) (using:0.5500) (walking in:0.0000) (walking on:0.5676) (watching:0.5833) 
--------------------------------------------------------
====================================================================================================

2023-02-22 06:57:32 - train.py[line:487] - INFO: 0.6753134453781513
2023-02-22 06:57:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 06:57:32 - progress_bar.py[line:282] - INFO: epoch 018 | valid on 'valid' subset | loss 0.236 | loss_v1 0 | loss_v2 0 | nll_loss 0.063 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.675313 | ppl 1.04 | vqa_score 0.4572 | wps 117.9 | wpb 72 | bsz 24 | num_updates 7000 | best_R@100 0.68379
2023-02-22 06:57:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 18 @ 7000 updates
2023-02-22 06:57:32 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_18_7000.pt
2023-02-22 06:57:38 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_18_7000.pt
2023-02-22 06:57:41 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_18_7000.pt (epoch 18 @ 7000 updates, score 0.6753134453781513) (writing took 9.010823400691152 seconds)
2023-02-22 06:57:52 - progress_bar.py[line:274] - INFO: epoch 018:     14 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=110.7, bsz=40, num_updates=7010, lr=3.43598e-05, gnorm=0.356, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38514
2023-02-22 06:58:03 - progress_bar.py[line:274] - INFO: epoch 018:     24 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=112.1, bsz=40, num_updates=7020, lr=3.43345e-05, gnorm=0.459, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38525
2023-02-22 06:58:14 - progress_bar.py[line:274] - INFO: epoch 018:     34 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=7030, lr=3.43093e-05, gnorm=0.448, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38536
2023-02-22 06:58:26 - progress_bar.py[line:274] - INFO: epoch 018:     44 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.88, wpb=111.5, bsz=40, num_updates=7040, lr=3.4284e-05, gnorm=0.453, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38547
2023-02-22 06:58:37 - progress_bar.py[line:274] - INFO: epoch 018:     54 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.7, ups=0.93, wpb=111, bsz=40, num_updates=7050, lr=3.42587e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38558
2023-02-22 06:58:48 - progress_bar.py[line:274] - INFO: epoch 018:     64 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=7060, lr=3.42334e-05, gnorm=0.457, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38569
2023-02-22 06:58:59 - progress_bar.py[line:274] - INFO: epoch 018:     74 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.89, wpb=109.5, bsz=40, num_updates=7070, lr=3.42081e-05, gnorm=0.572, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38581
2023-02-22 06:59:10 - progress_bar.py[line:274] - INFO: epoch 018:     84 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=7080, lr=3.41828e-05, gnorm=0.378, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38592
2023-02-22 06:59:17 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 06:59:22 - progress_bar.py[line:274] - INFO: epoch 018:     95 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=94, ups=0.84, wpb=111.7, bsz=40, num_updates=7090, lr=3.41576e-05, gnorm=0.446, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=38604
2023-02-22 06:59:34 - progress_bar.py[line:274] - INFO: epoch 018:    105 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=7100, lr=3.41323e-05, gnorm=0.581, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38615
2023-02-22 06:59:45 - progress_bar.py[line:274] - INFO: epoch 018:    115 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=7110, lr=3.4107e-05, gnorm=0.559, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38626
2023-02-22 06:59:56 - progress_bar.py[line:274] - INFO: epoch 018:    125 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.88, wpb=111.2, bsz=40, num_updates=7120, lr=3.40817e-05, gnorm=0.542, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38638
2023-02-22 07:00:08 - progress_bar.py[line:274] - INFO: epoch 018:    135 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=7130, lr=3.40564e-05, gnorm=0.36, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38649
2023-02-22 07:00:19 - progress_bar.py[line:274] - INFO: epoch 018:    145 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=112, bsz=40, num_updates=7140, lr=3.40311e-05, gnorm=0.576, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38661
2023-02-22 07:00:30 - progress_bar.py[line:274] - INFO: epoch 018:    155 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=7150, lr=3.40059e-05, gnorm=0.533, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38672
2023-02-22 07:00:42 - progress_bar.py[line:274] - INFO: epoch 018:    165 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=110.9, bsz=40, num_updates=7160, lr=3.39806e-05, gnorm=0.514, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38683
2023-02-22 07:00:53 - progress_bar.py[line:274] - INFO: epoch 018:    175 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=7170, lr=3.39553e-05, gnorm=0.307, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38694
2023-02-22 07:01:04 - progress_bar.py[line:274] - INFO: epoch 018:    185 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.9, wpb=110.5, bsz=40, num_updates=7180, lr=3.393e-05, gnorm=0.455, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38705
2023-02-22 07:01:15 - progress_bar.py[line:274] - INFO: epoch 018:    195 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.92, wpb=110.9, bsz=40, num_updates=7190, lr=3.39047e-05, gnorm=0.628, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38717
2023-02-22 07:01:26 - progress_bar.py[line:274] - INFO: epoch 018:    205 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=7200, lr=3.38794e-05, gnorm=0.285, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38728
2023-02-22 07:01:38 - progress_bar.py[line:274] - INFO: epoch 018:    215 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.88, wpb=111, bsz=40, num_updates=7210, lr=3.38542e-05, gnorm=1.043, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38739
2023-02-22 07:01:48 - progress_bar.py[line:274] - INFO: epoch 018:    225 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.92, wpb=110.8, bsz=40, num_updates=7220, lr=3.38289e-05, gnorm=0.447, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38750
2023-02-22 07:02:00 - progress_bar.py[line:274] - INFO: epoch 018:    235 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=7230, lr=3.38036e-05, gnorm=0.631, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38761
2023-02-22 07:02:11 - progress_bar.py[line:274] - INFO: epoch 018:    245 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.87, wpb=111.4, bsz=40, num_updates=7240, lr=3.37783e-05, gnorm=0.402, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=38773
2023-02-22 07:02:23 - progress_bar.py[line:274] - INFO: epoch 018:    255 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=111.8, bsz=40, num_updates=7250, lr=3.3753e-05, gnorm=0.336, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38784
2023-02-22 07:02:34 - progress_bar.py[line:274] - INFO: epoch 018:    265 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.88, wpb=111.7, bsz=40, num_updates=7260, lr=3.37278e-05, gnorm=0.5, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38796
2023-02-22 07:02:45 - progress_bar.py[line:274] - INFO: epoch 018:    275 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.3, ups=0.88, wpb=110.7, bsz=40, num_updates=7270, lr=3.37025e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38807
2023-02-22 07:02:56 - progress_bar.py[line:274] - INFO: epoch 018:    285 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.91, wpb=109.6, bsz=40, num_updates=7280, lr=3.36772e-05, gnorm=0.569, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38818
2023-02-22 07:03:07 - progress_bar.py[line:274] - INFO: epoch 018:    295 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.5, ups=0.92, wpb=112, bsz=40, num_updates=7290, lr=3.36519e-05, gnorm=0.345, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38829
2023-02-22 07:03:18 - progress_bar.py[line:274] - INFO: epoch 018:    305 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.9, wpb=111.5, bsz=40, num_updates=7300, lr=3.36266e-05, gnorm=0.318, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38840
2023-02-22 07:03:29 - progress_bar.py[line:274] - INFO: epoch 018:    315 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.93, wpb=111, bsz=40, num_updates=7310, lr=3.36013e-05, gnorm=0.298, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=38851
2023-02-22 07:03:40 - progress_bar.py[line:274] - INFO: epoch 018:    325 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.9, wpb=109.1, bsz=40, num_updates=7320, lr=3.35761e-05, gnorm=0.239, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38862
2023-02-22 07:03:51 - progress_bar.py[line:274] - INFO: epoch 018:    335 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.93, wpb=112.2, bsz=40, num_updates=7330, lr=3.35508e-05, gnorm=0.342, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38873
2023-02-22 07:04:03 - progress_bar.py[line:274] - INFO: epoch 018:    345 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.88, wpb=112.2, bsz=40, num_updates=7340, lr=3.35255e-05, gnorm=0.345, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38884
2023-02-22 07:04:14 - progress_bar.py[line:274] - INFO: epoch 018:    355 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111.9, bsz=40, num_updates=7350, lr=3.35002e-05, gnorm=0.243, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38895
2023-02-22 07:04:25 - progress_bar.py[line:274] - INFO: epoch 018:    365 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=111.5, bsz=40, num_updates=7360, lr=3.34749e-05, gnorm=0.275, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38907
2023-02-22 07:04:36 - progress_bar.py[line:274] - INFO: epoch 018:    375 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=7370, lr=3.34496e-05, gnorm=0.236, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38918
2023-02-22 07:04:47 - progress_bar.py[line:274] - INFO: epoch 018:    385 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=111.2, bsz=40, num_updates=7380, lr=3.34244e-05, gnorm=0.418, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38929
2023-02-22 07:04:58 - progress_bar.py[line:274] - INFO: epoch 018:    395 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.2, ups=0.91, wpb=111.8, bsz=40, num_updates=7390, lr=3.33991e-05, gnorm=0.305, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38940
2023-02-22 07:05:10 - progress_bar.py[line:274] - INFO: epoch 018:    405 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.5, bsz=40, num_updates=7400, lr=3.33738e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38951
2023-02-22 07:05:17 - train.py[line:339] - INFO: end of epoch 18 (average epoch stats below)
2023-02-22 07:05:17 - progress_bar.py[line:282] - INFO: epoch 018 | loss 0.24 | loss_v1 0 | loss_v2 0 | nll_loss 0.054 | ntokens 111.136 | nsentences 40 | sample_size 111.136 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 10.7 | ups 0.1 | wpb 111.1 | bsz 40 | num_updates 7407 | lr 3.33561e-05 | gnorm 0.443 | clip 9.2 | loss_scale 512 | train_wall 458 | gb_free 10.7 | ema_decay 0.9999 | wall 38959
2023-02-22 07:05:17 - trainer.py[line:694] - INFO: loading train data for epoch 19
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv slice_id 1 row count 8240 total row count 16480
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E18.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 07:05:18 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 07:05:18 - trainer.py[line:758] - INFO: begin training epoch 19
2023-02-22 07:05:18 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 07:05:23 - progress_bar.py[line:274] - INFO: epoch 019:      3 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=82.1, ups=0.74, wpb=111.4, bsz=40, num_updates=7410, lr=3.33485e-05, gnorm=0.606, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38965
2023-02-22 07:05:34 - progress_bar.py[line:274] - INFO: epoch 019:     13 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=7420, lr=3.33232e-05, gnorm=0.483, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38976
2023-02-22 07:05:46 - progress_bar.py[line:274] - INFO: epoch 019:     23 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.88, wpb=111.5, bsz=40, num_updates=7430, lr=3.32979e-05, gnorm=0.505, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38987
2023-02-22 07:05:57 - progress_bar.py[line:274] - INFO: epoch 019:     33 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.9, wpb=112.6, bsz=40, num_updates=7440, lr=3.32727e-05, gnorm=0.259, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38998
2023-02-22 07:06:08 - progress_bar.py[line:274] - INFO: epoch 019:     43 / 412 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.92, wpb=109.7, bsz=40, num_updates=7450, lr=3.32474e-05, gnorm=0.355, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39009
2023-02-22 07:06:19 - progress_bar.py[line:274] - INFO: epoch 019:     53 / 412 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.2, ups=0.88, wpb=109.2, bsz=40, num_updates=7460, lr=3.32221e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39021
2023-02-22 07:06:30 - progress_bar.py[line:274] - INFO: epoch 019:     63 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=7470, lr=3.31968e-05, gnorm=0.498, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39032
2023-02-22 07:06:41 - progress_bar.py[line:274] - INFO: epoch 019:     73 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.91, wpb=110.4, bsz=40, num_updates=7480, lr=3.31715e-05, gnorm=0.375, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39043
2023-02-22 07:06:52 - progress_bar.py[line:274] - INFO: epoch 019:     83 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.8, ups=0.93, wpb=112, bsz=40, num_updates=7490, lr=3.31462e-05, gnorm=0.521, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39054
2023-02-22 07:07:03 - progress_bar.py[line:274] - INFO: epoch 019:     93 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=7500, lr=3.3121e-05, gnorm=0.326, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=39065
2023-02-22 07:07:15 - progress_bar.py[line:274] - INFO: epoch 019:    103 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=7510, lr=3.30957e-05, gnorm=0.563, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39076
2023-02-22 07:07:26 - progress_bar.py[line:274] - INFO: epoch 019:    113 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.91, wpb=110.8, bsz=40, num_updates=7520, lr=3.30704e-05, gnorm=0.447, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39087
2023-02-22 07:07:37 - progress_bar.py[line:274] - INFO: epoch 019:    123 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=7530, lr=3.30451e-05, gnorm=0.28, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39098
2023-02-22 07:07:48 - progress_bar.py[line:274] - INFO: epoch 019:    133 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=7540, lr=3.30198e-05, gnorm=0.441, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=39110
2023-02-22 07:07:59 - progress_bar.py[line:274] - INFO: epoch 019:    143 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=111.3, bsz=40, num_updates=7550, lr=3.29945e-05, gnorm=0.707, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39121
2023-02-22 07:08:10 - progress_bar.py[line:274] - INFO: epoch 019:    153 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.4, bsz=40, num_updates=7560, lr=3.29693e-05, gnorm=0.41, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39132
2023-02-22 07:08:22 - progress_bar.py[line:274] - INFO: epoch 019:    163 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.9, wpb=110.5, bsz=40, num_updates=7570, lr=3.2944e-05, gnorm=0.317, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39143
2023-02-22 07:08:33 - progress_bar.py[line:274] - INFO: epoch 019:    173 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=7580, lr=3.29187e-05, gnorm=0.188, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39154
2023-02-22 07:08:44 - progress_bar.py[line:274] - INFO: epoch 019:    183 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=7590, lr=3.28934e-05, gnorm=0.331, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39166
2023-02-22 07:08:55 - progress_bar.py[line:274] - INFO: epoch 019:    193 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=111.4, bsz=40, num_updates=7600, lr=3.28681e-05, gnorm=0.378, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=39177
2023-02-22 07:09:06 - progress_bar.py[line:274] - INFO: epoch 019:    203 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=7610, lr=3.28428e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39188
2023-02-22 07:09:18 - progress_bar.py[line:274] - INFO: epoch 019:    213 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.88, wpb=112.4, bsz=40, num_updates=7620, lr=3.28176e-05, gnorm=0.383, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39199
2023-02-22 07:09:29 - progress_bar.py[line:274] - INFO: epoch 019:    223 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.92, wpb=112, bsz=40, num_updates=7630, lr=3.27923e-05, gnorm=0.759, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39210
2023-02-22 07:09:40 - progress_bar.py[line:274] - INFO: epoch 019:    233 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.92, wpb=110.7, bsz=40, num_updates=7640, lr=3.2767e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39221
2023-02-22 07:09:51 - progress_bar.py[line:274] - INFO: epoch 019:    243 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97, ups=0.88, wpb=110.3, bsz=40, num_updates=7650, lr=3.27417e-05, gnorm=0.831, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39233
2023-02-22 07:10:02 - progress_bar.py[line:274] - INFO: epoch 019:    253 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=7660, lr=3.27164e-05, gnorm=0.635, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39244
2023-02-22 07:10:14 - progress_bar.py[line:274] - INFO: epoch 019:    263 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96, ups=0.87, wpb=109.9, bsz=40, num_updates=7670, lr=3.26911e-05, gnorm=0.449, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39255
2023-02-22 07:10:25 - progress_bar.py[line:274] - INFO: epoch 019:    273 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.89, wpb=110.5, bsz=40, num_updates=7680, lr=3.26659e-05, gnorm=0.359, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39266
2023-02-22 07:10:36 - progress_bar.py[line:274] - INFO: epoch 019:    283 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=7690, lr=3.26406e-05, gnorm=0.52, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39278
2023-02-22 07:10:47 - progress_bar.py[line:274] - INFO: epoch 019:    293 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112, bsz=40, num_updates=7700, lr=3.26153e-05, gnorm=0.367, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39289
2023-02-22 07:10:58 - progress_bar.py[line:274] - INFO: epoch 019:    303 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.89, wpb=109.8, bsz=40, num_updates=7710, lr=3.259e-05, gnorm=0.332, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39300
2023-02-22 07:11:10 - progress_bar.py[line:274] - INFO: epoch 019:    313 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=7720, lr=3.25647e-05, gnorm=0.346, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39311
2023-02-22 07:11:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 07:11:22 - progress_bar.py[line:274] - INFO: epoch 019:    324 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=91.4, ups=0.83, wpb=110.4, bsz=40, num_updates=7730, lr=3.25394e-05, gnorm=0.392, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39323
2023-02-22 07:11:33 - progress_bar.py[line:274] - INFO: epoch 019:    334 / 412 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.92, wpb=111.7, bsz=40, num_updates=7740, lr=3.25142e-05, gnorm=0.981, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39334
2023-02-22 07:11:44 - progress_bar.py[line:274] - INFO: epoch 019:    344 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=7750, lr=3.24889e-05, gnorm=0.373, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39345
2023-02-22 07:11:55 - progress_bar.py[line:274] - INFO: epoch 019:    354 / 412 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.2, bsz=40, num_updates=7760, lr=3.24636e-05, gnorm=0.279, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=39357
2023-02-22 07:12:06 - progress_bar.py[line:274] - INFO: epoch 019:    364 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.93, wpb=111.4, bsz=40, num_updates=7770, lr=3.24383e-05, gnorm=0.323, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39368
2023-02-22 07:12:17 - progress_bar.py[line:274] - INFO: epoch 019:    374 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.88, wpb=111.3, bsz=40, num_updates=7780, lr=3.2413e-05, gnorm=0.619, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39379
2023-02-22 07:12:29 - progress_bar.py[line:274] - INFO: epoch 019:    384 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.87, wpb=112.2, bsz=40, num_updates=7790, lr=3.23877e-05, gnorm=0.351, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39390
2023-02-22 07:12:40 - progress_bar.py[line:274] - INFO: epoch 019:    394 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=7800, lr=3.23625e-05, gnorm=0.388, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39402
2023-02-22 07:12:51 - progress_bar.py[line:274] - INFO: epoch 019:    404 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.92, wpb=112, bsz=40, num_updates=7810, lr=3.23372e-05, gnorm=0.297, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39413
2023-02-22 07:12:59 - train.py[line:339] - INFO: end of epoch 19 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 07:12:59 - progress_bar.py[line:282] - INFO: epoch 019 | loss 0.238 | loss_v1 0 | loss_v2 0 | nll_loss 0.053 | ntokens 111.124 | nsentences 40 | sample_size 111.124 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 98.9 | ups 0.89 | wpb 111.1 | bsz 40 | num_updates 7818 | lr 3.23169e-05 | gnorm 0.44 | clip 8.8 | loss_scale 512 | train_wall 457 | gb_free 10.6 | ema_decay 0.9999 | wall 39421
2023-02-22 07:12:59 - trainer.py[line:694] - INFO: loading train data for epoch 20
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E19.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 07:12:59 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 07:13:00 - trainer.py[line:758] - INFO: begin training epoch 20
2023-02-22 07:13:00 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 07:13:04 - progress_bar.py[line:274] - INFO: epoch 020:      2 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=83.3, ups=0.76, wpb=109.9, bsz=40, num_updates=7820, lr=3.23119e-05, gnorm=0.364, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39426
2023-02-22 07:13:15 - progress_bar.py[line:274] - INFO: epoch 020:     12 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=110.8, bsz=40, num_updates=7830, lr=3.22866e-05, gnorm=0.558, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39437
2023-02-22 07:13:27 - progress_bar.py[line:274] - INFO: epoch 020:     22 / 412 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.7, ups=0.88, wpb=112, bsz=40, num_updates=7840, lr=3.22613e-05, gnorm=0.147, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39448
2023-02-22 07:13:38 - progress_bar.py[line:274] - INFO: epoch 020:     32 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.3, bsz=40, num_updates=7850, lr=3.2236e-05, gnorm=0.353, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39459
2023-02-22 07:13:49 - progress_bar.py[line:274] - INFO: epoch 020:     42 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=111.1, bsz=40, num_updates=7860, lr=3.22108e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39470
2023-02-22 07:14:00 - progress_bar.py[line:274] - INFO: epoch 020:     52 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.7, ups=0.88, wpb=110, bsz=40, num_updates=7870, lr=3.21855e-05, gnorm=0.263, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39482
2023-02-22 07:14:11 - progress_bar.py[line:274] - INFO: epoch 020:     62 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.89, wpb=110.3, bsz=40, num_updates=7880, lr=3.21602e-05, gnorm=0.28, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39493
2023-02-22 07:14:22 - progress_bar.py[line:274] - INFO: epoch 020:     72 / 412 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.5, bsz=40, num_updates=7890, lr=3.21349e-05, gnorm=0.231, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39504
2023-02-22 07:14:34 - progress_bar.py[line:274] - INFO: epoch 020:     82 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=7900, lr=3.21096e-05, gnorm=0.374, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39515
2023-02-22 07:14:45 - progress_bar.py[line:274] - INFO: epoch 020:     92 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.3, bsz=40, num_updates=7910, lr=3.20843e-05, gnorm=0.971, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39526
2023-02-22 07:14:56 - progress_bar.py[line:274] - INFO: epoch 020:    102 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=7920, lr=3.20591e-05, gnorm=0.426, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39537
2023-02-22 07:15:07 - progress_bar.py[line:274] - INFO: epoch 020:    112 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.2, bsz=40, num_updates=7930, lr=3.20338e-05, gnorm=0.546, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39548
2023-02-22 07:15:18 - progress_bar.py[line:274] - INFO: epoch 020:    122 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=7940, lr=3.20085e-05, gnorm=0.616, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39560
2023-02-22 07:15:29 - progress_bar.py[line:274] - INFO: epoch 020:    132 / 412 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=7950, lr=3.19832e-05, gnorm=0.615, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39571
2023-02-22 07:15:40 - progress_bar.py[line:274] - INFO: epoch 020:    142 / 412 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.91, wpb=111.5, bsz=40, num_updates=7960, lr=3.19579e-05, gnorm=0.322, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39582
2023-02-22 07:15:51 - progress_bar.py[line:274] - INFO: epoch 020:    152 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=7970, lr=3.19326e-05, gnorm=0.328, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39593
2023-02-22 07:16:03 - progress_bar.py[line:274] - INFO: epoch 020:    162 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.7, ups=0.86, wpb=111, bsz=40, num_updates=7980, lr=3.19074e-05, gnorm=0.42, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39605
2023-02-22 07:16:14 - progress_bar.py[line:274] - INFO: epoch 020:    172 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.89, wpb=110.2, bsz=40, num_updates=7990, lr=3.18821e-05, gnorm=0.222, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39616
2023-02-22 07:16:25 - progress_bar.py[line:274] - INFO: epoch 020:    182 / 412 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=8000, lr=3.18568e-05, gnorm=0.245, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39627
2023-02-22 07:16:25 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 07:16:27 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 07:16:27 - train.py[line:551] - INFO: load:1.17 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 07:16:42 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.97 GiB (GPU 0; 39.59 GiB total capacity; 8.00 GiB already allocated; 4.81 GiB free; 32.29 GiB reserved in total by PyTorch)
2023-02-22 07:16:42 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8190 MB |   12373 MB |    5669 TB |    5669 TB |
|       from large pool |    8045 MB |   12229 MB |    5667 TB |    5667 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8190 MB |   12373 MB |    5669 TB |    5669 TB |
|       from large pool |    8045 MB |   12229 MB |    5667 TB |    5667 TB |
|       from small pool |     144 MB |     145 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   33070 MB |   34874 MB |  146818 MB |  113748 MB |
|       from large pool |   32924 MB |   34728 MB |  146542 MB |  113618 MB |
|       from small pool |     146 MB |     146 MB |     276 MB |     130 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24879 MB |   24879 MB |    5459 TB |    5459 TB |
|       from large pool |   24878 MB |   24878 MB |    5458 TB |    5458 TB |
|       from small pool |       1 MB |       1 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |  286669 K  |  286666 K  |
|       from large pool |     563    |     575    |  100275 K  |  100274 K  |
|       from small pool |    3106    |    3116    |  186394 K  |  186391 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |  286669 K  |  286666 K  |
|       from large pool |     563    |     575    |  100275 K  |  100274 K  |
|       from small pool |    3106    |    3116    |  186394 K  |  186391 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     182    |     485    |     305    |
|       from large pool |     107    |     109    |     347    |     240    |
|       from small pool |      73    |      73    |     138    |      65    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     132    |  193890 K  |  193890 K  |
|       from large pool |      92    |      95    |   32739 K  |   32739 K  |
|       from small pool |      26    |      40    |  161150 K  |  161150 K  |
|===========================================================================|

2023-02-22 07:16:42 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-22 07:16:42 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-02-22 07:18:30 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 07:18:30 - train.py[line:551] - INFO: load:1.19 valid_run:123.48 task_valid:119.51 collect_output:1.85
2023-02-22 07:20:30 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 07:20:30 - train.py[line:551] - INFO: load:1.21 valid_run:243.62 task_valid:235.58 collect_output:4.90
2023-02-22 07:22:33 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 07:22:33 - train.py[line:551] - INFO: load:1.24 valid_run:365.92 task_valid:352.55 collect_output:9.22
2023-02-22 07:24:35 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 07:24:35 - train.py[line:551] - INFO: load:1.26 valid_run:488.15 task_valid:466.72 collect_output:16.27
2023-02-22 07:26:36 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 07:26:36 - train.py[line:551] - INFO: load:1.28 valid_run:609.02 task_valid:584.43 collect_output:18.42
2023-02-22 07:28:39 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 07:28:39 - train.py[line:551] - INFO: load:1.31 valid_run:732.35 task_valid:703.63 collect_output:21.53
2023-02-22 07:30:43 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 07:30:43 - train.py[line:551] - INFO: load:1.34 valid_run:855.75 task_valid:822.23 collect_output:25.30
2023-02-22 07:32:45 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 07:32:45 - train.py[line:551] - INFO: load:1.36 valid_run:977.90 task_valid:939.15 collect_output:29.53
2023-02-22 07:34:49 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 07:34:49 - train.py[line:551] - INFO: load:1.38 valid_run:1101.99 task_valid:1056.66 collect_output:35.11
2023-02-22 07:36:51 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 07:36:51 - train.py[line:551] - INFO: load:1.41 valid_run:1223.98 task_valid:1169.78 collect_output:42.97
2023-02-22 07:38:52 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 07:38:52 - train.py[line:551] - INFO: load:1.43 valid_run:1344.55 task_valid:1285.79 collect_output:46.54
2023-02-22 07:40:54 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 07:40:54 - train.py[line:551] - INFO: load:1.46 valid_run:1466.46 task_valid:1403.22 collect_output:50.00
2023-02-22 07:42:53 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 07:42:53 - train.py[line:551] - INFO: load:1.48 valid_run:1585.67 task_valid:1517.34 collect_output:54.09
2023-02-22 07:44:54 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 07:44:54 - train.py[line:551] - INFO: load:1.50 valid_run:1707.04 task_valid:1635.53 collect_output:56.26
2023-02-22 07:46:56 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 07:46:56 - train.py[line:551] - INFO: load:1.53 valid_run:1828.33 task_valid:1751.99 collect_output:60.09
2023-02-22 07:48:57 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 07:48:57 - train.py[line:551] - INFO: load:1.55 valid_run:1949.77 task_valid:1866.35 collect_output:66.15
2023-02-22 07:50:59 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 07:50:59 - train.py[line:551] - INFO: load:1.58 valid_run:2071.42 task_valid:1982.91 collect_output:70.24
2023-02-22 07:53:00 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 07:53:00 - train.py[line:551] - INFO: load:1.60 valid_run:2192.39 task_valid:2101.14 collect_output:71.97
2023-02-22 07:55:02 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 07:55:02 - train.py[line:551] - INFO: load:1.63 valid_run:2314.03 task_valid:2218.49 collect_output:75.24
2023-02-22 07:57:03 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 07:57:03 - train.py[line:551] - INFO: load:1.65 valid_run:2434.84 task_valid:2335.67 collect_output:77.86
2023-02-22 07:59:05 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 07:59:05 - train.py[line:551] - INFO: load:1.68 valid_run:2556.87 task_valid:2452.56 collect_output:81.99
2023-02-22 08:01:07 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 08:01:07 - train.py[line:551] - INFO: load:1.70 valid_run:2679.41 task_valid:2572.09 collect_output:83.95
2023-02-22 08:03:08 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 08:03:08 - train.py[line:551] - INFO: load:1.73 valid_run:2800.20 task_valid:2686.94 collect_output:88.85
2023-02-22 08:05:09 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 08:05:09 - train.py[line:551] - INFO: load:1.75 valid_run:2920.53 task_valid:2803.64 collect_output:91.46
2023-02-22 08:07:11 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 08:07:11 - train.py[line:551] - INFO: load:1.78 valid_run:3042.60 task_valid:2920.39 collect_output:95.75
2023-02-22 08:09:14 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 08:09:14 - train.py[line:551] - INFO: load:1.80 valid_run:3165.83 task_valid:3036.84 collect_output:101.52
2023-02-22 08:11:14 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 08:11:14 - train.py[line:551] - INFO: load:1.83 valid_run:3285.70 task_valid:3151.39 collect_output:105.82
2023-02-22 08:13:16 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 08:13:16 - train.py[line:551] - INFO: load:1.85 valid_run:3408.02 task_valid:3271.36 collect_output:107.17
2023-02-22 08:15:18 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 08:15:18 - train.py[line:551] - INFO: load:1.88 valid_run:3530.12 task_valid:3387.37 collect_output:112.24
2023-02-22 08:17:21 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 08:17:21 - train.py[line:551] - INFO: load:1.90 valid_run:3652.32 task_valid:3506.10 collect_output:114.72
2023-02-22 08:19:22 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 08:19:22 - train.py[line:551] - INFO: load:1.93 valid_run:3773.84 task_valid:3624.91 collect_output:116.41

====================================================================================================
SGG eval:     R @ 50: 0.6390;     R @ 100: 0.6784;     R @ 500: 0.7060;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4326;    mR @ 100: 0.4953;    mR @ 500: 0.5445;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6042) (covering:0.2286) (eating:0.7647) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4129) (lying on:0.4000) (mounted on:0.0000) (painted on:0.5000) (parked on:1.0000) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.7069) (standing on:0.4763) (using:0.6000) (walking in:0.0000) (walking on:0.5135) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2023-02-22 08:19:53 - train.py[line:487] - INFO: 0.6784134453781512

====================================================================================================
SGG eval:     R @ 50: 0.6390;     R @ 100: 0.6784;     R @ 500: 0.7060;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4326;    mR @ 100: 0.4953;    mR @ 500: 0.5445;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6042) (covering:0.2286) (eating:0.7647) (flying in:1.0000) (growing on:0.3750) (hanging from:0.4129) (lying on:0.4000) (mounted on:0.0000) (painted on:0.5000) (parked on:1.0000) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.7069) (standing on:0.4763) (using:0.6000) (walking in:0.0000) (walking on:0.5135) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2023-02-22 08:19:53 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 08:19:53 - progress_bar.py[line:282] - INFO: epoch 020 | valid on 'valid' subset | loss 0.231 | loss_v1 0 | loss_v2 0 | nll_loss 0.06 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.678413 | ppl 1.04 | vqa_score 0.4628 | wps 117.8 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.68379
2023-02-22 08:19:53 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 8000 updates
2023-02-22 08:19:53 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_20_8000.pt
2023-02-22 08:19:59 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_20_8000.pt
2023-02-22 08:20:02 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_20_8000.pt (epoch 20 @ 8000 updates, score 0.6784134453781512) (writing took 8.332877952605486 seconds)
2023-02-22 08:20:14 - progress_bar.py[line:274] - INFO: epoch 020:    192 / 412 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=111.6, bsz=40, num_updates=8010, lr=3.18315e-05, gnorm=0.504, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=43456
2023-02-22 08:20:25 - progress_bar.py[line:274] - INFO: epoch 020:    202 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.89, wpb=109.7, bsz=40, num_updates=8020, lr=3.18062e-05, gnorm=0.495, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43467
2023-02-22 08:20:37 - progress_bar.py[line:274] - INFO: epoch 020:    212 / 412 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.1, ups=0.88, wpb=111.5, bsz=40, num_updates=8030, lr=3.17809e-05, gnorm=0.25, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43478
2023-02-22 08:20:48 - progress_bar.py[line:274] - INFO: epoch 020:    222 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112, bsz=40, num_updates=8040, lr=3.17557e-05, gnorm=0.436, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43490
2023-02-22 08:21:00 - progress_bar.py[line:274] - INFO: epoch 020:    232 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96, ups=0.87, wpb=110.4, bsz=40, num_updates=8050, lr=3.17304e-05, gnorm=0.943, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43501
2023-02-22 08:21:11 - progress_bar.py[line:274] - INFO: epoch 020:    242 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=8060, lr=3.17051e-05, gnorm=0.831, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43513
2023-02-22 08:21:22 - progress_bar.py[line:274] - INFO: epoch 020:    252 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=8070, lr=3.16798e-05, gnorm=0.602, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43523
2023-02-22 08:21:33 - progress_bar.py[line:274] - INFO: epoch 020:    262 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.1, ups=0.94, wpb=110.7, bsz=40, num_updates=8080, lr=3.16545e-05, gnorm=0.233, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=43534
2023-02-22 08:21:44 - progress_bar.py[line:274] - INFO: epoch 020:    272 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=8090, lr=3.16292e-05, gnorm=0.313, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43545
2023-02-22 08:21:55 - progress_bar.py[line:274] - INFO: epoch 020:    282 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.9, ups=0.91, wpb=111.5, bsz=40, num_updates=8100, lr=3.1604e-05, gnorm=0.396, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43556
2023-02-22 08:22:06 - progress_bar.py[line:274] - INFO: epoch 020:    292 / 412 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=8110, lr=3.15787e-05, gnorm=0.472, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43568
2023-02-22 08:22:17 - progress_bar.py[line:274] - INFO: epoch 020:    302 / 412 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=111.6, bsz=40, num_updates=8120, lr=3.15534e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43579
2023-02-22 08:22:29 - progress_bar.py[line:274] - INFO: epoch 020:    312 / 412 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=111.7, bsz=40, num_updates=8130, lr=3.15281e-05, gnorm=0.349, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43590
2023-02-22 08:22:39 - progress_bar.py[line:274] - INFO: epoch 020:    322 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113, bsz=40, num_updates=8140, lr=3.15028e-05, gnorm=0.472, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43601
2023-02-22 08:22:51 - progress_bar.py[line:274] - INFO: epoch 020:    332 / 412 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.9, wpb=110.2, bsz=40, num_updates=8150, lr=3.14775e-05, gnorm=0.534, clip=20, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=43612
2023-02-22 08:23:02 - progress_bar.py[line:274] - INFO: epoch 020:    342 / 412 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=8160, lr=3.14523e-05, gnorm=0.231, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43623
2023-02-22 08:23:13 - progress_bar.py[line:274] - INFO: epoch 020:    352 / 412 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.2, ups=0.87, wpb=110.2, bsz=40, num_updates=8170, lr=3.1427e-05, gnorm=0.262, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43635
2023-02-22 08:23:25 - progress_bar.py[line:274] - INFO: epoch 020:    362 / 412 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.4, bsz=40, num_updates=8180, lr=3.14017e-05, gnorm=0.462, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43646
2023-02-22 08:23:36 - progress_bar.py[line:274] - INFO: epoch 020:    372 / 412 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.6, ups=0.87, wpb=110.1, bsz=40, num_updates=8190, lr=3.13764e-05, gnorm=0.341, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=43658
2023-02-22 08:23:48 - progress_bar.py[line:274] - INFO: epoch 020:    382 / 412 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=8200, lr=3.13511e-05, gnorm=0.466, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43669
2023-02-22 08:23:59 - progress_bar.py[line:274] - INFO: epoch 020:    392 / 412 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.88, wpb=112.1, bsz=40, num_updates=8210, lr=3.13258e-05, gnorm=0.308, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=43681
2023-02-22 08:24:10 - progress_bar.py[line:274] - INFO: epoch 020:    402 / 412 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.1, ups=0.87, wpb=111.2, bsz=40, num_updates=8220, lr=3.13006e-05, gnorm=0.407, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43692
2023-02-22 08:24:22 - progress_bar.py[line:274] - INFO: epoch 020:    412 / 412 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.8, bsz=40, num_updates=8230, lr=3.12753e-05, gnorm=0.432, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43703
2023-02-22 08:24:22 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 08:24:24 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 08:24:24 - train.py[line:551] - INFO: load:1.39 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 08:26:27 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 08:26:27 - train.py[line:551] - INFO: load:1.41 valid_run:123.87 task_valid:121.04 collect_output:1.69
2023-02-22 08:28:28 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 08:28:28 - train.py[line:551] - INFO: load:1.44 valid_run:244.06 task_valid:237.28 collect_output:4.63
2023-02-22 08:30:30 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 08:30:30 - train.py[line:551] - INFO: load:1.46 valid_run:366.40 task_valid:354.18 collect_output:9.05
2023-02-22 08:32:32 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 08:32:32 - train.py[line:551] - INFO: load:1.48 valid_run:488.70 task_valid:468.25 collect_output:16.24
2023-02-22 08:34:33 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 08:34:33 - train.py[line:551] - INFO: load:1.51 valid_run:609.54 task_valid:585.93 collect_output:18.39
2023-02-22 08:36:37 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 08:36:37 - train.py[line:551] - INFO: load:1.53 valid_run:732.75 task_valid:705.03 collect_output:21.50
2023-02-22 08:38:40 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 08:38:40 - train.py[line:551] - INFO: load:1.55 valid_run:856.11 task_valid:823.51 collect_output:25.36
2023-02-22 08:40:42 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 08:40:42 - train.py[line:551] - INFO: load:1.58 valid_run:978.21 task_valid:940.33 collect_output:29.63
2023-02-22 08:42:46 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 08:42:46 - train.py[line:551] - INFO: load:1.60 valid_run:1102.18 task_valid:1058.03 collect_output:34.89
2023-02-22 08:44:48 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 08:44:48 - train.py[line:551] - INFO: load:1.63 valid_run:1223.90 task_valid:1171.03 collect_output:42.61
2023-02-22 08:46:49 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 08:46:49 - train.py[line:551] - INFO: load:1.65 valid_run:1344.41 task_valid:1287.11 collect_output:46.02
2023-02-22 08:48:51 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 08:48:51 - train.py[line:551] - INFO: load:1.67 valid_run:1466.33 task_valid:1404.56 collect_output:49.47
2023-02-22 08:50:50 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 08:50:50 - train.py[line:551] - INFO: load:1.70 valid_run:1585.53 task_valid:1518.80 collect_output:53.41
2023-02-22 08:52:51 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 08:52:51 - train.py[line:551] - INFO: load:1.72 valid_run:1706.86 task_valid:1637.07 collect_output:55.44
2023-02-22 08:54:52 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 08:54:52 - train.py[line:551] - INFO: load:1.75 valid_run:1828.09 task_valid:1753.68 collect_output:59.03
2023-02-22 08:56:54 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 08:56:54 - train.py[line:551] - INFO: load:1.77 valid_run:1949.72 task_valid:1868.29 collect_output:65.01
2023-02-22 08:58:56 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 08:58:56 - train.py[line:551] - INFO: load:1.80 valid_run:2071.46 task_valid:1984.82 collect_output:69.20
2023-02-22 09:00:57 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 09:00:57 - train.py[line:551] - INFO: load:1.82 valid_run:2192.48 task_valid:2103.10 collect_output:70.92
2023-02-22 09:02:59 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 09:02:59 - train.py[line:551] - INFO: load:1.85 valid_run:2314.00 task_valid:2220.35 collect_output:74.19
2023-02-22 09:04:59 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 09:04:59 - train.py[line:551] - INFO: load:1.87 valid_run:2434.78 task_valid:2337.31 collect_output:76.99
2023-02-22 09:07:02 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 09:07:02 - train.py[line:551] - INFO: load:1.90 valid_run:2556.90 task_valid:2454.40 collect_output:81.00
2023-02-22 09:09:04 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 09:09:04 - train.py[line:551] - INFO: load:1.92 valid_run:2679.31 task_valid:2573.72 collect_output:83.06
2023-02-22 09:11:05 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 09:11:05 - train.py[line:551] - INFO: load:1.94 valid_run:2799.98 task_valid:2688.55 collect_output:87.88
2023-02-22 09:13:05 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 09:13:05 - train.py[line:551] - INFO: load:1.97 valid_run:2920.24 task_valid:2805.28 collect_output:90.36
2023-02-22 09:15:07 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 09:15:07 - train.py[line:551] - INFO: load:1.99 valid_run:3042.18 task_valid:2921.99 collect_output:94.57
2023-02-22 09:17:10 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 09:17:10 - train.py[line:551] - INFO: load:2.02 valid_run:3165.46 task_valid:3038.51 collect_output:100.31
2023-02-22 09:19:11 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 09:19:11 - train.py[line:551] - INFO: load:2.04 valid_run:3285.51 task_valid:3153.21 collect_output:104.64
2023-02-22 09:21:13 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 09:21:13 - train.py[line:551] - INFO: load:2.07 valid_run:3407.72 task_valid:3273.04 collect_output:105.99
2023-02-22 09:23:15 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 09:23:15 - train.py[line:551] - INFO: load:2.10 valid_run:3529.69 task_valid:3389.03 collect_output:110.97
2023-02-22 09:25:17 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 09:25:17 - train.py[line:551] - INFO: load:2.12 valid_run:3651.94 task_valid:3507.80 collect_output:113.44
2023-02-22 09:27:19 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 09:27:19 - train.py[line:551] - INFO: load:2.15 valid_run:3773.42 task_valid:3626.65 collect_output:115.05

====================================================================================================
SGG eval:     R @ 50: 0.6450;     R @ 100: 0.6798;     R @ 500: 0.7065;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4413;    mR @ 100: 0.4965;    mR @ 500: 0.5528;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6042) (covering:0.2286) (eating:0.7647) (flying in:0.9091) (growing on:0.5000) (hanging from:0.3935) (lying on:0.4000) (mounted on:0.0000) (painted on:0.5000) (parked on:1.0000) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.7069) (standing on:0.4863) (using:0.6000) (walking in:0.0000) (walking on:0.5135) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2023-02-22 09:27:50 - train.py[line:487] - INFO: 0.6798498090145149

====================================================================================================
SGG eval:     R @ 50: 0.6450;     R @ 100: 0.6798;     R @ 500: 0.7065;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4413;    mR @ 100: 0.4965;    mR @ 500: 0.5528;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7439) (covered in:0.6042) (covering:0.2286) (eating:0.7647) (flying in:0.9091) (growing on:0.5000) (hanging from:0.3935) (lying on:0.4000) (mounted on:0.0000) (painted on:0.5000) (parked on:1.0000) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.7069) (standing on:0.4863) (using:0.6000) (walking in:0.0000) (walking on:0.5135) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2023-02-22 09:27:50 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 09:27:50 - progress_bar.py[line:282] - INFO: epoch 020 | valid on 'valid' subset | loss 0.23 | loss_v1 0 | loss_v2 0 | nll_loss 0.058 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.67985 | ppl 1.04 | vqa_score 0.4583 | wps 117.9 | wpb 72 | bsz 24 | num_updates 8230 | best_R@100 0.68379
2023-02-22 09:27:50 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 20 @ 8230 updates
2023-02-22 09:27:50 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint20.pt
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv slice_id 1 row count 8240 total row count 16480
2023-02-22 09:27:56 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint20.pt
2023-02-22 09:27:58 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint20.pt (epoch 20 @ 8230 updates, score 0.6798498090145149) (writing took 8.532006982713938 seconds)
2023-02-22 09:27:58 - train.py[line:339] - INFO: end of epoch 20 (average epoch stats below)
2023-02-22 09:27:58 - progress_bar.py[line:282] - INFO: epoch 020 | loss 0.239 | loss_v1 0 | loss_v2 0 | nll_loss 0.054 | ntokens 111.126 | nsentences 40 | sample_size 111.126 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 5.7 | ups 0.05 | wpb 111.1 | bsz 40 | num_updates 8230 | lr 3.12753e-05 | gnorm 0.434 | clip 9.7 | loss_scale 512 | train_wall 461 | gb_free 10.7 | ema_decay 0.9999 | wall 47520
2023-02-22 09:27:58 - trainer.py[line:694] - INFO: loading train data for epoch 21
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_opt-new_train_NA1_E20.tsv slice_id 0 row count 8240 total row count 16480
2023-02-22 09:27:58 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 09:27:59 - trainer.py[line:758] - INFO: begin training epoch 21
2023-02-22 09:27:59 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 09:28:12 - progress_bar.py[line:274] - INFO: epoch 021:     10 / 412 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=111.3, bsz=40, num_updates=8240, lr=3.125e-05, gnorm=0.726, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=47533
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3922631
Killing subprocess 3922632
Main process received SIGINT, exiting
