2023-01-11 00:04:29 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-11 00:04:29 - utils.py[line:261] - INFO: Start init
2023-01-11 00:04:29 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-11 00:04:29 - utils.py[line:261] - INFO: Start init
2023-01-11 00:04:29 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-11 00:04:29 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-11 00:04:29 - utils.py[line:274] - INFO: initialized host node4 as rank 1
2023-01-11 00:04:29 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
single-machine distributed training is initialized.
2023-01-11 00:04:34 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_combine55_momentum0.95_alpha1.0', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_combine55_momentum0.95_alpha1.0', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-01-11 00:04:35 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-01-11 00:04:35 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-11 00:04:39 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-01-11 00:04:39 - train.py[line:118] - INFO: task: VqaGenTask
2023-01-11 00:04:39 - train.py[line:119] - INFO: model: OFAModel
2023-01-11 00:04:39 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-01-11 00:04:39 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-01-11 00:04:39 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-11 00:04:39 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-01-11 00:04:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-01-11 00:04:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-01-11 00:04:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-01-11 00:04:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-01-11 00:04:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-01-11 00:04:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-01-11 00:04:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-01-11 00:04:40 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-01-11 00:04:41 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-11 00:04:41 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-11 00:04:41 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-11 00:04:41 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-01-11 00:04:42 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-01-11 00:04:42 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-01-11 00:04:42 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-01-11 00:04:45 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-01-11 00:04:45 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-11 00:04:46 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-11 00:04:46 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-01-11 00:04:46 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-01-11 00:04:47 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-01-11 00:04:47 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv slice_id 1 row count 2000000 total row count 4000000
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv slice_id 0 row count 2000000 total row count 4000000
2023-01-11 00:04:50 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 100000, warmup steps 4000, warmup_factor 0.00025
Total steps 100000, warmup steps 4000, warmup_factor 0.00025
2023-01-11 00:04:51 - trainer.py[line:758] - INFO: begin training epoch 1
2023-01-11 00:04:51 - train.py[line:312] - INFO: Start iterating over samples
From cpu to cuda:1
From cpu to cuda:0
2023-01-11 00:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:24 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 100000 loss=0.956, loss_v1=0, loss_v2=0, nll_loss=0.808, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.75, vqa_score=0.033, wps=63.1, ups=0.36, wpb=111.3, bsz=40, num_updates=10, lr=1.25e-07, gnorm=10.345, clip=100, loss_scale=128, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=42
2023-01-11 00:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:46 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 100000 loss=1.002, loss_v1=0, loss_v2=0, nll_loss=0.854, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.81, vqa_score=0.0312, wps=76.3, ups=0.46, wpb=111.1, bsz=40, num_updates=20, lr=2.5e-07, gnorm=10.578, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=64
2023-01-11 00:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:05:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:08 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 100000 loss=1.074, loss_v1=0, loss_v2=0, nll_loss=0.931, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.91, vqa_score=0.0297, wps=75.7, ups=0.46, wpb=109.7, bsz=40, num_updates=30, lr=3.75e-07, gnorm=11.189, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=86
2023-01-11 00:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:30 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 100000 loss=1.03, loss_v1=0, loss_v2=0, nll_loss=0.884, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.85, vqa_score=0.0202, wps=76.2, ups=0.46, wpb=111.2, bsz=40, num_updates=40, lr=5e-07, gnorm=10.565, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=109
2023-01-11 00:06:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:06:52 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 100000 loss=1.066, loss_v1=0, loss_v2=0, nll_loss=0.925, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.9, vqa_score=0.0417, wps=76.2, ups=0.46, wpb=110.3, bsz=40, num_updates=50, lr=6.25e-07, gnorm=11.082, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=131
2023-01-11 00:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:14 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 100000 loss=0.932, loss_v1=0, loss_v2=0, nll_loss=0.785, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.72, vqa_score=0.051, wps=76.6, ups=0.46, wpb=110.1, bsz=40, num_updates=60, lr=7.5e-07, gnorm=8.455, clip=100, loss_scale=128, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=153
2023-01-11 00:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:36 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 100000 loss=0.903, loss_v1=0, loss_v2=0, nll_loss=0.763, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.7, vqa_score=0.0556, wps=76.3, ups=0.46, wpb=110.6, bsz=40, num_updates=70, lr=8.75e-07, gnorm=9.684, clip=100, loss_scale=128, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=175
2023-01-11 00:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:07:58 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 100000 loss=0.972, loss_v1=0, loss_v2=0, nll_loss=0.847, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.8, vqa_score=0, wps=74.7, ups=0.46, wpb=108, bsz=40, num_updates=80, lr=1e-06, gnorm=8.85, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=196
2023-01-11 00:08:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:20 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 100000 loss=0.867, loss_v1=0, loss_v2=0, nll_loss=0.744, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.67, vqa_score=0, wps=74.8, ups=0.46, wpb=109.2, bsz=40, num_updates=90, lr=1.125e-06, gnorm=6.623, clip=100, loss_scale=128, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=219
2023-01-11 00:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:42 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 100000 loss=0.815, loss_v1=0, loss_v2=0, nll_loss=0.693, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.62, vqa_score=0.0426, wps=76.6, ups=0.46, wpb=110.3, bsz=40, num_updates=100, lr=1.25e-06, gnorm=6.119, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=240
2023-01-11 00:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:04 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 100000 loss=0.853, loss_v1=0, loss_v2=0, nll_loss=0.746, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.68, vqa_score=0.0636, wps=75.5, ups=0.46, wpb=109.1, bsz=40, num_updates=110, lr=1.375e-06, gnorm=5.818, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=262
2023-01-11 00:09:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:26 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 100000 loss=0.763, loss_v1=0, loss_v2=0, nll_loss=0.651, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.57, vqa_score=0.0286, wps=73.8, ups=0.45, wpb=109.5, bsz=40, num_updates=120, lr=1.5e-06, gnorm=4.811, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=285
2023-01-11 00:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:49 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 100000 loss=0.708, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.51, vqa_score=0.0108, wps=73.9, ups=0.45, wpb=110.2, bsz=40, num_updates=130, lr=1.625e-06, gnorm=4.463, clip=100, loss_scale=128, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=307
2023-01-11 00:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:11 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 100000 loss=0.756, loss_v1=0, loss_v2=0, nll_loss=0.649, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.57, vqa_score=0.0566, wps=74.5, ups=0.45, wpb=110.1, bsz=40, num_updates=140, lr=1.75e-06, gnorm=4.317, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=330
2023-01-11 00:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:33 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 100000 loss=0.777, loss_v1=0, loss_v2=0, nll_loss=0.674, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.59, vqa_score=0, wps=74.3, ups=0.46, wpb=107.6, bsz=40, num_updates=150, lr=1.875e-06, gnorm=4.085, clip=100, loss_scale=128, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=352
2023-01-11 00:10:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:10:55 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0505, wps=75.3, ups=0.46, wpb=108.9, bsz=40, num_updates=160, lr=2e-06, gnorm=3.681, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=374
2023-01-11 00:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:17 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 100000 loss=0.701, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.51, vqa_score=0.0198, wps=74.1, ups=0.45, wpb=109.1, bsz=40, num_updates=170, lr=2.125e-06, gnorm=3.655, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=396
2023-01-11 00:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:40 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 100000 loss=0.697, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.51, vqa_score=0.0273, wps=75, ups=0.45, wpb=110.4, bsz=40, num_updates=180, lr=2.25e-06, gnorm=3.453, clip=100, loss_scale=128, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=418
2023-01-11 00:11:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:02 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 100000 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.514, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.43, vqa_score=0.0633, wps=76.5, ups=0.46, wpb=111.8, bsz=40, num_updates=190, lr=2.375e-06, gnorm=3.399, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=440
2023-01-11 00:12:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:24 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 100000 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.48, vqa_score=0, wps=75.5, ups=0.46, wpb=109.6, bsz=40, num_updates=200, lr=2.5e-06, gnorm=3.008, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=462
2023-01-11 00:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:47 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 100000 loss=0.717, loss_v1=0, loss_v2=0, nll_loss=0.622, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.54, vqa_score=0.0172, wps=70, ups=0.43, wpb=108, bsz=40, num_updates=210, lr=2.625e-06, gnorm=3.417, clip=100, loss_scale=128, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=486
2023-01-11 00:12:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:09 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.067, nsentences=40, sample_size=107.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0351, wps=73.9, ups=0.46, wpb=107.1, bsz=40, num_updates=220, lr=2.75e-06, gnorm=3.063, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=508
2023-01-11 00:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:31 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 100000 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.0103, wps=76.3, ups=0.46, wpb=109.9, bsz=40, num_updates=230, lr=2.875e-06, gnorm=3.049, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=529
2023-01-11 00:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:13:53 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0294, wps=74.9, ups=0.46, wpb=108.8, bsz=40, num_updates=240, lr=3e-06, gnorm=3.185, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=552
2023-01-11 00:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:15 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0297, wps=75.4, ups=0.46, wpb=109.7, bsz=40, num_updates=250, lr=3.125e-06, gnorm=2.538, clip=100, loss_scale=128, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=574
2023-01-11 00:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:37 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 100000 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.0222, wps=74.2, ups=0.45, wpb=109.9, bsz=40, num_updates=260, lr=3.25e-06, gnorm=2.759, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=596
2023-01-11 00:14:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:14:59 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 100000 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.42, vqa_score=0.0619, wps=74.5, ups=0.46, wpb=109, bsz=40, num_updates=270, lr=3.375e-06, gnorm=2.669, clip=100, loss_scale=128, train_wall=22, gb_free=9.5, ema_decay=0.9999, wall=618
2023-01-11 00:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:21 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 100000 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.02, wps=76, ups=0.46, wpb=109.9, bsz=40, num_updates=280, lr=3.5e-06, gnorm=2.423, clip=100, loss_scale=128, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=640
2023-01-11 00:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:43 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0385, wps=76.8, ups=0.47, wpb=110.1, bsz=40, num_updates=290, lr=3.625e-06, gnorm=2.702, clip=100, loss_scale=128, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=662
2023-01-11 00:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:05 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 100000 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.43, vqa_score=0.0306, wps=76.3, ups=0.46, wpb=109.7, bsz=40, num_updates=300, lr=3.75e-06, gnorm=2.689, clip=100, loss_scale=128, train_wall=22, gb_free=10, ema_decay=0.9999, wall=684
2023-01-11 00:16:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:27 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 100000 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.42, vqa_score=0.0198, wps=75.4, ups=0.46, wpb=110.3, bsz=40, num_updates=310, lr=3.875e-06, gnorm=2.36, clip=100, loss_scale=128, train_wall=22, gb_free=10.7, ema_decay=0.9999, wall=706
2023-01-11 00:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:49 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 100000 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.027, wps=76.1, ups=0.46, wpb=109.8, bsz=40, num_updates=320, lr=4e-06, gnorm=2.727, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=728
2023-01-11 00:16:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:11 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0297, wps=76, ups=0.46, wpb=109.1, bsz=40, num_updates=330, lr=4.125e-06, gnorm=2.525, clip=100, loss_scale=128, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=749
2023-01-11 00:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:33 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 100000 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, vqa_score=0.051, wps=75.6, ups=0.46, wpb=109.6, bsz=40, num_updates=340, lr=4.25e-06, gnorm=2.459, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=771
2023-01-11 00:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:17:54 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 100000 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.0093, wps=77.5, ups=0.47, wpb=109.9, bsz=40, num_updates=350, lr=4.375e-06, gnorm=2.468, clip=100, loss_scale=128, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=793
2023-01-11 00:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:16 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 100000 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0.0435, wps=77.9, ups=0.47, wpb=110.9, bsz=40, num_updates=360, lr=4.5e-06, gnorm=2.312, clip=100, loss_scale=128, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=814
2023-01-11 00:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:37 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 100000 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0, wps=78.8, ups=0.47, wpb=111.9, bsz=40, num_updates=370, lr=4.625e-06, gnorm=2.192, clip=100, loss_scale=128, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=836
2023-01-11 00:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:18:59 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 100000 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.41, vqa_score=0.0202, wps=76.1, ups=0.46, wpb=109.5, bsz=40, num_updates=380, lr=4.75e-06, gnorm=2.616, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=858
2023-01-11 00:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:21 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 100000 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.543, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.46, vqa_score=0.0085, wps=74.5, ups=0.46, wpb=108.2, bsz=40, num_updates=390, lr=4.875e-06, gnorm=2.439, clip=100, loss_scale=128, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=880
2023-01-11 00:19:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:43 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 100000 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, vqa_score=0.0374, wps=76.6, ups=0.46, wpb=109.8, bsz=40, num_updates=400, lr=5e-06, gnorm=2.221, clip=100, loss_scale=128, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=902
2023-01-11 00:19:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:19:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:05 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 100000 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.39, vqa_score=0, wps=76.5, ups=0.46, wpb=110.7, bsz=40, num_updates=410, lr=5.125e-06, gnorm=2.371, clip=100, loss_scale=128, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=924
2023-01-11 00:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:27 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 100000 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.39, vqa_score=0.0316, wps=76.7, ups=0.46, wpb=111, bsz=40, num_updates=420, lr=5.25e-06, gnorm=2.45, clip=100, loss_scale=128, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=945
2023-01-11 00:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:50 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 100000 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.38, vqa_score=0.0098, wps=73.5, ups=0.44, wpb=110.5, bsz=40, num_updates=430, lr=5.375e-06, gnorm=2.305, clip=100, loss_scale=128, train_wall=23, gb_free=9.4, ema_decay=0.9999, wall=968
2023-01-11 00:20:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:11 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 100000 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, vqa_score=0.01, wps=77.3, ups=0.47, wpb=110.2, bsz=40, num_updates=440, lr=5.5e-06, gnorm=2.177, clip=100, loss_scale=128, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=990
2023-01-11 00:21:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:34 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 100000 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0194, wps=74.4, ups=0.45, wpb=110.3, bsz=40, num_updates=450, lr=5.625e-06, gnorm=2.032, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1012
2023-01-11 00:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:21:56 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 100000 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0306, wps=74.7, ups=0.45, wpb=110.3, bsz=40, num_updates=460, lr=5.75e-06, gnorm=2.186, clip=100, loss_scale=128, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=1035
2023-01-11 00:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:18 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 100000 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, vqa_score=0, wps=76.2, ups=0.46, wpb=110.4, bsz=40, num_updates=470, lr=5.875e-06, gnorm=2.108, clip=100, loss_scale=128, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=1057
2023-01-11 00:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:41 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 100000 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0.0098, wps=73.5, ups=0.45, wpb=109.1, bsz=40, num_updates=480, lr=6e-06, gnorm=2.204, clip=100, loss_scale=128, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1079
2023-01-11 00:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:04 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 100000 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0532, wps=73.4, ups=0.44, wpb=110.5, bsz=40, num_updates=490, lr=6.125e-06, gnorm=2.376, clip=100, loss_scale=128, train_wall=23, gb_free=10.3, ema_decay=0.9999, wall=1102
2023-01-11 00:23:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:26 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 100000 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.049, wps=73.8, ups=0.45, wpb=109.5, bsz=40, num_updates=500, lr=6.25e-06, gnorm=1.944, clip=90, loss_scale=128, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=1125
2023-01-11 00:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:49 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0495, wps=73.9, ups=0.45, wpb=109.7, bsz=40, num_updates=510, lr=6.375e-06, gnorm=2.264, clip=100, loss_scale=128, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=1147
2023-01-11 00:23:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:11 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 100000 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0495, wps=73.8, ups=0.45, wpb=109.9, bsz=40, num_updates=520, lr=6.5e-06, gnorm=2.196, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1170
2023-01-11 00:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:34 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 100000 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.0104, wps=75.6, ups=0.45, wpb=110.9, bsz=40, num_updates=530, lr=6.625e-06, gnorm=2.063, clip=100, loss_scale=256, train_wall=22, gb_free=10, ema_decay=0.9999, wall=1192
2023-01-11 00:24:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:24:57 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 100000 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0, wps=73, ups=0.44, wpb=109.9, bsz=40, num_updates=540, lr=6.75e-06, gnorm=2.367, clip=100, loss_scale=256, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=1215
2023-01-11 00:25:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:19 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 100000 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.4, vqa_score=0.0481, wps=73.6, ups=0.45, wpb=109.1, bsz=40, num_updates=550, lr=6.875e-06, gnorm=2.19, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1238
2023-01-11 00:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:41 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 100000 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0309, wps=75.6, ups=0.46, wpb=109, bsz=40, num_updates=560, lr=7e-06, gnorm=2.04, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1260
2023-01-11 00:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:03 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 100000 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.028, wps=75.3, ups=0.46, wpb=109.5, bsz=40, num_updates=570, lr=7.125e-06, gnorm=1.928, clip=90, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1282
2023-01-11 00:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:25 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 100000 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.0435, wps=76.8, ups=0.46, wpb=111, bsz=40, num_updates=580, lr=7.25e-06, gnorm=2.058, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1304
2023-01-11 00:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:48 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0187, wps=75.3, ups=0.46, wpb=110.3, bsz=40, num_updates=590, lr=7.375e-06, gnorm=2.233, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1326
2023-01-11 00:26:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:26:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:10 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 100000 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0202, wps=73.6, ups=0.45, wpb=109, bsz=40, num_updates=600, lr=7.5e-06, gnorm=2.02, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1349
2023-01-11 00:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:33 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 100000 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.01, wps=73, ups=0.44, wpb=111.1, bsz=40, num_updates=610, lr=7.625e-06, gnorm=1.881, clip=100, loss_scale=256, train_wall=23, gb_free=10.3, ema_decay=0.9999, wall=1372
2023-01-11 00:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:27:56 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 100000 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.05, wps=74, ups=0.45, wpb=110, bsz=40, num_updates=620, lr=7.75e-06, gnorm=2.094, clip=100, loss_scale=256, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=1395
2023-01-11 00:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:18 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.01, wps=73.6, ups=0.45, wpb=109.3, bsz=40, num_updates=630, lr=7.875e-06, gnorm=1.692, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1417
2023-01-11 00:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:41 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 100000 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0202, wps=75.9, ups=0.46, wpb=109.9, bsz=40, num_updates=640, lr=8e-06, gnorm=1.776, clip=100, loss_scale=256, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=1439
2023-01-11 00:28:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:04 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 100000 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0185, wps=71.7, ups=0.44, wpb=108.9, bsz=40, num_updates=650, lr=8.125e-06, gnorm=1.879, clip=100, loss_scale=256, train_wall=23, gb_free=10.5, ema_decay=0.9999, wall=1462
2023-01-11 00:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:26 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0286, wps=74.9, ups=0.45, wpb=109.8, bsz=40, num_updates=660, lr=8.25e-06, gnorm=1.924, clip=100, loss_scale=256, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=1485
2023-01-11 00:29:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:48 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 100000 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.03, wps=75.5, ups=0.45, wpb=110.6, bsz=40, num_updates=670, lr=8.375e-06, gnorm=1.597, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1507
2023-01-11 00:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:11 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0196, wps=74.3, ups=0.45, wpb=110.9, bsz=40, num_updates=680, lr=8.5e-06, gnorm=1.7, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1530
2023-01-11 00:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:34 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 100000 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.03, wps=72, ups=0.44, wpb=109.7, bsz=40, num_updates=690, lr=8.625e-06, gnorm=1.878, clip=90, loss_scale=256, train_wall=23, gb_free=10.3, ema_decay=0.9999, wall=1553
2023-01-11 00:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:30:57 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 100000 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0101, wps=73.6, ups=0.45, wpb=109.5, bsz=40, num_updates=700, lr=8.75e-06, gnorm=1.732, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1575
2023-01-11 00:31:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:18 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 100000 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0, wps=77.1, ups=0.47, wpb=110, bsz=40, num_updates=710, lr=8.875e-06, gnorm=1.706, clip=100, loss_scale=256, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=1597
2023-01-11 00:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:41 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 100000 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, vqa_score=0.0286, wps=75.7, ups=0.45, wpb=111.2, bsz=40, num_updates=720, lr=9e-06, gnorm=1.891, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1619
2023-01-11 00:31:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:31:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:03 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 100000 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0217, wps=73.8, ups=0.45, wpb=108.6, bsz=40, num_updates=730, lr=9.125e-06, gnorm=1.871, clip=100, loss_scale=256, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=1642
2023-01-11 00:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:26 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 100000 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0485, wps=73.1, ups=0.45, wpb=108.8, bsz=40, num_updates=740, lr=9.25e-06, gnorm=1.77, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1664
2023-01-11 00:32:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:48 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 100000 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.044, wps=76.4, ups=0.46, wpb=110.9, bsz=40, num_updates=750, lr=9.375e-06, gnorm=1.787, clip=100, loss_scale=256, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=1686
2023-01-11 00:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:10 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 100000 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0326, wps=75, ups=0.46, wpb=109.3, bsz=40, num_updates=760, lr=9.5e-06, gnorm=1.853, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1709
2023-01-11 00:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:33 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 100000 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0536, wps=73, ups=0.45, wpb=107.9, bsz=40, num_updates=770, lr=9.625e-06, gnorm=1.794, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1731
2023-01-11 00:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:33:55 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 100000 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0549, wps=76.1, ups=0.45, wpb=111.7, bsz=40, num_updates=780, lr=9.75e-06, gnorm=1.907, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=1754
2023-01-11 00:34:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:18 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 100000 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0323, wps=72, ups=0.43, wpb=110.4, bsz=40, num_updates=790, lr=9.875e-06, gnorm=1.798, clip=100, loss_scale=256, train_wall=23, gb_free=9.7, ema_decay=0.9999, wall=1777
2023-01-11 00:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:41 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 100000 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0326, wps=74.4, ups=0.45, wpb=110.5, bsz=40, num_updates=800, lr=1e-05, gnorm=2.083, clip=90, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1800
2023-01-11 00:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:04 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.04, wps=73.2, ups=0.44, wpb=110.2, bsz=40, num_updates=810, lr=1.0125e-05, gnorm=1.791, clip=100, loss_scale=256, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=1823
2023-01-11 00:35:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:26 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 100000 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0, wps=75.5, ups=0.46, wpb=110.5, bsz=40, num_updates=820, lr=1.025e-05, gnorm=1.401, clip=90, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=1845
2023-01-11 00:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:49 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 100000 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.0183, wps=73.1, ups=0.44, wpb=110.1, bsz=40, num_updates=830, lr=1.0375e-05, gnorm=1.865, clip=100, loss_scale=256, train_wall=23, gb_free=10.4, ema_decay=0.9999, wall=1868
2023-01-11 00:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:35:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:12 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 100000 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.036, wps=73.4, ups=0.45, wpb=108.1, bsz=40, num_updates=840, lr=1.05e-05, gnorm=1.794, clip=100, loss_scale=256, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=1891
2023-01-11 00:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:35 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 100000 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0404, wps=74.3, ups=0.45, wpb=110.7, bsz=40, num_updates=850, lr=1.0625e-05, gnorm=1.866, clip=100, loss_scale=256, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=1913
2023-01-11 00:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:36:58 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 100000 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.05, wps=73.2, ups=0.44, wpb=109.8, bsz=40, num_updates=860, lr=1.075e-05, gnorm=1.773, clip=100, loss_scale=256, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=1936
2023-01-11 00:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:20 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 100000 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0105, wps=73.6, ups=0.45, wpb=109.8, bsz=40, num_updates=870, lr=1.0875e-05, gnorm=1.713, clip=100, loss_scale=256, train_wall=22, gb_free=10, ema_decay=0.9999, wall=1959
2023-01-11 00:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:43 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 100000 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0106, wps=75.4, ups=0.45, wpb=111.5, bsz=40, num_updates=880, lr=1.1e-05, gnorm=1.893, clip=100, loss_scale=256, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=1981
2023-01-11 00:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:05 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 100000 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0446, wps=75.1, ups=0.46, wpb=108.9, bsz=40, num_updates=890, lr=1.1125e-05, gnorm=1.603, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=2004
2023-01-11 00:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:28 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 100000 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0426, wps=73.5, ups=0.45, wpb=109.5, bsz=40, num_updates=900, lr=1.125e-05, gnorm=1.618, clip=90, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=2026
2023-01-11 00:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:49 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 100000 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.045, wps=76.1, ups=0.46, wpb=109.7, bsz=40, num_updates=910, lr=1.1375e-05, gnorm=1.888, clip=100, loss_scale=256, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=2048
2023-01-11 00:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:12 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 100000 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, vqa_score=0.0183, wps=74.8, ups=0.46, wpb=108.6, bsz=40, num_updates=920, lr=1.15e-05, gnorm=1.817, clip=100, loss_scale=256, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=2070
2023-01-11 00:39:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:34 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 100000 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0198, wps=76, ups=0.46, wpb=110.6, bsz=40, num_updates=930, lr=1.1625e-05, gnorm=1.625, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=2092
2023-01-11 00:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:39:57 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0098, wps=73.4, ups=0.44, wpb=110.4, bsz=40, num_updates=940, lr=1.175e-05, gnorm=1.764, clip=100, loss_scale=256, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=2115
2023-01-11 00:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:20 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 100000 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0291, wps=73.8, ups=0.45, wpb=110.1, bsz=40, num_updates=950, lr=1.1875e-05, gnorm=1.756, clip=100, loss_scale=256, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=2138
2023-01-11 00:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:42 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 100000 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0108, wps=77.5, ups=0.46, wpb=111.9, bsz=40, num_updates=960, lr=1.2e-05, gnorm=1.648, clip=100, loss_scale=256, train_wall=22, gb_free=10, ema_decay=0.9999, wall=2160
2023-01-11 00:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:04 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 100000 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.05, wps=76.7, ups=0.46, wpb=111.9, bsz=40, num_updates=970, lr=1.2125e-05, gnorm=1.563, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=2182
2023-01-11 00:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:26 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 100000 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0294, wps=74.2, ups=0.45, wpb=109.5, bsz=40, num_updates=980, lr=1.225e-05, gnorm=1.689, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=2205
2023-01-11 00:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:47 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 100000 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0194, wps=78.2, ups=0.48, wpb=109.3, bsz=40, num_updates=990, lr=1.2375e-05, gnorm=1.602, clip=100, loss_scale=256, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=2226
2023-01-11 00:41:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:42:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 00:42:09 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 100000 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.023, wps=77.2, ups=0.46, wpb=112.2, bsz=40, num_updates=1000, lr=1.25e-05, gnorm=1.789, clip=100, loss_scale=256, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=2248
2023-01-11 00:42:09 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 00:42:09 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-01-11 00:42:11 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 00:42:11 - train.py[line:551] - INFO: load:1.09 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 00:45:39 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 00:45:39 - train.py[line:551] - INFO: load:1.11 valid_run:208.51 task_valid:205.89 collect_output:0.57
2023-01-11 00:49:03 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 00:49:03 - train.py[line:551] - INFO: load:1.14 valid_run:412.30 task_valid:407.09 collect_output:1.14
2023-01-11 00:52:28 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 00:52:28 - train.py[line:551] - INFO: load:1.17 valid_run:616.80 task_valid:608.83 collect_output:1.89
2023-01-11 00:55:49 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 00:55:49 - train.py[line:551] - INFO: load:1.19 valid_run:817.35 task_valid:806.85 collect_output:2.45
2023-01-11 00:59:14 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 00:59:14 - train.py[line:551] - INFO: load:1.22 valid_run:1023.17 task_valid:1010.09 collect_output:3.01
2023-01-11 01:02:41 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 01:02:41 - train.py[line:551] - INFO: load:1.25 valid_run:1229.41 task_valid:1213.81 collect_output:3.56
2023-01-11 01:06:07 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 01:06:07 - train.py[line:551] - INFO: load:1.27 valid_run:1435.38 task_valid:1417.19 collect_output:4.11
2023-01-11 01:09:31 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 01:09:31 - train.py[line:551] - INFO: load:1.30 valid_run:1639.64 task_valid:1618.87 collect_output:4.67
2023-01-11 01:12:56 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 01:12:56 - train.py[line:551] - INFO: load:1.33 valid_run:1844.59 task_valid:1821.30 collect_output:5.23
2023-01-11 01:16:16 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 01:16:16 - train.py[line:551] - INFO: load:1.35 valid_run:2044.65 task_valid:2018.76 collect_output:5.79
2023-01-11 01:19:40 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 01:19:40 - train.py[line:551] - INFO: load:1.38 valid_run:2248.03 task_valid:2219.55 collect_output:6.35
2023-01-11 01:23:04 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 01:23:04 - train.py[line:551] - INFO: load:1.41 valid_run:2451.89 task_valid:2420.85 collect_output:6.91
2023-01-11 01:26:24 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 01:26:24 - train.py[line:551] - INFO: load:1.44 valid_run:2652.06 task_valid:2618.41 collect_output:7.48
2023-01-11 01:29:50 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 01:29:50 - train.py[line:551] - INFO: load:1.47 valid_run:2857.86 task_valid:2821.60 collect_output:8.07
2023-01-11 01:33:14 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 01:33:14 - train.py[line:551] - INFO: load:1.50 valid_run:3061.46 task_valid:3022.63 collect_output:8.63
2023-01-11 01:36:35 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 01:36:35 - train.py[line:551] - INFO: load:1.53 valid_run:3262.73 task_valid:3221.29 collect_output:9.20
2023-01-11 01:39:59 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 01:39:59 - train.py[line:551] - INFO: load:1.56 valid_run:3466.98 task_valid:3423.00 collect_output:9.77
2023-01-11 01:43:26 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 01:43:26 - train.py[line:551] - INFO: load:1.58 valid_run:3673.23 task_valid:3626.66 collect_output:10.34
2023-01-11 01:46:50 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 01:46:50 - train.py[line:551] - INFO: load:1.61 valid_run:3877.87 task_valid:3828.73 collect_output:10.92
2023-01-11 01:50:14 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 01:50:14 - train.py[line:551] - INFO: load:1.64 valid_run:4081.69 task_valid:4029.96 collect_output:11.48
2023-01-11 01:53:38 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 01:53:38 - train.py[line:551] - INFO: load:1.67 valid_run:4285.82 task_valid:4231.53 collect_output:12.05
2023-01-11 01:57:06 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 01:57:06 - train.py[line:551] - INFO: load:1.69 valid_run:4493.37 task_valid:4436.51 collect_output:12.61
2023-01-11 02:00:27 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 02:00:27 - train.py[line:551] - INFO: load:1.72 valid_run:4694.65 task_valid:4635.23 collect_output:13.17
2023-01-11 02:03:52 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 02:03:52 - train.py[line:551] - INFO: load:1.75 valid_run:4899.04 task_valid:4837.04 collect_output:13.74
2023-01-11 02:07:15 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 02:07:15 - train.py[line:551] - INFO: load:1.77 valid_run:5102.22 task_valid:5037.65 collect_output:14.30
2023-01-11 02:10:38 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 02:10:38 - train.py[line:551] - INFO: load:1.80 valid_run:5305.40 task_valid:5238.27 collect_output:14.86
2023-01-11 02:13:59 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 02:13:59 - train.py[line:551] - INFO: load:1.83 valid_run:5505.65 task_valid:5435.96 collect_output:15.43
2023-01-11 02:17:26 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 02:17:26 - train.py[line:551] - INFO: load:1.86 valid_run:5712.95 task_valid:5640.66 collect_output:16.00
2023-01-11 02:20:49 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 02:20:49 - train.py[line:551] - INFO: load:1.88 valid_run:5915.26 task_valid:5840.40 collect_output:16.58
2023-01-11 02:24:15 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 02:24:15 - train.py[line:551] - INFO: load:1.91 valid_run:6121.73 task_valid:6044.30 collect_output:17.14
2023-01-11 02:27:41 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 02:27:41 - train.py[line:551] - INFO: load:1.94 valid_run:6327.69 task_valid:6247.72 collect_output:17.70

====================================================================================================
SGG eval:     R @ 50: 0.1791;     R @ 100: 0.2608;     R @ 500: 0.3479;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0834;    mR @ 100: 0.1400;    mR @ 500: 0.1906;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.1429) (eating:0.2353) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3613) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.1250) (playing:0.0000) (riding:0.2059) (says:0.0000) (sitting on:0.3010) (standing on:0.5133) (using:0.1500) (walking in:0.0000) (walking on:0.0270) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-11 02:28:26 - train.py[line:487] - INFO: 0.26080000000000003

====================================================================================================
SGG eval:     R @ 50: 0.1791;     R @ 100: 0.2608;     R @ 500: 0.3479;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0834;    mR @ 100: 0.1400;    mR @ 500: 0.1906;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.1429) (eating:0.2353) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3613) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.1250) (playing:0.0000) (riding:0.2059) (says:0.0000) (sitting on:0.3010) (standing on:0.5133) (using:0.1500) (walking in:0.0000) (walking on:0.0270) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-11 02:28:26 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 02:28:26 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.269 | loss_v1 0 | loss_v2 0 | nll_loss 0.105 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.2608 | ppl 1.08 | vqa_score 0.1757 | wps 70.4 | wpb 72 | bsz 24 | num_updates 1000
2023-01-11 02:28:26 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1000 updates
2023-01-11 02:28:26 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_1000.pt
2023-01-11 02:29:10 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_1000.pt
2023-01-11 02:32:01 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 0.26080000000000003) (writing took 215.2865976113826 seconds)
2023-01-11 02:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:24 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0367, wps=0.2, ups=0, wpb=109.3, bsz=40, num_updates=1010, lr=1.2625e-05, gnorm=1.766, clip=100, loss_scale=256, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=8862
2023-01-11 02:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:47 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 100000 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.0294, wps=74.2, ups=0.45, wpb=109.9, bsz=40, num_updates=1020, lr=1.275e-05, gnorm=2.047, clip=100, loss_scale=256, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=8885
2023-01-11 02:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:11 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 100000 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0187, wps=72.8, ups=0.45, wpb=108.9, bsz=40, num_updates=1030, lr=1.2875e-05, gnorm=1.695, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=8909
2023-01-11 02:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:33 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 100000 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0283, wps=75.6, ups=0.47, wpb=108.3, bsz=40, num_updates=1040, lr=1.3e-05, gnorm=1.595, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=8931
2023-01-11 02:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:33:56 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0096, wps=73.4, ups=0.45, wpb=109.5, bsz=40, num_updates=1050, lr=1.3125e-05, gnorm=1.715, clip=90, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=8954
2023-01-11 02:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:19 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0385, wps=75, ups=0.45, wpb=110.7, bsz=40, num_updates=1060, lr=1.325e-05, gnorm=1.695, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=8977
2023-01-11 02:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:42 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0115, wps=77.3, ups=0.47, wpb=110.1, bsz=40, num_updates=1070, lr=1.3375e-05, gnorm=1.435, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=9000
2023-01-11 02:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:04 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 100000 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0495, wps=75.7, ups=0.46, wpb=108.9, bsz=40, num_updates=1080, lr=1.35e-05, gnorm=1.663, clip=100, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=9022
2023-01-11 02:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:27 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 100000 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0283, wps=77.5, ups=0.47, wpb=110.8, bsz=40, num_updates=1090, lr=1.3625e-05, gnorm=1.515, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=9045
2023-01-11 02:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:35:50 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0102, wps=75.1, ups=0.45, wpb=110.8, bsz=40, num_updates=1100, lr=1.375e-05, gnorm=1.466, clip=100, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=9068
2023-01-11 02:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:13 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 100000 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0, wps=72.9, ups=0.44, wpb=109.3, bsz=40, num_updates=1110, lr=1.3875e-05, gnorm=1.745, clip=90, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=9091
2023-01-11 02:36:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:36 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 100000 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0353, wps=75.8, ups=0.46, wpb=110.5, bsz=40, num_updates=1120, lr=1.4e-05, gnorm=1.6, clip=100, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=9114
2023-01-11 02:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:36:59 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 100000 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0204, wps=76.1, ups=0.46, wpb=109.5, bsz=40, num_updates=1130, lr=1.4125e-05, gnorm=1.63, clip=100, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=9137
2023-01-11 02:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:21 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 100000 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0495, wps=76.7, ups=0.46, wpb=110.2, bsz=40, num_updates=1140, lr=1.425e-05, gnorm=1.812, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=9159
2023-01-11 02:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:43 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0309, wps=76.6, ups=0.47, wpb=107.9, bsz=40, num_updates=1150, lr=1.4375e-05, gnorm=1.579, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=9181
2023-01-11 02:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:06 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 100000 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0495, wps=76.8, ups=0.47, wpb=110.1, bsz=40, num_updates=1160, lr=1.45e-05, gnorm=1.51, clip=90, loss_scale=512, train_wall=21, gb_free=10.6, ema_decay=0.9999, wall=9204
2023-01-11 02:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:28 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0388, wps=75.1, ups=0.46, wpb=109.8, bsz=40, num_updates=1170, lr=1.4625e-05, gnorm=1.453, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9227
2023-01-11 02:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:38:50 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 100000 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=112.467, nsentences=40, sample_size=112.467, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0103, wps=79.6, ups=0.47, wpb=112.5, bsz=40, num_updates=1180, lr=1.475e-05, gnorm=1.759, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=9248
2023-01-11 02:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:13 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0392, wps=75.4, ups=0.46, wpb=110.5, bsz=40, num_updates=1190, lr=1.4875e-05, gnorm=1.554, clip=100, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=9271
2023-01-11 02:39:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:37 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0103, wps=73.5, ups=0.45, wpb=109.8, bsz=40, num_updates=1200, lr=1.5e-05, gnorm=1.624, clip=100, loss_scale=512, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=9295
2023-01-11 02:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 100000 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0283, wps=75.6, ups=0.46, wpb=110, bsz=40, num_updates=1210, lr=1.5125e-05, gnorm=1.632, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9317
2023-01-11 02:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:22 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 100000 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0636, wps=75.8, ups=0.47, wpb=108.4, bsz=40, num_updates=1220, lr=1.525e-05, gnorm=1.952, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=9340
2023-01-11 02:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:44 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0204, wps=77.6, ups=0.47, wpb=110.4, bsz=40, num_updates=1230, lr=1.5375e-05, gnorm=1.579, clip=100, loss_scale=512, train_wall=21, gb_free=10.6, ema_decay=0.9999, wall=9362
2023-01-11 02:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:06 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 100000 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0323, wps=77.3, ups=0.46, wpb=110.9, bsz=40, num_updates=1240, lr=1.55e-05, gnorm=1.627, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=9384
2023-01-11 02:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:29 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0211, wps=76, ups=0.46, wpb=110.3, bsz=40, num_updates=1250, lr=1.5625e-05, gnorm=1.749, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9407
2023-01-11 02:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:41:52 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0106, wps=75, ups=0.45, wpb=110.5, bsz=40, num_updates=1260, lr=1.575e-05, gnorm=1.793, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9430
2023-01-11 02:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:15 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0769, wps=76.1, ups=0.45, wpb=111.8, bsz=40, num_updates=1270, lr=1.5875e-05, gnorm=1.513, clip=100, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=9453
2023-01-11 02:42:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:38 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 100000 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0412, wps=75.9, ups=0.46, wpb=110.1, bsz=40, num_updates=1280, lr=1.6e-05, gnorm=1.735, clip=100, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=9476
2023-01-11 02:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:00 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 100000 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0187, wps=77, ups=0.47, wpb=109.1, bsz=40, num_updates=1290, lr=1.6125e-05, gnorm=1.482, clip=100, loss_scale=512, train_wall=21, gb_free=10.6, ema_decay=0.9999, wall=9498
2023-01-11 02:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:23 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0194, wps=73.8, ups=0.46, wpb=107.6, bsz=40, num_updates=1300, lr=1.625e-05, gnorm=1.703, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9521
2023-01-11 02:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:45 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 100000 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0103, wps=76.2, ups=0.47, wpb=108.9, bsz=40, num_updates=1310, lr=1.6375e-05, gnorm=1.936, clip=100, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=9543
2023-01-11 02:43:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:08 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 100000 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0337, wps=77, ups=0.46, wpb=110.6, bsz=40, num_updates=1320, lr=1.65e-05, gnorm=1.568, clip=100, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=9566
2023-01-11 02:44:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:30 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 100000 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0097, wps=75.7, ups=0.46, wpb=109.8, bsz=40, num_updates=1330, lr=1.6625e-05, gnorm=1.583, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9588
2023-01-11 02:44:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:44:53 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 100000 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0385, wps=78.1, ups=0.47, wpb=110.3, bsz=40, num_updates=1340, lr=1.675e-05, gnorm=1.668, clip=100, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=9611
2023-01-11 02:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:15 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 100000 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0309, wps=77.3, ups=0.47, wpb=109.7, bsz=40, num_updates=1350, lr=1.6875e-05, gnorm=1.945, clip=90, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=9633
2023-01-11 02:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:37 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 100000 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0515, wps=77.4, ups=0.47, wpb=110.2, bsz=40, num_updates=1360, lr=1.7e-05, gnorm=2.157, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=9655
2023-01-11 02:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:00 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 100000 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0541, wps=75.3, ups=0.46, wpb=109.9, bsz=40, num_updates=1370, lr=1.7125e-05, gnorm=1.642, clip=100, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=9678
2023-01-11 02:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:23 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0444, wps=77.7, ups=0.47, wpb=111.1, bsz=40, num_updates=1380, lr=1.725e-05, gnorm=1.66, clip=100, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=9701
2023-01-11 02:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:46 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 100000 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0105, wps=73.5, ups=0.45, wpb=109.1, bsz=40, num_updates=1390, lr=1.7375e-05, gnorm=1.696, clip=90, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=9724
2023-01-11 02:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:08 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 100000 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0108, wps=78.4, ups=0.48, wpb=109.5, bsz=40, num_updates=1400, lr=1.75e-05, gnorm=2.238, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=9746
2023-01-11 02:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:30 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 100000 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0198, wps=77.5, ups=0.47, wpb=109.7, bsz=40, num_updates=1410, lr=1.7625e-05, gnorm=1.882, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=9768
2023-01-11 02:47:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:47:53 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 100000 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0103, wps=76, ups=0.46, wpb=109.9, bsz=40, num_updates=1420, lr=1.775e-05, gnorm=1.378, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9791
2023-01-11 02:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:16 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 100000 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0385, wps=74.6, ups=0.46, wpb=109, bsz=40, num_updates=1430, lr=1.7875e-05, gnorm=1.459, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9814
2023-01-11 02:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:39 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0312, wps=75.5, ups=0.46, wpb=109.8, bsz=40, num_updates=1440, lr=1.8e-05, gnorm=1.544, clip=100, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=9837
2023-01-11 02:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:02 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0309, wps=75.5, ups=0.45, wpb=111.7, bsz=40, num_updates=1450, lr=1.8125e-05, gnorm=1.456, clip=90, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=9860
2023-01-11 02:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:25 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 100000 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.028, wps=76.2, ups=0.47, wpb=108.3, bsz=40, num_updates=1460, lr=1.825e-05, gnorm=1.637, clip=100, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=9882
2023-01-11 02:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:47 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 100000 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0099, wps=75.1, ups=0.46, wpb=109.5, bsz=40, num_updates=1470, lr=1.8375e-05, gnorm=1.58, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9905
2023-01-11 02:49:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:10 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0, wps=77, ups=0.47, wpb=110, bsz=40, num_updates=1480, lr=1.85e-05, gnorm=1.44, clip=100, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=9928
2023-01-11 02:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:33 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0714, wps=74.6, ups=0.45, wpb=110.9, bsz=40, num_updates=1490, lr=1.8625e-05, gnorm=1.512, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=9951
2023-01-11 02:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:50:56 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 100000 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0404, wps=75.5, ups=0.46, wpb=109.4, bsz=40, num_updates=1500, lr=1.875e-05, gnorm=1.395, clip=90, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=9974
2023-01-11 02:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:18 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 100000 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0114, wps=76.6, ups=0.46, wpb=110.9, bsz=40, num_updates=1510, lr=1.8875e-05, gnorm=1.388, clip=100, loss_scale=512, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=9996
2023-01-11 02:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:41 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0222, wps=74.6, ups=0.45, wpb=110.5, bsz=40, num_updates=1520, lr=1.9e-05, gnorm=1.603, clip=90, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=10020
2023-01-11 02:51:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:04 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 100000 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0189, wps=76.4, ups=0.46, wpb=110, bsz=40, num_updates=1530, lr=1.9125e-05, gnorm=1.288, clip=90, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=10042
2023-01-11 02:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:26 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 100000 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0333, wps=75.9, ups=0.46, wpb=110.8, bsz=40, num_updates=1540, lr=1.925e-05, gnorm=1.724, clip=100, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10065
2023-01-11 02:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:48 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 100000 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0309, wps=76, ups=0.46, wpb=110.7, bsz=40, num_updates=1550, lr=1.9375e-05, gnorm=1.571, clip=100, loss_scale=1024, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=10087
2023-01-11 02:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:10 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 100000 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.009, wps=76.1, ups=0.46, wpb=109.5, bsz=40, num_updates=1560, lr=1.95e-05, gnorm=2.168, clip=100, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10109
2023-01-11 02:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:32 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.05, wps=76.9, ups=0.46, wpb=110.7, bsz=40, num_updates=1570, lr=1.9625e-05, gnorm=1.46, clip=90, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10131
2023-01-11 02:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:53:55 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0549, wps=75.2, ups=0.46, wpb=109.9, bsz=40, num_updates=1580, lr=1.975e-05, gnorm=1.436, clip=90, loss_scale=1024, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=10153
2023-01-11 02:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:17 - progress_bar.py[line:274] - INFO: epoch 001:   1590 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0265, wps=74.2, ups=0.45, wpb=109.3, bsz=40, num_updates=1590, lr=1.9875e-05, gnorm=1.652, clip=100, loss_scale=1024, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=10176
2023-01-11 02:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:39 - progress_bar.py[line:274] - INFO: epoch 001:   1600 / 100000 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0174, wps=77, ups=0.47, wpb=109.3, bsz=40, num_updates=1600, lr=2e-05, gnorm=1.647, clip=100, loss_scale=1024, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=10197
2023-01-11 02:54:43 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 02:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:03 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 100000 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0198, wps=69, ups=0.42, wpb=108.9, bsz=40, num_updates=1610, lr=2.0125e-05, gnorm=1.507, clip=100, loss_scale=512, train_wall=24, gb_free=10.3, ema_decay=0.9999, wall=10221
2023-01-11 02:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:24 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 100000 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0196, wps=77.8, ups=0.47, wpb=109.7, bsz=40, num_updates=1620, lr=2.025e-05, gnorm=1.502, clip=90, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=10243
2023-01-11 02:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:47 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0225, wps=74.8, ups=0.45, wpb=110, bsz=40, num_updates=1630, lr=2.0375e-05, gnorm=1.332, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10265
2023-01-11 02:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:55:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:09 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0306, wps=76.7, ups=0.46, wpb=110.9, bsz=40, num_updates=1640, lr=2.05e-05, gnorm=1.563, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10288
2023-01-11 02:56:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:31 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0104, wps=75.6, ups=0.46, wpb=108.7, bsz=40, num_updates=1650, lr=2.0625e-05, gnorm=1.678, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=10310
2023-01-11 02:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:56:53 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0099, wps=76.7, ups=0.47, wpb=109.2, bsz=40, num_updates=1660, lr=2.075e-05, gnorm=1.516, clip=90, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=10331
2023-01-11 02:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:15 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 100000 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0291, wps=74.2, ups=0.45, wpb=109.9, bsz=40, num_updates=1670, lr=2.0875e-05, gnorm=1.599, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10354
2023-01-11 02:57:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:37 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0444, wps=78.7, ups=0.47, wpb=110.5, bsz=40, num_updates=1680, lr=2.1e-05, gnorm=1.317, clip=80, loss_scale=512, train_wall=21, gb_free=10.5, ema_decay=0.9999, wall=10375
2023-01-11 02:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:57:58 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0412, wps=79.8, ups=0.48, wpb=111.1, bsz=40, num_updates=1690, lr=2.1125e-05, gnorm=1.736, clip=90, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=10397
2023-01-11 02:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:21 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0092, wps=75.6, ups=0.46, wpb=110.1, bsz=40, num_updates=1700, lr=2.125e-05, gnorm=1.504, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10419
2023-01-11 02:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:42 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0638, wps=78.4, ups=0.47, wpb=110.9, bsz=40, num_updates=1710, lr=2.1375e-05, gnorm=1.324, clip=90, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=10441
2023-01-11 02:58:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:04 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0275, wps=74.9, ups=0.46, wpb=108.7, bsz=40, num_updates=1720, lr=2.15e-05, gnorm=1.42, clip=80, loss_scale=512, train_wall=22, gb_free=10.7, ema_decay=0.9999, wall=10463
2023-01-11 02:59:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:26 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0, wps=77.3, ups=0.47, wpb=109.4, bsz=40, num_updates=1730, lr=2.1625e-05, gnorm=1.595, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=10484
2023-01-11 02:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:48 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0095, wps=75.3, ups=0.46, wpb=108.4, bsz=40, num_updates=1740, lr=2.175e-05, gnorm=1.835, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10507
2023-01-11 02:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 02:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:10 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0099, wps=74.1, ups=0.45, wpb=108.7, bsz=40, num_updates=1750, lr=2.1875e-05, gnorm=1.74, clip=90, loss_scale=512, train_wall=22, gb_free=9.8, ema_decay=0.9999, wall=10529
2023-01-11 03:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:33 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0532, wps=75, ups=0.46, wpb=109.6, bsz=40, num_updates=1760, lr=2.2e-05, gnorm=1.715, clip=80, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=10551
2023-01-11 03:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0412, wps=76.8, ups=0.46, wpb=110.7, bsz=40, num_updates=1770, lr=2.2125e-05, gnorm=1.843, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10573
2023-01-11 03:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:17 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0326, wps=74.6, ups=0.45, wpb=110.7, bsz=40, num_updates=1780, lr=2.225e-05, gnorm=1.811, clip=100, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=10596
2023-01-11 03:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:39 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0309, wps=78.7, ups=0.48, wpb=110, bsz=40, num_updates=1790, lr=2.2375e-05, gnorm=1.679, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=10617
2023-01-11 03:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:01 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 100000 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0179, wps=74.4, ups=0.45, wpb=109, bsz=40, num_updates=1800, lr=2.25e-05, gnorm=1.553, clip=100, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=10640
2023-01-11 03:02:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:23 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 100000 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0421, wps=77, ups=0.47, wpb=109.4, bsz=40, num_updates=1810, lr=2.2625e-05, gnorm=1.735, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=10661
2023-01-11 03:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:45 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0288, wps=74.7, ups=0.45, wpb=110.1, bsz=40, num_updates=1820, lr=2.275e-05, gnorm=1.629, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10684
2023-01-11 03:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:07 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 100000 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0294, wps=76.5, ups=0.47, wpb=108.3, bsz=40, num_updates=1830, lr=2.2875e-05, gnorm=1.614, clip=100, loss_scale=512, train_wall=21, gb_free=10, ema_decay=0.9999, wall=10705
2023-01-11 03:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:29 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 100000 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0095, wps=76.9, ups=0.47, wpb=109.3, bsz=40, num_updates=1840, lr=2.3e-05, gnorm=1.371, clip=80, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=10727
2023-01-11 03:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:51 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0306, wps=73.8, ups=0.45, wpb=109.1, bsz=40, num_updates=1850, lr=2.3125e-05, gnorm=1.297, clip=70, loss_scale=512, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=10750
2023-01-11 03:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:13 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0515, wps=75, ups=0.46, wpb=109.5, bsz=40, num_updates=1860, lr=2.325e-05, gnorm=1.427, clip=90, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=10772
2023-01-11 03:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:36 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 100000 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0095, wps=72.3, ups=0.44, wpb=108.4, bsz=40, num_updates=1870, lr=2.3375e-05, gnorm=1.851, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10795
2023-01-11 03:04:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:04:59 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0316, wps=73.9, ups=0.45, wpb=110.4, bsz=40, num_updates=1880, lr=2.35e-05, gnorm=1.293, clip=80, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=10818
2023-01-11 03:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:21 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0115, wps=76.2, ups=0.46, wpb=111.3, bsz=40, num_updates=1890, lr=2.3625e-05, gnorm=1.485, clip=90, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=10840
2023-01-11 03:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:43 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0196, wps=78.7, ups=0.47, wpb=111.5, bsz=40, num_updates=1900, lr=2.375e-05, gnorm=1.564, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=10861
2023-01-11 03:05:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:05 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0351, wps=74.7, ups=0.46, wpb=108.9, bsz=40, num_updates=1910, lr=2.3875e-05, gnorm=1.748, clip=100, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=10884
2023-01-11 03:06:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:26 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0204, wps=77.7, ups=0.47, wpb=109.1, bsz=40, num_updates=1920, lr=2.4e-05, gnorm=1.484, clip=100, loss_scale=512, train_wall=21, gb_free=10.5, ema_decay=0.9999, wall=10905
2023-01-11 03:06:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:48 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.037, wps=76.3, ups=0.47, wpb=109, bsz=40, num_updates=1930, lr=2.4125e-05, gnorm=1.573, clip=80, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=10927
2023-01-11 03:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:10 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0208, wps=77.4, ups=0.47, wpb=109.1, bsz=40, num_updates=1940, lr=2.425e-05, gnorm=1.4, clip=70, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=10948
2023-01-11 03:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:32 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.028, wps=75.9, ups=0.46, wpb=109.5, bsz=40, num_updates=1950, lr=2.4375e-05, gnorm=1.311, clip=80, loss_scale=512, train_wall=22, gb_free=10.8, ema_decay=0.9999, wall=10970
2023-01-11 03:07:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:07:54 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 100000 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0096, wps=75.4, ups=0.46, wpb=110.3, bsz=40, num_updates=1960, lr=2.45e-05, gnorm=1.456, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=10993
2023-01-11 03:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:16 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 100000 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0273, wps=76.1, ups=0.46, wpb=109.3, bsz=40, num_updates=1970, lr=2.4625e-05, gnorm=1.429, clip=90, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=11015
2023-01-11 03:08:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:38 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0566, wps=75.7, ups=0.46, wpb=109.7, bsz=40, num_updates=1980, lr=2.475e-05, gnorm=1.436, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=11037
2023-01-11 03:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:09:02 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0192, wps=78.4, ups=0.48, wpb=109.5, bsz=40, num_updates=1990, lr=2.4875e-05, gnorm=1.49, clip=100, loss_scale=512, train_wall=21, gb_free=9.5, ema_decay=0.9999, wall=11058
2023-01-11 03:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:09:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:09:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 03:09:26 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0476, wps=75.8, ups=0.46, wpb=110.9, bsz=40, num_updates=2000, lr=2.5e-05, gnorm=1.81, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=11083
2023-01-11 03:09:26 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 03:09:28 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 03:09:28 - train.py[line:551] - INFO: load:0.92 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 03:12:54 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 03:12:55 - train.py[line:551] - INFO: load:0.95 valid_run:206.87 task_valid:204.35 collect_output:0.54
2023-01-11 03:16:17 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 03:16:17 - train.py[line:551] - INFO: load:0.97 valid_run:409.67 task_valid:404.64 collect_output:1.06
2023-01-11 03:19:41 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 03:19:41 - train.py[line:551] - INFO: load:1.00 valid_run:613.44 task_valid:605.90 collect_output:1.60
2023-01-11 03:23:01 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 03:23:01 - train.py[line:551] - INFO: load:1.02 valid_run:813.40 task_valid:803.37 collect_output:2.13
2023-01-11 03:26:27 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 03:26:27 - train.py[line:551] - INFO: load:1.05 valid_run:1018.57 task_valid:1006.08 collect_output:2.64
2023-01-11 03:29:53 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 03:29:53 - train.py[line:551] - INFO: load:1.07 valid_run:1225.21 task_valid:1210.23 collect_output:3.17
2023-01-11 03:33:19 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 03:33:19 - train.py[line:551] - INFO: load:1.10 valid_run:1431.34 task_valid:1413.85 collect_output:3.70
2023-01-11 03:36:43 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 03:36:43 - train.py[line:551] - INFO: load:1.12 valid_run:1635.15 task_valid:1615.18 collect_output:4.23
2023-01-11 03:40:08 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 03:40:08 - train.py[line:551] - INFO: load:1.15 valid_run:1839.29 task_valid:1816.84 collect_output:4.75
2023-01-11 03:43:28 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 03:43:28 - train.py[line:551] - INFO: load:1.17 valid_run:2039.17 task_valid:2014.17 collect_output:5.29
2023-01-11 03:46:51 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 03:46:51 - train.py[line:551] - INFO: load:1.20 valid_run:2242.49 task_valid:2214.95 collect_output:5.84
2023-01-11 03:50:15 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 03:50:15 - train.py[line:551] - INFO: load:1.22 valid_run:2446.68 task_valid:2416.62 collect_output:6.38
2023-01-11 03:53:35 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 03:53:35 - train.py[line:551] - INFO: load:1.25 valid_run:2646.65 task_valid:2614.03 collect_output:6.92
2023-01-11 03:57:01 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 03:57:01 - train.py[line:551] - INFO: load:1.28 valid_run:2851.82 task_valid:2816.67 collect_output:7.46
2023-01-11 04:00:24 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 04:00:24 - train.py[line:551] - INFO: load:1.30 valid_run:3054.92 task_valid:3017.26 collect_output:7.99
2023-01-11 04:03:45 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 04:03:45 - train.py[line:551] - INFO: load:1.33 valid_run:3255.83 task_valid:3215.62 collect_output:8.54
2023-01-11 04:07:09 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 04:07:09 - train.py[line:551] - INFO: load:1.35 valid_run:3459.56 task_valid:3416.83 collect_output:9.09
2023-01-11 04:10:34 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 04:10:35 - train.py[line:551] - INFO: load:1.38 valid_run:3665.33 task_valid:3620.09 collect_output:9.63
2023-01-11 04:13:59 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 04:13:59 - train.py[line:551] - INFO: load:1.40 valid_run:3870.11 task_valid:3822.34 collect_output:10.18
2023-01-11 04:17:23 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 04:17:23 - train.py[line:551] - INFO: load:1.43 valid_run:4073.90 task_valid:4023.62 collect_output:10.72
2023-01-11 04:20:47 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 04:20:47 - train.py[line:551] - INFO: load:1.46 valid_run:4277.62 task_valid:4224.83 collect_output:11.25
2023-01-11 04:24:14 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 04:24:14 - train.py[line:551] - INFO: load:1.48 valid_run:4484.43 task_valid:4429.13 collect_output:11.80
2023-01-11 04:27:36 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 04:27:36 - train.py[line:551] - INFO: load:1.51 valid_run:4685.94 task_valid:4628.11 collect_output:12.34
2023-01-11 04:30:59 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 04:30:59 - train.py[line:551] - INFO: load:1.53 valid_run:4889.70 task_valid:4829.28 collect_output:12.89
2023-01-11 04:34:23 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 04:34:23 - train.py[line:551] - INFO: load:1.56 valid_run:5092.74 task_valid:5029.75 collect_output:13.45
2023-01-11 04:37:46 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 04:37:46 - train.py[line:551] - INFO: load:1.59 valid_run:5296.52 task_valid:5230.96 collect_output:14.01
2023-01-11 04:41:07 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 04:41:07 - train.py[line:551] - INFO: load:1.61 valid_run:5497.27 task_valid:5429.11 collect_output:14.57
2023-01-11 04:44:35 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 04:44:35 - train.py[line:551] - INFO: load:1.64 valid_run:5704.73 task_valid:5634.04 collect_output:15.12
2023-01-11 04:47:57 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 04:47:57 - train.py[line:551] - INFO: load:1.67 valid_run:5907.10 task_valid:5833.87 collect_output:15.67
2023-01-11 04:51:23 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 04:51:23 - train.py[line:551] - INFO: load:1.69 valid_run:6112.85 task_valid:6037.09 collect_output:16.23
2023-01-11 04:54:49 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 04:54:49 - train.py[line:551] - INFO: load:1.72 valid_run:6318.51 task_valid:6240.23 collect_output:16.78

====================================================================================================
SGG eval:     R @ 50: 0.3496;     R @ 100: 0.3924;     R @ 500: 0.4589;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1572;    mR @ 100: 0.2069;    mR @ 500: 0.2461;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0732) (covered in:0.0000) (covering:0.1429) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.2604) (playing:0.0000) (riding:0.5000) (says:0.0000) (sitting on:0.4887) (standing on:0.5083) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.3496;     R @ 100: 0.3924;     R @ 500: 0.4589;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1572;    mR @ 100: 0.2069;    mR @ 500: 0.2461;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0732) (covered in:0.0000) (covering:0.1429) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.2604) (playing:0.0000) (riding:0.5000) (says:0.0000) (sitting on:0.4887) (standing on:0.5083) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-11 04:55:34 - train.py[line:487] - INFO: 0.39243333333333336
2023-01-11 04:55:34 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 04:55:34 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.274 | loss_v1 0 | loss_v2 0 | nll_loss 0.11 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.392433 | ppl 1.08 | vqa_score 0.1926 | wps 70.5 | wpb 72 | bsz 24 | num_updates 2000 | best_R@100 0.392433
2023-01-11 04:55:34 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-01-11 04:55:34 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-11 04:56:20 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-11 04:59:11 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.39243333333333336) (writing took 217.31290394440293 seconds)
2023-01-11 04:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:33 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 100000 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0303, wps=0.2, ups=0, wpb=109.9, bsz=40, num_updates=2010, lr=2.5125e-05, gnorm=2.033, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=17691
2023-01-11 04:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 04:59:54 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 100000 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0, wps=76.8, ups=0.47, wpb=109.9, bsz=40, num_updates=2020, lr=2.525e-05, gnorm=1.659, clip=90, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=17713
2023-01-11 04:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:16 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 100000 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0105, wps=77.6, ups=0.46, wpb=111.4, bsz=40, num_updates=2030, lr=2.5375e-05, gnorm=1.361, clip=70, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=17735
2023-01-11 05:00:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:38 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 100000 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0095, wps=77, ups=0.47, wpb=110.3, bsz=40, num_updates=2040, lr=2.55e-05, gnorm=1.497, clip=100, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=17757
2023-01-11 05:00:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:00 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0087, wps=73.4, ups=0.45, wpb=108.1, bsz=40, num_updates=2050, lr=2.5625e-05, gnorm=1.673, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=17779
2023-01-11 05:01:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:22 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 100000 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0208, wps=75.5, ups=0.46, wpb=109.7, bsz=40, num_updates=2060, lr=2.575e-05, gnorm=1.526, clip=90, loss_scale=512, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=17801
2023-01-11 05:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:44 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0309, wps=76.5, ups=0.46, wpb=110.7, bsz=40, num_updates=2070, lr=2.5875e-05, gnorm=1.703, clip=100, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=17823
2023-01-11 05:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:06 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 100000 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0446, wps=74.2, ups=0.45, wpb=109.1, bsz=40, num_updates=2080, lr=2.6e-05, gnorm=1.755, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=17845
2023-01-11 05:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:28 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0098, wps=78, ups=0.47, wpb=111.1, bsz=40, num_updates=2090, lr=2.6125e-05, gnorm=1.393, clip=80, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=17867
2023-01-11 05:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:50 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0294, wps=75.8, ups=0.46, wpb=109.6, bsz=40, num_updates=2100, lr=2.625e-05, gnorm=1.856, clip=100, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=17889
2023-01-11 05:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:02:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:12 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 100000 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0288, wps=76.9, ups=0.47, wpb=109.4, bsz=40, num_updates=2110, lr=2.6375e-05, gnorm=1.546, clip=100, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=17910
2023-01-11 05:03:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:33 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0115, wps=77.8, ups=0.46, wpb=111.5, bsz=40, num_updates=2120, lr=2.65e-05, gnorm=1.392, clip=80, loss_scale=1024, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=17932
2023-01-11 05:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:03:56 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0211, wps=74.4, ups=0.45, wpb=109.4, bsz=40, num_updates=2130, lr=2.6625e-05, gnorm=1.551, clip=100, loss_scale=1024, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=17954
2023-01-11 05:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:18 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0532, wps=76.9, ups=0.46, wpb=110.9, bsz=40, num_updates=2140, lr=2.675e-05, gnorm=1.41, clip=90, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=17976
2023-01-11 05:04:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:40 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0102, wps=75.9, ups=0.46, wpb=110.5, bsz=40, num_updates=2150, lr=2.6875e-05, gnorm=1.734, clip=100, loss_scale=1024, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=17998
2023-01-11 05:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:01 - progress_bar.py[line:274] - INFO: epoch 001:   2161 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0312, wps=76.9, ups=0.47, wpb=109.7, bsz=40, num_updates=2160, lr=2.7e-05, gnorm=1.452, clip=90, loss_scale=1024, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=18020
2023-01-11 05:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:23 - progress_bar.py[line:274] - INFO: epoch 001:   2171 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0381, wps=75.8, ups=0.46, wpb=110.4, bsz=40, num_updates=2170, lr=2.7125e-05, gnorm=1.341, clip=80, loss_scale=1024, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=18042
2023-01-11 05:05:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:46 - progress_bar.py[line:274] - INFO: epoch 001:   2181 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0526, wps=74.6, ups=0.45, wpb=109.8, bsz=40, num_updates=2180, lr=2.725e-05, gnorm=1.207, clip=60, loss_scale=1024, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=18064
2023-01-11 05:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:07 - progress_bar.py[line:274] - INFO: epoch 001:   2191 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0215, wps=76.5, ups=0.46, wpb=109.9, bsz=40, num_updates=2190, lr=2.7375e-05, gnorm=1.354, clip=80, loss_scale=1024, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=18086
2023-01-11 05:06:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:30 - progress_bar.py[line:274] - INFO: epoch 001:   2201 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0306, wps=74.3, ups=0.45, wpb=110.3, bsz=40, num_updates=2200, lr=2.75e-05, gnorm=1.517, clip=90, loss_scale=1024, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=18109
2023-01-11 05:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:51 - progress_bar.py[line:274] - INFO: epoch 001:   2211 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0204, wps=77.3, ups=0.47, wpb=110, bsz=40, num_updates=2210, lr=2.7625e-05, gnorm=1.513, clip=80, loss_scale=1024, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=18130
2023-01-11 05:06:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:13 - progress_bar.py[line:274] - INFO: epoch 001:   2221 / 100000 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0, wps=79, ups=0.47, wpb=111.7, bsz=40, num_updates=2220, lr=2.775e-05, gnorm=1.479, clip=90, loss_scale=1024, train_wall=21, gb_free=10.7, ema_decay=0.9999, wall=18152
2023-01-11 05:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:35 - progress_bar.py[line:274] - INFO: epoch 001:   2231 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0273, wps=74.9, ups=0.46, wpb=107.7, bsz=40, num_updates=2230, lr=2.7875e-05, gnorm=1.389, clip=80, loss_scale=1024, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=18173
2023-01-11 05:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:07:57 - progress_bar.py[line:274] - INFO: epoch 001:   2241 / 100000 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0, wps=74.3, ups=0.45, wpb=109.1, bsz=40, num_updates=2240, lr=2.8e-05, gnorm=1.667, clip=100, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18196
2023-01-11 05:08:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:19 - progress_bar.py[line:274] - INFO: epoch 001:   2251 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0202, wps=76.4, ups=0.46, wpb=110.8, bsz=40, num_updates=2250, lr=2.8125e-05, gnorm=2.05, clip=90, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18218
2023-01-11 05:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:41 - progress_bar.py[line:274] - INFO: epoch 001:   2261 / 100000 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0194, wps=75.6, ups=0.46, wpb=110.7, bsz=40, num_updates=2260, lr=2.825e-05, gnorm=1.559, clip=90, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18240
2023-01-11 05:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:08:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:03 - progress_bar.py[line:274] - INFO: epoch 001:   2271 / 100000 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0, wps=75.9, ups=0.46, wpb=109.9, bsz=40, num_updates=2270, lr=2.8375e-05, gnorm=1.709, clip=90, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18262
2023-01-11 05:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:25 - progress_bar.py[line:274] - INFO: epoch 001:   2281 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0, wps=76.6, ups=0.46, wpb=110.1, bsz=40, num_updates=2280, lr=2.85e-05, gnorm=1.25, clip=90, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18284
2023-01-11 05:09:29 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 05:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:48 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0192, wps=70.9, ups=0.44, wpb=108.5, bsz=40, num_updates=2290, lr=2.8625e-05, gnorm=1.613, clip=90, loss_scale=512, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=18307
2023-01-11 05:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:10 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0404, wps=77, ups=0.47, wpb=110.3, bsz=40, num_updates=2300, lr=2.875e-05, gnorm=1.523, clip=70, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=18329
2023-01-11 05:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:31 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 100000 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0185, wps=76.6, ups=0.47, wpb=108.3, bsz=40, num_updates=2310, lr=2.8875e-05, gnorm=1.293, clip=70, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=18350
2023-01-11 05:10:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:10:53 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 100000 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0686, wps=76.1, ups=0.46, wpb=110.2, bsz=40, num_updates=2320, lr=2.9e-05, gnorm=1.592, clip=90, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=18372
2023-01-11 05:10:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:15 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 100000 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.051, wps=77.1, ups=0.47, wpb=110.3, bsz=40, num_updates=2330, lr=2.9125e-05, gnorm=1.342, clip=80, loss_scale=512, train_wall=21, gb_free=10.7, ema_decay=0.9999, wall=18394
2023-01-11 05:11:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:37 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 100000 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0213, wps=74.8, ups=0.45, wpb=110.5, bsz=40, num_updates=2340, lr=2.925e-05, gnorm=1.502, clip=90, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=18416
2023-01-11 05:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:11:59 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0286, wps=74.6, ups=0.46, wpb=108.9, bsz=40, num_updates=2350, lr=2.9375e-05, gnorm=1.524, clip=100, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=18438
2023-01-11 05:12:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:22 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0288, wps=71.5, ups=0.44, wpb=108.9, bsz=40, num_updates=2360, lr=2.95e-05, gnorm=1.461, clip=80, loss_scale=512, train_wall=23, gb_free=10.4, ema_decay=0.9999, wall=18461
2023-01-11 05:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:44 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 100000 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0196, wps=75.8, ups=0.46, wpb=110, bsz=40, num_updates=2370, lr=2.9625e-05, gnorm=1.383, clip=90, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=18483
2023-01-11 05:12:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:12:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:06 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0189, wps=75.3, ups=0.46, wpb=109.2, bsz=40, num_updates=2380, lr=2.975e-05, gnorm=1.26, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=18505
2023-01-11 05:13:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:28 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 100000 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0388, wps=75.6, ups=0.46, wpb=108.4, bsz=40, num_updates=2390, lr=2.9875e-05, gnorm=1.807, clip=80, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=18527
2023-01-11 05:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:50 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 100000 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0, wps=76.7, ups=0.47, wpb=109, bsz=40, num_updates=2400, lr=3e-05, gnorm=1.387, clip=90, loss_scale=512, train_wall=21, gb_free=10.5, ema_decay=0.9999, wall=18548
2023-01-11 05:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:13:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:12 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.03, wps=76.4, ups=0.46, wpb=110.1, bsz=40, num_updates=2410, lr=3.0125e-05, gnorm=1.375, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=18570
2023-01-11 05:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:33 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 100000 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0103, wps=76.7, ups=0.47, wpb=109.7, bsz=40, num_updates=2420, lr=3.025e-05, gnorm=1.28, clip=80, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=18592
2023-01-11 05:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:14:55 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.05, wps=75.5, ups=0.46, wpb=109, bsz=40, num_updates=2430, lr=3.0375e-05, gnorm=1.211, clip=80, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=18614
2023-01-11 05:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:17 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 100000 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0095, wps=75.7, ups=0.46, wpb=109.5, bsz=40, num_updates=2440, lr=3.05e-05, gnorm=1.442, clip=90, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=18636
2023-01-11 05:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:39 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0294, wps=75.1, ups=0.45, wpb=110.4, bsz=40, num_updates=2450, lr=3.0625e-05, gnorm=1.292, clip=80, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=18658
2023-01-11 05:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:02 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 100000 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0194, wps=73.7, ups=0.45, wpb=109.3, bsz=40, num_updates=2460, lr=3.075e-05, gnorm=1.299, clip=80, loss_scale=512, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=18681
2023-01-11 05:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:24 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.022, wps=74.5, ups=0.45, wpb=110.1, bsz=40, num_updates=2470, lr=3.0875e-05, gnorm=1.514, clip=90, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=18703
2023-01-11 05:16:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:46 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 100000 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0103, wps=76.5, ups=0.47, wpb=109, bsz=40, num_updates=2480, lr=3.1e-05, gnorm=1.297, clip=90, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=18725
2023-01-11 05:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:09 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0215, wps=73.5, ups=0.44, wpb=110.4, bsz=40, num_updates=2490, lr=3.1125e-05, gnorm=1.646, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18748
2023-01-11 05:17:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:31 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 100000 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0377, wps=76.1, ups=0.46, wpb=109.9, bsz=40, num_updates=2500, lr=3.125e-05, gnorm=1.399, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18769
2023-01-11 05:17:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:17:53 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 100000 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0412, wps=75.8, ups=0.46, wpb=109.5, bsz=40, num_updates=2510, lr=3.1375e-05, gnorm=1.833, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18791
2023-01-11 05:17:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:15 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0, wps=74.7, ups=0.45, wpb=109.7, bsz=40, num_updates=2520, lr=3.15e-05, gnorm=1.404, clip=90, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=18814
2023-01-11 05:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:37 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.023, wps=76.7, ups=0.46, wpb=110.7, bsz=40, num_updates=2530, lr=3.1625e-05, gnorm=1.61, clip=100, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=18836
2023-01-11 05:18:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:18:59 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0103, wps=77, ups=0.46, wpb=110.5, bsz=40, num_updates=2540, lr=3.175e-05, gnorm=1.357, clip=70, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=18857
2023-01-11 05:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:21 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0215, wps=74.6, ups=0.45, wpb=109.7, bsz=40, num_updates=2550, lr=3.1875e-05, gnorm=1.459, clip=100, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=18880
2023-01-11 05:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:43 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0495, wps=75, ups=0.45, wpb=110.9, bsz=40, num_updates=2560, lr=3.2e-05, gnorm=1.376, clip=90, loss_scale=512, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=18902
2023-01-11 05:19:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:06 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0698, wps=74.8, ups=0.45, wpb=110.6, bsz=40, num_updates=2570, lr=3.2125e-05, gnorm=1.027, clip=50, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18924
2023-01-11 05:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:28 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0202, wps=75.3, ups=0.46, wpb=109.3, bsz=40, num_updates=2580, lr=3.225e-05, gnorm=1.27, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=18946
2023-01-11 05:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:50 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0185, wps=74.9, ups=0.46, wpb=108.9, bsz=40, num_updates=2590, lr=3.2375e-05, gnorm=1.17, clip=70, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=18969
2023-01-11 05:20:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:12 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.01, wps=75.6, ups=0.46, wpb=109.3, bsz=40, num_updates=2600, lr=3.25e-05, gnorm=1.578, clip=100, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=18990
2023-01-11 05:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0196, wps=77.1, ups=0.47, wpb=110, bsz=40, num_updates=2610, lr=3.2625e-05, gnorm=1.336, clip=70, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=19012
2023-01-11 05:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:21:56 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 100000 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.044, wps=74.5, ups=0.45, wpb=110.7, bsz=40, num_updates=2620, lr=3.275e-05, gnorm=1.462, clip=100, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=19035
2023-01-11 05:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:18 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.03, wps=74.7, ups=0.46, wpb=109.3, bsz=40, num_updates=2630, lr=3.2875e-05, gnorm=1.429, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=19057
2023-01-11 05:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:40 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0215, wps=77.6, ups=0.47, wpb=110, bsz=40, num_updates=2640, lr=3.3e-05, gnorm=1.301, clip=90, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=19078
2023-01-11 05:22:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:01 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.01, wps=76.7, ups=0.47, wpb=109.5, bsz=40, num_updates=2650, lr=3.3125e-05, gnorm=1.939, clip=90, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=19100
2023-01-11 05:23:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:23 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 100000 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0196, wps=77.1, ups=0.47, wpb=109.9, bsz=40, num_updates=2660, lr=3.325e-05, gnorm=1.254, clip=90, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=19122
2023-01-11 05:23:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:45 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.009, wps=74, ups=0.45, wpb=109.1, bsz=40, num_updates=2670, lr=3.3375e-05, gnorm=1.539, clip=70, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=19144
2023-01-11 05:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:08 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 100000 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0273, wps=72.9, ups=0.45, wpb=108.2, bsz=40, num_updates=2680, lr=3.35e-05, gnorm=1.767, clip=100, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=19167
2023-01-11 05:24:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:30 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 100000 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0319, wps=74.5, ups=0.46, wpb=108.6, bsz=40, num_updates=2690, lr=3.3625e-05, gnorm=1.906, clip=100, loss_scale=512, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=19189
2023-01-11 05:24:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:52 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 100000 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0, wps=75.3, ups=0.46, wpb=109.6, bsz=40, num_updates=2700, lr=3.375e-05, gnorm=1.658, clip=90, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=19211
2023-01-11 05:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:14 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.04, wps=76, ups=0.46, wpb=109.4, bsz=40, num_updates=2710, lr=3.3875e-05, gnorm=1.37, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=19233
2023-01-11 05:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:36 - progress_bar.py[line:274] - INFO: epoch 001:   2722 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0097, wps=75.8, ups=0.47, wpb=108.1, bsz=40, num_updates=2720, lr=3.4e-05, gnorm=1.461, clip=80, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=19254
2023-01-11 05:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:25:58 - progress_bar.py[line:274] - INFO: epoch 001:   2732 / 100000 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0183, wps=75.1, ups=0.46, wpb=109.5, bsz=40, num_updates=2730, lr=3.4125e-05, gnorm=1.417, clip=80, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=19276
2023-01-11 05:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:19 - progress_bar.py[line:274] - INFO: epoch 001:   2742 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0291, wps=76.6, ups=0.46, wpb=110.1, bsz=40, num_updates=2740, lr=3.425e-05, gnorm=1.29, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=19298
2023-01-11 05:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:41 - progress_bar.py[line:274] - INFO: epoch 001:   2752 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0297, wps=77.4, ups=0.47, wpb=110.5, bsz=40, num_updates=2750, lr=3.4375e-05, gnorm=1.331, clip=70, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=19320
2023-01-11 05:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:04 - progress_bar.py[line:274] - INFO: epoch 001:   2762 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0, wps=72.4, ups=0.44, wpb=109.1, bsz=40, num_updates=2760, lr=3.45e-05, gnorm=1.203, clip=70, loss_scale=512, train_wall=23, gb_free=10.6, ema_decay=0.9999, wall=19343
2023-01-11 05:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:27 - progress_bar.py[line:274] - INFO: epoch 001:   2772 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0, wps=74.5, ups=0.45, wpb=110.5, bsz=40, num_updates=2770, lr=3.4625e-05, gnorm=1.582, clip=80, loss_scale=512, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=19365
2023-01-11 05:27:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:48 - progress_bar.py[line:274] - INFO: epoch 001:   2782 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0, wps=78.5, ups=0.48, wpb=110.1, bsz=40, num_updates=2780, lr=3.475e-05, gnorm=1.229, clip=60, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=19386
2023-01-11 05:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:10 - progress_bar.py[line:274] - INFO: epoch 001:   2792 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0472, wps=75.3, ups=0.46, wpb=110.3, bsz=40, num_updates=2790, lr=3.4875e-05, gnorm=1.363, clip=90, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=19409
2023-01-11 05:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:32 - progress_bar.py[line:274] - INFO: epoch 001:   2802 / 100000 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0175, wps=74.2, ups=0.46, wpb=108.5, bsz=40, num_updates=2800, lr=3.5e-05, gnorm=1.882, clip=100, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=19431
2023-01-11 05:28:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:28:54 - progress_bar.py[line:274] - INFO: epoch 001:   2812 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0, wps=74.8, ups=0.45, wpb=109.9, bsz=40, num_updates=2810, lr=3.5125e-05, gnorm=1.368, clip=80, loss_scale=1024, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=19453
2023-01-11 05:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:16 - progress_bar.py[line:274] - INFO: epoch 001:   2822 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0179, wps=74.7, ups=0.46, wpb=108.1, bsz=40, num_updates=2820, lr=3.525e-05, gnorm=1.381, clip=60, loss_scale=1024, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=19475
2023-01-11 05:29:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:39 - progress_bar.py[line:274] - INFO: epoch 001:   2832 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0388, wps=75.1, ups=0.45, wpb=110.4, bsz=40, num_updates=2830, lr=3.5375e-05, gnorm=1.631, clip=100, loss_scale=1024, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=19497
2023-01-11 05:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:01 - progress_bar.py[line:274] - INFO: epoch 001:   2842 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0345, wps=76.3, ups=0.46, wpb=110.5, bsz=40, num_updates=2840, lr=3.55e-05, gnorm=1.281, clip=80, loss_scale=1024, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=19519
2023-01-11 05:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:22 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 05:30:24 - progress_bar.py[line:274] - INFO: epoch 001:   2853 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0288, wps=71.3, ups=0.43, wpb=109.9, bsz=40, num_updates=2850, lr=3.5625e-05, gnorm=1.82, clip=80, loss_scale=512, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=19543
2023-01-11 05:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:46 - progress_bar.py[line:274] - INFO: epoch 001:   2863 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0098, wps=77.2, ups=0.47, wpb=110.2, bsz=40, num_updates=2860, lr=3.575e-05, gnorm=1.357, clip=80, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=19564
2023-01-11 05:30:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:08 - progress_bar.py[line:274] - INFO: epoch 001:   2873 / 100000 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0104, wps=75.7, ups=0.46, wpb=109.8, bsz=40, num_updates=2870, lr=3.5875e-05, gnorm=1.691, clip=100, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=19586
2023-01-11 05:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:29 - progress_bar.py[line:274] - INFO: epoch 001:   2883 / 100000 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0303, wps=77.2, ups=0.47, wpb=110.4, bsz=40, num_updates=2880, lr=3.6e-05, gnorm=1.839, clip=90, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=19608
2023-01-11 05:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:51 - progress_bar.py[line:274] - INFO: epoch 001:   2893 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0104, wps=76, ups=0.46, wpb=109.6, bsz=40, num_updates=2890, lr=3.6125e-05, gnorm=1.56, clip=90, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=19630
2023-01-11 05:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:12 - progress_bar.py[line:274] - INFO: epoch 001:   2903 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0194, wps=80.1, ups=0.48, wpb=110.8, bsz=40, num_updates=2900, lr=3.625e-05, gnorm=1.431, clip=80, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=19651
2023-01-11 05:32:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:34 - progress_bar.py[line:274] - INFO: epoch 001:   2913 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0465, wps=76, ups=0.45, wpb=111.7, bsz=40, num_updates=2910, lr=3.6375e-05, gnorm=1.57, clip=70, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=19673
2023-01-11 05:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:32:56 - progress_bar.py[line:274] - INFO: epoch 001:   2923 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0543, wps=76.8, ups=0.46, wpb=110.4, bsz=40, num_updates=2920, lr=3.65e-05, gnorm=1.644, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=19695
2023-01-11 05:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:18 - progress_bar.py[line:274] - INFO: epoch 001:   2933 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0198, wps=76.1, ups=0.46, wpb=111, bsz=40, num_updates=2930, lr=3.6625e-05, gnorm=1.238, clip=50, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=19717
2023-01-11 05:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:40 - progress_bar.py[line:274] - INFO: epoch 001:   2943 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0531, wps=76.9, ups=0.46, wpb=110.7, bsz=40, num_updates=2940, lr=3.675e-05, gnorm=1.224, clip=80, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=19739
2023-01-11 05:33:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:02 - progress_bar.py[line:274] - INFO: epoch 001:   2953 / 100000 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0294, wps=76.6, ups=0.46, wpb=110.2, bsz=40, num_updates=2950, lr=3.6875e-05, gnorm=1.209, clip=70, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=19761
2023-01-11 05:34:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:23 - progress_bar.py[line:274] - INFO: epoch 001:   2963 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0404, wps=78.1, ups=0.47, wpb=110.8, bsz=40, num_updates=2960, lr=3.7e-05, gnorm=1.756, clip=80, loss_scale=512, train_wall=21, gb_free=10.5, ema_decay=0.9999, wall=19782
2023-01-11 05:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:45 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0288, wps=76.5, ups=0.46, wpb=109.7, bsz=40, num_updates=2970, lr=3.7125e-05, gnorm=1.369, clip=80, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=19804
2023-01-11 05:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:08 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0421, wps=75.2, ups=0.45, wpb=111.2, bsz=40, num_updates=2980, lr=3.725e-05, gnorm=1.312, clip=80, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=19826
2023-01-11 05:35:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:30 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0189, wps=73, ups=0.44, wpb=109.8, bsz=40, num_updates=2990, lr=3.7375e-05, gnorm=1.703, clip=90, loss_scale=512, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=19849
2023-01-11 05:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 05:35:53 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 100000 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0098, wps=74.4, ups=0.45, wpb=109.9, bsz=40, num_updates=3000, lr=3.75e-05, gnorm=1.572, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=19872
2023-01-11 05:35:53 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 05:35:54 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 05:35:54 - train.py[line:551] - INFO: load:0.86 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 05:39:21 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 05:39:21 - train.py[line:551] - INFO: load:0.88 valid_run:207.10 task_valid:204.51 collect_output:0.57
2023-01-11 05:42:44 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 05:42:44 - train.py[line:551] - INFO: load:0.91 valid_run:410.29 task_valid:405.19 collect_output:1.11
2023-01-11 05:46:09 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 05:46:09 - train.py[line:551] - INFO: load:0.93 valid_run:614.78 task_valid:607.14 collect_output:1.65
2023-01-11 05:49:30 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 05:49:30 - train.py[line:551] - INFO: load:0.96 valid_run:815.29 task_valid:805.13 collect_output:2.18
2023-01-11 05:52:55 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 05:52:55 - train.py[line:551] - INFO: load:0.99 valid_run:1020.36 task_valid:1007.68 collect_output:2.71
2023-01-11 05:56:22 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 05:56:22 - train.py[line:551] - INFO: load:1.01 valid_run:1227.18 task_valid:1211.97 collect_output:3.25
2023-01-11 05:59:47 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 05:59:47 - train.py[line:551] - INFO: load:1.04 valid_run:1432.39 task_valid:1414.66 collect_output:3.78
2023-01-11 06:03:11 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 06:03:11 - train.py[line:551] - INFO: load:1.07 valid_run:1636.25 task_valid:1616.03 collect_output:4.31
2023-01-11 06:06:35 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 06:06:35 - train.py[line:551] - INFO: load:1.09 valid_run:1840.52 task_valid:1817.77 collect_output:4.85
2023-01-11 06:09:55 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 06:09:55 - train.py[line:551] - INFO: load:1.12 valid_run:2040.27 task_valid:2014.99 collect_output:5.39
2023-01-11 06:13:19 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 06:13:19 - train.py[line:551] - INFO: load:1.15 valid_run:2243.71 task_valid:2215.88 collect_output:5.94
2023-01-11 06:16:43 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 06:16:43 - train.py[line:551] - INFO: load:1.18 valid_run:2447.86 task_valid:2417.47 collect_output:6.49
2023-01-11 06:20:03 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 06:20:03 - train.py[line:551] - INFO: load:1.20 valid_run:2647.87 task_valid:2614.95 collect_output:7.03
2023-01-11 06:23:28 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 06:23:28 - train.py[line:551] - INFO: load:1.23 valid_run:2852.45 task_valid:2816.97 collect_output:7.59
2023-01-11 06:26:51 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 06:26:51 - train.py[line:551] - INFO: load:1.26 valid_run:3055.54 task_valid:3017.53 collect_output:8.13
2023-01-11 06:30:12 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 06:30:12 - train.py[line:551] - INFO: load:1.28 valid_run:3256.52 task_valid:3215.94 collect_output:8.69
2023-01-11 06:33:36 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 06:33:36 - train.py[line:551] - INFO: load:1.31 valid_run:3460.31 task_valid:3417.16 collect_output:9.24
2023-01-11 06:37:01 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 06:37:01 - train.py[line:551] - INFO: load:1.34 valid_run:3666.09 task_valid:3620.38 collect_output:9.81
2023-01-11 06:40:26 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 06:40:26 - train.py[line:551] - INFO: load:1.36 valid_run:3870.91 task_valid:3822.61 collect_output:10.37
2023-01-11 06:43:50 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 06:43:50 - train.py[line:551] - INFO: load:1.39 valid_run:4074.60 task_valid:4023.74 collect_output:10.93
2023-01-11 06:47:13 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 06:47:13 - train.py[line:551] - INFO: load:1.42 valid_run:4277.86 task_valid:4224.44 collect_output:11.49
2023-01-11 06:50:41 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 06:50:41 - train.py[line:551] - INFO: load:1.45 valid_run:4485.61 task_valid:4429.63 collect_output:12.05
2023-01-11 06:54:03 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 06:54:03 - train.py[line:551] - INFO: load:1.47 valid_run:4686.86 task_valid:4628.34 collect_output:12.61
2023-01-11 06:57:27 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 06:57:27 - train.py[line:551] - INFO: load:1.50 valid_run:4890.80 task_valid:4829.68 collect_output:13.18
2023-01-11 07:00:51 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 07:00:51 - train.py[line:551] - INFO: load:1.53 valid_run:5094.56 task_valid:5030.89 collect_output:13.74
2023-01-11 07:04:14 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 07:04:14 - train.py[line:551] - INFO: load:1.55 valid_run:5298.34 task_valid:5232.07 collect_output:14.30
2023-01-11 07:07:35 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 07:07:35 - train.py[line:551] - INFO: load:1.58 valid_run:5498.50 task_valid:5429.65 collect_output:14.87
2023-01-11 07:11:03 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 07:11:03 - train.py[line:551] - INFO: load:1.61 valid_run:5706.33 task_valid:5634.91 collect_output:15.44
2023-01-11 07:14:26 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 07:14:26 - train.py[line:551] - INFO: load:1.63 valid_run:5909.22 task_valid:5835.24 collect_output:16.01
2023-01-11 07:17:52 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 07:17:52 - train.py[line:551] - INFO: load:1.66 valid_run:6115.64 task_valid:6039.07 collect_output:16.57
2023-01-11 07:21:19 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 07:21:19 - train.py[line:551] - INFO: load:1.69 valid_run:6322.05 task_valid:6242.89 collect_output:17.15

====================================================================================================
SGG eval:     R @ 50: 0.4760;     R @ 100: 0.5313;     R @ 500: 0.5866;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2392;    mR @ 100: 0.3142;    mR @ 500: 0.3758;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3122) (covered in:0.0625) (covering:0.5143) (eating:0.7059) (flying in:0.6364) (growing on:0.2500) (hanging from:0.4839) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.7725) (says:0.0000) (sitting on:0.6341) (standing on:0.5350) (using:0.3000) (walking in:0.0000) (walking on:0.2838) (watching:0.0694) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4760;     R @ 100: 0.5313;     R @ 500: 0.5866;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2392;    mR @ 100: 0.3142;    mR @ 500: 0.3758;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3122) (covered in:0.0625) (covering:0.5143) (eating:0.7059) (flying in:0.6364) (growing on:0.2500) (hanging from:0.4839) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.7725) (says:0.0000) (sitting on:0.6341) (standing on:0.5350) (using:0.3000) (walking in:0.0000) (walking on:0.2838) (watching:0.0694) 
--------------------------------------------------------
====================================================================================================

2023-01-11 07:22:04 - train.py[line:487] - INFO: 0.5313025974025973
2023-01-11 07:22:04 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 07:22:04 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.293 | loss_v1 0 | loss_v2 0 | nll_loss 0.135 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.531303 | ppl 1.1 | vqa_score 0.2748 | wps 70.4 | wpb 72 | bsz 24 | num_updates 3000 | best_R@100 0.531303
2023-01-11 07:22:04 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3000 updates
2023-01-11 07:22:04 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_3000.pt
2023-01-11 07:22:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_3000.pt
2023-01-11 07:25:38 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 0.5313025974025973) (writing took 213.71468507684767 seconds)
2023-01-11 07:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:25:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:00 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0632, wps=0.2, ups=0, wpb=109.8, bsz=40, num_updates=3010, lr=3.7625e-05, gnorm=1.179, clip=60, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=26479
2023-01-11 07:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:23 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0297, wps=74.1, ups=0.45, wpb=109.9, bsz=40, num_updates=3020, lr=3.775e-05, gnorm=1.195, clip=60, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=26501
2023-01-11 07:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:45 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 100000 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.01, wps=74.4, ups=0.45, wpb=109.3, bsz=40, num_updates=3030, lr=3.7875e-05, gnorm=1.318, clip=70, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=26524
2023-01-11 07:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:26:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:06 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0099, wps=78.3, ups=0.47, wpb=110.4, bsz=40, num_updates=3040, lr=3.8e-05, gnorm=1.421, clip=90, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=26545
2023-01-11 07:27:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:28 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 100000 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0561, wps=75.8, ups=0.46, wpb=109.5, bsz=40, num_updates=3050, lr=3.8125e-05, gnorm=1.366, clip=60, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=26567
2023-01-11 07:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:51 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0294, wps=74.5, ups=0.45, wpb=109.7, bsz=40, num_updates=3060, lr=3.825e-05, gnorm=1.794, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=26589
2023-01-11 07:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0309, wps=77.5, ups=0.47, wpb=111.1, bsz=40, num_updates=3070, lr=3.8375e-05, gnorm=1.944, clip=80, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=26611
2023-01-11 07:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:35 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0202, wps=72.3, ups=0.45, wpb=107, bsz=40, num_updates=3080, lr=3.85e-05, gnorm=1.516, clip=90, loss_scale=512, train_wall=22, gb_free=10.7, ema_decay=0.9999, wall=26633
2023-01-11 07:28:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:28:57 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0114, wps=76.4, ups=0.46, wpb=110.9, bsz=40, num_updates=3090, lr=3.8625e-05, gnorm=1.331, clip=70, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=26655
2023-01-11 07:28:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:18 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 100000 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0404, wps=76.6, ups=0.47, wpb=108.1, bsz=40, num_updates=3100, lr=3.875e-05, gnorm=1.314, clip=70, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=26677
2023-01-11 07:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:40 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0227, wps=77.1, ups=0.46, wpb=111.7, bsz=40, num_updates=3110, lr=3.8875e-05, gnorm=1.839, clip=100, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=26699
2023-01-11 07:29:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:02 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0208, wps=75.5, ups=0.46, wpb=109.5, bsz=40, num_updates=3120, lr=3.9e-05, gnorm=1.428, clip=80, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=26721
2023-01-11 07:30:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:24 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0306, wps=74.5, ups=0.45, wpb=109.6, bsz=40, num_updates=3130, lr=3.9125e-05, gnorm=1.275, clip=60, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=26743
2023-01-11 07:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:47 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 100000 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0632, wps=74.2, ups=0.45, wpb=109.7, bsz=40, num_updates=3140, lr=3.925e-05, gnorm=1.256, clip=90, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=26766
2023-01-11 07:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:09 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 100000 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0306, wps=75.1, ups=0.45, wpb=110.1, bsz=40, num_updates=3150, lr=3.9375e-05, gnorm=1.242, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=26788
2023-01-11 07:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:32 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 100000 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0449, wps=75.5, ups=0.45, wpb=111.9, bsz=40, num_updates=3160, lr=3.95e-05, gnorm=1.495, clip=80, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=26810
2023-01-11 07:31:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:53 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0309, wps=77.2, ups=0.46, wpb=111.3, bsz=40, num_updates=3170, lr=3.9625e-05, gnorm=1.431, clip=80, loss_scale=512, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=26832
2023-01-11 07:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:16 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.011, wps=75.8, ups=0.46, wpb=110.7, bsz=40, num_updates=3180, lr=3.975e-05, gnorm=1.28, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=26854
2023-01-11 07:32:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:38 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0396, wps=75.9, ups=0.46, wpb=109.7, bsz=40, num_updates=3190, lr=3.9875e-05, gnorm=1.337, clip=60, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=26876
2023-01-11 07:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:32:59 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.023, wps=76.1, ups=0.46, wpb=110.1, bsz=40, num_updates=3200, lr=4e-05, gnorm=1.587, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=26898
2023-01-11 07:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:21 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0092, wps=77.3, ups=0.47, wpb=108.7, bsz=40, num_updates=3210, lr=4.0125e-05, gnorm=1.265, clip=80, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=26919
2023-01-11 07:33:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:43 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0396, wps=73.9, ups=0.45, wpb=108.5, bsz=40, num_updates=3220, lr=4.025e-05, gnorm=1.368, clip=70, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=26942
2023-01-11 07:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:05 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.022, wps=75.9, ups=0.46, wpb=110.5, bsz=40, num_updates=3230, lr=4.0375e-05, gnorm=1.426, clip=90, loss_scale=512, train_wall=22, gb_free=9.5, ema_decay=0.9999, wall=26964
2023-01-11 07:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:27 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.055, wps=76.3, ups=0.46, wpb=110.7, bsz=40, num_updates=3240, lr=4.05e-05, gnorm=1.417, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=26986
2023-01-11 07:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:49 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0215, wps=76.6, ups=0.46, wpb=110.8, bsz=40, num_updates=3250, lr=4.0625e-05, gnorm=1.257, clip=80, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=27008
2023-01-11 07:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:10 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0306, wps=77.8, ups=0.48, wpb=108.3, bsz=40, num_updates=3260, lr=4.075e-05, gnorm=1.088, clip=70, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=27029
2023-01-11 07:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:32 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.037, wps=76.1, ups=0.46, wpb=109.7, bsz=40, num_updates=3270, lr=4.0875e-05, gnorm=1.181, clip=60, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27051
2023-01-11 07:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:35:54 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0583, wps=75.3, ups=0.46, wpb=110.2, bsz=40, num_updates=3280, lr=4.1e-05, gnorm=1.211, clip=60, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=27073
2023-01-11 07:35:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:16 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0105, wps=77, ups=0.46, wpb=111, bsz=40, num_updates=3290, lr=4.1125e-05, gnorm=1.483, clip=70, loss_scale=512, train_wall=22, gb_free=9.6, ema_decay=0.9999, wall=27095
2023-01-11 07:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:39 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0515, wps=75.1, ups=0.45, wpb=111.4, bsz=40, num_updates=3300, lr=4.125e-05, gnorm=1.393, clip=60, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=27117
2023-01-11 07:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:01 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0194, wps=72.7, ups=0.45, wpb=108.9, bsz=40, num_updates=3310, lr=4.1375e-05, gnorm=2.384, clip=70, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=27140
2023-01-11 07:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:23 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0465, wps=77.6, ups=0.46, wpb=112.2, bsz=40, num_updates=3320, lr=4.15e-05, gnorm=1.101, clip=60, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=27162
2023-01-11 07:37:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:45 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0571, wps=78, ups=0.47, wpb=110.6, bsz=40, num_updates=3330, lr=4.1625e-05, gnorm=1.385, clip=60, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=27184
2023-01-11 07:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:06 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 100000 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0283, wps=78.3, ups=0.47, wpb=110.9, bsz=40, num_updates=3340, lr=4.175e-05, gnorm=1.385, clip=60, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=27205
2023-01-11 07:38:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:28 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.04, wps=76.4, ups=0.46, wpb=109.6, bsz=40, num_updates=3350, lr=4.1875e-05, gnorm=1.425, clip=60, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=27227
2023-01-11 07:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:50 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.05, wps=76.4, ups=0.46, wpb=111.9, bsz=40, num_updates=3360, lr=4.2e-05, gnorm=1.305, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=27249
2023-01-11 07:38:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:13 - progress_bar.py[line:274] - INFO: epoch 001:   3373 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0103, wps=73.5, ups=0.44, wpb=111.1, bsz=40, num_updates=3370, lr=4.2125e-05, gnorm=1.664, clip=90, loss_scale=1024, train_wall=23, gb_free=10.2, ema_decay=0.9999, wall=27272
2023-01-11 07:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:35 - progress_bar.py[line:274] - INFO: epoch 001:   3383 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0096, wps=77.3, ups=0.47, wpb=109.7, bsz=40, num_updates=3380, lr=4.225e-05, gnorm=1.018, clip=50, loss_scale=1024, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=27294
2023-01-11 07:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:39:57 - progress_bar.py[line:274] - INFO: epoch 001:   3393 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0, wps=72.4, ups=0.45, wpb=107.9, bsz=40, num_updates=3390, lr=4.2375e-05, gnorm=1.082, clip=40, loss_scale=1024, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=27316
2023-01-11 07:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:20 - progress_bar.py[line:274] - INFO: epoch 001:   3403 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0568, wps=76.3, ups=0.46, wpb=111.3, bsz=40, num_updates=3400, lr=4.25e-05, gnorm=1.258, clip=60, loss_scale=1024, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27338
2023-01-11 07:40:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:41 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 07:40:43 - progress_bar.py[line:274] - INFO: epoch 001:   3414 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0526, wps=71.1, ups=0.43, wpb=109.1, bsz=40, num_updates=3410, lr=4.2625e-05, gnorm=1.414, clip=90, loss_scale=512, train_wall=23, gb_free=10.3, ema_decay=0.9999, wall=27362
2023-01-11 07:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:05 - progress_bar.py[line:274] - INFO: epoch 001:   3424 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0112, wps=73.7, ups=0.45, wpb=109.3, bsz=40, num_updates=3420, lr=4.275e-05, gnorm=1.069, clip=50, loss_scale=512, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=27384
2023-01-11 07:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:28 - progress_bar.py[line:274] - INFO: epoch 001:   3434 / 100000 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0808, wps=74.2, ups=0.45, wpb=111.2, bsz=40, num_updates=3430, lr=4.2875e-05, gnorm=1.226, clip=50, loss_scale=512, train_wall=22, gb_free=9.9, ema_decay=0.9999, wall=27407
2023-01-11 07:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:50 - progress_bar.py[line:274] - INFO: epoch 001:   3444 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0652, wps=75.5, ups=0.46, wpb=110.3, bsz=40, num_updates=3440, lr=4.3e-05, gnorm=1.096, clip=50, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=27429
2023-01-11 07:41:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:12 - progress_bar.py[line:274] - INFO: epoch 001:   3454 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.01, wps=75.5, ups=0.46, wpb=109.7, bsz=40, num_updates=3450, lr=4.3125e-05, gnorm=1.026, clip=50, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=27451
2023-01-11 07:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:34 - progress_bar.py[line:274] - INFO: epoch 001:   3464 / 100000 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0748, wps=77.5, ups=0.47, wpb=110.4, bsz=40, num_updates=3460, lr=4.325e-05, gnorm=1.559, clip=90, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=27473
2023-01-11 07:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:42:56 - progress_bar.py[line:274] - INFO: epoch 001:   3474 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.044, wps=76.8, ups=0.46, wpb=110.1, bsz=40, num_updates=3470, lr=4.3375e-05, gnorm=1.324, clip=70, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=27494
2023-01-11 07:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:18 - progress_bar.py[line:274] - INFO: epoch 001:   3484 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0096, wps=73, ups=0.44, wpb=109.4, bsz=40, num_updates=3480, lr=4.35e-05, gnorm=1.388, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27517
2023-01-11 07:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:40 - progress_bar.py[line:274] - INFO: epoch 001:   3494 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0099, wps=75.5, ups=0.46, wpb=110, bsz=40, num_updates=3490, lr=4.3625e-05, gnorm=1.417, clip=90, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27539
2023-01-11 07:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:02 - progress_bar.py[line:274] - INFO: epoch 001:   3504 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0208, wps=77.5, ups=0.48, wpb=108.6, bsz=40, num_updates=3500, lr=4.375e-05, gnorm=1.121, clip=50, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=27560
2023-01-11 07:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:24 - progress_bar.py[line:274] - INFO: epoch 001:   3514 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0312, wps=76.3, ups=0.46, wpb=110.7, bsz=40, num_updates=3510, lr=4.3875e-05, gnorm=1.014, clip=40, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27582
2023-01-11 07:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:46 - progress_bar.py[line:274] - INFO: epoch 001:   3524 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0385, wps=75.6, ups=0.46, wpb=108.7, bsz=40, num_updates=3520, lr=4.4e-05, gnorm=1.428, clip=80, loss_scale=512, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=27604
2023-01-11 07:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:08 - progress_bar.py[line:274] - INFO: epoch 001:   3534 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0632, wps=75.7, ups=0.46, wpb=110.2, bsz=40, num_updates=3530, lr=4.4125e-05, gnorm=1.889, clip=90, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=27626
2023-01-11 07:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:29 - progress_bar.py[line:274] - INFO: epoch 001:   3544 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.028, wps=77.1, ups=0.47, wpb=108.9, bsz=40, num_updates=3540, lr=4.425e-05, gnorm=1.24, clip=60, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=27648
2023-01-11 07:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:51 - progress_bar.py[line:274] - INFO: epoch 001:   3554 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0449, wps=76.9, ups=0.46, wpb=111.4, bsz=40, num_updates=3550, lr=4.4375e-05, gnorm=1.755, clip=60, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=27670
2023-01-11 07:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:45:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:13 - progress_bar.py[line:274] - INFO: epoch 001:   3564 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0319, wps=75.1, ups=0.46, wpb=109.5, bsz=40, num_updates=3560, lr=4.45e-05, gnorm=1.461, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=27692
2023-01-11 07:46:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:35 - progress_bar.py[line:274] - INFO: epoch 001:   3574 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0377, wps=75.6, ups=0.46, wpb=110.1, bsz=40, num_updates=3570, lr=4.4625e-05, gnorm=1.28, clip=60, loss_scale=512, train_wall=22, gb_free=10.6, ema_decay=0.9999, wall=27714
2023-01-11 07:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:46:58 - progress_bar.py[line:274] - INFO: epoch 001:   3584 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0096, wps=74.3, ups=0.45, wpb=110.3, bsz=40, num_updates=3580, lr=4.475e-05, gnorm=1.212, clip=60, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27737
2023-01-11 07:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:20 - progress_bar.py[line:274] - INFO: epoch 001:   3594 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0465, wps=74.7, ups=0.45, wpb=110.2, bsz=40, num_updates=3590, lr=4.4875e-05, gnorm=1.43, clip=70, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=27759
2023-01-11 07:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:42 - progress_bar.py[line:274] - INFO: epoch 001:   3604 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0532, wps=76.8, ups=0.46, wpb=110.5, bsz=40, num_updates=3600, lr=4.5e-05, gnorm=1.417, clip=70, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27781
2023-01-11 07:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:04 - progress_bar.py[line:274] - INFO: epoch 001:   3614 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0109, wps=77.9, ups=0.46, wpb=111.7, bsz=40, num_updates=3610, lr=4.5125e-05, gnorm=1.285, clip=70, loss_scale=512, train_wall=21, gb_free=10.5, ema_decay=0.9999, wall=27803
2023-01-11 07:48:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:26 - progress_bar.py[line:274] - INFO: epoch 001:   3624 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0192, wps=76.7, ups=0.46, wpb=110, bsz=40, num_updates=3620, lr=4.525e-05, gnorm=1.427, clip=90, loss_scale=512, train_wall=21, gb_free=10.2, ema_decay=0.9999, wall=27824
2023-01-11 07:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:48 - progress_bar.py[line:274] - INFO: epoch 001:   3634 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.037, wps=75.5, ups=0.46, wpb=109.5, bsz=40, num_updates=3630, lr=4.5375e-05, gnorm=1.112, clip=70, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=27846
2023-01-11 07:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:10 - progress_bar.py[line:274] - INFO: epoch 001:   3644 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.019, wps=75.7, ups=0.46, wpb=108.7, bsz=40, num_updates=3640, lr=4.55e-05, gnorm=1.896, clip=90, loss_scale=512, train_wall=21, gb_free=10.6, ema_decay=0.9999, wall=27868
2023-01-11 07:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:32 - progress_bar.py[line:274] - INFO: epoch 001:   3654 / 100000 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0189, wps=75.2, ups=0.46, wpb=109.4, bsz=40, num_updates=3650, lr=4.5625e-05, gnorm=1.209, clip=60, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=27890
2023-01-11 07:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:54 - progress_bar.py[line:274] - INFO: epoch 001:   3664 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0275, wps=74.1, ups=0.45, wpb=109.1, bsz=40, num_updates=3660, lr=4.575e-05, gnorm=1.166, clip=60, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=27913
2023-01-11 07:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:16 - progress_bar.py[line:274] - INFO: epoch 001:   3674 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0202, wps=75.5, ups=0.46, wpb=109.7, bsz=40, num_updates=3670, lr=4.5875e-05, gnorm=1.493, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=27935
2023-01-11 07:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:38 - progress_bar.py[line:274] - INFO: epoch 001:   3684 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0099, wps=74.5, ups=0.46, wpb=109.1, bsz=40, num_updates=3680, lr=4.6e-05, gnorm=1.272, clip=60, loss_scale=512, train_wall=22, gb_free=10.7, ema_decay=0.9999, wall=27957
2023-01-11 07:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:01 - progress_bar.py[line:274] - INFO: epoch 001:   3694 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0521, wps=74.4, ups=0.45, wpb=109.4, bsz=40, num_updates=3690, lr=4.6125e-05, gnorm=1.494, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=27979
2023-01-11 07:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:23 - progress_bar.py[line:274] - INFO: epoch 001:   3704 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0189, wps=74.1, ups=0.45, wpb=109.7, bsz=40, num_updates=3700, lr=4.625e-05, gnorm=1.517, clip=90, loss_scale=512, train_wall=22, gb_free=10.5, ema_decay=0.9999, wall=28002
2023-01-11 07:51:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:45 - progress_bar.py[line:274] - INFO: epoch 001:   3714 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0648, wps=75.1, ups=0.46, wpb=107.9, bsz=40, num_updates=3710, lr=4.6375e-05, gnorm=1.347, clip=80, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=28023
2023-01-11 07:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:07 - progress_bar.py[line:274] - INFO: epoch 001:   3724 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0333, wps=73, ups=0.45, wpb=108.2, bsz=40, num_updates=3720, lr=4.65e-05, gnorm=1.352, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=28046
2023-01-11 07:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:29 - progress_bar.py[line:274] - INFO: epoch 001:   3734 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.02, wps=77.3, ups=0.47, wpb=109.6, bsz=40, num_updates=3730, lr=4.6625e-05, gnorm=1.342, clip=80, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=28067
2023-01-11 07:52:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:51 - progress_bar.py[line:274] - INFO: epoch 001:   3744 / 100000 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0505, wps=75.7, ups=0.46, wpb=109.2, bsz=40, num_updates=3740, lr=4.675e-05, gnorm=1.383, clip=90, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=28089
2023-01-11 07:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:13 - progress_bar.py[line:274] - INFO: epoch 001:   3754 / 100000 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0377, wps=75.1, ups=0.46, wpb=109.4, bsz=40, num_updates=3750, lr=4.6875e-05, gnorm=1.48, clip=70, loss_scale=512, train_wall=22, gb_free=9.6, ema_decay=0.9999, wall=28112
2023-01-11 07:53:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:35 - progress_bar.py[line:274] - INFO: epoch 001:   3764 / 100000 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0521, wps=75.8, ups=0.46, wpb=109.5, bsz=40, num_updates=3760, lr=4.7e-05, gnorm=1.049, clip=50, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=28133
2023-01-11 07:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:53:57 - progress_bar.py[line:274] - INFO: epoch 001:   3774 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0288, wps=76.3, ups=0.47, wpb=109.1, bsz=40, num_updates=3770, lr=4.7125e-05, gnorm=1.193, clip=60, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=28155
2023-01-11 07:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:18 - progress_bar.py[line:274] - INFO: epoch 001:   3784 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0211, wps=77.4, ups=0.47, wpb=110.3, bsz=40, num_updates=3780, lr=4.725e-05, gnorm=1.249, clip=60, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=28177
2023-01-11 07:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:41 - progress_bar.py[line:274] - INFO: epoch 001:   3794 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0538, wps=75.8, ups=0.46, wpb=110.7, bsz=40, num_updates=3790, lr=4.7375e-05, gnorm=1.289, clip=80, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=28199
2023-01-11 07:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:54:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:03 - progress_bar.py[line:274] - INFO: epoch 001:   3804 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0297, wps=75.4, ups=0.46, wpb=110, bsz=40, num_updates=3800, lr=4.75e-05, gnorm=1.414, clip=70, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=28221
2023-01-11 07:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:25 - progress_bar.py[line:274] - INFO: epoch 001:   3814 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0326, wps=76.1, ups=0.46, wpb=110.5, bsz=40, num_updates=3810, lr=4.7625e-05, gnorm=1.034, clip=60, loss_scale=512, train_wall=22, gb_free=10.4, ema_decay=0.9999, wall=28244
2023-01-11 07:55:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:47 - progress_bar.py[line:274] - INFO: epoch 001:   3824 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0104, wps=75.3, ups=0.45, wpb=110.5, bsz=40, num_updates=3820, lr=4.775e-05, gnorm=1.278, clip=70, loss_scale=512, train_wall=22, gb_free=10, ema_decay=0.9999, wall=28266
2023-01-11 07:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:09 - progress_bar.py[line:274] - INFO: epoch 001:   3834 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0288, wps=76.5, ups=0.47, wpb=109.3, bsz=40, num_updates=3830, lr=4.7875e-05, gnorm=1.458, clip=70, loss_scale=512, train_wall=21, gb_free=10.1, ema_decay=0.9999, wall=28288
2023-01-11 07:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:30 - progress_bar.py[line:274] - INFO: epoch 001:   3844 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0481, wps=78.2, ups=0.47, wpb=110.5, bsz=40, num_updates=3840, lr=4.8e-05, gnorm=1.928, clip=70, loss_scale=512, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=28309
2023-01-11 07:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:52 - progress_bar.py[line:274] - INFO: epoch 001:   3854 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0667, wps=77.1, ups=0.47, wpb=109.9, bsz=40, num_updates=3850, lr=4.8125e-05, gnorm=1.134, clip=50, loss_scale=512, train_wall=21, gb_free=10, ema_decay=0.9999, wall=28331
2023-01-11 07:56:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:14 - progress_bar.py[line:274] - INFO: epoch 001:   3864 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0521, wps=75.9, ups=0.46, wpb=110.1, bsz=40, num_updates=3860, lr=4.825e-05, gnorm=1.1, clip=50, loss_scale=512, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=28353
2023-01-11 07:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:37 - progress_bar.py[line:274] - INFO: epoch 001:   3874 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0189, wps=75.6, ups=0.46, wpb=109.7, bsz=40, num_updates=3870, lr=4.8375e-05, gnorm=1.256, clip=70, loss_scale=512, train_wall=22, gb_free=10.1, ema_decay=0.9999, wall=28375
2023-01-11 07:57:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:57:58 - progress_bar.py[line:274] - INFO: epoch 001:   3884 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0388, wps=78.4, ups=0.48, wpb=109.9, bsz=40, num_updates=3880, lr=4.85e-05, gnorm=1.175, clip=50, loss_scale=512, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=28397
2023-01-11 07:58:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:21 - progress_bar.py[line:274] - INFO: epoch 001:   3894 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.037, wps=76.5, ups=0.46, wpb=110.2, bsz=40, num_updates=3890, lr=4.8625e-05, gnorm=1.247, clip=60, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=28419
2023-01-11 07:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:43 - progress_bar.py[line:274] - INFO: epoch 001:   3904 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0583, wps=73.9, ups=0.46, wpb=107.9, bsz=40, num_updates=3900, lr=4.875e-05, gnorm=1.05, clip=50, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=28442
2023-01-11 07:58:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:06 - progress_bar.py[line:274] - INFO: epoch 001:   3914 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0261, wps=75.1, ups=0.46, wpb=108.9, bsz=40, num_updates=3910, lr=4.8875e-05, gnorm=1.221, clip=70, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=28464
2023-01-11 07:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:28 - progress_bar.py[line:274] - INFO: epoch 001:   3924 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.099, wps=76.4, ups=0.46, wpb=110.2, bsz=40, num_updates=3920, lr=4.9e-05, gnorm=1.419, clip=80, loss_scale=512, train_wall=22, gb_free=10.2, ema_decay=0.9999, wall=28486
2023-01-11 07:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:52 - progress_bar.py[line:274] - INFO: epoch 001:   3934 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0341, wps=73.2, ups=0.44, wpb=110.5, bsz=40, num_updates=3930, lr=4.9125e-05, gnorm=0.967, clip=40, loss_scale=1024, train_wall=23, gb_free=10.1, ema_decay=0.9999, wall=28510
2023-01-11 07:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 07:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:14 - progress_bar.py[line:274] - INFO: epoch 001:   3944 / 100000 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0101, wps=77.3, ups=0.47, wpb=110.7, bsz=40, num_updates=3940, lr=4.925e-05, gnorm=1.529, clip=90, loss_scale=1024, train_wall=21, gb_free=10.5, ema_decay=0.9999, wall=28532
2023-01-11 08:00:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:37 - progress_bar.py[line:274] - INFO: epoch 001:   3954 / 100000 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0561, wps=73.2, ups=0.45, wpb=108, bsz=40, num_updates=3950, lr=4.9375e-05, gnorm=1.058, clip=30, loss_scale=1024, train_wall=22, gb_free=10.3, ema_decay=0.9999, wall=28555
2023-01-11 08:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:00:59 - progress_bar.py[line:274] - INFO: epoch 001:   3964 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0192, wps=75.2, ups=0.46, wpb=108.2, bsz=40, num_updates=3960, lr=4.95e-05, gnorm=1.143, clip=60, loss_scale=1024, train_wall=22, gb_free=9.7, ema_decay=0.9999, wall=28577
2023-01-11 08:01:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:21 - progress_bar.py[line:274] - INFO: epoch 001:   3974 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0472, wps=76.2, ups=0.47, wpb=108.8, bsz=40, num_updates=3970, lr=4.9625e-05, gnorm=1.201, clip=70, loss_scale=1024, train_wall=21, gb_free=10.4, ema_decay=0.9999, wall=28600
2023-01-11 08:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:45 - progress_bar.py[line:274] - INFO: epoch 001:   3984 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0288, wps=72.8, ups=0.45, wpb=108.9, bsz=40, num_updates=3980, lr=4.975e-05, gnorm=1.26, clip=50, loss_scale=1024, train_wall=22, gb_free=9.8, ema_decay=0.9999, wall=28623
2023-01-11 08:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:02:07 - progress_bar.py[line:274] - INFO: epoch 001:   3994 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0549, wps=78.4, ups=0.47, wpb=111.3, bsz=40, num_updates=3990, lr=4.9875e-05, gnorm=0.967, clip=40, loss_scale=1024, train_wall=21, gb_free=10.3, ema_decay=0.9999, wall=28645
2023-01-11 08:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:02:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:02:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 08:02:29 - progress_bar.py[line:274] - INFO: epoch 001:   4004 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0392, wps=76.4, ups=0.47, wpb=108.5, bsz=40, num_updates=4000, lr=5e-05, gnorm=1.266, clip=70, loss_scale=1024, train_wall=21, gb_free=10.6, ema_decay=0.9999, wall=28667
2023-01-11 08:02:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 08:02:30 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 08:02:30 - train.py[line:551] - INFO: load:1.22 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 08:05:58 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 08:05:58 - train.py[line:551] - INFO: load:1.24 valid_run:207.33 task_valid:204.76 collect_output:0.55
2023-01-11 08:09:21 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 08:09:21 - train.py[line:551] - INFO: load:1.27 valid_run:410.30 task_valid:405.24 collect_output:1.08
2023-01-11 08:12:45 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 08:12:45 - train.py[line:551] - INFO: load:1.29 valid_run:614.02 task_valid:606.48 collect_output:1.61
2023-01-11 08:16:05 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 08:16:05 - train.py[line:551] - INFO: load:1.32 valid_run:814.51 task_valid:804.43 collect_output:2.16
2023-01-11 08:19:31 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 08:19:31 - train.py[line:551] - INFO: load:1.34 valid_run:1019.86 task_valid:1007.25 collect_output:2.70
2023-01-11 08:22:58 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 08:22:58 - train.py[line:551] - INFO: load:1.37 valid_run:1227.44 task_valid:1212.26 collect_output:3.25
2023-01-11 08:26:26 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 08:26:26 - train.py[line:551] - INFO: load:1.39 valid_run:1434.50 task_valid:1416.72 collect_output:3.80
2023-01-11 08:29:50 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 08:29:50 - train.py[line:551] - INFO: load:1.42 valid_run:1638.94 task_valid:1618.62 collect_output:4.35
2023-01-11 08:33:15 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 08:33:15 - train.py[line:551] - INFO: load:1.45 valid_run:1843.78 task_valid:1820.88 collect_output:4.91
2023-01-11 08:36:35 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 08:36:36 - train.py[line:551] - INFO: load:1.47 valid_run:2044.21 task_valid:2018.68 collect_output:5.48
2023-01-11 08:40:00 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 08:40:00 - train.py[line:551] - INFO: load:1.50 valid_run:2248.19 task_valid:2220.05 collect_output:6.05
2023-01-11 08:43:24 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 08:43:24 - train.py[line:551] - INFO: load:1.52 valid_run:2452.50 task_valid:2421.79 collect_output:6.61
2023-01-11 08:46:45 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 08:46:45 - train.py[line:551] - INFO: load:1.55 valid_run:2653.20 task_valid:2619.90 collect_output:7.18
2023-01-11 08:50:10 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 08:50:10 - train.py[line:551] - INFO: load:1.58 valid_run:2858.75 task_valid:2822.91 collect_output:7.74
2023-01-11 08:53:34 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 08:53:34 - train.py[line:551] - INFO: load:1.61 valid_run:3062.06 task_valid:3023.67 collect_output:8.29
2023-01-11 08:56:55 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 08:56:55 - train.py[line:551] - INFO: load:1.64 valid_run:3263.25 task_valid:3222.27 collect_output:8.86
2023-01-11 09:00:19 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 09:00:19 - train.py[line:551] - INFO: load:1.66 valid_run:3467.42 task_valid:3423.88 collect_output:9.42
2023-01-11 09:03:45 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 09:03:45 - train.py[line:551] - INFO: load:1.69 valid_run:3673.33 task_valid:3627.25 collect_output:9.97
2023-01-11 09:07:11 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 09:07:11 - train.py[line:551] - INFO: load:1.71 valid_run:3878.51 task_valid:3829.85 collect_output:10.56
2023-01-11 09:10:36 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 09:10:36 - train.py[line:551] - INFO: load:1.74 valid_run:4083.35 task_valid:4032.10 collect_output:11.13
2023-01-11 09:13:27 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 09:13:27 - train.py[line:551] - INFO: load:1.77 valid_run:4254.68 task_valid:4199.01 collect_output:13.88
2023-01-11 09:15:29 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 09:15:29 - train.py[line:551] - INFO: load:1.79 valid_run:4376.40 task_valid:4317.40 collect_output:16.14
2023-01-11 09:17:29 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 09:17:29 - train.py[line:551] - INFO: load:1.81 valid_run:4496.54 task_valid:4431.37 collect_output:21.26
2023-01-11 09:19:29 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 09:19:29 - train.py[line:551] - INFO: load:1.84 valid_run:4616.17 task_valid:4547.20 collect_output:23.99
2023-01-11 09:21:30 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 09:21:30 - train.py[line:551] - INFO: load:1.86 valid_run:4737.58 task_valid:4663.05 collect_output:28.47
2023-01-11 09:23:33 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 09:23:33 - train.py[line:551] - INFO: load:1.89 valid_run:4860.11 task_valid:4778.61 collect_output:34.40
2023-01-11 09:25:32 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 09:25:32 - train.py[line:551] - INFO: load:1.92 valid_run:4979.33 task_valid:4892.30 collect_output:38.89
2023-01-11 09:27:34 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 09:27:34 - train.py[line:551] - INFO: load:1.94 valid_run:5100.93 task_valid:5011.44 collect_output:40.31
2023-01-11 09:29:35 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 09:29:35 - train.py[line:551] - INFO: load:1.97 valid_run:5222.25 task_valid:5126.67 collect_output:45.38
2023-01-11 09:31:37 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 09:31:37 - train.py[line:551] - INFO: load:1.99 valid_run:5343.71 task_valid:5244.60 collect_output:47.86
2023-01-11 09:33:37 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 09:33:37 - train.py[line:551] - INFO: load:2.02 valid_run:5464.34 task_valid:5362.44 collect_output:49.61

====================================================================================================
SGG eval:     R @ 50: 0.6109;     R @ 100: 0.6480;     R @ 500: 0.6927;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3594;    mR @ 100: 0.4295;    mR @ 500: 0.4707;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5732) (covered in:0.0625) (covering:0.7143) (eating:0.7059) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4452) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8333) (playing:0.0000) (riding:0.9408) (says:0.0000) (sitting on:0.7302) (standing on:0.4810) (using:0.3500) (walking in:0.0000) (walking on:0.6486) (watching:0.2917) 
--------------------------------------------------------
====================================================================================================

2023-01-11 09:34:08 - train.py[line:487] - INFO: 0.6479693913929209
2023-01-11 09:34:08 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6109;     R @ 100: 0.6480;     R @ 500: 0.6927;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3594;    mR @ 100: 0.4295;    mR @ 500: 0.4707;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5732) (covered in:0.0625) (covering:0.7143) (eating:0.7059) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4452) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8333) (playing:0.0000) (riding:0.9408) (says:0.0000) (sitting on:0.7302) (standing on:0.4810) (using:0.3500) (walking in:0.0000) (walking on:0.6486) (watching:0.2917) 
--------------------------------------------------------
====================================================================================================

2023-01-11 09:34:08 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.313 | loss_v1 0 | loss_v2 0 | nll_loss 0.163 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.647969 | ppl 1.12 | vqa_score 0.3896 | wps 81.6 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.647969
2023-01-11 09:34:08 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-01-11 09:34:08 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-11 09:34:45 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-11 09:37:29 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.6479693913929209) (writing took 201.00871311873198 seconds)
2023-01-11 09:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:46 - progress_bar.py[line:274] - INFO: epoch 001:   4014 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1111, wps=0.3, ups=0, wpb=109.1, bsz=40, num_updates=4010, lr=4.99948e-05, gnorm=1.246, clip=70, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34384
2023-01-11 09:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:02 - progress_bar.py[line:274] - INFO: epoch 001:   4024 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0286, wps=102.1, ups=0.62, wpb=110.1, bsz=40, num_updates=4020, lr=4.99896e-05, gnorm=1.033, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34401
2023-01-11 09:38:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:19 - progress_bar.py[line:274] - INFO: epoch 001:   4034 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0273, wps=97.9, ups=0.6, wpb=109.3, bsz=40, num_updates=4030, lr=4.99844e-05, gnorm=1.081, clip=50, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=34418
2023-01-11 09:38:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:36 - progress_bar.py[line:274] - INFO: epoch 001:   4044 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0326, wps=102.9, ups=0.62, wpb=110.9, bsz=40, num_updates=4040, lr=4.99792e-05, gnorm=1.205, clip=70, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=34434
2023-01-11 09:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:52 - progress_bar.py[line:274] - INFO: epoch 001:   4054 / 100000 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0737, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=4050, lr=4.9974e-05, gnorm=1.103, clip=60, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34451
2023-01-11 09:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:38:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:09 - progress_bar.py[line:274] - INFO: epoch 001:   4064 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0367, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=4060, lr=4.99688e-05, gnorm=1.102, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34468
2023-01-11 09:39:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:26 - progress_bar.py[line:274] - INFO: epoch 001:   4074 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0882, wps=99.2, ups=0.61, wpb=109.3, bsz=40, num_updates=4070, lr=4.99635e-05, gnorm=1.099, clip=50, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=34484
2023-01-11 09:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:42 - progress_bar.py[line:274] - INFO: epoch 001:   4084 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.04, wps=103.4, ups=0.62, wpb=110.9, bsz=40, num_updates=4080, lr=4.99583e-05, gnorm=1.524, clip=80, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34501
2023-01-11 09:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   4094 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=112.533, nsentences=40, sample_size=112.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0602, wps=101.3, ups=0.6, wpb=112.5, bsz=40, num_updates=4090, lr=4.99531e-05, gnorm=0.999, clip=40, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=34518
2023-01-11 09:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:16 - progress_bar.py[line:274] - INFO: epoch 001:   4104 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0172, wps=99.8, ups=0.6, wpb=110.5, bsz=40, num_updates=4100, lr=4.99479e-05, gnorm=1.352, clip=60, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=34535
2023-01-11 09:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:33 - progress_bar.py[line:274] - INFO: epoch 001:   4114 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0215, wps=99.1, ups=0.6, wpb=110, bsz=40, num_updates=4110, lr=4.99427e-05, gnorm=1.27, clip=70, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=34552
2023-01-11 09:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:50 - progress_bar.py[line:274] - INFO: epoch 001:   4124 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0093, wps=98.4, ups=0.6, wpb=108.7, bsz=40, num_updates=4120, lr=4.99375e-05, gnorm=1.381, clip=60, loss_scale=1024, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=34568
2023-01-11 09:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:06 - progress_bar.py[line:274] - INFO: epoch 001:   4134 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0561, wps=99.6, ups=0.62, wpb=107.9, bsz=40, num_updates=4130, lr=4.99323e-05, gnorm=1.491, clip=60, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34585
2023-01-11 09:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:23 - progress_bar.py[line:274] - INFO: epoch 001:   4144 / 100000 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0648, wps=101.2, ups=0.61, wpb=111.1, bsz=40, num_updates=4140, lr=4.99271e-05, gnorm=1.08, clip=50, loss_scale=1024, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=34602
2023-01-11 09:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:40 - progress_bar.py[line:274] - INFO: epoch 001:   4154 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0421, wps=100.3, ups=0.61, wpb=110, bsz=40, num_updates=4150, lr=4.99219e-05, gnorm=1.042, clip=20, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=34618
2023-01-11 09:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:56 - progress_bar.py[line:274] - INFO: epoch 001:   4164 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0632, wps=101.3, ups=0.61, wpb=110.6, bsz=40, num_updates=4160, lr=4.99167e-05, gnorm=1.703, clip=70, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34635
2023-01-11 09:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:14 - progress_bar.py[line:274] - INFO: epoch 001:   4174 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0481, wps=102.2, ups=0.62, wpb=109.9, bsz=40, num_updates=4170, lr=4.99115e-05, gnorm=0.831, clip=20, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34651
2023-01-11 09:42:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:30 - progress_bar.py[line:274] - INFO: epoch 001:   4184 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0455, wps=101.3, ups=0.61, wpb=110.3, bsz=40, num_updates=4180, lr=4.99063e-05, gnorm=1.407, clip=40, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34669
2023-01-11 09:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:47 - progress_bar.py[line:274] - INFO: epoch 001:   4194 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0495, wps=100.1, ups=0.6, wpb=110.7, bsz=40, num_updates=4190, lr=4.9901e-05, gnorm=1.051, clip=60, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=34686
2023-01-11 09:42:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:04 - progress_bar.py[line:274] - INFO: epoch 001:   4204 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0098, wps=98, ups=0.61, wpb=107.2, bsz=40, num_updates=4200, lr=4.98958e-05, gnorm=1.133, clip=50, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34702
2023-01-11 09:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:20 - progress_bar.py[line:274] - INFO: epoch 001:   4214 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0435, wps=103.1, ups=0.63, wpb=109.9, bsz=40, num_updates=4210, lr=4.98906e-05, gnorm=1.092, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34719
2023-01-11 09:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:37 - progress_bar.py[line:274] - INFO: epoch 001:   4224 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0638, wps=101.2, ups=0.62, wpb=109.7, bsz=40, num_updates=4220, lr=4.98854e-05, gnorm=1.067, clip=40, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34735
2023-01-11 09:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:53 - progress_bar.py[line:274] - INFO: epoch 001:   4234 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0204, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=4230, lr=4.98802e-05, gnorm=1.141, clip=50, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34752
2023-01-11 09:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:43:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:10 - progress_bar.py[line:274] - INFO: epoch 001:   4244 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0412, wps=101.7, ups=0.61, wpb=111.1, bsz=40, num_updates=4240, lr=4.9875e-05, gnorm=1.079, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34769
2023-01-11 09:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:27 - progress_bar.py[line:274] - INFO: epoch 001:   4254 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.06, wps=99.1, ups=0.6, wpb=109.4, bsz=40, num_updates=4250, lr=4.98698e-05, gnorm=1.629, clip=60, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=34786
2023-01-11 09:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:43 - progress_bar.py[line:274] - INFO: epoch 001:   4264 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0526, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=4260, lr=4.98646e-05, gnorm=1.325, clip=60, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34802
2023-01-11 09:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:00 - progress_bar.py[line:274] - INFO: epoch 001:   4274 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0495, wps=99.5, ups=0.6, wpb=110.9, bsz=40, num_updates=4270, lr=4.98594e-05, gnorm=1.201, clip=70, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=34819
2023-01-11 09:45:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:18 - progress_bar.py[line:274] - INFO: epoch 001:   4284 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0213, wps=96.5, ups=0.59, wpb=108.9, bsz=40, num_updates=4280, lr=4.98542e-05, gnorm=1.027, clip=40, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=34836
2023-01-11 09:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:34 - progress_bar.py[line:274] - INFO: epoch 001:   4294 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0385, wps=100.5, ups=0.61, wpb=110.3, bsz=40, num_updates=4290, lr=4.9849e-05, gnorm=1.279, clip=40, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34853
2023-01-11 09:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:37 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 09:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:52 - progress_bar.py[line:274] - INFO: epoch 001:   4305 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0667, wps=97.8, ups=0.59, wpb=110.8, bsz=40, num_updates=4300, lr=4.98438e-05, gnorm=1.52, clip=70, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=34870
2023-01-11 09:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:08 - progress_bar.py[line:274] - INFO: epoch 001:   4315 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.02, wps=100.6, ups=0.61, wpb=109.3, bsz=40, num_updates=4310, lr=4.98385e-05, gnorm=1.967, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34887
2023-01-11 09:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:25 - progress_bar.py[line:274] - INFO: epoch 001:   4325 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.1176, wps=101.7, ups=0.62, wpb=109.1, bsz=40, num_updates=4320, lr=4.98333e-05, gnorm=1.16, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34903
2023-01-11 09:46:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:41 - progress_bar.py[line:274] - INFO: epoch 001:   4335 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0426, wps=103.5, ups=0.63, wpb=109.9, bsz=40, num_updates=4330, lr=4.98281e-05, gnorm=1.655, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34919
2023-01-11 09:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:46:57 - progress_bar.py[line:274] - INFO: epoch 001:   4345 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.037, wps=99.1, ups=0.61, wpb=109.2, bsz=40, num_updates=4340, lr=4.98229e-05, gnorm=1.438, clip=70, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=34936
2023-01-11 09:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:14 - progress_bar.py[line:274] - INFO: epoch 001:   4355 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0288, wps=99, ups=0.61, wpb=108.9, bsz=40, num_updates=4350, lr=4.98177e-05, gnorm=1.37, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34953
2023-01-11 09:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:31 - progress_bar.py[line:274] - INFO: epoch 001:   4365 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.018, wps=104.1, ups=0.62, wpb=111.6, bsz=40, num_updates=4360, lr=4.98125e-05, gnorm=1.07, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34969
2023-01-11 09:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:47 - progress_bar.py[line:274] - INFO: epoch 001:   4375 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0261, wps=102.4, ups=0.63, wpb=108.9, bsz=40, num_updates=4370, lr=4.98073e-05, gnorm=1.873, clip=90, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=34985
2023-01-11 09:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:04 - progress_bar.py[line:274] - INFO: epoch 001:   4385 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.033, wps=100.4, ups=0.6, wpb=110.7, bsz=40, num_updates=4380, lr=4.98021e-05, gnorm=1.212, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=35002
2023-01-11 09:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:20 - progress_bar.py[line:274] - INFO: epoch 001:   4395 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0748, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=4390, lr=4.97969e-05, gnorm=1.319, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35019
2023-01-11 09:48:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:37 - progress_bar.py[line:274] - INFO: epoch 001:   4405 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0561, wps=99.6, ups=0.6, wpb=111.1, bsz=40, num_updates=4400, lr=4.97917e-05, gnorm=1.296, clip=60, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=35036
2023-01-11 09:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:54 - progress_bar.py[line:274] - INFO: epoch 001:   4415 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0472, wps=98.7, ups=0.6, wpb=110.4, bsz=40, num_updates=4410, lr=4.97865e-05, gnorm=1.13, clip=40, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=35053
2023-01-11 09:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:11 - progress_bar.py[line:274] - INFO: epoch 001:   4425 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.069, wps=101, ups=0.61, wpb=110.9, bsz=40, num_updates=4420, lr=4.97813e-05, gnorm=1.026, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35070
2023-01-11 09:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:27 - progress_bar.py[line:274] - INFO: epoch 001:   4435 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0312, wps=102.6, ups=0.62, wpb=110.1, bsz=40, num_updates=4430, lr=4.9776e-05, gnorm=1.506, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35086
2023-01-11 09:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:44 - progress_bar.py[line:274] - INFO: epoch 001:   4445 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0283, wps=99, ups=0.61, wpb=108.3, bsz=40, num_updates=4440, lr=4.97708e-05, gnorm=1.241, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35103
2023-01-11 09:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:49:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:01 - progress_bar.py[line:274] - INFO: epoch 001:   4455 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0364, wps=97.6, ups=0.6, wpb=109.1, bsz=40, num_updates=4450, lr=4.97656e-05, gnorm=1.5, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=35120
2023-01-11 09:50:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:18 - progress_bar.py[line:274] - INFO: epoch 001:   4465 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0392, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=4460, lr=4.97604e-05, gnorm=1.196, clip=70, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=35136
2023-01-11 09:50:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:34 - progress_bar.py[line:274] - INFO: epoch 001:   4475 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0215, wps=100.4, ups=0.6, wpb=110.9, bsz=40, num_updates=4470, lr=4.97552e-05, gnorm=0.772, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=35153
2023-01-11 09:50:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:51 - progress_bar.py[line:274] - INFO: epoch 001:   4485 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0857, wps=100.2, ups=0.61, wpb=110.4, bsz=40, num_updates=4480, lr=4.975e-05, gnorm=1.09, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35170
2023-01-11 09:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:08 - progress_bar.py[line:274] - INFO: epoch 001:   4495 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0571, wps=99.8, ups=0.61, wpb=109.4, bsz=40, num_updates=4490, lr=4.97448e-05, gnorm=1.112, clip=80, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=35187
2023-01-11 09:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:25 - progress_bar.py[line:274] - INFO: epoch 001:   4505 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0566, wps=94.8, ups=0.58, wpb=109.6, bsz=40, num_updates=4500, lr=4.97396e-05, gnorm=1.273, clip=50, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=35204
2023-01-11 09:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:42 - progress_bar.py[line:274] - INFO: epoch 001:   4515 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0784, wps=101.5, ups=0.62, wpb=110, bsz=40, num_updates=4510, lr=4.97344e-05, gnorm=1.179, clip=60, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=35221
2023-01-11 09:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:51:59 - progress_bar.py[line:274] - INFO: epoch 001:   4525 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1183, wps=100.5, ups=0.61, wpb=110.3, bsz=40, num_updates=4520, lr=4.97292e-05, gnorm=1.878, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35237
2023-01-11 09:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:16 - progress_bar.py[line:274] - INFO: epoch 001:   4535 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.086, wps=97.6, ups=0.6, wpb=109.1, bsz=40, num_updates=4530, lr=4.9724e-05, gnorm=1.093, clip=50, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=35255
2023-01-11 09:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:33 - progress_bar.py[line:274] - INFO: epoch 001:   4545 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0583, wps=98.4, ups=0.6, wpb=108.9, bsz=40, num_updates=4540, lr=4.97188e-05, gnorm=1.584, clip=90, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=35271
2023-01-11 09:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:50 - progress_bar.py[line:274] - INFO: epoch 001:   4555 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0222, wps=99.9, ups=0.6, wpb=111.4, bsz=40, num_updates=4550, lr=4.97135e-05, gnorm=1.294, clip=50, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=35288
2023-01-11 09:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:06 - progress_bar.py[line:274] - INFO: epoch 001:   4565 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0571, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=4560, lr=4.97083e-05, gnorm=1.191, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=35305
2023-01-11 09:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:23 - progress_bar.py[line:274] - INFO: epoch 001:   4575 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0556, wps=100.3, ups=0.6, wpb=111.1, bsz=40, num_updates=4570, lr=4.97031e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=35322
2023-01-11 09:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:39 - progress_bar.py[line:274] - INFO: epoch 001:   4585 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.051, wps=101.8, ups=0.62, wpb=109.1, bsz=40, num_updates=4580, lr=4.96979e-05, gnorm=1.117, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35338
2023-01-11 09:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:56 - progress_bar.py[line:274] - INFO: epoch 001:   4595 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0309, wps=100.8, ups=0.61, wpb=111, bsz=40, num_updates=4590, lr=4.96927e-05, gnorm=0.891, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35355
2023-01-11 09:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:13 - progress_bar.py[line:274] - INFO: epoch 001:   4605 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0303, wps=98.9, ups=0.6, wpb=109.7, bsz=40, num_updates=4600, lr=4.96875e-05, gnorm=1.859, clip=50, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=35372
2023-01-11 09:54:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:30 - progress_bar.py[line:274] - INFO: epoch 001:   4615 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0667, wps=99.3, ups=0.61, wpb=109.4, bsz=40, num_updates=4610, lr=4.96823e-05, gnorm=0.965, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35389
2023-01-11 09:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:46 - progress_bar.py[line:274] - INFO: epoch 001:   4625 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0957, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=4620, lr=4.96771e-05, gnorm=1.709, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35405
2023-01-11 09:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:03 - progress_bar.py[line:274] - INFO: epoch 001:   4635 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0816, wps=101.4, ups=0.62, wpb=109.5, bsz=40, num_updates=4630, lr=4.96719e-05, gnorm=0.966, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35421
2023-01-11 09:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:20 - progress_bar.py[line:274] - INFO: epoch 001:   4645 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.1296, wps=99.1, ups=0.6, wpb=109.5, bsz=40, num_updates=4640, lr=4.96667e-05, gnorm=1.727, clip=70, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=35438
2023-01-11 09:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:36 - progress_bar.py[line:274] - INFO: epoch 001:   4655 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0455, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=4650, lr=4.96615e-05, gnorm=0.991, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35455
2023-01-11 09:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:53 - progress_bar.py[line:274] - INFO: epoch 001:   4665 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.028, wps=99.7, ups=0.61, wpb=108.3, bsz=40, num_updates=4660, lr=4.96563e-05, gnorm=1.023, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35471
2023-01-11 09:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:09 - progress_bar.py[line:274] - INFO: epoch 001:   4675 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0612, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=4670, lr=4.9651e-05, gnorm=1.22, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35488
2023-01-11 09:56:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:26 - progress_bar.py[line:274] - INFO: epoch 001:   4685 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=106.867, nsentences=40, sample_size=106.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0565, wps=98.3, ups=0.61, wpb=106.9, bsz=40, num_updates=4680, lr=4.96458e-05, gnorm=1.401, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35504
2023-01-11 09:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:42 - progress_bar.py[line:274] - INFO: epoch 001:   4695 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0204, wps=99.2, ups=0.61, wpb=109.2, bsz=40, num_updates=4690, lr=4.96406e-05, gnorm=1.171, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35521
2023-01-11 09:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:56:59 - progress_bar.py[line:274] - INFO: epoch 001:   4705 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0444, wps=101.3, ups=0.61, wpb=111.2, bsz=40, num_updates=4700, lr=4.96354e-05, gnorm=1.11, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35538
2023-01-11 09:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:15 - progress_bar.py[line:274] - INFO: epoch 001:   4715 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0326, wps=101.1, ups=0.62, wpb=108.8, bsz=40, num_updates=4710, lr=4.96302e-05, gnorm=1.764, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35554
2023-01-11 09:57:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:32 - progress_bar.py[line:274] - INFO: epoch 001:   4725 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0323, wps=102.1, ups=0.62, wpb=110.6, bsz=40, num_updates=4720, lr=4.9625e-05, gnorm=0.82, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=35571
2023-01-11 09:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:49 - progress_bar.py[line:274] - INFO: epoch 001:   4735 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0755, wps=97.5, ups=0.6, wpb=107.9, bsz=40, num_updates=4730, lr=4.96198e-05, gnorm=1.161, clip=50, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=35587
2023-01-11 09:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:57:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:05 - progress_bar.py[line:274] - INFO: epoch 001:   4745 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0602, wps=105.4, ups=0.64, wpb=110.5, bsz=40, num_updates=4740, lr=4.96146e-05, gnorm=0.908, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35603
2023-01-11 09:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:22 - progress_bar.py[line:274] - INFO: epoch 001:   4755 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0583, wps=98.6, ups=0.6, wpb=109.8, bsz=40, num_updates=4750, lr=4.96094e-05, gnorm=1.049, clip=50, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=35620
2023-01-11 09:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:38 - progress_bar.py[line:274] - INFO: epoch 001:   4765 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.061, wps=103.4, ups=0.63, wpb=109.8, bsz=40, num_updates=4760, lr=4.96042e-05, gnorm=1.079, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=35636
2023-01-11 09:58:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:54 - progress_bar.py[line:274] - INFO: epoch 001:   4775 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0392, wps=99.7, ups=0.61, wpb=109.4, bsz=40, num_updates=4770, lr=4.9599e-05, gnorm=1.105, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=35653
2023-01-11 09:58:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:11 - progress_bar.py[line:274] - INFO: epoch 001:   4785 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.05, wps=99.2, ups=0.6, wpb=110.4, bsz=40, num_updates=4780, lr=4.95938e-05, gnorm=0.904, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=35670
2023-01-11 09:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:28 - progress_bar.py[line:274] - INFO: epoch 001:   4795 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0568, wps=100.9, ups=0.61, wpb=111, bsz=40, num_updates=4790, lr=4.95885e-05, gnorm=0.982, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35687
2023-01-11 09:59:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:45 - progress_bar.py[line:274] - INFO: epoch 001:   4805 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0741, wps=98.9, ups=0.6, wpb=109.3, bsz=40, num_updates=4800, lr=4.95833e-05, gnorm=1.348, clip=70, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=35704
2023-01-11 09:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 09:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:01 - progress_bar.py[line:274] - INFO: epoch 001:   4815 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0909, wps=103, ups=0.62, wpb=110, bsz=40, num_updates=4810, lr=4.95781e-05, gnorm=1.043, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35720
2023-01-11 10:00:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:18 - progress_bar.py[line:274] - INFO: epoch 001:   4825 / 100000 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0449, wps=100.4, ups=0.61, wpb=110, bsz=40, num_updates=4820, lr=4.95729e-05, gnorm=1.286, clip=70, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35737
2023-01-11 10:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:34 - progress_bar.py[line:274] - INFO: epoch 001:   4835 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0532, wps=102.1, ups=0.61, wpb=111.5, bsz=40, num_updates=4830, lr=4.95677e-05, gnorm=1.216, clip=40, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=35753
2023-01-11 10:00:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:51 - progress_bar.py[line:274] - INFO: epoch 001:   4845 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0729, wps=100.9, ups=0.6, wpb=111.2, bsz=40, num_updates=4840, lr=4.95625e-05, gnorm=1.351, clip=60, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=35770
2023-01-11 10:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:00:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:07 - progress_bar.py[line:274] - INFO: epoch 001:   4855 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0571, wps=102.5, ups=0.63, wpb=108.4, bsz=40, num_updates=4850, lr=4.95573e-05, gnorm=1.356, clip=60, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=35786
2023-01-11 10:01:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:24 - progress_bar.py[line:274] - INFO: epoch 001:   4865 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.055, wps=98.6, ups=0.6, wpb=108.7, bsz=40, num_updates=4860, lr=4.95521e-05, gnorm=1.681, clip=80, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=35803
2023-01-11 10:01:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:40 - progress_bar.py[line:274] - INFO: epoch 001:   4875 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0549, wps=103.7, ups=0.62, wpb=111.1, bsz=40, num_updates=4870, lr=4.95469e-05, gnorm=1.387, clip=60, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35819
2023-01-11 10:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:57 - progress_bar.py[line:274] - INFO: epoch 001:   4885 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0283, wps=102, ups=0.62, wpb=109.7, bsz=40, num_updates=4880, lr=4.95417e-05, gnorm=0.848, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35836
2023-01-11 10:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:13 - progress_bar.py[line:274] - INFO: epoch 001:   4895 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0722, wps=103.3, ups=0.63, wpb=109.9, bsz=40, num_updates=4890, lr=4.95365e-05, gnorm=0.971, clip=30, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=35852
2023-01-11 10:02:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:30 - progress_bar.py[line:274] - INFO: epoch 001:   4905 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0957, wps=100.4, ups=0.6, wpb=110.9, bsz=40, num_updates=4900, lr=4.95313e-05, gnorm=1.008, clip=50, loss_scale=1024, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=35869
2023-01-11 10:02:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:47 - progress_bar.py[line:274] - INFO: epoch 001:   4915 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0808, wps=98.2, ups=0.6, wpb=109.8, bsz=40, num_updates=4910, lr=4.9526e-05, gnorm=1.306, clip=60, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=35886
2023-01-11 10:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:02:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:04 - progress_bar.py[line:274] - INFO: epoch 001:   4925 / 100000 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0532, wps=100.5, ups=0.61, wpb=110, bsz=40, num_updates=4920, lr=4.95208e-05, gnorm=1.553, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35902
2023-01-11 10:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:20 - progress_bar.py[line:274] - INFO: epoch 001:   4935 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0971, wps=100.6, ups=0.61, wpb=109.3, bsz=40, num_updates=4930, lr=4.95156e-05, gnorm=1.494, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35919
2023-01-11 10:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:37 - progress_bar.py[line:274] - INFO: epoch 001:   4945 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0476, wps=99.6, ups=0.61, wpb=109.7, bsz=40, num_updates=4940, lr=4.95104e-05, gnorm=1.245, clip=50, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=35936
2023-01-11 10:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:53 - progress_bar.py[line:274] - INFO: epoch 001:   4955 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0202, wps=104, ups=0.62, wpb=111, bsz=40, num_updates=4950, lr=4.95052e-05, gnorm=1.078, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35952
2023-01-11 10:03:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:10 - progress_bar.py[line:274] - INFO: epoch 001:   4965 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1183, wps=101.3, ups=0.61, wpb=110.7, bsz=40, num_updates=4960, lr=4.95e-05, gnorm=1.042, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35969
2023-01-11 10:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:26 - progress_bar.py[line:274] - INFO: epoch 001:   4975 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0556, wps=100.9, ups=0.62, wpb=108.9, bsz=40, num_updates=4970, lr=4.94948e-05, gnorm=1.231, clip=70, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=35985
2023-01-11 10:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:43 - progress_bar.py[line:274] - INFO: epoch 001:   4985 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0686, wps=96.8, ups=0.6, wpb=108.3, bsz=40, num_updates=4980, lr=4.94896e-05, gnorm=0.78, clip=20, loss_scale=1024, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=36002
2023-01-11 10:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:04:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:05:00 - progress_bar.py[line:274] - INFO: epoch 001:   4995 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0686, wps=99, ups=0.6, wpb=110, bsz=40, num_updates=4990, lr=4.94844e-05, gnorm=1.293, clip=80, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=36019
2023-01-11 10:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 10:05:17 - progress_bar.py[line:274] - INFO: epoch 001:   5005 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0577, wps=102, ups=0.62, wpb=110.5, bsz=40, num_updates=5000, lr=4.94792e-05, gnorm=1.153, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=36035
2023-01-11 10:05:17 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 10:05:18 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 10:05:18 - train.py[line:551] - INFO: load:1.09 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 10:07:20 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 10:07:20 - train.py[line:551] - INFO: load:1.12 valid_run:122.00 task_valid:119.01 collect_output:1.84
2023-01-11 10:09:20 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 10:09:20 - train.py[line:551] - INFO: load:1.15 valid_run:241.77 task_valid:234.41 collect_output:5.17
2023-01-11 10:11:22 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 10:11:22 - train.py[line:551] - INFO: load:1.17 valid_run:363.95 task_valid:350.67 collect_output:10.04
2023-01-11 10:13:24 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 10:13:24 - train.py[line:551] - INFO: load:1.20 valid_run:485.92 task_valid:464.18 collect_output:17.41
2023-01-11 10:15:25 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 10:15:25 - train.py[line:551] - INFO: load:1.22 valid_run:606.16 task_valid:581.27 collect_output:19.52
2023-01-11 10:17:28 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 10:17:28 - train.py[line:551] - INFO: load:1.25 valid_run:728.97 task_valid:699.57 collect_output:22.97
2023-01-11 10:19:31 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 10:19:31 - train.py[line:551] - INFO: load:1.27 valid_run:851.94 task_valid:817.32 collect_output:27.12
2023-01-11 10:21:32 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 10:21:32 - train.py[line:551] - INFO: load:1.30 valid_run:973.56 task_valid:933.35 collect_output:31.65
2023-01-11 10:23:36 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 10:23:36 - train.py[line:551] - INFO: load:1.32 valid_run:1097.08 task_valid:1049.99 collect_output:37.46
2023-01-11 10:25:38 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 10:25:38 - train.py[line:551] - INFO: load:1.35 valid_run:1218.86 task_valid:1162.61 collect_output:45.56
2023-01-11 10:27:38 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 10:27:38 - train.py[line:551] - INFO: load:1.37 valid_run:1339.07 task_valid:1278.11 collect_output:49.18
2023-01-11 10:29:39 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 10:29:40 - train.py[line:551] - INFO: load:1.40 valid_run:1460.43 task_valid:1394.57 collect_output:53.02
2023-01-11 10:31:39 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 10:31:39 - train.py[line:551] - INFO: load:1.43 valid_run:1579.37 task_valid:1508.14 collect_output:57.32
2023-01-11 10:33:39 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 10:33:39 - train.py[line:551] - INFO: load:1.45 valid_run:1700.05 task_valid:1625.43 collect_output:59.68
2023-01-11 10:35:40 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 10:35:40 - train.py[line:551] - INFO: load:1.48 valid_run:1821.00 task_valid:1741.47 collect_output:63.46
2023-01-11 10:37:41 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 10:37:41 - train.py[line:551] - INFO: load:1.50 valid_run:1942.07 task_valid:1855.25 collect_output:69.69
2023-01-11 10:39:43 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 10:39:43 - train.py[line:551] - INFO: load:1.53 valid_run:2063.57 task_valid:1971.20 collect_output:74.15
2023-01-11 10:41:44 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 10:41:44 - train.py[line:551] - INFO: load:1.56 valid_run:2183.94 task_valid:2088.76 collect_output:75.87
2023-01-11 10:43:46 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 10:43:46 - train.py[line:551] - INFO: load:1.59 valid_run:2305.85 task_valid:2206.23 collect_output:79.04
2023-01-11 10:45:46 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 10:45:46 - train.py[line:551] - INFO: load:1.62 valid_run:2426.10 task_valid:2322.65 collect_output:81.78
2023-01-11 10:47:48 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 10:47:48 - train.py[line:551] - INFO: load:1.64 valid_run:2547.95 task_valid:2438.89 collect_output:86.29
2023-01-11 10:49:50 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 10:49:50 - train.py[line:551] - INFO: load:1.67 valid_run:2669.79 task_valid:2557.44 collect_output:88.52
2023-01-11 10:51:50 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 10:51:50 - train.py[line:551] - INFO: load:1.69 valid_run:2789.83 task_valid:2671.53 collect_output:93.43
2023-01-11 10:53:50 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 10:53:50 - train.py[line:551] - INFO: load:1.72 valid_run:2909.76 task_valid:2787.39 collect_output:96.43
2023-01-11 10:55:52 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 10:55:52 - train.py[line:551] - INFO: load:1.74 valid_run:3031.46 task_valid:2903.47 collect_output:100.92
2023-01-11 10:57:54 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 10:57:54 - train.py[line:551] - INFO: load:1.77 valid_run:3153.95 task_valid:3019.07 collect_output:106.71
2023-01-11 10:59:54 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 10:59:54 - train.py[line:551] - INFO: load:1.80 valid_run:3273.48 task_valid:3132.81 collect_output:111.40
2023-01-11 11:01:55 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 11:01:55 - train.py[line:551] - INFO: load:1.82 valid_run:3394.80 task_valid:3251.53 collect_output:112.95
2023-01-11 11:03:57 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 11:03:57 - train.py[line:551] - INFO: load:1.85 valid_run:3516.36 task_valid:3366.88 collect_output:118.08
2023-01-11 11:05:59 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 11:05:59 - train.py[line:551] - INFO: load:1.88 valid_run:3638.43 task_valid:3485.05 collect_output:120.87
2023-01-11 11:08:00 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 11:08:00 - train.py[line:551] - INFO: load:1.90 valid_run:3759.14 task_valid:3602.96 collect_output:122.61

====================================================================================================
SGG eval:     R @ 50: 0.6561;     R @ 100: 0.6901;     R @ 500: 0.7204;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4219;    mR @ 100: 0.4891;    mR @ 500: 0.5326;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6585) (covered in:0.6250) (covering:0.5714) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9477) (says:0.0000) (sitting on:0.7846) (standing on:0.4093) (using:0.4500) (walking in:0.0000) (walking on:0.7748) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6561;     R @ 100: 0.6901;     R @ 500: 0.7204;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4219;    mR @ 100: 0.4891;    mR @ 500: 0.5326;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6585) (covered in:0.6250) (covering:0.5714) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9477) (says:0.0000) (sitting on:0.7846) (standing on:0.4093) (using:0.4500) (walking in:0.0000) (walking on:0.7748) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================

2023-01-11 11:08:31 - train.py[line:487] - INFO: 0.6901360580595874
2023-01-11 11:08:31 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 11:08:32 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.272 | loss_v1 0 | loss_v2 0 | nll_loss 0.109 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.690136 | ppl 1.08 | vqa_score 0.4696 | wps 118.3 | wpb 72 | bsz 24 | num_updates 5000 | best_R@100 0.690136
2023-01-11 11:08:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5000 updates
2023-01-11 11:08:32 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_5000.pt
2023-01-11 11:09:12 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_5000.pt
2023-01-11 11:12:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 0.6901360580595874) (writing took 210.54399593919516 seconds)
2023-01-11 11:12:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:20 - progress_bar.py[line:274] - INFO: epoch 001:   5015 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0667, wps=0.4, ups=0, wpb=109.9, bsz=40, num_updates=5010, lr=4.9474e-05, gnorm=1.012, clip=50, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=40058
2023-01-11 11:12:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:36 - progress_bar.py[line:274] - INFO: epoch 001:   5025 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0459, wps=103.5, ups=0.62, wpb=111.5, bsz=40, num_updates=5020, lr=4.94687e-05, gnorm=1.402, clip=70, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40075
2023-01-11 11:12:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:53 - progress_bar.py[line:274] - INFO: epoch 001:   5035 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0323, wps=102.1, ups=0.61, wpb=111.1, bsz=40, num_updates=5030, lr=4.94635e-05, gnorm=0.928, clip=30, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40091
2023-01-11 11:12:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:12:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:09 - progress_bar.py[line:274] - INFO: epoch 001:   5045 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0286, wps=101.3, ups=0.61, wpb=110.7, bsz=40, num_updates=5040, lr=4.94583e-05, gnorm=0.981, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40108
2023-01-11 11:13:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:26 - progress_bar.py[line:274] - INFO: epoch 001:   5055 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.05, wps=100, ups=0.6, wpb=110.5, bsz=40, num_updates=5050, lr=4.94531e-05, gnorm=1.263, clip=60, loss_scale=1024, train_wall=17, gb_free=10.7, ema_decay=0.9999, wall=40125
2023-01-11 11:13:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:43 - progress_bar.py[line:274] - INFO: epoch 001:   5065 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0485, wps=99.2, ups=0.6, wpb=110.3, bsz=40, num_updates=5060, lr=4.94479e-05, gnorm=0.962, clip=30, loss_scale=1024, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=40142
2023-01-11 11:13:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:00 - progress_bar.py[line:274] - INFO: epoch 001:   5075 / 100000 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.04, wps=99.1, ups=0.6, wpb=110, bsz=40, num_updates=5070, lr=4.94427e-05, gnorm=1.583, clip=60, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40159
2023-01-11 11:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:17 - progress_bar.py[line:274] - INFO: epoch 001:   5085 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0901, wps=100.6, ups=0.61, wpb=110.5, bsz=40, num_updates=5080, lr=4.94375e-05, gnorm=0.932, clip=40, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=40175
2023-01-11 11:14:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:33 - progress_bar.py[line:274] - INFO: epoch 001:   5095 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.087, wps=103, ups=0.62, wpb=109.9, bsz=40, num_updates=5090, lr=4.94323e-05, gnorm=0.959, clip=40, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40192
2023-01-11 11:14:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:49 - progress_bar.py[line:274] - INFO: epoch 001:   5105 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0682, wps=106.1, ups=0.64, wpb=111.2, bsz=40, num_updates=5100, lr=4.94271e-05, gnorm=0.767, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40208
2023-01-11 11:14:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:05 - progress_bar.py[line:274] - INFO: epoch 001:   5115 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0769, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=5110, lr=4.94219e-05, gnorm=1.575, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40224
2023-01-11 11:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:22 - progress_bar.py[line:274] - INFO: epoch 001:   5125 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0918, wps=100.2, ups=0.6, wpb=110.7, bsz=40, num_updates=5120, lr=4.94167e-05, gnorm=1.29, clip=50, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=40241
2023-01-11 11:15:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:39 - progress_bar.py[line:274] - INFO: epoch 001:   5135 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0388, wps=102.2, ups=0.62, wpb=109.1, bsz=40, num_updates=5130, lr=4.94115e-05, gnorm=1.105, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40257
2023-01-11 11:15:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:55 - progress_bar.py[line:274] - INFO: epoch 001:   5145 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0917, wps=99.5, ups=0.61, wpb=109.2, bsz=40, num_updates=5140, lr=4.94063e-05, gnorm=1.108, clip=60, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40274
2023-01-11 11:15:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:15:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:12 - progress_bar.py[line:274] - INFO: epoch 001:   5155 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0495, wps=98.9, ups=0.6, wpb=110.7, bsz=40, num_updates=5150, lr=4.9401e-05, gnorm=2.977, clip=70, loss_scale=1024, train_wall=17, gb_free=10, ema_decay=0.9999, wall=40291
2023-01-11 11:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:29 - progress_bar.py[line:274] - INFO: epoch 001:   5165 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0625, wps=100.9, ups=0.61, wpb=109.8, bsz=40, num_updates=5160, lr=4.93958e-05, gnorm=1.684, clip=80, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40308
2023-01-11 11:16:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:46 - progress_bar.py[line:274] - INFO: epoch 001:   5175 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=107.733, nsentences=40, sample_size=107.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.05, wps=98.3, ups=0.61, wpb=107.7, bsz=40, num_updates=5170, lr=4.93906e-05, gnorm=1.649, clip=60, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40324
2023-01-11 11:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:03 - progress_bar.py[line:274] - INFO: epoch 001:   5185 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0833, wps=97.4, ups=0.59, wpb=109.3, bsz=40, num_updates=5180, lr=4.93854e-05, gnorm=2.293, clip=80, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40342
2023-01-11 11:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:19 - progress_bar.py[line:274] - INFO: epoch 001:   5195 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0594, wps=104, ups=0.63, wpb=109.3, bsz=40, num_updates=5190, lr=4.93802e-05, gnorm=1.824, clip=50, loss_scale=1024, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=40358
2023-01-11 11:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:36 - progress_bar.py[line:274] - INFO: epoch 001:   5205 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0515, wps=99.1, ups=0.6, wpb=109.3, bsz=40, num_updates=5200, lr=4.9375e-05, gnorm=0.969, clip=30, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40375
2023-01-11 11:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:52 - progress_bar.py[line:274] - INFO: epoch 001:   5215 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0729, wps=103.6, ups=0.62, wpb=111.7, bsz=40, num_updates=5210, lr=4.93698e-05, gnorm=1.784, clip=80, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=40391
2023-01-11 11:17:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:09 - progress_bar.py[line:274] - INFO: epoch 001:   5225 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0636, wps=101.5, ups=0.61, wpb=110.9, bsz=40, num_updates=5220, lr=4.93646e-05, gnorm=2.25, clip=80, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40408
2023-01-11 11:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:26 - progress_bar.py[line:274] - INFO: epoch 001:   5235 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.0333, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=5230, lr=4.93594e-05, gnorm=0.845, clip=20, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=40424
2023-01-11 11:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:43 - progress_bar.py[line:274] - INFO: epoch 001:   5245 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.037, wps=97.7, ups=0.6, wpb=108.9, bsz=40, num_updates=5240, lr=4.93542e-05, gnorm=2.114, clip=70, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40441
2023-01-11 11:18:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:18:59 - progress_bar.py[line:274] - INFO: epoch 001:   5255 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0476, wps=100.7, ups=0.62, wpb=109.1, bsz=40, num_updates=5250, lr=4.9349e-05, gnorm=1.98, clip=60, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40458
2023-01-11 11:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:16 - progress_bar.py[line:274] - INFO: epoch 001:   5265 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0361, wps=100.2, ups=0.61, wpb=110.3, bsz=40, num_updates=5260, lr=4.93437e-05, gnorm=1.119, clip=40, loss_scale=1024, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=40474
2023-01-11 11:19:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:32 - progress_bar.py[line:274] - INFO: epoch 001:   5275 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0412, wps=100.8, ups=0.61, wpb=109.9, bsz=40, num_updates=5270, lr=4.93385e-05, gnorm=1.376, clip=60, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=40491
2023-01-11 11:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:49 - progress_bar.py[line:274] - INFO: epoch 001:   5285 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0388, wps=98.5, ups=0.61, wpb=108.1, bsz=40, num_updates=5280, lr=4.93333e-05, gnorm=1.041, clip=40, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40508
2023-01-11 11:19:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:06 - progress_bar.py[line:274] - INFO: epoch 001:   5295 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0505, wps=100.2, ups=0.61, wpb=110.3, bsz=40, num_updates=5290, lr=4.93281e-05, gnorm=1.558, clip=80, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=40525
2023-01-11 11:20:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:23 - progress_bar.py[line:274] - INFO: epoch 001:   5305 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0833, wps=99.5, ups=0.61, wpb=108.4, bsz=40, num_updates=5300, lr=4.93229e-05, gnorm=0.676, clip=10, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40541
2023-01-11 11:20:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:39 - progress_bar.py[line:274] - INFO: epoch 001:   5315 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0206, wps=98.7, ups=0.61, wpb=108.7, bsz=40, num_updates=5310, lr=4.93177e-05, gnorm=1.257, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40558
2023-01-11 11:20:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:56 - progress_bar.py[line:274] - INFO: epoch 001:   5325 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1224, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=5320, lr=4.93125e-05, gnorm=1.06, clip=70, loss_scale=2048, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=40574
2023-01-11 11:20:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:20:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:12 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-01-11 11:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:15 - progress_bar.py[line:274] - INFO: epoch 001:   5336 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0846, wps=96.4, ups=0.55, wpb=110, bsz=40, num_updates=5330, lr=4.93073e-05, gnorm=1.31, clip=60, loss_scale=1024, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=40593
2023-01-11 11:21:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:31 - progress_bar.py[line:274] - INFO: epoch 001:   5346 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0761, wps=103.2, ups=0.62, wpb=111.7, bsz=40, num_updates=5340, lr=4.93021e-05, gnorm=1.045, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40610
2023-01-11 11:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:48 - progress_bar.py[line:274] - INFO: epoch 001:   5356 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0901, wps=99, ups=0.61, wpb=108.4, bsz=40, num_updates=5350, lr=4.92969e-05, gnorm=0.985, clip=50, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=40626
2023-01-11 11:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:04 - progress_bar.py[line:274] - INFO: epoch 001:   5366 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0874, wps=101.7, ups=0.61, wpb=110.9, bsz=40, num_updates=5360, lr=4.92917e-05, gnorm=1.607, clip=50, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=40643
2023-01-11 11:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:22 - progress_bar.py[line:274] - INFO: epoch 001:   5376 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.044, wps=99, ups=0.6, wpb=110.3, bsz=40, num_updates=5370, lr=4.92865e-05, gnorm=1.075, clip=40, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40660
2023-01-11 11:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:38 - progress_bar.py[line:274] - INFO: epoch 001:   5386 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.087, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=5380, lr=4.92813e-05, gnorm=1.002, clip=30, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40677
2023-01-11 11:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:55 - progress_bar.py[line:274] - INFO: epoch 001:   5396 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0753, wps=100.3, ups=0.61, wpb=110.2, bsz=40, num_updates=5390, lr=4.9276e-05, gnorm=2.049, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40694
2023-01-11 11:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:12 - progress_bar.py[line:274] - INFO: epoch 001:   5406 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0673, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=5400, lr=4.92708e-05, gnorm=0.906, clip=30, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40710
2023-01-11 11:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:29 - progress_bar.py[line:274] - INFO: epoch 001:   5416 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0792, wps=97.4, ups=0.59, wpb=110.1, bsz=40, num_updates=5410, lr=4.92656e-05, gnorm=0.922, clip=50, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40728
2023-01-11 11:23:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:45 - progress_bar.py[line:274] - INFO: epoch 001:   5426 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0891, wps=100.2, ups=0.62, wpb=108.6, bsz=40, num_updates=5420, lr=4.92604e-05, gnorm=1.063, clip=40, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40744
2023-01-11 11:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:02 - progress_bar.py[line:274] - INFO: epoch 001:   5436 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0667, wps=100.6, ups=0.61, wpb=110.5, bsz=40, num_updates=5430, lr=4.92552e-05, gnorm=0.963, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40761
2023-01-11 11:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:19 - progress_bar.py[line:274] - INFO: epoch 001:   5446 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.068, wps=99, ups=0.61, wpb=108.5, bsz=40, num_updates=5440, lr=4.925e-05, gnorm=1.12, clip=60, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40777
2023-01-11 11:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:35 - progress_bar.py[line:274] - INFO: epoch 001:   5456 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.086, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=5450, lr=4.92448e-05, gnorm=1.216, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40794
2023-01-11 11:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:52 - progress_bar.py[line:274] - INFO: epoch 001:   5466 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0288, wps=100, ups=0.61, wpb=110.1, bsz=40, num_updates=5460, lr=4.92396e-05, gnorm=0.933, clip=30, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40811
2023-01-11 11:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:09 - progress_bar.py[line:274] - INFO: epoch 001:   5476 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0217, wps=101.5, ups=0.61, wpb=111.4, bsz=40, num_updates=5470, lr=4.92344e-05, gnorm=1.151, clip=40, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40827
2023-01-11 11:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:25 - progress_bar.py[line:274] - INFO: epoch 001:   5486 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0521, wps=101.7, ups=0.62, wpb=109.1, bsz=40, num_updates=5480, lr=4.92292e-05, gnorm=1.047, clip=40, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40844
2023-01-11 11:25:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   5496 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0889, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=5490, lr=4.9224e-05, gnorm=1.4, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40861
2023-01-11 11:25:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:25:59 - progress_bar.py[line:274] - INFO: epoch 001:   5506 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0381, wps=99, ups=0.61, wpb=108.3, bsz=40, num_updates=5500, lr=4.92188e-05, gnorm=1.945, clip=70, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40877
2023-01-11 11:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:15 - progress_bar.py[line:274] - INFO: epoch 001:   5516 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0353, wps=102.7, ups=0.62, wpb=111.3, bsz=40, num_updates=5510, lr=4.92135e-05, gnorm=1.431, clip=60, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40894
2023-01-11 11:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:31 - progress_bar.py[line:274] - INFO: epoch 001:   5526 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0935, wps=103.2, ups=0.63, wpb=109.5, bsz=40, num_updates=5520, lr=4.92083e-05, gnorm=1.732, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40910
2023-01-11 11:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:48 - progress_bar.py[line:274] - INFO: epoch 001:   5536 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0737, wps=99.8, ups=0.6, wpb=110.7, bsz=40, num_updates=5530, lr=4.92031e-05, gnorm=1.191, clip=60, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40927
2023-01-11 11:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:26:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:06 - progress_bar.py[line:274] - INFO: epoch 001:   5546 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0396, wps=97.7, ups=0.6, wpb=109, bsz=40, num_updates=5540, lr=4.91979e-05, gnorm=1.282, clip=50, loss_scale=1024, train_wall=17, gb_free=9.6, ema_decay=0.9999, wall=40944
2023-01-11 11:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:22 - progress_bar.py[line:274] - INFO: epoch 001:   5556 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1443, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=5550, lr=4.91927e-05, gnorm=1.55, clip=70, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40961
2023-01-11 11:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:39 - progress_bar.py[line:274] - INFO: epoch 001:   5566 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0714, wps=100.3, ups=0.61, wpb=109.1, bsz=40, num_updates=5560, lr=4.91875e-05, gnorm=1.03, clip=50, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40978
2023-01-11 11:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:56 - progress_bar.py[line:274] - INFO: epoch 001:   5576 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1031, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=5570, lr=4.91823e-05, gnorm=0.817, clip=30, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40994
2023-01-11 11:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   5586 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0467, wps=101.1, ups=0.61, wpb=110.4, bsz=40, num_updates=5580, lr=4.91771e-05, gnorm=1.4, clip=60, loss_scale=1024, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=41011
2023-01-11 11:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:30 - progress_bar.py[line:274] - INFO: epoch 001:   5596 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.11, wps=94.6, ups=0.58, wpb=109.6, bsz=40, num_updates=5590, lr=4.91719e-05, gnorm=0.883, clip=30, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=41029
2023-01-11 11:28:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:47 - progress_bar.py[line:274] - INFO: epoch 001:   5606 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.061, wps=100.4, ups=0.6, wpb=111, bsz=40, num_updates=5600, lr=4.91667e-05, gnorm=0.902, clip=40, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=41045
2023-01-11 11:28:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:03 - progress_bar.py[line:274] - INFO: epoch 001:   5616 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0762, wps=102, ups=0.62, wpb=109.7, bsz=40, num_updates=5610, lr=4.91615e-05, gnorm=0.767, clip=20, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=41062
2023-01-11 11:29:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:20 - progress_bar.py[line:274] - INFO: epoch 001:   5626 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1383, wps=99.8, ups=0.6, wpb=110.7, bsz=40, num_updates=5620, lr=4.91563e-05, gnorm=1.269, clip=60, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=41079
2023-01-11 11:29:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:37 - progress_bar.py[line:274] - INFO: epoch 001:   5636 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0968, wps=100.3, ups=0.6, wpb=111.8, bsz=40, num_updates=5630, lr=4.9151e-05, gnorm=0.679, clip=10, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=41096
2023-01-11 11:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:54 - progress_bar.py[line:274] - INFO: epoch 001:   5646 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0515, wps=98, ups=0.6, wpb=109.6, bsz=40, num_updates=5640, lr=4.91458e-05, gnorm=0.94, clip=30, loss_scale=1024, train_wall=17, gb_free=10, ema_decay=0.9999, wall=41113
2023-01-11 11:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:11 - progress_bar.py[line:274] - INFO: epoch 001:   5656 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0693, wps=102.5, ups=0.62, wpb=110.1, bsz=40, num_updates=5650, lr=4.91406e-05, gnorm=0.826, clip=20, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=41130
2023-01-11 11:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:28 - progress_bar.py[line:274] - INFO: epoch 001:   5666 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0737, wps=97.1, ups=0.6, wpb=108.7, bsz=40, num_updates=5660, lr=4.91354e-05, gnorm=1.763, clip=80, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=41147
2023-01-11 11:30:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:45 - progress_bar.py[line:274] - INFO: epoch 001:   5676 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.098, wps=99.1, ups=0.6, wpb=109.3, bsz=40, num_updates=5670, lr=4.91302e-05, gnorm=0.986, clip=40, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=41163
2023-01-11 11:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:02 - progress_bar.py[line:274] - INFO: epoch 001:   5686 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0404, wps=97.9, ups=0.6, wpb=108.8, bsz=40, num_updates=5680, lr=4.9125e-05, gnorm=1.291, clip=60, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=41181
2023-01-11 11:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:04 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 11:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:21 - progress_bar.py[line:274] - INFO: epoch 001:   5697 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0413, wps=95, ups=0.54, wpb=110, bsz=40, num_updates=5690, lr=4.91198e-05, gnorm=1.453, clip=70, loss_scale=512, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=41199
2023-01-11 11:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:37 - progress_bar.py[line:274] - INFO: epoch 001:   5707 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0808, wps=100.6, ups=0.61, wpb=109.7, bsz=40, num_updates=5700, lr=4.91146e-05, gnorm=1.285, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41216
2023-01-11 11:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:55 - progress_bar.py[line:274] - INFO: epoch 001:   5717 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1058, wps=97.6, ups=0.6, wpb=109.1, bsz=40, num_updates=5710, lr=4.91094e-05, gnorm=1.337, clip=70, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=41233
2023-01-11 11:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:11 - progress_bar.py[line:274] - INFO: epoch 001:   5727 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0673, wps=99.5, ups=0.61, wpb=109, bsz=40, num_updates=5720, lr=4.91042e-05, gnorm=1.442, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41250
2023-01-11 11:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:28 - progress_bar.py[line:274] - INFO: epoch 001:   5737 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0652, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=5730, lr=4.9099e-05, gnorm=1.162, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41267
2023-01-11 11:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:45 - progress_bar.py[line:274] - INFO: epoch 001:   5747 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1319, wps=99.9, ups=0.6, wpb=110.1, bsz=40, num_updates=5740, lr=4.90938e-05, gnorm=1.004, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=41284
2023-01-11 11:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:03 - progress_bar.py[line:274] - INFO: epoch 001:   5757 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0297, wps=98.6, ups=0.6, wpb=108.8, bsz=40, num_updates=5750, lr=4.90885e-05, gnorm=1.1, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=41301
2023-01-11 11:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:20 - progress_bar.py[line:274] - INFO: epoch 001:   5767 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0851, wps=99.9, ups=0.6, wpb=111, bsz=40, num_updates=5760, lr=4.90833e-05, gnorm=1.542, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=41319
2023-01-11 11:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:38 - progress_bar.py[line:274] - INFO: epoch 001:   5777 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1165, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=5770, lr=4.90781e-05, gnorm=1.266, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=41335
2023-01-11 11:33:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:55 - progress_bar.py[line:274] - INFO: epoch 001:   5787 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0796, wps=101.3, ups=0.62, wpb=108.7, bsz=40, num_updates=5780, lr=4.90729e-05, gnorm=1.221, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=41353
2023-01-11 11:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:12 - progress_bar.py[line:274] - INFO: epoch 001:   5797 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.09, wps=101.1, ups=0.61, wpb=111.1, bsz=40, num_updates=5790, lr=4.90677e-05, gnorm=0.959, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=41370
2023-01-11 11:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:30 - progress_bar.py[line:274] - INFO: epoch 001:   5807 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0202, wps=98.7, ups=0.6, wpb=109.6, bsz=40, num_updates=5800, lr=4.90625e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=41388
2023-01-11 11:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:48 - progress_bar.py[line:274] - INFO: epoch 001:   5817 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0702, wps=99.5, ups=0.61, wpb=108.4, bsz=40, num_updates=5810, lr=4.90573e-05, gnorm=1.217, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=41406
2023-01-11 11:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:05 - progress_bar.py[line:274] - INFO: epoch 001:   5827 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0909, wps=98.9, ups=0.6, wpb=109.5, bsz=40, num_updates=5820, lr=4.90521e-05, gnorm=1.065, clip=50, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=41424
2023-01-11 11:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:22 - progress_bar.py[line:274] - INFO: epoch 001:   5837 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0784, wps=101.7, ups=0.63, wpb=108.3, bsz=40, num_updates=5830, lr=4.90469e-05, gnorm=1.108, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41440
2023-01-11 11:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:38 - progress_bar.py[line:274] - INFO: epoch 001:   5847 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0729, wps=102.8, ups=0.62, wpb=110.8, bsz=40, num_updates=5840, lr=4.90417e-05, gnorm=0.813, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=41457
2023-01-11 11:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:55 - progress_bar.py[line:274] - INFO: epoch 001:   5857 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.134, wps=99.6, ups=0.61, wpb=109.7, bsz=40, num_updates=5850, lr=4.90365e-05, gnorm=1.274, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41474
2023-01-11 11:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:12 - progress_bar.py[line:274] - INFO: epoch 001:   5867 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0625, wps=100.8, ups=0.61, wpb=109.9, bsz=40, num_updates=5860, lr=4.90313e-05, gnorm=1.136, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41491
2023-01-11 11:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:29 - progress_bar.py[line:274] - INFO: epoch 001:   5877 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=107.333, nsentences=40, sample_size=107.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0769, wps=98.5, ups=0.61, wpb=107.3, bsz=40, num_updates=5870, lr=4.9026e-05, gnorm=0.973, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41508
2023-01-11 11:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:46 - progress_bar.py[line:274] - INFO: epoch 001:   5887 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0388, wps=100, ups=0.61, wpb=109.6, bsz=40, num_updates=5880, lr=4.90208e-05, gnorm=1.248, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=41525
2023-01-11 11:36:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:03 - progress_bar.py[line:274] - INFO: epoch 001:   5897 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.086, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=5890, lr=4.90156e-05, gnorm=1.158, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=41542
2023-01-11 11:37:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:20 - progress_bar.py[line:274] - INFO: epoch 001:   5907 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0316, wps=100, ups=0.61, wpb=110.2, bsz=40, num_updates=5900, lr=4.90104e-05, gnorm=0.773, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=41558
2023-01-11 11:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:37 - progress_bar.py[line:274] - INFO: epoch 001:   5917 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0965, wps=100, ups=0.6, wpb=110.3, bsz=40, num_updates=5910, lr=4.90052e-05, gnorm=1.226, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=41575
2023-01-11 11:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:53 - progress_bar.py[line:274] - INFO: epoch 001:   5927 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1, wps=103.2, ups=0.62, wpb=110.1, bsz=40, num_updates=5920, lr=4.9e-05, gnorm=0.983, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=41592
2023-01-11 11:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:37:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:09 - progress_bar.py[line:274] - INFO: epoch 001:   5937 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0515, wps=102.9, ups=0.62, wpb=110.1, bsz=40, num_updates=5930, lr=4.89948e-05, gnorm=1.043, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=41608
2023-01-11 11:38:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:26 - progress_bar.py[line:274] - INFO: epoch 001:   5947 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0784, wps=102.7, ups=0.62, wpb=109.8, bsz=40, num_updates=5940, lr=4.89896e-05, gnorm=0.908, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41624
2023-01-11 11:38:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:43 - progress_bar.py[line:274] - INFO: epoch 001:   5957 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0654, wps=97.7, ups=0.6, wpb=108, bsz=40, num_updates=5950, lr=4.89844e-05, gnorm=1.237, clip=50, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=41641
2023-01-11 11:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:38:59 - progress_bar.py[line:274] - INFO: epoch 001:   5967 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1111, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=5960, lr=4.89792e-05, gnorm=1.12, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41658
2023-01-11 11:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:16 - progress_bar.py[line:274] - INFO: epoch 001:   5977 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0323, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=5970, lr=4.8974e-05, gnorm=1.463, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=41675
2023-01-11 11:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:33 - progress_bar.py[line:274] - INFO: epoch 001:   5987 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.066, wps=100.6, ups=0.61, wpb=109.3, bsz=40, num_updates=5980, lr=4.89688e-05, gnorm=0.971, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=41691
2023-01-11 11:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:49 - progress_bar.py[line:274] - INFO: epoch 001:   5997 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1196, wps=102.3, ups=0.62, wpb=110.6, bsz=40, num_updates=5990, lr=4.89635e-05, gnorm=1.113, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=41708
2023-01-11 11:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 11:40:06 - progress_bar.py[line:274] - INFO: epoch 001:   6007 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0808, wps=101.1, ups=0.61, wpb=110.2, bsz=40, num_updates=6000, lr=4.89583e-05, gnorm=0.994, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=41725
2023-01-11 11:40:06 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 11:40:07 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 11:40:07 - train.py[line:551] - INFO: load:0.92 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 11:42:09 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 11:42:09 - train.py[line:551] - INFO: load:0.95 valid_run:122.07 task_valid:119.05 collect_output:1.97
2023-01-11 11:44:10 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 11:44:10 - train.py[line:551] - INFO: load:0.97 valid_run:242.56 task_valid:234.94 collect_output:5.46
2023-01-11 11:46:12 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 11:46:12 - train.py[line:551] - INFO: load:1.00 valid_run:364.27 task_valid:351.06 collect_output:10.00
2023-01-11 11:48:14 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 11:48:14 - train.py[line:551] - INFO: load:1.02 valid_run:486.23 task_valid:464.55 collect_output:17.37
2023-01-11 11:50:14 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 11:50:14 - train.py[line:551] - INFO: load:1.05 valid_run:606.86 task_valid:581.60 collect_output:19.85
2023-01-11 11:52:17 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 11:52:17 - train.py[line:551] - INFO: load:1.07 valid_run:729.67 task_valid:700.02 collect_output:23.18
2023-01-11 11:54:21 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 11:54:21 - train.py[line:551] - INFO: load:1.10 valid_run:852.96 task_valid:817.97 collect_output:27.38
2023-01-11 11:56:23 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 11:56:23 - train.py[line:551] - INFO: load:1.12 valid_run:974.77 task_valid:934.28 collect_output:31.80
2023-01-11 11:58:26 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 11:58:26 - train.py[line:551] - INFO: load:1.15 valid_run:1098.20 task_valid:1051.04 collect_output:37.42
2023-01-11 12:00:28 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 12:00:28 - train.py[line:551] - INFO: load:1.18 valid_run:1220.07 task_valid:1163.52 collect_output:45.70
2023-01-11 12:02:28 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 12:02:28 - train.py[line:551] - INFO: load:1.20 valid_run:1340.48 task_valid:1278.92 collect_output:49.62
2023-01-11 12:04:30 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 12:04:30 - train.py[line:551] - INFO: load:1.23 valid_run:1461.61 task_valid:1395.26 collect_output:53.38
2023-01-11 12:06:29 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 12:06:29 - train.py[line:551] - INFO: load:1.25 valid_run:1580.76 task_valid:1509.10 collect_output:57.55
2023-01-11 12:08:30 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 12:08:30 - train.py[line:551] - INFO: load:1.28 valid_run:1701.78 task_valid:1626.84 collect_output:59.75
2023-01-11 12:10:31 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 12:10:31 - train.py[line:551] - INFO: load:1.30 valid_run:1822.84 task_valid:1742.98 collect_output:63.58
2023-01-11 12:12:32 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 12:12:32 - train.py[line:551] - INFO: load:1.33 valid_run:1943.91 task_valid:1856.84 collect_output:69.70
2023-01-11 12:14:33 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 12:14:33 - train.py[line:551] - INFO: load:1.36 valid_run:2064.98 task_valid:1972.79 collect_output:73.75
2023-01-11 12:16:34 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 12:16:34 - train.py[line:551] - INFO: load:1.38 valid_run:2185.63 task_valid:2090.65 collect_output:75.45
2023-01-11 12:18:35 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 12:18:35 - train.py[line:551] - INFO: load:1.41 valid_run:2306.63 task_valid:2207.39 collect_output:78.64
2023-01-11 12:20:36 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 12:20:36 - train.py[line:551] - INFO: load:1.43 valid_run:2426.92 task_valid:2323.72 collect_output:81.54
2023-01-11 12:22:37 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 12:22:37 - train.py[line:551] - INFO: load:1.46 valid_run:2548.41 task_valid:2439.90 collect_output:85.80
2023-01-11 12:24:39 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 12:24:39 - train.py[line:551] - INFO: load:1.48 valid_run:2670.34 task_valid:2558.47 collect_output:88.10
2023-01-11 12:26:40 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 12:26:40 - train.py[line:551] - INFO: load:1.51 valid_run:2790.57 task_valid:2672.56 collect_output:93.15
2023-01-11 12:28:40 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 12:28:40 - train.py[line:551] - INFO: load:1.53 valid_run:2910.47 task_valid:2788.69 collect_output:95.86
2023-01-11 12:30:41 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 12:30:41 - train.py[line:551] - INFO: load:1.56 valid_run:3032.18 task_valid:2905.06 collect_output:100.11
2023-01-11 12:32:44 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 12:32:44 - train.py[line:551] - INFO: load:1.59 valid_run:3155.12 task_valid:3021.03 collect_output:106.00
2023-01-11 12:34:44 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 12:34:44 - train.py[line:551] - INFO: load:1.61 valid_run:3274.64 task_valid:3135.29 collect_output:110.18
2023-01-11 12:36:46 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 12:36:46 - train.py[line:551] - INFO: load:1.64 valid_run:3396.46 task_valid:3254.40 collect_output:111.79
2023-01-11 12:38:48 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 12:38:48 - train.py[line:551] - INFO: load:1.66 valid_run:3518.13 task_valid:3369.78 collect_output:117.01
2023-01-11 12:40:50 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 12:40:50 - train.py[line:551] - INFO: load:1.69 valid_run:3640.37 task_valid:3488.44 collect_output:119.45
2023-01-11 12:42:51 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 12:42:51 - train.py[line:551] - INFO: load:1.72 valid_run:3761.26 task_valid:3606.68 collect_output:121.02

====================================================================================================
SGG eval:     R @ 50: 0.6643;     R @ 100: 0.6976;     R @ 500: 0.7297;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4396;    mR @ 100: 0.5029;    mR @ 500: 0.5419;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.5143) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9167) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.8095) (standing on:0.3643) (using:0.5500) (walking in:0.0000) (walking on:0.7838) (watching:0.5000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6643;     R @ 100: 0.6976;     R @ 500: 0.7297;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4396;    mR @ 100: 0.5029;    mR @ 500: 0.5419;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.5143) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9167) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.8095) (standing on:0.3643) (using:0.5500) (walking in:0.0000) (walking on:0.7838) (watching:0.5000) 
--------------------------------------------------------
====================================================================================================

2023-01-11 12:43:22 - train.py[line:487] - INFO: 0.6975836771072066
2023-01-11 12:43:22 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 12:43:23 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.286 | loss_v1 0 | loss_v2 0 | nll_loss 0.127 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.697584 | ppl 1.09 | vqa_score 0.545 | wps 118.2 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.697584
2023-01-11 12:43:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2023-01-11 12:43:23 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-11 12:44:07 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-11 12:46:58 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6975836771072066) (writing took 215.1278370730579 seconds)
2023-01-11 12:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:14 - progress_bar.py[line:274] - INFO: epoch 001:   6017 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0727, wps=0.4, ups=0, wpb=108.8, bsz=40, num_updates=6010, lr=4.89531e-05, gnorm=1.33, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45753
2023-01-11 12:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:31 - progress_bar.py[line:274] - INFO: epoch 001:   6027 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0901, wps=100.4, ups=0.62, wpb=108.3, bsz=40, num_updates=6020, lr=4.89479e-05, gnorm=1.002, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45770
2023-01-11 12:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:48 - progress_bar.py[line:274] - INFO: epoch 001:   6037 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0909, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=6030, lr=4.89427e-05, gnorm=0.87, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45786
2023-01-11 12:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:05 - progress_bar.py[line:274] - INFO: epoch 001:   6047 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0645, wps=99.9, ups=0.6, wpb=111.4, bsz=40, num_updates=6040, lr=4.89375e-05, gnorm=0.916, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=45803
2023-01-11 12:48:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:22 - progress_bar.py[line:274] - INFO: epoch 001:   6057 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0909, wps=99.2, ups=0.6, wpb=110.1, bsz=40, num_updates=6050, lr=4.89323e-05, gnorm=1.646, clip=70, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=45820
2023-01-11 12:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:39 - progress_bar.py[line:274] - INFO: epoch 001:   6067 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0648, wps=99.3, ups=0.61, wpb=109, bsz=40, num_updates=6060, lr=4.89271e-05, gnorm=2.088, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45837
2023-01-11 12:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:56 - progress_bar.py[line:274] - INFO: epoch 001:   6077 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0612, wps=99.6, ups=0.6, wpb=110.7, bsz=40, num_updates=6070, lr=4.89219e-05, gnorm=1.009, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=45854
2023-01-11 12:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:12 - progress_bar.py[line:274] - INFO: epoch 001:   6087 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0561, wps=101.8, ups=0.61, wpb=110.4, bsz=40, num_updates=6080, lr=4.89167e-05, gnorm=0.935, clip=50, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=45871
2023-01-11 12:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:29 - progress_bar.py[line:274] - INFO: epoch 001:   6097 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1287, wps=101, ups=0.61, wpb=110.1, bsz=40, num_updates=6090, lr=4.89115e-05, gnorm=0.797, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45887
2023-01-11 12:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:45 - progress_bar.py[line:274] - INFO: epoch 001:   6107 / 100000 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0667, wps=101.7, ups=0.62, wpb=108.7, bsz=40, num_updates=6100, lr=4.89063e-05, gnorm=1.678, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45904
2023-01-11 12:49:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:49:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:02 - progress_bar.py[line:274] - INFO: epoch 001:   6117 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0495, wps=101.5, ups=0.62, wpb=109.5, bsz=40, num_updates=6110, lr=4.8901e-05, gnorm=0.972, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45920
2023-01-11 12:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:18 - progress_bar.py[line:274] - INFO: epoch 001:   6127 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.049, wps=101.5, ups=0.62, wpb=108.7, bsz=40, num_updates=6120, lr=4.88958e-05, gnorm=0.898, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45937
2023-01-11 12:50:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:35 - progress_bar.py[line:274] - INFO: epoch 001:   6137 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0421, wps=99.4, ups=0.6, wpb=110, bsz=40, num_updates=6130, lr=4.88906e-05, gnorm=1.196, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=45954
2023-01-11 12:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:52 - progress_bar.py[line:274] - INFO: epoch 001:   6147 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1196, wps=102.6, ups=0.61, wpb=111.3, bsz=40, num_updates=6140, lr=4.88854e-05, gnorm=0.941, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45971
2023-01-11 12:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:09 - progress_bar.py[line:274] - INFO: epoch 001:   6157 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0727, wps=97.8, ups=0.6, wpb=108.5, bsz=40, num_updates=6150, lr=4.88802e-05, gnorm=1.159, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=45988
2023-01-11 12:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:26 - progress_bar.py[line:274] - INFO: epoch 001:   6167 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0583, wps=98.7, ups=0.6, wpb=109.7, bsz=40, num_updates=6160, lr=4.8875e-05, gnorm=1.208, clip=40, loss_scale=512, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=46004
2023-01-11 12:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:43 - progress_bar.py[line:274] - INFO: epoch 001:   6177 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0808, wps=99.8, ups=0.6, wpb=110.1, bsz=40, num_updates=6170, lr=4.88698e-05, gnorm=1.124, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46021
2023-01-11 12:51:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:51:59 - progress_bar.py[line:274] - INFO: epoch 001:   6187 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.07, wps=99.5, ups=0.61, wpb=108.3, bsz=40, num_updates=6180, lr=4.88646e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46038
2023-01-11 12:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:16 - progress_bar.py[line:274] - INFO: epoch 001:   6197 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0734, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=6190, lr=4.88594e-05, gnorm=0.803, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46055
2023-01-11 12:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:32 - progress_bar.py[line:274] - INFO: epoch 001:   6207 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0288, wps=102.8, ups=0.62, wpb=110, bsz=40, num_updates=6200, lr=4.88542e-05, gnorm=0.884, clip=40, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46071
2023-01-11 12:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:49 - progress_bar.py[line:274] - INFO: epoch 001:   6217 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1038, wps=101.1, ups=0.62, wpb=109.2, bsz=40, num_updates=6210, lr=4.8849e-05, gnorm=1.056, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46088
2023-01-11 12:52:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:06 - progress_bar.py[line:274] - INFO: epoch 001:   6227 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.0864, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=6220, lr=4.88438e-05, gnorm=0.733, clip=10, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46104
2023-01-11 12:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:14 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 12:53:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:25 - progress_bar.py[line:274] - INFO: epoch 001:   6238 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.938, nsentences=40, sample_size=109.938, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1081, wps=94.6, ups=0.54, wpb=109.9, bsz=40, num_updates=6230, lr=4.88385e-05, gnorm=1.094, clip=50, loss_scale=512, train_wall=19, gb_free=10.3, ema_decay=0.9999, wall=46123
2023-01-11 12:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:41 - progress_bar.py[line:274] - INFO: epoch 001:   6248 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1224, wps=101, ups=0.61, wpb=110.6, bsz=40, num_updates=6240, lr=4.88333e-05, gnorm=1.131, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46140
2023-01-11 12:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:53:58 - progress_bar.py[line:274] - INFO: epoch 001:   6258 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0891, wps=101.3, ups=0.61, wpb=110.9, bsz=40, num_updates=6250, lr=4.88281e-05, gnorm=0.971, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46157
2023-01-11 12:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:14 - progress_bar.py[line:274] - INFO: epoch 001:   6268 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0755, wps=103.3, ups=0.63, wpb=109.1, bsz=40, num_updates=6260, lr=4.88229e-05, gnorm=1.105, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46173
2023-01-11 12:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:30 - progress_bar.py[line:274] - INFO: epoch 001:   6278 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0571, wps=101.1, ups=0.62, wpb=108.2, bsz=40, num_updates=6270, lr=4.88177e-05, gnorm=0.992, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46189
2023-01-11 12:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:47 - progress_bar.py[line:274] - INFO: epoch 001:   6288 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0714, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=6280, lr=4.88125e-05, gnorm=1.128, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46206
2023-01-11 12:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:04 - progress_bar.py[line:274] - INFO: epoch 001:   6298 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0682, wps=100.9, ups=0.61, wpb=110.6, bsz=40, num_updates=6290, lr=4.88073e-05, gnorm=1.212, clip=60, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=46223
2023-01-11 12:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:20 - progress_bar.py[line:274] - INFO: epoch 001:   6308 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0543, wps=102.6, ups=0.62, wpb=111.1, bsz=40, num_updates=6300, lr=4.88021e-05, gnorm=0.978, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46239
2023-01-11 12:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:37 - progress_bar.py[line:274] - INFO: epoch 001:   6318 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0238, wps=102.3, ups=0.61, wpb=110.9, bsz=40, num_updates=6310, lr=4.87969e-05, gnorm=1.193, clip=50, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=46256
2023-01-11 12:55:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:54 - progress_bar.py[line:274] - INFO: epoch 001:   6328 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1209, wps=101.1, ups=0.61, wpb=111.3, bsz=40, num_updates=6320, lr=4.87917e-05, gnorm=0.88, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46272
2023-01-11 12:55:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:55:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:10 - progress_bar.py[line:274] - INFO: epoch 001:   6338 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0632, wps=103.5, ups=0.62, wpb=110.5, bsz=40, num_updates=6330, lr=4.87865e-05, gnorm=0.796, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=46289
2023-01-11 12:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:27 - progress_bar.py[line:274] - INFO: epoch 001:   6348 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1391, wps=97.6, ups=0.6, wpb=108.9, bsz=40, num_updates=6340, lr=4.87813e-05, gnorm=0.936, clip=30, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=46306
2023-01-11 12:56:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:43 - progress_bar.py[line:274] - INFO: epoch 001:   6358 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0748, wps=102, ups=0.62, wpb=109.5, bsz=40, num_updates=6350, lr=4.8776e-05, gnorm=1.129, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46322
2023-01-11 12:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:00 - progress_bar.py[line:274] - INFO: epoch 001:   6368 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0659, wps=101, ups=0.61, wpb=110.4, bsz=40, num_updates=6360, lr=4.87708e-05, gnorm=1.044, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46339
2023-01-11 12:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:17 - progress_bar.py[line:274] - INFO: epoch 001:   6378 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0811, wps=100, ups=0.62, wpb=108, bsz=40, num_updates=6370, lr=4.87656e-05, gnorm=1.147, clip=50, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=46355
2023-01-11 12:57:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:33 - progress_bar.py[line:274] - INFO: epoch 001:   6388 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0686, wps=103.4, ups=0.63, wpb=109.8, bsz=40, num_updates=6380, lr=4.87604e-05, gnorm=1.219, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=46372
2023-01-11 12:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:49 - progress_bar.py[line:274] - INFO: epoch 001:   6398 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0449, wps=103.8, ups=0.63, wpb=110.3, bsz=40, num_updates=6390, lr=4.87552e-05, gnorm=0.974, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46388
2023-01-11 12:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:06 - progress_bar.py[line:274] - INFO: epoch 001:   6408 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1398, wps=101, ups=0.61, wpb=110, bsz=40, num_updates=6400, lr=4.875e-05, gnorm=1.374, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46404
2023-01-11 12:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:23 - progress_bar.py[line:274] - INFO: epoch 001:   6418 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0495, wps=99.3, ups=0.61, wpb=109.3, bsz=40, num_updates=6410, lr=4.87448e-05, gnorm=1.282, clip=40, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=46421
2023-01-11 12:58:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:40 - progress_bar.py[line:274] - INFO: epoch 001:   6428 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.098, wps=97.7, ups=0.6, wpb=108.7, bsz=40, num_updates=6420, lr=4.87396e-05, gnorm=0.994, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=46438
2023-01-11 12:58:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:56 - progress_bar.py[line:274] - INFO: epoch 001:   6438 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0777, wps=100.5, ups=0.61, wpb=110.5, bsz=40, num_updates=6430, lr=4.87344e-05, gnorm=0.837, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46455
2023-01-11 12:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:14 - progress_bar.py[line:274] - INFO: epoch 001:   6448 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1064, wps=97.3, ups=0.59, wpb=109.5, bsz=40, num_updates=6440, lr=4.87292e-05, gnorm=0.951, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=46472
2023-01-11 12:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:30 - progress_bar.py[line:274] - INFO: epoch 001:   6458 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0857, wps=100.8, ups=0.62, wpb=109.3, bsz=40, num_updates=6450, lr=4.8724e-05, gnorm=1.529, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46489
2023-01-11 12:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:47 - progress_bar.py[line:274] - INFO: epoch 001:   6468 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1313, wps=100.8, ups=0.61, wpb=109.5, bsz=40, num_updates=6460, lr=4.87188e-05, gnorm=0.802, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46505
2023-01-11 12:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 12:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:04 - progress_bar.py[line:274] - INFO: epoch 001:   6478 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1064, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=6470, lr=4.87135e-05, gnorm=1.531, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46522
2023-01-11 13:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:21 - progress_bar.py[line:274] - INFO: epoch 001:   6488 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1215, wps=97.6, ups=0.6, wpb=109.3, bsz=40, num_updates=6480, lr=4.87083e-05, gnorm=1.005, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=46539
2023-01-11 13:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:37 - progress_bar.py[line:274] - INFO: epoch 001:   6498 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1091, wps=99.7, ups=0.61, wpb=108.8, bsz=40, num_updates=6490, lr=4.87031e-05, gnorm=1.291, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=46556
2023-01-11 13:00:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:54 - progress_bar.py[line:274] - INFO: epoch 001:   6508 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0707, wps=99.9, ups=0.61, wpb=109.8, bsz=40, num_updates=6500, lr=4.86979e-05, gnorm=1.065, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46573
2023-01-11 13:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:10 - progress_bar.py[line:274] - INFO: epoch 001:   6518 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0495, wps=103.1, ups=0.62, wpb=110.2, bsz=40, num_updates=6510, lr=4.86927e-05, gnorm=1.044, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46589
2023-01-11 13:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:27 - progress_bar.py[line:274] - INFO: epoch 001:   6528 / 100000 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0948, wps=98.2, ups=0.61, wpb=107.9, bsz=40, num_updates=6520, lr=4.86875e-05, gnorm=0.993, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46606
2023-01-11 13:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:44 - progress_bar.py[line:274] - INFO: epoch 001:   6538 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1143, wps=101.5, ups=0.61, wpb=110.9, bsz=40, num_updates=6530, lr=4.86823e-05, gnorm=0.882, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46623
2023-01-11 13:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:00 - progress_bar.py[line:274] - INFO: epoch 001:   6548 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0682, wps=104.1, ups=0.63, wpb=110.5, bsz=40, num_updates=6540, lr=4.86771e-05, gnorm=1.356, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46639
2023-01-11 13:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:17 - progress_bar.py[line:274] - INFO: epoch 001:   6558 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1485, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=6550, lr=4.86719e-05, gnorm=1.164, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46655
2023-01-11 13:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:34 - progress_bar.py[line:274] - INFO: epoch 001:   6568 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0935, wps=96.3, ups=0.59, wpb=108.3, bsz=40, num_updates=6560, lr=4.86667e-05, gnorm=1.049, clip=50, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=46672
2023-01-11 13:02:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:51 - progress_bar.py[line:274] - INFO: epoch 001:   6578 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0753, wps=99.7, ups=0.59, wpb=111.7, bsz=40, num_updates=6570, lr=4.86615e-05, gnorm=1.21, clip=30, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=46690
2023-01-11 13:02:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:07 - progress_bar.py[line:274] - INFO: epoch 001:   6588 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0889, wps=102.8, ups=0.62, wpb=110.9, bsz=40, num_updates=6580, lr=4.86563e-05, gnorm=1.543, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46706
2023-01-11 13:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:24 - progress_bar.py[line:274] - INFO: epoch 001:   6598 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.125, wps=103.7, ups=0.63, wpb=110.3, bsz=40, num_updates=6590, lr=4.8651e-05, gnorm=0.73, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46722
2023-01-11 13:03:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:40 - progress_bar.py[line:274] - INFO: epoch 001:   6608 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1124, wps=101.8, ups=0.61, wpb=110.5, bsz=40, num_updates=6600, lr=4.86458e-05, gnorm=0.861, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46739
2023-01-11 13:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:03:58 - progress_bar.py[line:274] - INFO: epoch 001:   6618 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.07, wps=95.1, ups=0.57, wpb=110.5, bsz=40, num_updates=6610, lr=4.86406e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=46757
2023-01-11 13:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:15 - progress_bar.py[line:274] - INFO: epoch 001:   6628 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0816, wps=100.9, ups=0.61, wpb=110.9, bsz=40, num_updates=6620, lr=4.86354e-05, gnorm=1.689, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46773
2023-01-11 13:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:32 - progress_bar.py[line:274] - INFO: epoch 001:   6638 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0485, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=6630, lr=4.86302e-05, gnorm=1.485, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46790
2023-01-11 13:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   6648 / 100000 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1238, wps=100.8, ups=0.62, wpb=108, bsz=40, num_updates=6640, lr=4.8625e-05, gnorm=0.821, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46807
2023-01-11 13:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:04:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:04 - progress_bar.py[line:274] - INFO: epoch 001:   6658 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.12, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=6650, lr=4.86198e-05, gnorm=1.019, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46823
2023-01-11 13:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:22 - progress_bar.py[line:274] - INFO: epoch 001:   6668 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1215, wps=97.8, ups=0.59, wpb=110.1, bsz=40, num_updates=6660, lr=4.86146e-05, gnorm=0.901, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=46840
2023-01-11 13:05:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:38 - progress_bar.py[line:274] - INFO: epoch 001:   6678 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1058, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=6670, lr=4.86094e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46857
2023-01-11 13:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:55 - progress_bar.py[line:274] - INFO: epoch 001:   6688 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1064, wps=103.2, ups=0.63, wpb=109.7, bsz=40, num_updates=6680, lr=4.86042e-05, gnorm=1.612, clip=60, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=46873
2023-01-11 13:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:11 - progress_bar.py[line:274] - INFO: epoch 001:   6698 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0842, wps=101, ups=0.61, wpb=110.8, bsz=40, num_updates=6690, lr=4.8599e-05, gnorm=1.203, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46890
2023-01-11 13:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:28 - progress_bar.py[line:274] - INFO: epoch 001:   6708 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1415, wps=100.7, ups=0.61, wpb=109.2, bsz=40, num_updates=6700, lr=4.85938e-05, gnorm=0.979, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=46907
2023-01-11 13:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:44 - progress_bar.py[line:274] - INFO: epoch 001:   6718 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1078, wps=104, ups=0.63, wpb=109.6, bsz=40, num_updates=6710, lr=4.85885e-05, gnorm=0.942, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46923
2023-01-11 13:06:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:00 - progress_bar.py[line:274] - INFO: epoch 001:   6728 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0625, wps=102.6, ups=0.62, wpb=110.3, bsz=40, num_updates=6720, lr=4.85833e-05, gnorm=1.079, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=46939
2023-01-11 13:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:17 - progress_bar.py[line:274] - INFO: epoch 001:   6738 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0467, wps=99.9, ups=0.61, wpb=109.5, bsz=40, num_updates=6730, lr=4.85781e-05, gnorm=1.256, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46956
2023-01-11 13:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:34 - progress_bar.py[line:274] - INFO: epoch 001:   6748 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1087, wps=101.6, ups=0.62, wpb=109.8, bsz=40, num_updates=6740, lr=4.85729e-05, gnorm=0.897, clip=30, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=46972
2023-01-11 13:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:50 - progress_bar.py[line:274] - INFO: epoch 001:   6758 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0693, wps=99.2, ups=0.6, wpb=109.9, bsz=40, num_updates=6750, lr=4.85677e-05, gnorm=0.924, clip=40, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=46989
2023-01-11 13:07:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:07 - progress_bar.py[line:274] - INFO: epoch 001:   6768 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.099, wps=100.4, ups=0.61, wpb=110, bsz=40, num_updates=6760, lr=4.85625e-05, gnorm=0.85, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=47006
2023-01-11 13:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:24 - progress_bar.py[line:274] - INFO: epoch 001:   6778 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1294, wps=101.7, ups=0.61, wpb=111.6, bsz=40, num_updates=6770, lr=4.85573e-05, gnorm=0.88, clip=30, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=47022
2023-01-11 13:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:40 - progress_bar.py[line:274] - INFO: epoch 001:   6788 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1263, wps=102.4, ups=0.62, wpb=110.9, bsz=40, num_updates=6780, lr=4.85521e-05, gnorm=1.439, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=47039
2023-01-11 13:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:08:57 - progress_bar.py[line:274] - INFO: epoch 001:   6798 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1176, wps=98.8, ups=0.6, wpb=109.7, bsz=40, num_updates=6790, lr=4.85469e-05, gnorm=0.995, clip=40, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=47056
2023-01-11 13:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:14 - progress_bar.py[line:274] - INFO: epoch 001:   6808 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.11, wps=100.4, ups=0.6, wpb=111.2, bsz=40, num_updates=6800, lr=4.85417e-05, gnorm=1.384, clip=40, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=47073
2023-01-11 13:09:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:31 - progress_bar.py[line:274] - INFO: epoch 001:   6818 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1122, wps=102.9, ups=0.62, wpb=110.1, bsz=40, num_updates=6810, lr=4.85365e-05, gnorm=1.355, clip=60, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=47089
2023-01-11 13:09:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:48 - progress_bar.py[line:274] - INFO: epoch 001:   6828 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1562, wps=99, ups=0.6, wpb=109.6, bsz=40, num_updates=6820, lr=4.85312e-05, gnorm=0.783, clip=30, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=47106
2023-01-11 13:09:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:04 - progress_bar.py[line:274] - INFO: epoch 001:   6838 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.114, wps=102.9, ups=0.62, wpb=109.8, bsz=40, num_updates=6830, lr=4.8526e-05, gnorm=1.28, clip=60, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=47122
2023-01-11 13:10:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:09 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 13:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:22 - progress_bar.py[line:274] - INFO: epoch 001:   6849 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.875, nsentences=40, sample_size=109.875, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.1393, wps=96.9, ups=0.55, wpb=109.9, bsz=40, num_updates=6840, lr=4.85208e-05, gnorm=1.476, clip=60, loss_scale=512, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=47141
2023-01-11 13:10:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:39 - progress_bar.py[line:274] - INFO: epoch 001:   6859 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1359, wps=98.2, ups=0.6, wpb=108.7, bsz=40, num_updates=6850, lr=4.85156e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=47158
2023-01-11 13:10:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:10:56 - progress_bar.py[line:274] - INFO: epoch 001:   6869 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0879, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=6860, lr=4.85104e-05, gnorm=1.018, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=47175
2023-01-11 13:10:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:13 - progress_bar.py[line:274] - INFO: epoch 001:   6879 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1744, wps=100.1, ups=0.6, wpb=110.7, bsz=40, num_updates=6870, lr=4.85052e-05, gnorm=0.93, clip=50, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=47191
2023-01-11 13:11:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:29 - progress_bar.py[line:274] - INFO: epoch 001:   6889 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1143, wps=103.5, ups=0.63, wpb=109.9, bsz=40, num_updates=6880, lr=4.85e-05, gnorm=0.673, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=47208
2023-01-11 13:11:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:46 - progress_bar.py[line:274] - INFO: epoch 001:   6899 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0722, wps=99.1, ups=0.6, wpb=111.1, bsz=40, num_updates=6890, lr=4.84948e-05, gnorm=0.721, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=47225
2023-01-11 13:11:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:03 - progress_bar.py[line:274] - INFO: epoch 001:   6909 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1146, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=6900, lr=4.84896e-05, gnorm=0.7, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=47242
2023-01-11 13:12:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:20 - progress_bar.py[line:274] - INFO: epoch 001:   6919 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1313, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=6910, lr=4.84844e-05, gnorm=0.961, clip=30, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=47259
2023-01-11 13:12:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:36 - progress_bar.py[line:274] - INFO: epoch 001:   6929 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1078, wps=102.4, ups=0.62, wpb=109.7, bsz=40, num_updates=6920, lr=4.84792e-05, gnorm=1.243, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=47275
2023-01-11 13:12:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:12:53 - progress_bar.py[line:274] - INFO: epoch 001:   6939 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1048, wps=100.1, ups=0.61, wpb=110.1, bsz=40, num_updates=6930, lr=4.8474e-05, gnorm=0.714, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=47292
2023-01-11 13:12:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:10 - progress_bar.py[line:274] - INFO: epoch 001:   6949 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0826, wps=102, ups=0.61, wpb=110.6, bsz=40, num_updates=6940, lr=4.84688e-05, gnorm=1.182, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=47308
2023-01-11 13:13:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:26 - progress_bar.py[line:274] - INFO: epoch 001:   6959 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.14, wps=99.2, ups=0.6, wpb=110.5, bsz=40, num_updates=6950, lr=4.84635e-05, gnorm=1.272, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=47325
2023-01-11 13:13:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:43 - progress_bar.py[line:274] - INFO: epoch 001:   6969 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1461, wps=103.7, ups=0.62, wpb=111.3, bsz=40, num_updates=6960, lr=4.84583e-05, gnorm=0.929, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=47342
2023-01-11 13:13:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:13:59 - progress_bar.py[line:274] - INFO: epoch 001:   6979 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1333, wps=100.5, ups=0.61, wpb=110.1, bsz=40, num_updates=6970, lr=4.84531e-05, gnorm=1.748, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=47358
2023-01-11 13:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:16 - progress_bar.py[line:274] - INFO: epoch 001:   6989 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1489, wps=100.4, ups=0.6, wpb=111.1, bsz=40, num_updates=6980, lr=4.84479e-05, gnorm=0.975, clip=60, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=47375
2023-01-11 13:14:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:33 - progress_bar.py[line:274] - INFO: epoch 001:   6999 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1293, wps=102.3, ups=0.62, wpb=109.4, bsz=40, num_updates=6990, lr=4.84427e-05, gnorm=0.889, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=47391
2023-01-11 13:14:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 13:14:49 - progress_bar.py[line:274] - INFO: epoch 001:   7009 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0909, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=7000, lr=4.84375e-05, gnorm=0.806, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=47408
2023-01-11 13:14:49 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 13:14:51 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 13:14:51 - train.py[line:551] - INFO: load:1.32 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 13:16:54 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 13:16:54 - train.py[line:551] - INFO: load:1.34 valid_run:122.67 task_valid:119.66 collect_output:1.88
2023-01-11 13:18:53 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 13:18:53 - train.py[line:551] - INFO: load:1.37 valid_run:242.49 task_valid:235.38 collect_output:4.93
2023-01-11 13:20:56 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 13:20:56 - train.py[line:551] - INFO: load:1.39 valid_run:364.99 task_valid:351.94 collect_output:9.79
2023-01-11 13:22:58 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 13:22:58 - train.py[line:551] - INFO: load:1.41 valid_run:486.97 task_valid:465.47 collect_output:17.21
2023-01-11 13:24:59 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 13:24:59 - train.py[line:551] - INFO: load:1.44 valid_run:607.70 task_valid:582.97 collect_output:19.36
2023-01-11 13:27:02 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 13:27:02 - train.py[line:551] - INFO: load:1.46 valid_run:730.66 task_valid:701.66 collect_output:22.56
2023-01-11 13:29:05 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 13:29:05 - train.py[line:551] - INFO: load:1.49 valid_run:853.86 task_valid:819.45 collect_output:26.93
2023-01-11 13:31:07 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 13:31:07 - train.py[line:551] - INFO: load:1.51 valid_run:975.71 task_valid:935.76 collect_output:31.44
2023-01-11 13:33:11 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 13:33:11 - train.py[line:551] - INFO: load:1.54 valid_run:1099.24 task_valid:1052.80 collect_output:36.88
2023-01-11 13:35:13 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 13:35:13 - train.py[line:551] - INFO: load:1.56 valid_run:1221.58 task_valid:1165.56 collect_output:45.36
2023-01-11 13:37:14 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 13:37:14 - train.py[line:551] - INFO: load:1.58 valid_run:1342.09 task_valid:1281.00 collect_output:49.36
2023-01-11 13:39:15 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 13:39:15 - train.py[line:551] - INFO: load:1.61 valid_run:1463.60 task_valid:1397.63 collect_output:53.15
2023-01-11 13:41:15 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 13:41:15 - train.py[line:551] - INFO: load:1.63 valid_run:1582.75 task_valid:1511.46 collect_output:57.35
2023-01-11 13:43:15 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 13:43:15 - train.py[line:551] - INFO: load:1.66 valid_run:1703.35 task_valid:1628.81 collect_output:59.60
2023-01-11 13:45:16 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 13:45:16 - train.py[line:551] - INFO: load:1.68 valid_run:1824.46 task_valid:1744.80 collect_output:63.59
2023-01-11 13:47:18 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 13:47:18 - train.py[line:551] - INFO: load:1.71 valid_run:1945.46 task_valid:1858.71 collect_output:69.60
2023-01-11 13:49:19 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 13:49:19 - train.py[line:551] - INFO: load:1.73 valid_run:2066.38 task_valid:1974.53 collect_output:73.66
2023-01-11 13:51:19 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 13:51:19 - train.py[line:551] - INFO: load:1.76 valid_run:2186.90 task_valid:2092.31 collect_output:75.33
2023-01-11 13:53:21 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 13:53:21 - train.py[line:551] - INFO: load:1.78 valid_run:2308.33 task_valid:2209.20 collect_output:78.82
2023-01-11 13:55:21 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 13:55:21 - train.py[line:551] - INFO: load:1.81 valid_run:2428.42 task_valid:2325.37 collect_output:81.70
2023-01-11 13:57:23 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 13:57:23 - train.py[line:551] - INFO: load:1.83 valid_run:2550.35 task_valid:2441.71 collect_output:86.22
2023-01-11 13:59:25 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 13:59:25 - train.py[line:551] - INFO: load:1.85 valid_run:2672.13 task_valid:2560.25 collect_output:88.38
2023-01-11 14:01:25 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 14:01:25 - train.py[line:551] - INFO: load:1.88 valid_run:2792.09 task_valid:2674.24 collect_output:93.35
2023-01-11 14:03:25 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 14:03:25 - train.py[line:551] - INFO: load:1.91 valid_run:2911.94 task_valid:2790.31 collect_output:95.96
2023-01-11 14:05:26 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 14:05:26 - train.py[line:551] - INFO: load:1.93 valid_run:3033.05 task_valid:2905.98 collect_output:100.39
2023-01-11 14:07:28 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 14:07:28 - train.py[line:551] - INFO: load:1.96 valid_run:3155.50 task_valid:3021.39 collect_output:106.38
2023-01-11 14:09:27 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 14:09:28 - train.py[line:551] - INFO: load:1.98 valid_run:3274.46 task_valid:3134.78 collect_output:110.94
2023-01-11 14:11:29 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 14:11:29 - train.py[line:551] - INFO: load:2.01 valid_run:3395.86 task_valid:3253.64 collect_output:112.45
2023-01-11 14:13:30 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 14:13:30 - train.py[line:551] - INFO: load:2.03 valid_run:3517.10 task_valid:3368.63 collect_output:117.67
2023-01-11 14:15:32 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 14:15:32 - train.py[line:551] - INFO: load:2.06 valid_run:3638.78 task_valid:3486.97 collect_output:119.92
2023-01-11 14:17:33 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 14:17:33 - train.py[line:551] - INFO: load:2.08 valid_run:3759.51 task_valid:3605.05 collect_output:121.51

====================================================================================================
SGG eval:     R @ 50: 0.6439;     R @ 100: 0.6795;     R @ 500: 0.7102;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4136;    mR @ 100: 0.4612;    mR @ 500: 0.5037;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.3636) (growing on:0.5000) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9477) (says:0.0000) (sitting on:0.7993) (standing on:0.3343) (using:0.5500) (walking in:0.0000) (walking on:0.7658) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 14:18:04 - train.py[line:487] - INFO: 0.6794503437738733

====================================================================================================
SGG eval:     R @ 50: 0.6439;     R @ 100: 0.6795;     R @ 500: 0.7102;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4136;    mR @ 100: 0.4612;    mR @ 500: 0.5037;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.3636) (growing on:0.5000) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9477) (says:0.0000) (sitting on:0.7993) (standing on:0.3343) (using:0.5500) (walking in:0.0000) (walking on:0.7658) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 14:18:04 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 14:18:04 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.312 | loss_v1 0 | loss_v2 0 | nll_loss 0.163 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.67945 | ppl 1.12 | vqa_score 0.5698 | wps 118.3 | wpb 72 | bsz 24 | num_updates 7000 | best_R@100 0.697584
2023-01-11 14:18:04 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7000 updates
2023-01-11 14:18:04 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_7000.pt
2023-01-11 14:18:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_7000.pt
2023-01-11 14:20:14 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 0.6794503437738733) (writing took 130.280876589939 seconds)
2023-01-11 14:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:32 - progress_bar.py[line:274] - INFO: epoch 001:   7019 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1415, wps=0.4, ups=0, wpb=110.2, bsz=40, num_updates=7010, lr=4.84323e-05, gnorm=0.882, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51349
2023-01-11 14:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:48 - progress_bar.py[line:274] - INFO: epoch 001:   7029 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1237, wps=101.2, ups=0.61, wpb=109.8, bsz=40, num_updates=7020, lr=4.84271e-05, gnorm=1.326, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51367
2023-01-11 14:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:20:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   7039 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1068, wps=99.2, ups=0.6, wpb=109.5, bsz=40, num_updates=7030, lr=4.84219e-05, gnorm=0.934, clip=50, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=51384
2023-01-11 14:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:22 - progress_bar.py[line:274] - INFO: epoch 001:   7049 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0753, wps=99.2, ups=0.61, wpb=109.1, bsz=40, num_updates=7040, lr=4.84167e-05, gnorm=0.592, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=51401
2023-01-11 14:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:39 - progress_bar.py[line:274] - INFO: epoch 001:   7059 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1359, wps=100.6, ups=0.61, wpb=110.8, bsz=40, num_updates=7050, lr=4.84115e-05, gnorm=0.988, clip=60, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=51418
2023-01-11 14:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   7069 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0971, wps=97.9, ups=0.6, wpb=109.6, bsz=40, num_updates=7060, lr=4.84063e-05, gnorm=1.068, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=51435
2023-01-11 14:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:14 - progress_bar.py[line:274] - INFO: epoch 001:   7079 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0882, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=7070, lr=4.8401e-05, gnorm=0.986, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51452
2023-01-11 14:22:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:31 - progress_bar.py[line:274] - INFO: epoch 001:   7089 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1217, wps=99.9, ups=0.61, wpb=108.6, bsz=40, num_updates=7080, lr=4.83958e-05, gnorm=1.005, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51469
2023-01-11 14:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:48 - progress_bar.py[line:274] - INFO: epoch 001:   7099 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1209, wps=99.8, ups=0.6, wpb=111.1, bsz=40, num_updates=7090, lr=4.83906e-05, gnorm=0.882, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=51486
2023-01-11 14:22:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:22:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:05 - progress_bar.py[line:274] - INFO: epoch 001:   7109 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1196, wps=101.6, ups=0.61, wpb=110.5, bsz=40, num_updates=7100, lr=4.83854e-05, gnorm=1.335, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51503
2023-01-11 14:23:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:22 - progress_bar.py[line:274] - INFO: epoch 001:   7119 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1395, wps=101.2, ups=0.61, wpb=110, bsz=40, num_updates=7110, lr=4.83802e-05, gnorm=1.283, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51521
2023-01-11 14:23:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:39 - progress_bar.py[line:274] - INFO: epoch 001:   7129 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1429, wps=100.1, ups=0.61, wpb=110.3, bsz=40, num_updates=7120, lr=4.8375e-05, gnorm=1.3, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51538
2023-01-11 14:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:23:57 - progress_bar.py[line:274] - INFO: epoch 001:   7139 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.165, wps=99.5, ups=0.6, wpb=110.5, bsz=40, num_updates=7130, lr=4.83698e-05, gnorm=1.021, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=51555
2023-01-11 14:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:14 - progress_bar.py[line:274] - INFO: epoch 001:   7149 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1089, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=7140, lr=4.83646e-05, gnorm=1.215, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51572
2023-01-11 14:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:30 - progress_bar.py[line:274] - INFO: epoch 001:   7159 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1389, wps=104.6, ups=0.64, wpb=109.3, bsz=40, num_updates=7150, lr=4.83594e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51588
2023-01-11 14:24:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:47 - progress_bar.py[line:274] - INFO: epoch 001:   7169 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0816, wps=100.6, ups=0.61, wpb=109.2, bsz=40, num_updates=7160, lr=4.83542e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51606
2023-01-11 14:24:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:04 - progress_bar.py[line:274] - INFO: epoch 001:   7179 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1304, wps=103.7, ups=0.62, wpb=111.8, bsz=40, num_updates=7170, lr=4.8349e-05, gnorm=0.974, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51622
2023-01-11 14:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:21 - progress_bar.py[line:274] - INFO: epoch 001:   7189 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1379, wps=104.9, ups=0.63, wpb=111.2, bsz=40, num_updates=7180, lr=4.83438e-05, gnorm=1.12, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51639
2023-01-11 14:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:38 - progress_bar.py[line:274] - INFO: epoch 001:   7199 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1364, wps=98.7, ups=0.6, wpb=109.3, bsz=40, num_updates=7190, lr=4.83385e-05, gnorm=0.921, clip=30, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=51656
2023-01-11 14:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:25:55 - progress_bar.py[line:274] - INFO: epoch 001:   7209 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1196, wps=102.7, ups=0.62, wpb=110.2, bsz=40, num_updates=7200, lr=4.83333e-05, gnorm=0.861, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51673
2023-01-11 14:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:12 - progress_bar.py[line:274] - INFO: epoch 001:   7219 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.125, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=7210, lr=4.83281e-05, gnorm=1.182, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51690
2023-01-11 14:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:29 - progress_bar.py[line:274] - INFO: epoch 001:   7229 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1053, wps=101, ups=0.61, wpb=110.8, bsz=40, num_updates=7220, lr=4.83229e-05, gnorm=0.848, clip=30, loss_scale=512, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=51707
2023-01-11 14:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:46 - progress_bar.py[line:274] - INFO: epoch 001:   7239 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1287, wps=102.1, ups=0.62, wpb=110.3, bsz=40, num_updates=7230, lr=4.83177e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=51724
2023-01-11 14:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:03 - progress_bar.py[line:274] - INFO: epoch 001:   7249 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1165, wps=98, ups=0.6, wpb=108.7, bsz=40, num_updates=7240, lr=4.83125e-05, gnorm=0.886, clip=40, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=51741
2023-01-11 14:27:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:20 - progress_bar.py[line:274] - INFO: epoch 001:   7259 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1313, wps=100.8, ups=0.6, wpb=111.6, bsz=40, num_updates=7250, lr=4.83073e-05, gnorm=0.954, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=51759
2023-01-11 14:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:37 - progress_bar.py[line:274] - INFO: epoch 001:   7269 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1132, wps=99.4, ups=0.61, wpb=109.2, bsz=40, num_updates=7260, lr=4.83021e-05, gnorm=0.999, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51776
2023-01-11 14:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:27:55 - progress_bar.py[line:274] - INFO: epoch 001:   7279 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1176, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=7270, lr=4.82969e-05, gnorm=1.008, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=51793
2023-01-11 14:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   7289 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.134, wps=98.7, ups=0.6, wpb=109.2, bsz=40, num_updates=7280, lr=4.82917e-05, gnorm=1.082, clip=40, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=51810
2023-01-11 14:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:29 - progress_bar.py[line:274] - INFO: epoch 001:   7299 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1778, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=7290, lr=4.82865e-05, gnorm=0.981, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=51827
2023-01-11 14:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:46 - progress_bar.py[line:274] - INFO: epoch 001:   7309 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1048, wps=98.5, ups=0.6, wpb=109.5, bsz=40, num_updates=7300, lr=4.82812e-05, gnorm=1.835, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=51845
2023-01-11 14:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:03 - progress_bar.py[line:274] - INFO: epoch 001:   7319 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0495, wps=102.9, ups=0.62, wpb=110.9, bsz=40, num_updates=7310, lr=4.8276e-05, gnorm=1.181, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51861
2023-01-11 14:29:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:21 - progress_bar.py[line:274] - INFO: epoch 001:   7329 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1215, wps=98, ups=0.59, wpb=109.9, bsz=40, num_updates=7320, lr=4.82708e-05, gnorm=0.84, clip=20, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=51879
2023-01-11 14:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   7339 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1228, wps=98.9, ups=0.61, wpb=108.5, bsz=40, num_updates=7330, lr=4.82656e-05, gnorm=1.478, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51896
2023-01-11 14:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:29:54 - progress_bar.py[line:274] - INFO: epoch 001:   7349 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0652, wps=104.4, ups=0.64, wpb=109.4, bsz=40, num_updates=7340, lr=4.82604e-05, gnorm=1.243, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=51913
2023-01-11 14:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:12 - progress_bar.py[line:274] - INFO: epoch 001:   7359 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0826, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=7350, lr=4.82552e-05, gnorm=1.325, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51930
2023-01-11 14:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:29 - progress_bar.py[line:274] - INFO: epoch 001:   7369 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.12, wps=99.5, ups=0.61, wpb=109.1, bsz=40, num_updates=7360, lr=4.825e-05, gnorm=1.738, clip=50, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51947
2023-01-11 14:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:46 - progress_bar.py[line:274] - INFO: epoch 001:   7379 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0808, wps=98.9, ups=0.61, wpb=108.9, bsz=40, num_updates=7370, lr=4.82448e-05, gnorm=0.81, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51964
2023-01-11 14:30:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:02 - progress_bar.py[line:274] - INFO: epoch 001:   7389 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0864, wps=106.6, ups=0.64, wpb=111.9, bsz=40, num_updates=7380, lr=4.82396e-05, gnorm=0.72, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51981
2023-01-11 14:31:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:19 - progress_bar.py[line:274] - INFO: epoch 001:   7399 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1858, wps=101.3, ups=0.62, wpb=109.4, bsz=40, num_updates=7390, lr=4.82344e-05, gnorm=0.652, clip=10, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=51998
2023-01-11 14:31:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:36 - progress_bar.py[line:274] - INFO: epoch 001:   7409 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2424, wps=101.6, ups=0.61, wpb=110.4, bsz=40, num_updates=7400, lr=4.82292e-05, gnorm=0.682, clip=20, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52015
2023-01-11 14:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:48 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 14:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:31:55 - progress_bar.py[line:274] - INFO: epoch 001:   7420 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=111.125, nsentences=40, sample_size=111.125, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0865, wps=97.4, ups=0.55, wpb=111.1, bsz=40, num_updates=7410, lr=4.8224e-05, gnorm=0.867, clip=40, loss_scale=512, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=52033
2023-01-11 14:32:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:12 - progress_bar.py[line:274] - INFO: epoch 001:   7430 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1778, wps=101.1, ups=0.61, wpb=111, bsz=40, num_updates=7420, lr=4.82188e-05, gnorm=0.781, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52051
2023-01-11 14:32:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:29 - progress_bar.py[line:274] - INFO: epoch 001:   7440 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1333, wps=98.4, ups=0.6, wpb=109.3, bsz=40, num_updates=7430, lr=4.82135e-05, gnorm=0.558, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=52068
2023-01-11 14:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:47 - progress_bar.py[line:274] - INFO: epoch 001:   7450 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1209, wps=99.7, ups=0.6, wpb=110.9, bsz=40, num_updates=7440, lr=4.82083e-05, gnorm=1.642, clip=50, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=52085
2023-01-11 14:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:04 - progress_bar.py[line:274] - INFO: epoch 001:   7460 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1782, wps=102.3, ups=0.61, wpb=111.3, bsz=40, num_updates=7450, lr=4.82031e-05, gnorm=0.715, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52102
2023-01-11 14:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:21 - progress_bar.py[line:274] - INFO: epoch 001:   7470 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1443, wps=101.1, ups=0.6, wpb=111.5, bsz=40, num_updates=7460, lr=4.81979e-05, gnorm=1.001, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52119
2023-01-11 14:33:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:38 - progress_bar.py[line:274] - INFO: epoch 001:   7480 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.1481, wps=101.5, ups=0.62, wpb=110, bsz=40, num_updates=7470, lr=4.81927e-05, gnorm=1.079, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52136
2023-01-11 14:33:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:33:55 - progress_bar.py[line:274] - INFO: epoch 001:   7490 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1474, wps=99.3, ups=0.6, wpb=110.4, bsz=40, num_updates=7480, lr=4.81875e-05, gnorm=0.576, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=52153
2023-01-11 14:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:12 - progress_bar.py[line:274] - INFO: epoch 001:   7500 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1474, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=7490, lr=4.81823e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52170
2023-01-11 14:34:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:30 - progress_bar.py[line:274] - INFO: epoch 001:   7510 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.09, wps=97.9, ups=0.6, wpb=109.6, bsz=40, num_updates=7500, lr=4.81771e-05, gnorm=1.042, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=52188
2023-01-11 14:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:47 - progress_bar.py[line:274] - INFO: epoch 001:   7520 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1546, wps=100, ups=0.61, wpb=109.8, bsz=40, num_updates=7510, lr=4.81719e-05, gnorm=0.943, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=52205
2023-01-11 14:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:01 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 14:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:04 - progress_bar.py[line:274] - INFO: epoch 001:   7531 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1373, wps=96.3, ups=0.58, wpb=109.8, bsz=40, num_updates=7520, lr=4.81667e-05, gnorm=1.157, clip=50, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=52223
2023-01-11 14:35:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:21 - progress_bar.py[line:274] - INFO: epoch 001:   7541 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1277, wps=100.9, ups=0.61, wpb=109.6, bsz=40, num_updates=7530, lr=4.81615e-05, gnorm=0.792, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52240
2023-01-11 14:35:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:38 - progress_bar.py[line:274] - INFO: epoch 001:   7551 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1485, wps=99.6, ups=0.6, wpb=109.9, bsz=40, num_updates=7540, lr=4.81563e-05, gnorm=1.103, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=52257
2023-01-11 14:35:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:35:56 - progress_bar.py[line:274] - INFO: epoch 001:   7561 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1548, wps=99.2, ups=0.59, wpb=111.5, bsz=40, num_updates=7550, lr=4.8151e-05, gnorm=0.886, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=52274
2023-01-11 14:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:13 - progress_bar.py[line:274] - INFO: epoch 001:   7571 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1031, wps=102.3, ups=0.62, wpb=110.5, bsz=40, num_updates=7560, lr=4.81458e-05, gnorm=1.516, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52291
2023-01-11 14:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:30 - progress_bar.py[line:274] - INFO: epoch 001:   7581 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1485, wps=99.1, ups=0.61, wpb=109, bsz=40, num_updates=7570, lr=4.81406e-05, gnorm=1.073, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52308
2023-01-11 14:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:47 - progress_bar.py[line:274] - INFO: epoch 001:   7591 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1364, wps=98.5, ups=0.6, wpb=108.9, bsz=40, num_updates=7580, lr=4.81354e-05, gnorm=1.259, clip=40, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=52325
2023-01-11 14:36:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   7601 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1682, wps=98.9, ups=0.6, wpb=109.6, bsz=40, num_updates=7590, lr=4.81302e-05, gnorm=0.869, clip=50, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=52342
2023-01-11 14:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:22 - progress_bar.py[line:274] - INFO: epoch 001:   7611 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1068, wps=98.5, ups=0.6, wpb=109.1, bsz=40, num_updates=7600, lr=4.8125e-05, gnorm=1.169, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=52360
2023-01-11 14:37:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:38 - progress_bar.py[line:274] - INFO: epoch 001:   7621 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2, wps=102.6, ups=0.63, wpb=109.4, bsz=40, num_updates=7610, lr=4.81198e-05, gnorm=0.911, clip=30, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=52376
2023-01-11 14:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:37:55 - progress_bar.py[line:274] - INFO: epoch 001:   7631 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0865, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=7620, lr=4.81146e-05, gnorm=1.169, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52393
2023-01-11 14:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:11 - progress_bar.py[line:274] - INFO: epoch 001:   7641 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1649, wps=104.1, ups=0.63, wpb=109.3, bsz=40, num_updates=7630, lr=4.81094e-05, gnorm=1.232, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=52410
2023-01-11 14:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:29 - progress_bar.py[line:274] - INFO: epoch 001:   7651 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0917, wps=96.9, ups=0.6, wpb=108.2, bsz=40, num_updates=7640, lr=4.81042e-05, gnorm=0.957, clip=20, loss_scale=256, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=52427
2023-01-11 14:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:46 - progress_bar.py[line:274] - INFO: epoch 001:   7661 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0841, wps=101, ups=0.61, wpb=110.3, bsz=40, num_updates=7650, lr=4.8099e-05, gnorm=1.114, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=52444
2023-01-11 14:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:03 - progress_bar.py[line:274] - INFO: epoch 001:   7671 / 100000 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.1062, wps=100.2, ups=0.61, wpb=109.7, bsz=40, num_updates=7660, lr=4.80938e-05, gnorm=0.775, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52461
2023-01-11 14:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:20 - progress_bar.py[line:274] - INFO: epoch 001:   7681 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.117, wps=99.9, ups=0.61, wpb=108.9, bsz=40, num_updates=7670, lr=4.80885e-05, gnorm=1.06, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52478
2023-01-11 14:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:37 - progress_bar.py[line:274] - INFO: epoch 001:   7691 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1354, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=7680, lr=4.80833e-05, gnorm=1.493, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52495
2023-01-11 14:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:39:54 - progress_bar.py[line:274] - INFO: epoch 001:   7701 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1121, wps=99.8, ups=0.6, wpb=110.9, bsz=40, num_updates=7690, lr=4.80781e-05, gnorm=0.638, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=52512
2023-01-11 14:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:11 - progress_bar.py[line:274] - INFO: epoch 001:   7711 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1441, wps=100.9, ups=0.61, wpb=110.5, bsz=40, num_updates=7700, lr=4.80729e-05, gnorm=0.559, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52529
2023-01-11 14:40:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:28 - progress_bar.py[line:274] - INFO: epoch 001:   7721 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1562, wps=102.9, ups=0.62, wpb=109.8, bsz=40, num_updates=7710, lr=4.80677e-05, gnorm=0.653, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52546
2023-01-11 14:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:45 - progress_bar.py[line:274] - INFO: epoch 001:   7731 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1522, wps=101.7, ups=0.61, wpb=110.8, bsz=40, num_updates=7720, lr=4.80625e-05, gnorm=0.898, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52563
2023-01-11 14:40:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:01 - progress_bar.py[line:274] - INFO: epoch 001:   7741 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.219, wps=102.5, ups=0.63, wpb=109.1, bsz=40, num_updates=7730, lr=4.80573e-05, gnorm=1.023, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52579
2023-01-11 14:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:17 - progress_bar.py[line:274] - INFO: epoch 001:   7751 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1604, wps=101.5, ups=0.62, wpb=109.8, bsz=40, num_updates=7740, lr=4.80521e-05, gnorm=0.56, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=52596
2023-01-11 14:41:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:34 - progress_bar.py[line:274] - INFO: epoch 001:   7761 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2088, wps=101.4, ups=0.61, wpb=110.2, bsz=40, num_updates=7750, lr=4.80469e-05, gnorm=0.99, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52613
2023-01-11 14:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:50 - progress_bar.py[line:274] - INFO: epoch 001:   7771 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1143, wps=103, ups=0.64, wpb=107.7, bsz=40, num_updates=7760, lr=4.80417e-05, gnorm=0.975, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52628
2023-01-11 14:41:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:07 - progress_bar.py[line:274] - INFO: epoch 001:   7781 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0789, wps=96.9, ups=0.59, wpb=108.9, bsz=40, num_updates=7770, lr=4.80365e-05, gnorm=1.637, clip=50, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=52646
2023-01-11 14:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:24 - progress_bar.py[line:274] - INFO: epoch 001:   7791 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1705, wps=101.8, ups=0.61, wpb=110.9, bsz=40, num_updates=7780, lr=4.80312e-05, gnorm=0.932, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=52662
2023-01-11 14:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:40 - progress_bar.py[line:274] - INFO: epoch 001:   7801 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0841, wps=98.8, ups=0.61, wpb=108.1, bsz=40, num_updates=7790, lr=4.8026e-05, gnorm=1.461, clip=60, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52679
2023-01-11 14:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:42:57 - progress_bar.py[line:274] - INFO: epoch 001:   7811 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1545, wps=100.3, ups=0.61, wpb=109.6, bsz=40, num_updates=7800, lr=4.80208e-05, gnorm=1.972, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52695
2023-01-11 14:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:13 - progress_bar.py[line:274] - INFO: epoch 001:   7821 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1414, wps=104.3, ups=0.63, wpb=110.1, bsz=40, num_updates=7810, lr=4.80156e-05, gnorm=0.935, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52712
2023-01-11 14:43:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:29 - progress_bar.py[line:274] - INFO: epoch 001:   7831 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.125, wps=102.1, ups=0.61, wpb=111.7, bsz=40, num_updates=7820, lr=4.80104e-05, gnorm=1.494, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52728
2023-01-11 14:43:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:46 - progress_bar.py[line:274] - INFO: epoch 001:   7841 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.19, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=7830, lr=4.80052e-05, gnorm=1.283, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52745
2023-01-11 14:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:43:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:03 - progress_bar.py[line:274] - INFO: epoch 001:   7851 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2037, wps=99.1, ups=0.61, wpb=108.5, bsz=40, num_updates=7840, lr=4.8e-05, gnorm=0.838, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52761
2023-01-11 14:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:19 - progress_bar.py[line:274] - INFO: epoch 001:   7861 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2041, wps=102.3, ups=0.62, wpb=110.3, bsz=40, num_updates=7850, lr=4.79948e-05, gnorm=0.878, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52778
2023-01-11 14:44:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:35 - progress_bar.py[line:274] - INFO: epoch 001:   7871 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2083, wps=103.8, ups=0.63, wpb=110, bsz=40, num_updates=7860, lr=4.79896e-05, gnorm=0.969, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52794
2023-01-11 14:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:51 - progress_bar.py[line:274] - INFO: epoch 001:   7881 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1744, wps=105.3, ups=0.63, wpb=110.7, bsz=40, num_updates=7870, lr=4.79844e-05, gnorm=0.723, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52810
2023-01-11 14:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:44:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:08 - progress_bar.py[line:274] - INFO: epoch 001:   7891 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0737, wps=100.1, ups=0.61, wpb=110.3, bsz=40, num_updates=7880, lr=4.79792e-05, gnorm=1.497, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52827
2023-01-11 14:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:25 - progress_bar.py[line:274] - INFO: epoch 001:   7901 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1939, wps=97.6, ups=0.59, wpb=110.1, bsz=40, num_updates=7890, lr=4.7974e-05, gnorm=1.549, clip=50, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=52844
2023-01-11 14:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:42 - progress_bar.py[line:274] - INFO: epoch 001:   7911 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1619, wps=98.2, ups=0.6, wpb=109, bsz=40, num_updates=7900, lr=4.79688e-05, gnorm=1.182, clip=50, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=52861
2023-01-11 14:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:45:59 - progress_bar.py[line:274] - INFO: epoch 001:   7921 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1961, wps=100, ups=0.61, wpb=110.1, bsz=40, num_updates=7910, lr=4.79635e-05, gnorm=0.646, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=52878
2023-01-11 14:46:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:16 - progress_bar.py[line:274] - INFO: epoch 001:   7931 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=112.267, nsentences=40, sample_size=112.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1266, wps=103.6, ups=0.62, wpb=112.3, bsz=40, num_updates=7920, lr=4.79583e-05, gnorm=1.363, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52894
2023-01-11 14:46:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:32 - progress_bar.py[line:274] - INFO: epoch 001:   7941 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1414, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=7930, lr=4.79531e-05, gnorm=1.434, clip=50, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52911
2023-01-11 14:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:49 - progress_bar.py[line:274] - INFO: epoch 001:   7951 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1622, wps=99.2, ups=0.61, wpb=108.9, bsz=40, num_updates=7940, lr=4.79479e-05, gnorm=0.749, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=52927
2023-01-11 14:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:05 - progress_bar.py[line:274] - INFO: epoch 001:   7961 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1613, wps=101.2, ups=0.61, wpb=110.6, bsz=40, num_updates=7950, lr=4.79427e-05, gnorm=1.017, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52944
2023-01-11 14:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:22 - progress_bar.py[line:274] - INFO: epoch 001:   7971 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.23, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=7960, lr=4.79375e-05, gnorm=0.932, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=52961
2023-01-11 14:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:39 - progress_bar.py[line:274] - INFO: epoch 001:   7981 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1579, wps=100.5, ups=0.61, wpb=110.1, bsz=40, num_updates=7970, lr=4.79323e-05, gnorm=0.813, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52978
2023-01-11 14:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:47:56 - progress_bar.py[line:274] - INFO: epoch 001:   7991 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1386, wps=100.4, ups=0.61, wpb=109.7, bsz=40, num_updates=7980, lr=4.79271e-05, gnorm=0.909, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52994
2023-01-11 14:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:12 - progress_bar.py[line:274] - INFO: epoch 001:   8001 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1574, wps=99.9, ups=0.61, wpb=108.9, bsz=40, num_updates=7990, lr=4.79219e-05, gnorm=0.801, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=53011
2023-01-11 14:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 14:48:28 - progress_bar.py[line:274] - INFO: epoch 001:   8011 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.16, wps=103.4, ups=0.62, wpb=110.7, bsz=40, num_updates=8000, lr=4.79167e-05, gnorm=0.739, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=53027
2023-01-11 14:48:28 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 14:48:30 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 14:48:30 - train.py[line:551] - INFO: load:1.22 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 14:50:31 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 14:50:31 - train.py[line:551] - INFO: load:1.25 valid_run:121.51 task_valid:118.52 collect_output:1.91
2023-01-11 14:52:32 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 14:52:32 - train.py[line:551] - INFO: load:1.27 valid_run:241.90 task_valid:234.28 collect_output:5.37
2023-01-11 14:54:34 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 14:54:34 - train.py[line:551] - INFO: load:1.30 valid_run:364.24 task_valid:350.54 collect_output:10.33
2023-01-11 14:56:36 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 14:56:36 - train.py[line:551] - INFO: load:1.32 valid_run:486.05 task_valid:463.96 collect_output:17.67
2023-01-11 14:58:37 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 14:58:37 - train.py[line:551] - INFO: load:1.35 valid_run:606.82 task_valid:581.18 collect_output:20.12
2023-01-11 15:00:40 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 15:00:40 - train.py[line:551] - INFO: load:1.37 valid_run:729.42 task_valid:699.44 collect_output:23.42
2023-01-11 15:02:43 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 15:02:43 - train.py[line:551] - INFO: load:1.40 valid_run:852.54 task_valid:817.12 collect_output:27.80
2023-01-11 15:04:45 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 15:04:45 - train.py[line:551] - INFO: load:1.42 valid_run:974.63 task_valid:933.35 collect_output:32.60
2023-01-11 15:06:49 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 15:06:49 - train.py[line:551] - INFO: load:1.45 valid_run:1098.32 task_valid:1050.03 collect_output:38.59
2023-01-11 15:08:50 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 15:08:50 - train.py[line:551] - INFO: load:1.47 valid_run:1219.72 task_valid:1162.23 collect_output:46.77
2023-01-11 15:10:51 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 15:10:51 - train.py[line:551] - INFO: load:1.50 valid_run:1340.35 task_valid:1277.73 collect_output:50.79
2023-01-11 15:12:52 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 15:12:52 - train.py[line:551] - INFO: load:1.52 valid_run:1461.60 task_valid:1394.05 collect_output:54.70
2023-01-11 15:14:52 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 15:14:52 - train.py[line:551] - INFO: load:1.55 valid_run:1581.30 task_valid:1507.92 collect_output:59.32
2023-01-11 15:16:54 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 15:16:54 - train.py[line:551] - INFO: load:1.57 valid_run:1702.58 task_valid:1625.50 collect_output:61.93
2023-01-11 15:18:54 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 15:18:54 - train.py[line:551] - INFO: load:1.60 valid_run:1823.15 task_valid:1741.19 collect_output:65.76
2023-01-11 15:20:56 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 15:20:56 - train.py[line:551] - INFO: load:1.63 valid_run:1945.18 task_valid:1855.26 collect_output:72.57
2023-01-11 15:22:58 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 15:22:58 - train.py[line:551] - INFO: load:1.65 valid_run:2066.88 task_valid:1970.92 collect_output:77.54
2023-01-11 15:24:59 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 15:24:59 - train.py[line:551] - INFO: load:1.68 valid_run:2187.35 task_valid:2088.33 collect_output:79.52
2023-01-11 15:27:00 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 15:27:00 - train.py[line:551] - INFO: load:1.70 valid_run:2308.71 task_valid:2205.04 collect_output:83.08
2023-01-11 15:29:00 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 15:29:00 - train.py[line:551] - INFO: load:1.73 valid_run:2428.81 task_valid:2321.08 collect_output:86.10
2023-01-11 15:31:02 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 15:31:02 - train.py[line:551] - INFO: load:1.75 valid_run:2550.74 task_valid:2437.29 collect_output:90.76
2023-01-11 15:33:04 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 15:33:04 - train.py[line:551] - INFO: load:1.78 valid_run:2672.38 task_valid:2555.58 collect_output:93.04
2023-01-11 15:35:04 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 15:35:04 - train.py[line:551] - INFO: load:1.80 valid_run:2792.33 task_valid:2669.42 collect_output:98.11
2023-01-11 15:37:04 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 15:37:04 - train.py[line:551] - INFO: load:1.83 valid_run:2912.23 task_valid:2785.19 collect_output:101.19
2023-01-11 15:39:06 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 15:39:06 - train.py[line:551] - INFO: load:1.86 valid_run:3034.10 task_valid:2901.15 collect_output:105.99
2023-01-11 15:41:09 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 15:41:09 - train.py[line:551] - INFO: load:1.88 valid_run:3157.07 task_valid:3017.02 collect_output:112.00
2023-01-11 15:43:09 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 15:43:09 - train.py[line:551] - INFO: load:1.91 valid_run:3276.57 task_valid:3130.69 collect_output:116.77
2023-01-11 15:45:10 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 15:45:10 - train.py[line:551] - INFO: load:1.93 valid_run:3398.12 task_valid:3249.54 collect_output:118.40
2023-01-11 15:47:12 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 15:47:12 - train.py[line:551] - INFO: load:1.96 valid_run:3519.97 task_valid:3364.89 collect_output:123.84
2023-01-11 15:49:14 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 15:49:14 - train.py[line:551] - INFO: load:1.98 valid_run:3641.47 task_valid:3482.93 collect_output:126.26
2023-01-11 15:51:15 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 15:51:15 - train.py[line:551] - INFO: load:2.01 valid_run:3762.67 task_valid:3600.98 collect_output:128.30

====================================================================================================
SGG eval:     R @ 50: 0.6339;     R @ 100: 0.6780;     R @ 500: 0.7077;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4062;    mR @ 100: 0.4534;    mR @ 500: 0.4983;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7353) (flying in:0.2727) (growing on:0.3750) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9369) (says:0.0000) (sitting on:0.8005) (standing on:0.3293) (using:0.6500) (walking in:0.0000) (walking on:0.7838) (watching:0.3194) 
--------------------------------------------------------
====================================================================================================

2023-01-11 15:51:46 - train.py[line:487] - INFO: 0.6780200407435701
2023-01-11 15:51:46 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6339;     R @ 100: 0.6780;     R @ 500: 0.7077;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4062;    mR @ 100: 0.4534;    mR @ 500: 0.4983;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7353) (flying in:0.2727) (growing on:0.3750) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9369) (says:0.0000) (sitting on:0.8005) (standing on:0.3293) (using:0.6500) (walking in:0.0000) (walking on:0.7838) (watching:0.3194) 
--------------------------------------------------------
====================================================================================================

2023-01-11 15:51:46 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.324 | loss_v1 0 | loss_v2 0 | nll_loss 0.178 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.67802 | ppl 1.13 | vqa_score 0.589 | wps 118.2 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.697584
2023-01-11 15:51:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2023-01-11 15:51:46 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-11 15:52:36 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-11 15:54:05 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.6780200407435701) (writing took 139.2845044247806 seconds)
2023-01-11 15:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:22 - progress_bar.py[line:274] - INFO: epoch 001:   8021 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1596, wps=0.4, ups=0, wpb=109.7, bsz=40, num_updates=8010, lr=4.79115e-05, gnorm=0.844, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56980
2023-01-11 15:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:39 - progress_bar.py[line:274] - INFO: epoch 001:   8031 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1818, wps=100.8, ups=0.6, wpb=111.1, bsz=40, num_updates=8020, lr=4.79063e-05, gnorm=0.831, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56997
2023-01-11 15:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:54:55 - progress_bar.py[line:274] - INFO: epoch 001:   8041 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.099, wps=99.7, ups=0.61, wpb=108.3, bsz=40, num_updates=8030, lr=4.7901e-05, gnorm=0.787, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57014
2023-01-11 15:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:12 - progress_bar.py[line:274] - INFO: epoch 001:   8051 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1522, wps=101.7, ups=0.61, wpb=110.5, bsz=40, num_updates=8040, lr=4.78958e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57030
2023-01-11 15:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:28 - progress_bar.py[line:274] - INFO: epoch 001:   8061 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1518, wps=102, ups=0.62, wpb=109.1, bsz=40, num_updates=8050, lr=4.78906e-05, gnorm=1.099, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=57047
2023-01-11 15:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:45 - progress_bar.py[line:274] - INFO: epoch 001:   8071 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1565, wps=99.9, ups=0.61, wpb=108.4, bsz=40, num_updates=8060, lr=4.78854e-05, gnorm=1.121, clip=50, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=57063
2023-01-11 15:55:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:55:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:01 - progress_bar.py[line:274] - INFO: epoch 001:   8081 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.125, wps=99.6, ups=0.6, wpb=109.9, bsz=40, num_updates=8070, lr=4.78802e-05, gnorm=1.1, clip=60, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=57080
2023-01-11 15:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:18 - progress_bar.py[line:274] - INFO: epoch 001:   8091 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1702, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=8080, lr=4.7875e-05, gnorm=0.878, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57097
2023-01-11 15:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:35 - progress_bar.py[line:274] - INFO: epoch 001:   8101 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1613, wps=99.2, ups=0.6, wpb=110.2, bsz=40, num_updates=8090, lr=4.78698e-05, gnorm=1.165, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=57114
2023-01-11 15:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:52 - progress_bar.py[line:274] - INFO: epoch 001:   8111 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0762, wps=100.3, ups=0.61, wpb=109.7, bsz=40, num_updates=8100, lr=4.78646e-05, gnorm=1.25, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=57130
2023-01-11 15:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:09 - progress_bar.py[line:274] - INFO: epoch 001:   8121 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1552, wps=98, ups=0.6, wpb=109.1, bsz=40, num_updates=8110, lr=4.78594e-05, gnorm=0.937, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=57148
2023-01-11 15:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:26 - progress_bar.py[line:274] - INFO: epoch 001:   8131 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1442, wps=101.4, ups=0.61, wpb=110.7, bsz=40, num_updates=8120, lr=4.78542e-05, gnorm=0.783, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57164
2023-01-11 15:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:43 - progress_bar.py[line:274] - INFO: epoch 001:   8141 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1705, wps=99.3, ups=0.6, wpb=110.4, bsz=40, num_updates=8130, lr=4.7849e-05, gnorm=1.166, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=57181
2023-01-11 15:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:57:59 - progress_bar.py[line:274] - INFO: epoch 001:   8151 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1134, wps=102.8, ups=0.62, wpb=111, bsz=40, num_updates=8140, lr=4.78438e-05, gnorm=0.949, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57198
2023-01-11 15:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:15 - progress_bar.py[line:274] - INFO: epoch 001:   8161 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1628, wps=103, ups=0.62, wpb=111.3, bsz=40, num_updates=8150, lr=4.78385e-05, gnorm=0.726, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=57214
2023-01-11 15:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:32 - progress_bar.py[line:274] - INFO: epoch 001:   8171 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1881, wps=98.4, ups=0.6, wpb=109, bsz=40, num_updates=8160, lr=4.78333e-05, gnorm=1.512, clip=30, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=57231
2023-01-11 15:58:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:49 - progress_bar.py[line:274] - INFO: epoch 001:   8181 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.22, wps=100, ups=0.61, wpb=109.4, bsz=40, num_updates=8170, lr=4.78281e-05, gnorm=0.81, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57248
2023-01-11 15:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:05 - progress_bar.py[line:274] - INFO: epoch 001:   8191 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1702, wps=104.5, ups=0.63, wpb=109.7, bsz=40, num_updates=8180, lr=4.78229e-05, gnorm=1.586, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57264
2023-01-11 15:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:22 - progress_bar.py[line:274] - INFO: epoch 001:   8201 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2095, wps=99.1, ups=0.61, wpb=108.5, bsz=40, num_updates=8190, lr=4.78177e-05, gnorm=1.009, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57280
2023-01-11 15:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:39 - progress_bar.py[line:274] - INFO: epoch 001:   8211 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.125, wps=95.8, ups=0.59, wpb=108.5, bsz=40, num_updates=8200, lr=4.78125e-05, gnorm=1.743, clip=60, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=57298
2023-01-11 15:59:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 15:59:55 - progress_bar.py[line:274] - INFO: epoch 001:   8221 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1727, wps=101.4, ups=0.62, wpb=108.9, bsz=40, num_updates=8210, lr=4.78073e-05, gnorm=0.795, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57314
2023-01-11 16:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:12 - progress_bar.py[line:274] - INFO: epoch 001:   8231 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1633, wps=101.6, ups=0.61, wpb=111, bsz=40, num_updates=8220, lr=4.78021e-05, gnorm=0.684, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=57331
2023-01-11 16:00:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:29 - progress_bar.py[line:274] - INFO: epoch 001:   8241 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1193, wps=99.5, ups=0.61, wpb=108.8, bsz=40, num_updates=8230, lr=4.77969e-05, gnorm=0.83, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57347
2023-01-11 16:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:46 - progress_bar.py[line:274] - INFO: epoch 001:   8251 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2, wps=99.9, ups=0.6, wpb=110.6, bsz=40, num_updates=8240, lr=4.77917e-05, gnorm=0.905, clip=10, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=57364
2023-01-11 16:00:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:00:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:02 - progress_bar.py[line:274] - INFO: epoch 001:   8261 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1895, wps=100.5, ups=0.61, wpb=110.6, bsz=40, num_updates=8250, lr=4.77865e-05, gnorm=0.859, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57381
2023-01-11 16:01:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:19 - progress_bar.py[line:274] - INFO: epoch 001:   8271 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1354, wps=102.6, ups=0.62, wpb=109.7, bsz=40, num_updates=8260, lr=4.77812e-05, gnorm=1.15, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=57397
2023-01-11 16:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:36 - progress_bar.py[line:274] - INFO: epoch 001:   8281 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1633, wps=100.8, ups=0.61, wpb=110.4, bsz=40, num_updates=8270, lr=4.7776e-05, gnorm=0.712, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=57414
2023-01-11 16:01:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:52 - progress_bar.py[line:274] - INFO: epoch 001:   8291 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1651, wps=99.8, ups=0.61, wpb=108.7, bsz=40, num_updates=8280, lr=4.77708e-05, gnorm=1.074, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=57431
2023-01-11 16:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:09 - progress_bar.py[line:274] - INFO: epoch 001:   8301 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.186, wps=99.3, ups=0.6, wpb=110.5, bsz=40, num_updates=8290, lr=4.77656e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=57448
2023-01-11 16:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:16 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 16:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:27 - progress_bar.py[line:274] - INFO: epoch 001:   8312 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2475, wps=90.5, ups=0.56, wpb=107.9, bsz=40, num_updates=8300, lr=4.77604e-05, gnorm=0.863, clip=30, loss_scale=256, train_wall=18, gb_free=10.5, ema_decay=0.9999, wall=57466
2023-01-11 16:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:44 - progress_bar.py[line:274] - INFO: epoch 001:   8322 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2294, wps=97.9, ups=0.6, wpb=109, bsz=40, num_updates=8310, lr=4.77552e-05, gnorm=0.564, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=57483
2023-01-11 16:02:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:02:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:01 - progress_bar.py[line:274] - INFO: epoch 001:   8332 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.0978, wps=101.4, ups=0.61, wpb=111.3, bsz=40, num_updates=8320, lr=4.775e-05, gnorm=0.935, clip=30, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=57499
2023-01-11 16:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:17 - progress_bar.py[line:274] - INFO: epoch 001:   8342 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.117, wps=102.6, ups=0.62, wpb=110, bsz=40, num_updates=8330, lr=4.77448e-05, gnorm=0.927, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57516
2023-01-11 16:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:34 - progress_bar.py[line:274] - INFO: epoch 001:   8352 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.12, wps=98.4, ups=0.6, wpb=110.2, bsz=40, num_updates=8340, lr=4.77396e-05, gnorm=1.474, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=57533
2023-01-11 16:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:51 - progress_bar.py[line:274] - INFO: epoch 001:   8362 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1648, wps=102.1, ups=0.62, wpb=110.6, bsz=40, num_updates=8350, lr=4.77344e-05, gnorm=0.945, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57549
2023-01-11 16:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:07 - progress_bar.py[line:274] - INFO: epoch 001:   8372 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1739, wps=99.6, ups=0.61, wpb=109.5, bsz=40, num_updates=8360, lr=4.77292e-05, gnorm=1.107, clip=50, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=57566
2023-01-11 16:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:24 - progress_bar.py[line:274] - INFO: epoch 001:   8382 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2396, wps=102.5, ups=0.62, wpb=110.9, bsz=40, num_updates=8370, lr=4.7724e-05, gnorm=0.941, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57582
2023-01-11 16:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:41 - progress_bar.py[line:274] - INFO: epoch 001:   8392 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1456, wps=98.5, ups=0.6, wpb=108.9, bsz=40, num_updates=8380, lr=4.77187e-05, gnorm=0.647, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=57599
2023-01-11 16:04:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:04:57 - progress_bar.py[line:274] - INFO: epoch 001:   8402 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1443, wps=103, ups=0.62, wpb=110.4, bsz=40, num_updates=8390, lr=4.77135e-05, gnorm=1.177, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57616
2023-01-11 16:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:14 - progress_bar.py[line:274] - INFO: epoch 001:   8412 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1224, wps=101.1, ups=0.61, wpb=109.9, bsz=40, num_updates=8400, lr=4.77083e-05, gnorm=0.769, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57632
2023-01-11 16:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:30 - progress_bar.py[line:274] - INFO: epoch 001:   8422 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2143, wps=99.8, ups=0.61, wpb=109.4, bsz=40, num_updates=8410, lr=4.77031e-05, gnorm=0.86, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57649
2023-01-11 16:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:47 - progress_bar.py[line:274] - INFO: epoch 001:   8432 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1458, wps=99.8, ups=0.6, wpb=111.4, bsz=40, num_updates=8420, lr=4.76979e-05, gnorm=1.417, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=57666
2023-01-11 16:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:04 - progress_bar.py[line:274] - INFO: epoch 001:   8442 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1827, wps=98.4, ups=0.61, wpb=107.5, bsz=40, num_updates=8430, lr=4.76927e-05, gnorm=1.526, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57683
2023-01-11 16:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:21 - progress_bar.py[line:274] - INFO: epoch 001:   8452 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2178, wps=99.1, ups=0.6, wpb=109.4, bsz=40, num_updates=8440, lr=4.76875e-05, gnorm=1.393, clip=40, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=57699
2023-01-11 16:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:38 - progress_bar.py[line:274] - INFO: epoch 001:   8462 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2151, wps=99.8, ups=0.6, wpb=110.5, bsz=40, num_updates=8450, lr=4.76823e-05, gnorm=0.715, clip=10, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=57716
2023-01-11 16:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:06:54 - progress_bar.py[line:274] - INFO: epoch 001:   8472 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1913, wps=100, ups=0.62, wpb=108.2, bsz=40, num_updates=8460, lr=4.76771e-05, gnorm=0.861, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=57733
2023-01-11 16:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:11 - progress_bar.py[line:274] - INFO: epoch 001:   8482 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2165, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=8470, lr=4.76719e-05, gnorm=0.753, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57749
2023-01-11 16:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:28 - progress_bar.py[line:274] - INFO: epoch 001:   8492 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2636, wps=99.2, ups=0.61, wpb=109.3, bsz=40, num_updates=8480, lr=4.76667e-05, gnorm=1.572, clip=60, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57766
2023-01-11 16:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:44 - progress_bar.py[line:274] - INFO: epoch 001:   8502 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1579, wps=101.1, ups=0.61, wpb=110.3, bsz=40, num_updates=8490, lr=4.76615e-05, gnorm=1.67, clip=60, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=57783
2023-01-11 16:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:01 - progress_bar.py[line:274] - INFO: epoch 001:   8512 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2617, wps=99.5, ups=0.61, wpb=108.9, bsz=40, num_updates=8500, lr=4.76563e-05, gnorm=1.052, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57800
2023-01-11 16:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:18 - progress_bar.py[line:274] - INFO: epoch 001:   8522 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1667, wps=99.7, ups=0.61, wpb=109.7, bsz=40, num_updates=8510, lr=4.7651e-05, gnorm=1.034, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57817
2023-01-11 16:08:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:35 - progress_bar.py[line:274] - INFO: epoch 001:   8532 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1058, wps=99, ups=0.61, wpb=108.7, bsz=40, num_updates=8520, lr=4.76458e-05, gnorm=0.621, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57833
2023-01-11 16:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:51 - progress_bar.py[line:274] - INFO: epoch 001:   8542 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1698, wps=101.2, ups=0.62, wpb=108.8, bsz=40, num_updates=8530, lr=4.76406e-05, gnorm=1.874, clip=70, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57850
2023-01-11 16:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:08 - progress_bar.py[line:274] - INFO: epoch 001:   8552 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1978, wps=100.5, ups=0.61, wpb=109.3, bsz=40, num_updates=8540, lr=4.76354e-05, gnorm=1.263, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57867
2023-01-11 16:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:24 - progress_bar.py[line:274] - INFO: epoch 001:   8562 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1316, wps=103.3, ups=0.62, wpb=110.7, bsz=40, num_updates=8550, lr=4.76302e-05, gnorm=1.112, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57883
2023-01-11 16:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:41 - progress_bar.py[line:274] - INFO: epoch 001:   8572 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2157, wps=99.7, ups=0.6, wpb=110.1, bsz=40, num_updates=8560, lr=4.7625e-05, gnorm=0.93, clip=40, loss_scale=256, train_wall=17, gb_free=10.7, ema_decay=0.9999, wall=57900
2023-01-11 16:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:09:59 - progress_bar.py[line:274] - INFO: epoch 001:   8582 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1359, wps=95.9, ups=0.58, wpb=109.7, bsz=40, num_updates=8570, lr=4.76198e-05, gnorm=1.153, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=57917
2023-01-11 16:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:16 - progress_bar.py[line:274] - INFO: epoch 001:   8592 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1188, wps=97.5, ups=0.6, wpb=107.9, bsz=40, num_updates=8580, lr=4.76146e-05, gnorm=1.443, clip=50, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=57934
2023-01-11 16:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:32 - progress_bar.py[line:274] - INFO: epoch 001:   8602 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2245, wps=102.9, ups=0.63, wpb=109.7, bsz=40, num_updates=8590, lr=4.76094e-05, gnorm=1.051, clip=40, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=57950
2023-01-11 16:10:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:48 - progress_bar.py[line:274] - INFO: epoch 001:   8612 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1978, wps=104.9, ups=0.63, wpb=111.3, bsz=40, num_updates=8600, lr=4.76042e-05, gnorm=0.927, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57967
2023-01-11 16:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:10:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:04 - progress_bar.py[line:274] - INFO: epoch 001:   8622 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1607, wps=101.8, ups=0.62, wpb=110, bsz=40, num_updates=8610, lr=4.7599e-05, gnorm=0.871, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57983
2023-01-11 16:11:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:21 - progress_bar.py[line:274] - INFO: epoch 001:   8632 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1889, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=8620, lr=4.75938e-05, gnorm=0.85, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58000
2023-01-11 16:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:37 - progress_bar.py[line:274] - INFO: epoch 001:   8642 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1759, wps=100.7, ups=0.62, wpb=108.1, bsz=40, num_updates=8630, lr=4.75885e-05, gnorm=0.973, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58016
2023-01-11 16:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:11:55 - progress_bar.py[line:274] - INFO: epoch 001:   8652 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2, wps=98.9, ups=0.6, wpb=110.4, bsz=40, num_updates=8640, lr=4.75833e-05, gnorm=0.688, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=58033
2023-01-11 16:11:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:11 - progress_bar.py[line:274] - INFO: epoch 001:   8662 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2323, wps=101.9, ups=0.62, wpb=110.4, bsz=40, num_updates=8650, lr=4.75781e-05, gnorm=0.512, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58050
2023-01-11 16:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:28 - progress_bar.py[line:274] - INFO: epoch 001:   8672 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2524, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=8660, lr=4.75729e-05, gnorm=0.955, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58066
2023-01-11 16:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:44 - progress_bar.py[line:274] - INFO: epoch 001:   8682 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1714, wps=100.6, ups=0.62, wpb=108.9, bsz=40, num_updates=8670, lr=4.75677e-05, gnorm=0.839, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58083
2023-01-11 16:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   8692 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1122, wps=101.6, ups=0.61, wpb=110.5, bsz=40, num_updates=8680, lr=4.75625e-05, gnorm=0.895, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58099
2023-01-11 16:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:17 - progress_bar.py[line:274] - INFO: epoch 001:   8702 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2407, wps=99.1, ups=0.6, wpb=109.3, bsz=40, num_updates=8690, lr=4.75573e-05, gnorm=0.917, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58116
2023-01-11 16:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:34 - progress_bar.py[line:274] - INFO: epoch 001:   8712 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.198, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=8700, lr=4.75521e-05, gnorm=1.29, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58133
2023-01-11 16:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:51 - progress_bar.py[line:274] - INFO: epoch 001:   8722 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.191, wps=101.1, ups=0.6, wpb=111.8, bsz=40, num_updates=8710, lr=4.75469e-05, gnorm=0.851, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=58150
2023-01-11 16:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:08 - progress_bar.py[line:274] - INFO: epoch 001:   8732 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1622, wps=97.7, ups=0.6, wpb=108.4, bsz=40, num_updates=8720, lr=4.75417e-05, gnorm=0.712, clip=20, loss_scale=256, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=58166
2023-01-11 16:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:24 - progress_bar.py[line:274] - INFO: epoch 001:   8742 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2, wps=99.8, ups=0.61, wpb=109.4, bsz=40, num_updates=8730, lr=4.75365e-05, gnorm=0.856, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58183
2023-01-11 16:14:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:41 - progress_bar.py[line:274] - INFO: epoch 001:   8752 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=112.333, nsentences=40, sample_size=112.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1809, wps=104.8, ups=0.62, wpb=112.3, bsz=40, num_updates=8740, lr=4.75313e-05, gnorm=1.242, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58199
2023-01-11 16:14:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:14:57 - progress_bar.py[line:274] - INFO: epoch 001:   8762 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2475, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=8750, lr=4.7526e-05, gnorm=0.761, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58216
2023-01-11 16:15:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:15 - progress_bar.py[line:274] - INFO: epoch 001:   8772 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.28, wps=98.1, ups=0.6, wpb=109.9, bsz=40, num_updates=8760, lr=4.75208e-05, gnorm=1.343, clip=50, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=58233
2023-01-11 16:15:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:31 - progress_bar.py[line:274] - INFO: epoch 001:   8782 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2407, wps=99.9, ups=0.61, wpb=109.9, bsz=40, num_updates=8770, lr=4.75156e-05, gnorm=0.991, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58250
2023-01-11 16:15:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:48 - progress_bar.py[line:274] - INFO: epoch 001:   8792 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1593, wps=100.2, ups=0.62, wpb=108.5, bsz=40, num_updates=8780, lr=4.75104e-05, gnorm=0.752, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58267
2023-01-11 16:15:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:15:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:05 - progress_bar.py[line:274] - INFO: epoch 001:   8802 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2, wps=102.5, ups=0.61, wpb=111.7, bsz=40, num_updates=8790, lr=4.75052e-05, gnorm=0.806, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58283
2023-01-11 16:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:21 - progress_bar.py[line:274] - INFO: epoch 001:   8812 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2072, wps=102.1, ups=0.62, wpb=109.3, bsz=40, num_updates=8800, lr=4.75e-05, gnorm=1.368, clip=60, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=58300
2023-01-11 16:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:38 - progress_bar.py[line:274] - INFO: epoch 001:   8822 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2451, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=8810, lr=4.74948e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58316
2023-01-11 16:16:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:16:54 - progress_bar.py[line:274] - INFO: epoch 001:   8832 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1727, wps=98.9, ups=0.6, wpb=109.1, bsz=40, num_updates=8820, lr=4.74896e-05, gnorm=1.24, clip=50, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=58333
2023-01-11 16:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:11 - progress_bar.py[line:274] - INFO: epoch 001:   8842 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1546, wps=98.2, ups=0.6, wpb=109.3, bsz=40, num_updates=8830, lr=4.74844e-05, gnorm=1.781, clip=70, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=58350
2023-01-11 16:17:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:28 - progress_bar.py[line:274] - INFO: epoch 001:   8852 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2604, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=8840, lr=4.74792e-05, gnorm=1.06, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58367
2023-01-11 16:17:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:45 - progress_bar.py[line:274] - INFO: epoch 001:   8862 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.211, wps=98.3, ups=0.6, wpb=108.5, bsz=40, num_updates=8850, lr=4.7474e-05, gnorm=0.686, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58383
2023-01-11 16:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:01 - progress_bar.py[line:274] - INFO: epoch 001:   8872 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2018, wps=99.9, ups=0.61, wpb=109.8, bsz=40, num_updates=8860, lr=4.74687e-05, gnorm=0.766, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58400
2023-01-11 16:18:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:18 - progress_bar.py[line:274] - INFO: epoch 001:   8882 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.27, wps=100.3, ups=0.61, wpb=109.4, bsz=40, num_updates=8870, lr=4.74635e-05, gnorm=1.088, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=58417
2023-01-11 16:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:35 - progress_bar.py[line:274] - INFO: epoch 001:   8892 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2065, wps=101.5, ups=0.61, wpb=110.1, bsz=40, num_updates=8880, lr=4.74583e-05, gnorm=0.999, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=58433
2023-01-11 16:18:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:52 - progress_bar.py[line:274] - INFO: epoch 001:   8902 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2569, wps=96.6, ups=0.6, wpb=108.1, bsz=40, num_updates=8890, lr=4.74531e-05, gnorm=0.759, clip=40, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=58450
2023-01-11 16:18:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:08 - progress_bar.py[line:274] - INFO: epoch 001:   8912 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1748, wps=101.2, ups=0.61, wpb=109.8, bsz=40, num_updates=8900, lr=4.74479e-05, gnorm=1.306, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=58467
2023-01-11 16:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:25 - progress_bar.py[line:274] - INFO: epoch 001:   8922 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.177, wps=101.8, ups=0.62, wpb=109.3, bsz=40, num_updates=8910, lr=4.74427e-05, gnorm=0.71, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58483
2023-01-11 16:19:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:41 - progress_bar.py[line:274] - INFO: epoch 001:   8932 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2451, wps=100.5, ups=0.61, wpb=110, bsz=40, num_updates=8920, lr=4.74375e-05, gnorm=0.903, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58500
2023-01-11 16:19:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:19:58 - progress_bar.py[line:274] - INFO: epoch 001:   8942 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1584, wps=100.6, ups=0.61, wpb=109.6, bsz=40, num_updates=8930, lr=4.74323e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58517
2023-01-11 16:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:15 - progress_bar.py[line:274] - INFO: epoch 001:   8952 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2, wps=101.9, ups=0.62, wpb=109.1, bsz=40, num_updates=8940, lr=4.74271e-05, gnorm=0.633, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58533
2023-01-11 16:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:32 - progress_bar.py[line:274] - INFO: epoch 001:   8962 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1875, wps=99.8, ups=0.61, wpb=109.5, bsz=40, num_updates=8950, lr=4.74219e-05, gnorm=0.718, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=58550
2023-01-11 16:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:48 - progress_bar.py[line:274] - INFO: epoch 001:   8972 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1635, wps=102.1, ups=0.62, wpb=110.3, bsz=40, num_updates=8960, lr=4.74167e-05, gnorm=0.625, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=58567
2023-01-11 16:20:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:20:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:04 - progress_bar.py[line:274] - INFO: epoch 001:   8982 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2967, wps=104.2, ups=0.63, wpb=111.1, bsz=40, num_updates=8970, lr=4.74115e-05, gnorm=0.915, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=58583
2023-01-11 16:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:21 - progress_bar.py[line:274] - INFO: epoch 001:   8992 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1979, wps=98.3, ups=0.6, wpb=108.7, bsz=40, num_updates=8980, lr=4.74063e-05, gnorm=1.865, clip=20, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=58600
2023-01-11 16:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:37 - progress_bar.py[line:274] - INFO: epoch 001:   9002 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1456, wps=102.6, ups=0.62, wpb=110.2, bsz=40, num_updates=8990, lr=4.7401e-05, gnorm=2.017, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58616
2023-01-11 16:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 16:21:54 - progress_bar.py[line:274] - INFO: epoch 001:   9012 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2613, wps=100.1, ups=0.61, wpb=109.4, bsz=40, num_updates=9000, lr=4.73958e-05, gnorm=0.749, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=58633
2023-01-11 16:21:54 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 16:21:55 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 16:21:55 - train.py[line:551] - INFO: load:1.25 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 16:23:57 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 16:23:57 - train.py[line:551] - INFO: load:1.28 valid_run:121.88 task_valid:118.87 collect_output:1.90
2023-01-11 16:25:58 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 16:25:58 - train.py[line:551] - INFO: load:1.30 valid_run:241.93 task_valid:234.39 collect_output:5.34
2023-01-11 16:28:00 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 16:28:00 - train.py[line:551] - INFO: load:1.33 valid_run:363.92 task_valid:350.54 collect_output:10.11
2023-01-11 16:30:01 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 16:30:01 - train.py[line:551] - INFO: load:1.35 valid_run:485.69 task_valid:463.98 collect_output:17.36
2023-01-11 16:32:02 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 16:32:02 - train.py[line:551] - INFO: load:1.38 valid_run:606.51 task_valid:581.26 collect_output:19.76
2023-01-11 16:34:06 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 16:34:06 - train.py[line:551] - INFO: load:1.40 valid_run:729.75 task_valid:699.63 collect_output:23.49
2023-01-11 16:36:08 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 16:36:08 - train.py[line:551] - INFO: load:1.43 valid_run:852.41 task_valid:817.14 collect_output:27.59
2023-01-11 16:38:11 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 16:38:11 - train.py[line:551] - INFO: load:1.45 valid_run:974.50 task_valid:933.46 collect_output:32.27
2023-01-11 16:40:14 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 16:40:14 - train.py[line:551] - INFO: load:1.48 valid_run:1098.12 task_valid:1050.14 collect_output:38.17
2023-01-11 16:42:16 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 16:42:16 - train.py[line:551] - INFO: load:1.51 valid_run:1220.06 task_valid:1162.64 collect_output:46.53
2023-01-11 16:44:17 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 16:44:17 - train.py[line:551] - INFO: load:1.53 valid_run:1340.25 task_valid:1278.21 collect_output:50.06
2023-01-11 16:46:18 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 16:46:18 - train.py[line:551] - INFO: load:1.55 valid_run:1461.47 task_valid:1394.75 collect_output:53.69
2023-01-11 16:48:17 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 16:48:17 - train.py[line:551] - INFO: load:1.58 valid_run:1580.56 task_valid:1508.46 collect_output:57.94
2023-01-11 16:50:18 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 16:50:18 - train.py[line:551] - INFO: load:1.61 valid_run:1701.56 task_valid:1625.89 collect_output:60.42
2023-01-11 16:52:19 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 16:52:19 - train.py[line:551] - INFO: load:1.63 valid_run:1822.67 task_valid:1741.86 collect_output:64.45
2023-01-11 16:54:21 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 16:54:21 - train.py[line:551] - INFO: load:1.66 valid_run:1944.21 task_valid:1855.68 collect_output:71.07
2023-01-11 16:56:22 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 16:56:22 - train.py[line:551] - INFO: load:1.69 valid_run:2065.13 task_valid:1971.42 collect_output:75.19
2023-01-11 16:58:23 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 16:58:23 - train.py[line:551] - INFO: load:1.71 valid_run:2185.62 task_valid:2088.86 collect_output:77.15
2023-01-11 17:00:24 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 17:00:24 - train.py[line:551] - INFO: load:1.74 valid_run:2307.03 task_valid:2205.59 collect_output:80.74
2023-01-11 17:02:24 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 17:02:24 - train.py[line:551] - INFO: load:1.76 valid_run:2427.26 task_valid:2321.75 collect_output:83.73
2023-01-11 17:04:26 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 17:04:26 - train.py[line:551] - INFO: load:1.79 valid_run:2548.88 task_valid:2438.00 collect_output:88.02
2023-01-11 17:06:28 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 17:06:28 - train.py[line:551] - INFO: load:1.82 valid_run:2670.71 task_valid:2556.47 collect_output:90.32
2023-01-11 17:08:28 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 17:08:28 - train.py[line:551] - INFO: load:1.84 valid_run:2791.03 task_valid:2670.40 collect_output:95.67
2023-01-11 17:10:28 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 17:10:28 - train.py[line:551] - INFO: load:1.87 valid_run:2910.55 task_valid:2786.26 collect_output:98.26
2023-01-11 17:12:30 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 17:12:30 - train.py[line:551] - INFO: load:1.89 valid_run:3032.30 task_valid:2902.00 collect_output:103.21
2023-01-11 17:14:33 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 17:14:33 - train.py[line:551] - INFO: load:1.92 valid_run:3155.14 task_valid:3017.48 collect_output:109.48
2023-01-11 17:16:32 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 17:16:32 - train.py[line:551] - INFO: load:1.94 valid_run:3274.49 task_valid:3131.02 collect_output:114.24
2023-01-11 17:18:34 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 17:18:34 - train.py[line:551] - INFO: load:1.97 valid_run:3396.24 task_valid:3250.14 collect_output:115.76
2023-01-11 17:20:35 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 17:20:35 - train.py[line:551] - INFO: load:1.99 valid_run:3517.40 task_valid:3365.21 collect_output:120.81
2023-01-11 17:22:37 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 17:22:37 - train.py[line:551] - INFO: load:2.02 valid_run:3639.34 task_valid:3483.53 collect_output:123.31
2023-01-11 17:24:39 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 17:24:39 - train.py[line:551] - INFO: load:2.04 valid_run:3760.49 task_valid:3601.80 collect_output:125.07

====================================================================================================
SGG eval:     R @ 50: 0.6292;     R @ 100: 0.6768;     R @ 500: 0.7082;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4012;    mR @ 100: 0.4355;    mR @ 500: 0.4761;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9271) (says:0.0000) (sitting on:0.8056) (standing on:0.3343) (using:0.6500) (walking in:0.0000) (walking on:0.7838) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6292;     R @ 100: 0.6768;     R @ 500: 0.7082;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4012;    mR @ 100: 0.4355;    mR @ 500: 0.4761;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9271) (says:0.0000) (sitting on:0.8056) (standing on:0.3343) (using:0.6500) (walking in:0.0000) (walking on:0.7838) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 17:25:09 - train.py[line:487] - INFO: 0.6767624649859943
2023-01-11 17:25:10 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 17:25:10 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.319 | loss_v1 0 | loss_v2 0 | nll_loss 0.17 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.676762 | ppl 1.13 | vqa_score 0.6025 | wps 118.2 | wpb 72 | bsz 24 | num_updates 9000 | best_R@100 0.697584
2023-01-11 17:25:10 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 9000 updates
2023-01-11 17:25:10 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_9000.pt
2023-01-11 17:25:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_9000.pt
2023-01-11 17:27:08 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_9000.pt (epoch 1 @ 9000 updates, score 0.6767624649859943) (writing took 118.16530347056687 seconds)
2023-01-11 17:27:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:24 - progress_bar.py[line:274] - INFO: epoch 001:   9022 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2021, wps=0.4, ups=0, wpb=109.9, bsz=40, num_updates=9010, lr=4.73906e-05, gnorm=2.02, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62563
2023-01-11 17:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:41 - progress_bar.py[line:274] - INFO: epoch 001:   9032 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.292, wps=98.8, ups=0.6, wpb=109.5, bsz=40, num_updates=9020, lr=4.73854e-05, gnorm=0.629, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=62580
2023-01-11 17:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:27:58 - progress_bar.py[line:274] - INFO: epoch 001:   9042 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.236, wps=100, ups=0.61, wpb=109.2, bsz=40, num_updates=9030, lr=4.73802e-05, gnorm=0.716, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62597
2023-01-11 17:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:15 - progress_bar.py[line:274] - INFO: epoch 001:   9052 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.2072, wps=98.4, ups=0.6, wpb=108.8, bsz=40, num_updates=9040, lr=4.7375e-05, gnorm=0.76, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=62613
2023-01-11 17:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:31 - progress_bar.py[line:274] - INFO: epoch 001:   9062 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2525, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=9050, lr=4.73698e-05, gnorm=0.971, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62630
2023-01-11 17:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:48 - progress_bar.py[line:274] - INFO: epoch 001:   9072 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2828, wps=98.7, ups=0.61, wpb=108.3, bsz=40, num_updates=9060, lr=4.73646e-05, gnorm=1.312, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62647
2023-01-11 17:28:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:05 - progress_bar.py[line:274] - INFO: epoch 001:   9082 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1771, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=9070, lr=4.73594e-05, gnorm=1.075, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62663
2023-01-11 17:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:21 - progress_bar.py[line:274] - INFO: epoch 001:   9092 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1982, wps=98.7, ups=0.61, wpb=108.3, bsz=40, num_updates=9080, lr=4.73542e-05, gnorm=0.73, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62680
2023-01-11 17:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   9102 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2553, wps=100.5, ups=0.61, wpb=110.5, bsz=40, num_updates=9090, lr=4.7349e-05, gnorm=1.629, clip=60, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=62697
2023-01-11 17:29:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:29:55 - progress_bar.py[line:274] - INFO: epoch 001:   9112 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2252, wps=98.7, ups=0.6, wpb=108.8, bsz=40, num_updates=9100, lr=4.73438e-05, gnorm=1.252, clip=20, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=62714
2023-01-11 17:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:11 - progress_bar.py[line:274] - INFO: epoch 001:   9122 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.215, wps=100.6, ups=0.61, wpb=109.3, bsz=40, num_updates=9110, lr=4.73385e-05, gnorm=0.692, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=62730
2023-01-11 17:30:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:28 - progress_bar.py[line:274] - INFO: epoch 001:   9132 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.25, wps=98.3, ups=0.6, wpb=109.7, bsz=40, num_updates=9120, lr=4.73333e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=17, gb_free=9.6, ema_decay=0.9999, wall=62747
2023-01-11 17:30:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:45 - progress_bar.py[line:274] - INFO: epoch 001:   9142 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2021, wps=99.5, ups=0.6, wpb=111.3, bsz=40, num_updates=9130, lr=4.73281e-05, gnorm=0.832, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=62764
2023-01-11 17:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:02 - progress_bar.py[line:274] - INFO: epoch 001:   9152 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.233, wps=101.5, ups=0.61, wpb=110.4, bsz=40, num_updates=9140, lr=4.73229e-05, gnorm=0.956, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62781
2023-01-11 17:31:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:19 - progress_bar.py[line:274] - INFO: epoch 001:   9162 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1919, wps=101.1, ups=0.6, wpb=111.9, bsz=40, num_updates=9150, lr=4.73177e-05, gnorm=1.61, clip=60, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=62797
2023-01-11 17:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:35 - progress_bar.py[line:274] - INFO: epoch 001:   9172 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2203, wps=104, ups=0.63, wpb=109.4, bsz=40, num_updates=9160, lr=4.73125e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62813
2023-01-11 17:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:51 - progress_bar.py[line:274] - INFO: epoch 001:   9182 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.25, wps=100.6, ups=0.62, wpb=108.9, bsz=40, num_updates=9170, lr=4.73073e-05, gnorm=0.915, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62830
2023-01-11 17:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:08 - progress_bar.py[line:274] - INFO: epoch 001:   9192 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1619, wps=100.5, ups=0.62, wpb=108.1, bsz=40, num_updates=9180, lr=4.73021e-05, gnorm=0.612, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62846
2023-01-11 17:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:24 - progress_bar.py[line:274] - INFO: epoch 001:   9202 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1616, wps=102.2, ups=0.62, wpb=109.9, bsz=40, num_updates=9190, lr=4.72969e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=62863
2023-01-11 17:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:40 - progress_bar.py[line:274] - INFO: epoch 001:   9212 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2243, wps=102.6, ups=0.63, wpb=109, bsz=40, num_updates=9200, lr=4.72917e-05, gnorm=1.377, clip=60, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=62879
2023-01-11 17:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:32:57 - progress_bar.py[line:274] - INFO: epoch 001:   9222 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1892, wps=98.3, ups=0.61, wpb=107.7, bsz=40, num_updates=9210, lr=4.72865e-05, gnorm=0.724, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62895
2023-01-11 17:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:14 - progress_bar.py[line:274] - INFO: epoch 001:   9232 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2804, wps=98.7, ups=0.6, wpb=108.9, bsz=40, num_updates=9220, lr=4.72813e-05, gnorm=1.48, clip=20, loss_scale=512, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=62912
2023-01-11 17:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:30 - progress_bar.py[line:274] - INFO: epoch 001:   9242 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2604, wps=100.3, ups=0.61, wpb=109.6, bsz=40, num_updates=9230, lr=4.7276e-05, gnorm=0.701, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62929
2023-01-11 17:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:47 - progress_bar.py[line:274] - INFO: epoch 001:   9252 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2136, wps=101.4, ups=0.61, wpb=110.6, bsz=40, num_updates=9240, lr=4.72708e-05, gnorm=2.448, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62946
2023-01-11 17:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:03 - progress_bar.py[line:274] - INFO: epoch 001:   9262 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2165, wps=101.2, ups=0.61, wpb=109.8, bsz=40, num_updates=9250, lr=4.72656e-05, gnorm=1.264, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62962
2023-01-11 17:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:20 - progress_bar.py[line:274] - INFO: epoch 001:   9272 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2157, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=9260, lr=4.72604e-05, gnorm=0.796, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62979
2023-01-11 17:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:37 - progress_bar.py[line:274] - INFO: epoch 001:   9282 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.15, wps=98.4, ups=0.59, wpb=110.5, bsz=40, num_updates=9270, lr=4.72552e-05, gnorm=1.943, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=62996
2023-01-11 17:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:34:54 - progress_bar.py[line:274] - INFO: epoch 001:   9292 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.268, wps=97.3, ups=0.59, wpb=109.1, bsz=40, num_updates=9280, lr=4.725e-05, gnorm=1.302, clip=40, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=63013
2023-01-11 17:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:10 - progress_bar.py[line:274] - INFO: epoch 001:   9302 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2727, wps=100, ups=0.62, wpb=108.1, bsz=40, num_updates=9290, lr=4.72448e-05, gnorm=0.766, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=63029
2023-01-11 17:35:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:27 - progress_bar.py[line:274] - INFO: epoch 001:   9312 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.268, wps=102.7, ups=0.62, wpb=110.5, bsz=40, num_updates=9300, lr=4.72396e-05, gnorm=0.781, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63046
2023-01-11 17:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:43 - progress_bar.py[line:274] - INFO: epoch 001:   9322 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2258, wps=103.3, ups=0.62, wpb=110.5, bsz=40, num_updates=9310, lr=4.72344e-05, gnorm=0.747, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63062
2023-01-11 17:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:35:59 - progress_bar.py[line:274] - INFO: epoch 001:   9332 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.29, wps=102.9, ups=0.62, wpb=109.7, bsz=40, num_updates=9320, lr=4.72292e-05, gnorm=0.643, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63078
2023-01-11 17:36:00 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 17:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:17 - progress_bar.py[line:274] - INFO: epoch 001:   9343 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2784, wps=94.9, ups=0.58, wpb=109.8, bsz=40, num_updates=9330, lr=4.7224e-05, gnorm=1.094, clip=40, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=63096
2023-01-11 17:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:33 - progress_bar.py[line:274] - INFO: epoch 001:   9353 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2091, wps=101.5, ups=0.61, wpb=110, bsz=40, num_updates=9340, lr=4.72187e-05, gnorm=0.637, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63112
2023-01-11 17:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:50 - progress_bar.py[line:274] - INFO: epoch 001:   9363 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2041, wps=100.8, ups=0.61, wpb=110.7, bsz=40, num_updates=9350, lr=4.72135e-05, gnorm=0.769, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63129
2023-01-11 17:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:07 - progress_bar.py[line:274] - INFO: epoch 001:   9373 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2376, wps=98.7, ups=0.6, wpb=109.7, bsz=40, num_updates=9360, lr=4.72083e-05, gnorm=0.858, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=63146
2023-01-11 17:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:24 - progress_bar.py[line:274] - INFO: epoch 001:   9383 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.21, wps=102.4, ups=0.62, wpb=110.6, bsz=40, num_updates=9370, lr=4.72031e-05, gnorm=0.778, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63162
2023-01-11 17:37:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:40 - progress_bar.py[line:274] - INFO: epoch 001:   9393 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.22, wps=100.2, ups=0.6, wpb=111.2, bsz=40, num_updates=9380, lr=4.71979e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=63179
2023-01-11 17:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:37:57 - progress_bar.py[line:274] - INFO: epoch 001:   9403 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1456, wps=102.8, ups=0.63, wpb=109.5, bsz=40, num_updates=9390, lr=4.71927e-05, gnorm=1.405, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63195
2023-01-11 17:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:13 - progress_bar.py[line:274] - INFO: epoch 001:   9413 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1717, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=9400, lr=4.71875e-05, gnorm=1.728, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=63212
2023-01-11 17:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:30 - progress_bar.py[line:274] - INFO: epoch 001:   9423 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2391, wps=98.9, ups=0.59, wpb=110.9, bsz=40, num_updates=9410, lr=4.71823e-05, gnorm=1.371, clip=50, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=63229
2023-01-11 17:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:47 - progress_bar.py[line:274] - INFO: epoch 001:   9433 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2455, wps=98.2, ups=0.6, wpb=108.4, bsz=40, num_updates=9420, lr=4.71771e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=63246
2023-01-11 17:38:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:38:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:04 - progress_bar.py[line:274] - INFO: epoch 001:   9443 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2347, wps=102.4, ups=0.62, wpb=110, bsz=40, num_updates=9430, lr=4.71719e-05, gnorm=2.021, clip=40, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=63262
2023-01-11 17:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:20 - progress_bar.py[line:274] - INFO: epoch 001:   9453 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2292, wps=100.6, ups=0.61, wpb=110.1, bsz=40, num_updates=9440, lr=4.71667e-05, gnorm=0.387, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=63279
2023-01-11 17:39:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:37 - progress_bar.py[line:274] - INFO: epoch 001:   9463 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2407, wps=102.4, ups=0.62, wpb=110, bsz=40, num_updates=9450, lr=4.71615e-05, gnorm=1.69, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=63295
2023-01-11 17:39:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:53 - progress_bar.py[line:274] - INFO: epoch 001:   9473 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2232, wps=101.9, ups=0.62, wpb=108.7, bsz=40, num_updates=9460, lr=4.71563e-05, gnorm=0.857, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63311
2023-01-11 17:39:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:10 - progress_bar.py[line:274] - INFO: epoch 001:   9483 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2222, wps=101.5, ups=0.61, wpb=111.5, bsz=40, num_updates=9470, lr=4.7151e-05, gnorm=0.85, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63328
2023-01-11 17:40:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:27 - progress_bar.py[line:274] - INFO: epoch 001:   9493 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2959, wps=100, ups=0.61, wpb=109.7, bsz=40, num_updates=9480, lr=4.71458e-05, gnorm=1.143, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63345
2023-01-11 17:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:44 - progress_bar.py[line:274] - INFO: epoch 001:   9503 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2083, wps=103.9, ups=0.62, wpb=111.1, bsz=40, num_updates=9490, lr=4.71406e-05, gnorm=1.953, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63362
2023-01-11 17:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:40:56 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 17:41:01 - progress_bar.py[line:274] - INFO: epoch 001:   9514 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2451, wps=97.4, ups=0.59, wpb=109.8, bsz=40, num_updates=9500, lr=4.71354e-05, gnorm=0.656, clip=20, loss_scale=256, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=63379
2023-01-11 17:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:18 - progress_bar.py[line:274] - INFO: epoch 001:   9524 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3333, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=9510, lr=4.71302e-05, gnorm=0.666, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63396
2023-01-11 17:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:35 - progress_bar.py[line:274] - INFO: epoch 001:   9534 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.22, wps=101.7, ups=0.61, wpb=111.5, bsz=40, num_updates=9520, lr=4.7125e-05, gnorm=1.619, clip=70, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63413
2023-01-11 17:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:52 - progress_bar.py[line:274] - INFO: epoch 001:   9544 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2632, wps=100.3, ups=0.61, wpb=110, bsz=40, num_updates=9530, lr=4.71198e-05, gnorm=0.9, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63431
2023-01-11 17:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:10 - progress_bar.py[line:274] - INFO: epoch 001:   9554 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2385, wps=98.1, ups=0.6, wpb=108.4, bsz=40, num_updates=9540, lr=4.71146e-05, gnorm=0.806, clip=20, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=63448
2023-01-11 17:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:26 - progress_bar.py[line:274] - INFO: epoch 001:   9564 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3214, wps=102.2, ups=0.63, wpb=108.7, bsz=40, num_updates=9550, lr=4.71094e-05, gnorm=0.594, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63464
2023-01-11 17:42:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:43 - progress_bar.py[line:274] - INFO: epoch 001:   9574 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2407, wps=100.5, ups=0.62, wpb=108.3, bsz=40, num_updates=9560, lr=4.71042e-05, gnorm=0.784, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63481
2023-01-11 17:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:00 - progress_bar.py[line:274] - INFO: epoch 001:   9584 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2062, wps=99.8, ups=0.6, wpb=111.1, bsz=40, num_updates=9570, lr=4.7099e-05, gnorm=0.745, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=63499
2023-01-11 17:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:18 - progress_bar.py[line:274] - INFO: epoch 001:   9594 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2079, wps=98.7, ups=0.6, wpb=109.9, bsz=40, num_updates=9580, lr=4.70938e-05, gnorm=1.052, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=63516
2023-01-11 17:43:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:35 - progress_bar.py[line:274] - INFO: epoch 001:   9604 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2755, wps=99.1, ups=0.6, wpb=110.3, bsz=40, num_updates=9590, lr=4.70885e-05, gnorm=0.827, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=63533
2023-01-11 17:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:52 - progress_bar.py[line:274] - INFO: epoch 001:   9614 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.17, wps=102.2, ups=0.62, wpb=109.8, bsz=40, num_updates=9600, lr=4.70833e-05, gnorm=0.97, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=63550
2023-01-11 17:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:09 - progress_bar.py[line:274] - INFO: epoch 001:   9624 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2083, wps=99.3, ups=0.6, wpb=110.3, bsz=40, num_updates=9610, lr=4.70781e-05, gnorm=0.693, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=63567
2023-01-11 17:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:26 - progress_bar.py[line:274] - INFO: epoch 001:   9634 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.301, wps=102.9, ups=0.62, wpb=110.7, bsz=40, num_updates=9620, lr=4.70729e-05, gnorm=1.121, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63584
2023-01-11 17:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:43 - progress_bar.py[line:274] - INFO: epoch 001:   9644 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2872, wps=101.1, ups=0.61, wpb=110.3, bsz=40, num_updates=9630, lr=4.70677e-05, gnorm=0.535, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63602
2023-01-11 17:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:00 - progress_bar.py[line:274] - INFO: epoch 001:   9654 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2688, wps=103, ups=0.62, wpb=110.5, bsz=40, num_updates=9640, lr=4.70625e-05, gnorm=0.66, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63618
2023-01-11 17:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:17 - progress_bar.py[line:274] - INFO: epoch 001:   9664 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2903, wps=104.4, ups=0.63, wpb=111.3, bsz=40, num_updates=9650, lr=4.70573e-05, gnorm=0.622, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63635
2023-01-11 17:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:34 - progress_bar.py[line:274] - INFO: epoch 001:   9674 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.27, wps=100.8, ups=0.61, wpb=109.5, bsz=40, num_updates=9660, lr=4.70521e-05, gnorm=1.439, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63652
2023-01-11 17:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:51 - progress_bar.py[line:274] - INFO: epoch 001:   9684 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2062, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=9670, lr=4.70469e-05, gnorm=1.594, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63669
2023-01-11 17:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:45:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:08 - progress_bar.py[line:274] - INFO: epoch 001:   9694 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1837, wps=98.4, ups=0.6, wpb=109.3, bsz=40, num_updates=9680, lr=4.70417e-05, gnorm=0.86, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=63686
2023-01-11 17:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:25 - progress_bar.py[line:274] - INFO: epoch 001:   9704 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2212, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=9690, lr=4.70365e-05, gnorm=1.295, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63703
2023-01-11 17:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:42 - progress_bar.py[line:274] - INFO: epoch 001:   9714 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2524, wps=102.4, ups=0.62, wpb=109.7, bsz=40, num_updates=9700, lr=4.70313e-05, gnorm=0.657, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=63720
2023-01-11 17:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:46:59 - progress_bar.py[line:274] - INFO: epoch 001:   9724 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.22, wps=101.1, ups=0.61, wpb=109.9, bsz=40, num_updates=9710, lr=4.7026e-05, gnorm=1.139, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=63737
2023-01-11 17:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:16 - progress_bar.py[line:274] - INFO: epoch 001:   9734 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2626, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=9720, lr=4.70208e-05, gnorm=1.724, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63754
2023-01-11 17:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:33 - progress_bar.py[line:274] - INFO: epoch 001:   9744 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1963, wps=99.2, ups=0.61, wpb=108.7, bsz=40, num_updates=9730, lr=4.70156e-05, gnorm=0.523, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=63771
2023-01-11 17:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:51 - progress_bar.py[line:274] - INFO: epoch 001:   9754 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2793, wps=97.9, ups=0.6, wpb=108.7, bsz=40, num_updates=9740, lr=4.70104e-05, gnorm=0.81, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=63789
2023-01-11 17:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:47:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:08 - progress_bar.py[line:274] - INFO: epoch 001:   9764 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.211, wps=100.9, ups=0.61, wpb=109.7, bsz=40, num_updates=9750, lr=4.70052e-05, gnorm=1.444, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63806
2023-01-11 17:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:25 - progress_bar.py[line:274] - INFO: epoch 001:   9774 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2336, wps=98.4, ups=0.6, wpb=108.9, bsz=40, num_updates=9760, lr=4.7e-05, gnorm=0.799, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=63823
2023-01-11 17:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:42 - progress_bar.py[line:274] - INFO: epoch 001:   9784 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2981, wps=98, ups=0.59, wpb=109.8, bsz=40, num_updates=9770, lr=4.69948e-05, gnorm=0.648, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=63841
2023-01-11 17:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:48:59 - progress_bar.py[line:274] - INFO: epoch 001:   9794 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.268, wps=102.6, ups=0.61, wpb=111.3, bsz=40, num_updates=9780, lr=4.69896e-05, gnorm=0.543, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=63858
2023-01-11 17:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:16 - progress_bar.py[line:274] - INFO: epoch 001:   9804 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1863, wps=102.3, ups=0.61, wpb=111.1, bsz=40, num_updates=9790, lr=4.69844e-05, gnorm=0.754, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63875
2023-01-11 17:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:33 - progress_bar.py[line:274] - INFO: epoch 001:   9814 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1979, wps=103.2, ups=0.62, wpb=110.8, bsz=40, num_updates=9800, lr=4.69792e-05, gnorm=0.945, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63891
2023-01-11 17:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:50 - progress_bar.py[line:274] - INFO: epoch 001:   9824 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2642, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=9810, lr=4.6974e-05, gnorm=1.145, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=63909
2023-01-11 17:49:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:07 - progress_bar.py[line:274] - INFO: epoch 001:   9834 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.23, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=9820, lr=4.69687e-05, gnorm=0.582, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63925
2023-01-11 17:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:24 - progress_bar.py[line:274] - INFO: epoch 001:   9844 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3119, wps=100.6, ups=0.61, wpb=110.1, bsz=40, num_updates=9830, lr=4.69635e-05, gnorm=0.845, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63942
2023-01-11 17:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:41 - progress_bar.py[line:274] - INFO: epoch 001:   9854 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.202, wps=98.5, ups=0.6, wpb=110.2, bsz=40, num_updates=9840, lr=4.69583e-05, gnorm=0.649, clip=10, loss_scale=256, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=63960
2023-01-11 17:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:50:58 - progress_bar.py[line:274] - INFO: epoch 001:   9864 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2766, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=9850, lr=4.69531e-05, gnorm=0.584, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63977
2023-01-11 17:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:16 - progress_bar.py[line:274] - INFO: epoch 001:   9874 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2222, wps=98.2, ups=0.6, wpb=109.3, bsz=40, num_updates=9860, lr=4.69479e-05, gnorm=1.188, clip=10, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=63994
2023-01-11 17:51:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   9884 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2234, wps=103.8, ups=0.63, wpb=109.9, bsz=40, num_updates=9870, lr=4.69427e-05, gnorm=0.977, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=64011
2023-01-11 17:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:50 - progress_bar.py[line:274] - INFO: epoch 001:   9894 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2308, wps=100.2, ups=0.6, wpb=110.9, bsz=40, num_updates=9880, lr=4.69375e-05, gnorm=0.472, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=64028
2023-01-11 17:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:07 - progress_bar.py[line:274] - INFO: epoch 001:   9904 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2059, wps=101.2, ups=0.61, wpb=109.9, bsz=40, num_updates=9890, lr=4.69323e-05, gnorm=0.885, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=64045
2023-01-11 17:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:24 - progress_bar.py[line:274] - INFO: epoch 001:   9914 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2386, wps=102.2, ups=0.61, wpb=110.8, bsz=40, num_updates=9900, lr=4.69271e-05, gnorm=0.638, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=64062
2023-01-11 17:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:41 - progress_bar.py[line:274] - INFO: epoch 001:   9924 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2474, wps=99, ups=0.6, wpb=109.7, bsz=40, num_updates=9910, lr=4.69219e-05, gnorm=0.746, clip=40, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=64079
2023-01-11 17:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:52:58 - progress_bar.py[line:274] - INFO: epoch 001:   9934 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2655, wps=98.8, ups=0.61, wpb=108.3, bsz=40, num_updates=9920, lr=4.69167e-05, gnorm=0.808, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=64096
2023-01-11 17:52:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:15 - progress_bar.py[line:274] - INFO: epoch 001:   9944 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2039, wps=100.6, ups=0.62, wpb=108.9, bsz=40, num_updates=9930, lr=4.69115e-05, gnorm=1.323, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=64113
2023-01-11 17:53:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:32 - progress_bar.py[line:274] - INFO: epoch 001:   9954 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2596, wps=99.7, ups=0.6, wpb=111, bsz=40, num_updates=9940, lr=4.69062e-05, gnorm=0.59, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=64131
2023-01-11 17:53:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   9964 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2571, wps=97.7, ups=0.6, wpb=107.9, bsz=40, num_updates=9950, lr=4.6901e-05, gnorm=0.936, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=64148
2023-01-11 17:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:07 - progress_bar.py[line:274] - INFO: epoch 001:   9974 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2, wps=99.7, ups=0.61, wpb=109.1, bsz=40, num_updates=9960, lr=4.68958e-05, gnorm=0.564, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=64165
2023-01-11 17:54:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:24 - progress_bar.py[line:274] - INFO: epoch 001:   9984 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2621, wps=100.9, ups=0.61, wpb=109.4, bsz=40, num_updates=9970, lr=4.68906e-05, gnorm=1.305, clip=50, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=64182
2023-01-11 17:54:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:41 - progress_bar.py[line:274] - INFO: epoch 001:   9994 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2551, wps=99.8, ups=0.61, wpb=109, bsz=40, num_updates=9980, lr=4.68854e-05, gnorm=0.761, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=64199
2023-01-11 17:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:54:58 - progress_bar.py[line:274] - INFO: epoch 001:  10004 / 100000 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.2407, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=9990, lr=4.68802e-05, gnorm=0.901, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=64216
2023-01-11 17:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 17:55:16 - progress_bar.py[line:274] - INFO: epoch 001:  10014 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.2474, wps=98.9, ups=0.6, wpb=109.1, bsz=40, num_updates=10000, lr=4.6875e-05, gnorm=0.765, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=64234
2023-01-11 17:55:16 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 17:55:17 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 17:55:17 - train.py[line:551] - INFO: load:0.89 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 17:57:19 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 17:57:19 - train.py[line:551] - INFO: load:0.91 valid_run:122.18 task_valid:119.15 collect_output:1.96
2023-01-11 17:59:19 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 17:59:19 - train.py[line:551] - INFO: load:0.94 valid_run:242.15 task_valid:234.86 collect_output:5.13
2023-01-11 18:01:22 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 18:01:22 - train.py[line:551] - INFO: load:0.96 valid_run:364.51 task_valid:351.24 collect_output:10.05
2023-01-11 18:03:24 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 18:03:24 - train.py[line:551] - INFO: load:0.99 valid_run:487.03 task_valid:464.83 collect_output:17.90
2023-01-11 18:05:25 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 18:05:25 - train.py[line:551] - INFO: load:1.01 valid_run:607.31 task_valid:581.92 collect_output:20.04
2023-01-11 18:07:28 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 18:07:28 - train.py[line:551] - INFO: load:1.04 valid_run:730.25 task_valid:700.48 collect_output:23.37
2023-01-11 18:09:31 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 18:09:31 - train.py[line:551] - INFO: load:1.06 valid_run:853.39 task_valid:818.27 collect_output:27.65
2023-01-11 18:11:32 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 18:11:32 - train.py[line:551] - INFO: load:1.08 valid_run:974.83 task_valid:934.30 collect_output:32.03
2023-01-11 18:13:36 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 18:13:36 - train.py[line:551] - INFO: load:1.11 valid_run:1098.62 task_valid:1051.07 collect_output:38.02
2023-01-11 18:15:38 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 18:15:38 - train.py[line:551] - INFO: load:1.13 valid_run:1220.41 task_valid:1163.67 collect_output:46.08
2023-01-11 18:17:38 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 18:17:38 - train.py[line:551] - INFO: load:1.16 valid_run:1340.31 task_valid:1278.98 collect_output:49.63
2023-01-11 18:19:40 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 18:19:40 - train.py[line:551] - INFO: load:1.18 valid_run:1461.91 task_valid:1395.36 collect_output:53.78
2023-01-11 18:21:39 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 18:21:39 - train.py[line:551] - INFO: load:1.21 valid_run:1581.14 task_valid:1509.23 collect_output:58.07
2023-01-11 18:23:40 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 18:23:40 - train.py[line:551] - INFO: load:1.23 valid_run:1701.99 task_valid:1626.62 collect_output:60.46
2023-01-11 18:25:41 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 18:25:41 - train.py[line:551] - INFO: load:1.26 valid_run:1823.01 task_valid:1742.57 collect_output:64.45
2023-01-11 18:27:42 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 18:27:42 - train.py[line:551] - INFO: load:1.28 valid_run:1944.03 task_valid:1856.26 collect_output:70.69
2023-01-11 18:29:44 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 18:29:44 - train.py[line:551] - INFO: load:1.31 valid_run:2065.63 task_valid:1972.25 collect_output:75.21
2023-01-11 18:31:44 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 18:31:44 - train.py[line:551] - INFO: load:1.33 valid_run:2185.93 task_valid:2089.62 collect_output:77.09
2023-01-11 18:33:45 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 18:33:45 - train.py[line:551] - INFO: load:1.36 valid_run:2306.89 task_valid:2206.12 collect_output:80.54
2023-01-11 18:35:46 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 18:35:46 - train.py[line:551] - INFO: load:1.38 valid_run:2427.25 task_valid:2322.31 collect_output:83.67
2023-01-11 18:37:47 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 18:37:47 - train.py[line:551] - INFO: load:1.41 valid_run:2548.62 task_valid:2438.51 collect_output:87.75
2023-01-11 18:39:49 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 18:39:49 - train.py[line:551] - INFO: load:1.43 valid_run:2670.67 task_valid:2557.10 collect_output:90.16
2023-01-11 18:41:49 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 18:41:49 - train.py[line:551] - INFO: load:1.46 valid_run:2790.57 task_valid:2671.05 collect_output:95.06
2023-01-11 18:43:49 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 18:43:49 - train.py[line:551] - INFO: load:1.49 valid_run:2910.24 task_valid:2786.92 collect_output:97.84
2023-01-11 18:45:51 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 18:45:51 - train.py[line:551] - INFO: load:1.52 valid_run:3031.98 task_valid:2902.96 collect_output:102.41
2023-01-11 18:47:53 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 18:47:53 - train.py[line:551] - INFO: load:1.54 valid_run:3154.44 task_valid:3018.46 collect_output:108.33
2023-01-11 18:49:53 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 18:49:53 - train.py[line:551] - INFO: load:1.57 valid_run:3274.25 task_valid:3132.49 collect_output:112.97
2023-01-11 18:51:55 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 18:51:55 - train.py[line:551] - INFO: load:1.59 valid_run:3395.59 task_valid:3251.40 collect_output:114.34
2023-01-11 18:53:57 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 18:53:57 - train.py[line:551] - INFO: load:1.62 valid_run:3517.27 task_valid:3366.79 collect_output:119.56
2023-01-11 18:55:58 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 18:55:58 - train.py[line:551] - INFO: load:1.64 valid_run:3638.97 task_valid:3484.85 collect_output:122.15
2023-01-11 18:57:59 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 18:57:59 - train.py[line:551] - INFO: load:1.67 valid_run:3759.40 task_valid:3602.61 collect_output:123.78

====================================================================================================
SGG eval:     R @ 50: 0.6208;     R @ 100: 0.6660;     R @ 500: 0.7055;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3992;    mR @ 100: 0.4399;    mR @ 500: 0.4763;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7902) (covered in:0.8750) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9222) (says:0.0000) (sitting on:0.7851) (standing on:0.3193) (using:0.6500) (walking in:0.0000) (walking on:0.7297) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 18:58:30 - train.py[line:487] - INFO: 0.6660148459383752

====================================================================================================
SGG eval:     R @ 50: 0.6208;     R @ 100: 0.6660;     R @ 500: 0.7055;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3992;    mR @ 100: 0.4399;    mR @ 500: 0.4763;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7902) (covered in:0.8750) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9222) (says:0.0000) (sitting on:0.7851) (standing on:0.3193) (using:0.6500) (walking in:0.0000) (walking on:0.7297) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 18:58:30 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 18:58:30 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.28 | loss_v1 0 | loss_v2 0 | nll_loss 0.134 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.666015 | ppl 1.1 | vqa_score 0.5856 | wps 118.3 | wpb 72 | bsz 24 | num_updates 10000 | best_R@100 0.697584
2023-01-11 18:58:30 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-01-11 18:58:30 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-11 18:59:12 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-11 19:00:50 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.6660148459383752) (writing took 140.1982490401715 seconds)
2023-01-11 19:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:00:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:07 - progress_bar.py[line:274] - INFO: epoch 001:  10024 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2525, wps=0.4, ups=0, wpb=109.9, bsz=40, num_updates=10010, lr=4.68698e-05, gnorm=1.159, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68185
2023-01-11 19:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:23 - progress_bar.py[line:274] - INFO: epoch 001:  10034 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2255, wps=102.6, ups=0.62, wpb=110.2, bsz=40, num_updates=10020, lr=4.68646e-05, gnorm=1.07, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68202
2023-01-11 19:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:40 - progress_bar.py[line:274] - INFO: epoch 001:  10044 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2222, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=10030, lr=4.68594e-05, gnorm=1.163, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68219
2023-01-11 19:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:01:57 - progress_bar.py[line:274] - INFO: epoch 001:  10054 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1964, wps=97.4, ups=0.6, wpb=108.9, bsz=40, num_updates=10040, lr=4.68542e-05, gnorm=0.882, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68236
2023-01-11 19:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:14 - progress_bar.py[line:274] - INFO: epoch 001:  10064 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.23, wps=98.7, ups=0.6, wpb=109, bsz=40, num_updates=10050, lr=4.6849e-05, gnorm=0.928, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=68253
2023-01-11 19:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:31 - progress_bar.py[line:274] - INFO: epoch 001:  10074 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3077, wps=101.9, ups=0.62, wpb=110.4, bsz=40, num_updates=10060, lr=4.68438e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68269
2023-01-11 19:02:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:47 - progress_bar.py[line:274] - INFO: epoch 001:  10084 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2418, wps=101.4, ups=0.61, wpb=110, bsz=40, num_updates=10070, lr=4.68385e-05, gnorm=1.086, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68286
2023-01-11 19:02:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:04 - progress_bar.py[line:274] - INFO: epoch 001:  10094 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2647, wps=105, ups=0.63, wpb=111.1, bsz=40, num_updates=10080, lr=4.68333e-05, gnorm=0.752, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68302
2023-01-11 19:03:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:20 - progress_bar.py[line:274] - INFO: epoch 001:  10104 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3187, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=10090, lr=4.68281e-05, gnorm=0.671, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68319
2023-01-11 19:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:37 - progress_bar.py[line:274] - INFO: epoch 001:  10114 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3068, wps=101.8, ups=0.61, wpb=111, bsz=40, num_updates=10100, lr=4.68229e-05, gnorm=0.633, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68336
2023-01-11 19:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:54 - progress_bar.py[line:274] - INFO: epoch 001:  10124 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3404, wps=100.1, ups=0.61, wpb=109.6, bsz=40, num_updates=10110, lr=4.68177e-05, gnorm=0.881, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68352
2023-01-11 19:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:11 - progress_bar.py[line:274] - INFO: epoch 001:  10134 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2234, wps=99.5, ups=0.61, wpb=109.6, bsz=40, num_updates=10120, lr=4.68125e-05, gnorm=0.845, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68369
2023-01-11 19:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:28 - progress_bar.py[line:274] - INFO: epoch 001:  10144 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.213, wps=99.3, ups=0.6, wpb=109.5, bsz=40, num_updates=10130, lr=4.68073e-05, gnorm=1.807, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68386
2023-01-11 19:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:45 - progress_bar.py[line:274] - INFO: epoch 001:  10154 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2935, wps=98.6, ups=0.6, wpb=109.2, bsz=40, num_updates=10140, lr=4.68021e-05, gnorm=0.899, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=68403
2023-01-11 19:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:04:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:01 - progress_bar.py[line:274] - INFO: epoch 001:  10164 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2921, wps=100.3, ups=0.61, wpb=108.9, bsz=40, num_updates=10150, lr=4.67969e-05, gnorm=1.215, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68420
2023-01-11 19:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:17 - progress_bar.py[line:274] - INFO: epoch 001:  10174 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3208, wps=101.9, ups=0.63, wpb=108, bsz=40, num_updates=10160, lr=4.67917e-05, gnorm=1.418, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68436
2023-01-11 19:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:35 - progress_bar.py[line:274] - INFO: epoch 001:  10184 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3271, wps=98.7, ups=0.6, wpb=109.3, bsz=40, num_updates=10170, lr=4.67865e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=68453
2023-01-11 19:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:51 - progress_bar.py[line:274] - INFO: epoch 001:  10194 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3061, wps=101.2, ups=0.61, wpb=110.4, bsz=40, num_updates=10180, lr=4.67813e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68470
2023-01-11 19:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:08 - progress_bar.py[line:274] - INFO: epoch 001:  10204 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.25, wps=100.4, ups=0.61, wpb=109.4, bsz=40, num_updates=10190, lr=4.6776e-05, gnorm=0.657, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68487
2023-01-11 19:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:25 - progress_bar.py[line:274] - INFO: epoch 001:  10214 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2692, wps=101.5, ups=0.62, wpb=108.8, bsz=40, num_updates=10200, lr=4.67708e-05, gnorm=1.865, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68503
2023-01-11 19:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:41 - progress_bar.py[line:274] - INFO: epoch 001:  10224 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.25, wps=102.6, ups=0.61, wpb=111.9, bsz=40, num_updates=10210, lr=4.67656e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=68520
2023-01-11 19:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:06:58 - progress_bar.py[line:274] - INFO: epoch 001:  10234 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2549, wps=102.5, ups=0.62, wpb=110.1, bsz=40, num_updates=10220, lr=4.67604e-05, gnorm=1.344, clip=30, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=68536
2023-01-11 19:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:15 - progress_bar.py[line:274] - INFO: epoch 001:  10244 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2844, wps=100.2, ups=0.61, wpb=110, bsz=40, num_updates=10230, lr=4.67552e-05, gnorm=2.608, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68553
2023-01-11 19:07:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:31 - progress_bar.py[line:274] - INFO: epoch 001:  10254 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2857, wps=100.6, ups=0.61, wpb=109.2, bsz=40, num_updates=10240, lr=4.675e-05, gnorm=0.741, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68570
2023-01-11 19:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:48 - progress_bar.py[line:274] - INFO: epoch 001:  10264 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2788, wps=99.6, ups=0.6, wpb=110.3, bsz=40, num_updates=10250, lr=4.67448e-05, gnorm=0.684, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68587
2023-01-11 19:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:07:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:05 - progress_bar.py[line:274] - INFO: epoch 001:  10274 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2245, wps=102.5, ups=0.62, wpb=109.5, bsz=40, num_updates=10260, lr=4.67396e-05, gnorm=0.79, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=68603
2023-01-11 19:08:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:21 - progress_bar.py[line:274] - INFO: epoch 001:  10284 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3241, wps=101.4, ups=0.61, wpb=109.9, bsz=40, num_updates=10270, lr=4.67344e-05, gnorm=0.563, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68620
2023-01-11 19:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:38 - progress_bar.py[line:274] - INFO: epoch 001:  10294 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2816, wps=101.5, ups=0.62, wpb=109.3, bsz=40, num_updates=10280, lr=4.67292e-05, gnorm=2.202, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68636
2023-01-11 19:08:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:54 - progress_bar.py[line:274] - INFO: epoch 001:  10304 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2979, wps=100.3, ups=0.61, wpb=110.5, bsz=40, num_updates=10290, lr=4.6724e-05, gnorm=0.505, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68653
2023-01-11 19:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:08:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:10 - progress_bar.py[line:274] - INFO: epoch 001:  10314 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3137, wps=104.5, ups=0.64, wpb=108.6, bsz=40, num_updates=10300, lr=4.67188e-05, gnorm=1.536, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68669
2023-01-11 19:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:27 - progress_bar.py[line:274] - INFO: epoch 001:  10324 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1237, wps=100.3, ups=0.61, wpb=109.4, bsz=40, num_updates=10310, lr=4.67135e-05, gnorm=0.775, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68686
2023-01-11 19:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:44 - progress_bar.py[line:274] - INFO: epoch 001:  10334 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2857, wps=98, ups=0.6, wpb=109.8, bsz=40, num_updates=10320, lr=4.67083e-05, gnorm=1.318, clip=40, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=68703
2023-01-11 19:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:01 - progress_bar.py[line:274] - INFO: epoch 001:  10344 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=112.267, nsentences=40, sample_size=112.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1919, wps=102.6, ups=0.61, wpb=112.3, bsz=40, num_updates=10330, lr=4.67031e-05, gnorm=0.964, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68720
2023-01-11 19:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:18 - progress_bar.py[line:274] - INFO: epoch 001:  10354 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2286, wps=100.1, ups=0.61, wpb=110, bsz=40, num_updates=10340, lr=4.66979e-05, gnorm=0.608, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68737
2023-01-11 19:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:35 - progress_bar.py[line:274] - INFO: epoch 001:  10364 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2524, wps=98.1, ups=0.6, wpb=109.1, bsz=40, num_updates=10350, lr=4.66927e-05, gnorm=0.719, clip=30, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=68754
2023-01-11 19:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:52 - progress_bar.py[line:274] - INFO: epoch 001:  10374 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3529, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=10360, lr=4.66875e-05, gnorm=0.99, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68770
2023-01-11 19:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:08 - progress_bar.py[line:274] - INFO: epoch 001:  10384 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2111, wps=100, ups=0.6, wpb=110.5, bsz=40, num_updates=10370, lr=4.66823e-05, gnorm=0.591, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=68787
2023-01-11 19:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:25 - progress_bar.py[line:274] - INFO: epoch 001:  10394 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2233, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=10380, lr=4.66771e-05, gnorm=0.926, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68804
2023-01-11 19:11:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:41 - progress_bar.py[line:274] - INFO: epoch 001:  10404 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.2222, wps=102.7, ups=0.63, wpb=109.1, bsz=40, num_updates=10390, lr=4.66719e-05, gnorm=1.155, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68820
2023-01-11 19:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:11:58 - progress_bar.py[line:274] - INFO: epoch 001:  10414 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3011, wps=100.6, ups=0.6, wpb=111.2, bsz=40, num_updates=10400, lr=4.66667e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=68837
2023-01-11 19:12:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:15 - progress_bar.py[line:274] - INFO: epoch 001:  10424 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2895, wps=97.5, ups=0.6, wpb=108.1, bsz=40, num_updates=10410, lr=4.66615e-05, gnorm=1.097, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68854
2023-01-11 19:12:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:32 - progress_bar.py[line:274] - INFO: epoch 001:  10434 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2453, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=10420, lr=4.66562e-05, gnorm=0.692, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68870
2023-01-11 19:12:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:49 - progress_bar.py[line:274] - INFO: epoch 001:  10444 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2115, wps=99.5, ups=0.61, wpb=109.3, bsz=40, num_updates=10430, lr=4.6651e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68887
2023-01-11 19:12:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:05 - progress_bar.py[line:274] - INFO: epoch 001:  10454 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2636, wps=99.9, ups=0.61, wpb=108.7, bsz=40, num_updates=10440, lr=4.66458e-05, gnorm=0.831, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68904
2023-01-11 19:13:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:22 - progress_bar.py[line:274] - INFO: epoch 001:  10464 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2358, wps=99.2, ups=0.6, wpb=109.4, bsz=40, num_updates=10450, lr=4.66406e-05, gnorm=0.941, clip=30, loss_scale=512, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=68921
2023-01-11 19:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:39 - progress_bar.py[line:274] - INFO: epoch 001:  10474 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2797, wps=98.7, ups=0.61, wpb=107.8, bsz=40, num_updates=10460, lr=4.66354e-05, gnorm=0.842, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68938
2023-01-11 19:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:56 - progress_bar.py[line:274] - INFO: epoch 001:  10484 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3103, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=10470, lr=4.66302e-05, gnorm=0.595, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68954
2023-01-11 19:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:12 - progress_bar.py[line:274] - INFO: epoch 001:  10494 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.25, wps=99.7, ups=0.61, wpb=109, bsz=40, num_updates=10480, lr=4.6625e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68971
2023-01-11 19:14:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:29 - progress_bar.py[line:274] - INFO: epoch 001:  10504 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3168, wps=102, ups=0.62, wpb=110.3, bsz=40, num_updates=10490, lr=4.66198e-05, gnorm=1.262, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68988
2023-01-11 19:14:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:46 - progress_bar.py[line:274] - INFO: epoch 001:  10514 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2871, wps=100.8, ups=0.61, wpb=109.9, bsz=40, num_updates=10500, lr=4.66146e-05, gnorm=1.04, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69004
2023-01-11 19:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:51 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 19:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:04 - progress_bar.py[line:274] - INFO: epoch 001:  10525 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3021, wps=94.1, ups=0.57, wpb=109.9, bsz=40, num_updates=10510, lr=4.66094e-05, gnorm=0.911, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=69022
2023-01-11 19:15:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:20 - progress_bar.py[line:274] - INFO: epoch 001:  10535 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3125, wps=102.3, ups=0.62, wpb=109.8, bsz=40, num_updates=10520, lr=4.66042e-05, gnorm=0.961, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69039
2023-01-11 19:15:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:37 - progress_bar.py[line:274] - INFO: epoch 001:  10545 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.266, wps=100.9, ups=0.61, wpb=110.7, bsz=40, num_updates=10530, lr=4.6599e-05, gnorm=0.859, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69055
2023-01-11 19:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:53 - progress_bar.py[line:274] - INFO: epoch 001:  10555 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3163, wps=99.4, ups=0.61, wpb=109.1, bsz=40, num_updates=10540, lr=4.65938e-05, gnorm=0.555, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69072
2023-01-11 19:15:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:15:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:10 - progress_bar.py[line:274] - INFO: epoch 001:  10565 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3, wps=97.3, ups=0.6, wpb=108.7, bsz=40, num_updates=10550, lr=4.65885e-05, gnorm=1.104, clip=20, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=69089
2023-01-11 19:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:27 - progress_bar.py[line:274] - INFO: epoch 001:  10575 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2947, wps=99.5, ups=0.6, wpb=110.4, bsz=40, num_updates=10560, lr=4.65833e-05, gnorm=0.659, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=69106
2023-01-11 19:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:44 - progress_bar.py[line:274] - INFO: epoch 001:  10585 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1978, wps=100.1, ups=0.61, wpb=109.7, bsz=40, num_updates=10570, lr=4.65781e-05, gnorm=0.908, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69123
2023-01-11 19:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:01 - progress_bar.py[line:274] - INFO: epoch 001:  10595 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2613, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=10580, lr=4.65729e-05, gnorm=0.54, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=69140
2023-01-11 19:17:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:18 - progress_bar.py[line:274] - INFO: epoch 001:  10605 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2871, wps=98.9, ups=0.6, wpb=109.3, bsz=40, num_updates=10590, lr=4.65677e-05, gnorm=0.617, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=69156
2023-01-11 19:17:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:34 - progress_bar.py[line:274] - INFO: epoch 001:  10615 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3587, wps=102.6, ups=0.62, wpb=109.7, bsz=40, num_updates=10600, lr=4.65625e-05, gnorm=2.052, clip=60, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69173
2023-01-11 19:17:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:51 - progress_bar.py[line:274] - INFO: epoch 001:  10625 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2747, wps=102.1, ups=0.62, wpb=109.8, bsz=40, num_updates=10610, lr=4.65573e-05, gnorm=1.176, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=69189
2023-01-11 19:17:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:07 - progress_bar.py[line:274] - INFO: epoch 001:  10635 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3119, wps=102, ups=0.63, wpb=108.8, bsz=40, num_updates=10620, lr=4.65521e-05, gnorm=0.665, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69206
2023-01-11 19:18:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:23 - progress_bar.py[line:274] - INFO: epoch 001:  10645 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.24, wps=104.3, ups=0.64, wpb=109.3, bsz=40, num_updates=10630, lr=4.65469e-05, gnorm=0.845, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69222
2023-01-11 19:18:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:39 - progress_bar.py[line:274] - INFO: epoch 001:  10655 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2917, wps=102, ups=0.62, wpb=109.9, bsz=40, num_updates=10640, lr=4.65417e-05, gnorm=0.531, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69238
2023-01-11 19:18:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:56 - progress_bar.py[line:274] - INFO: epoch 001:  10665 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.27, wps=101.5, ups=0.61, wpb=110.9, bsz=40, num_updates=10650, lr=4.65365e-05, gnorm=0.677, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69255
2023-01-11 19:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:18:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:13 - progress_bar.py[line:274] - INFO: epoch 001:  10675 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3, wps=101.1, ups=0.62, wpb=109.1, bsz=40, num_updates=10660, lr=4.65313e-05, gnorm=0.834, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=69271
2023-01-11 19:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:29 - progress_bar.py[line:274] - INFO: epoch 001:  10685 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2364, wps=100.6, ups=0.61, wpb=109.4, bsz=40, num_updates=10670, lr=4.6526e-05, gnorm=0.937, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69288
2023-01-11 19:19:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:46 - progress_bar.py[line:274] - INFO: epoch 001:  10695 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2396, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=10680, lr=4.65208e-05, gnorm=1.069, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69304
2023-01-11 19:19:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:02 - progress_bar.py[line:274] - INFO: epoch 001:  10705 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2947, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=10690, lr=4.65156e-05, gnorm=1.045, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69321
2023-01-11 19:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:19 - progress_bar.py[line:274] - INFO: epoch 001:  10715 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3684, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=10700, lr=4.65104e-05, gnorm=0.512, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69338
2023-01-11 19:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:36 - progress_bar.py[line:274] - INFO: epoch 001:  10725 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2547, wps=99.4, ups=0.61, wpb=109.2, bsz=40, num_updates=10710, lr=4.65052e-05, gnorm=1.525, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=69355
2023-01-11 19:20:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:53 - progress_bar.py[line:274] - INFO: epoch 001:  10735 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2745, wps=98, ups=0.59, wpb=109.9, bsz=40, num_updates=10720, lr=4.65e-05, gnorm=0.643, clip=20, loss_scale=256, train_wall=17, gb_free=10.9, ema_decay=0.9999, wall=69372
2023-01-11 19:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:10 - progress_bar.py[line:274] - INFO: epoch 001:  10745 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1818, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=10730, lr=4.64948e-05, gnorm=0.691, clip=30, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=69388
2023-01-11 19:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:27 - progress_bar.py[line:274] - INFO: epoch 001:  10755 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1919, wps=101.7, ups=0.61, wpb=111.3, bsz=40, num_updates=10740, lr=4.64896e-05, gnorm=0.625, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69405
2023-01-11 19:21:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:43 - progress_bar.py[line:274] - INFO: epoch 001:  10765 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2883, wps=100.4, ups=0.61, wpb=109, bsz=40, num_updates=10750, lr=4.64844e-05, gnorm=0.838, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69422
2023-01-11 19:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:00 - progress_bar.py[line:274] - INFO: epoch 001:  10775 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3571, wps=101.5, ups=0.61, wpb=110.4, bsz=40, num_updates=10760, lr=4.64792e-05, gnorm=0.554, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69438
2023-01-11 19:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  10785 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3604, wps=97.5, ups=0.6, wpb=108.7, bsz=40, num_updates=10770, lr=4.6474e-05, gnorm=0.795, clip=50, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=69455
2023-01-11 19:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:33 - progress_bar.py[line:274] - INFO: epoch 001:  10795 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3232, wps=102.6, ups=0.61, wpb=111.6, bsz=40, num_updates=10780, lr=4.64688e-05, gnorm=0.846, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69472
2023-01-11 19:22:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:50 - progress_bar.py[line:274] - INFO: epoch 001:  10805 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2685, wps=98.7, ups=0.6, wpb=109.6, bsz=40, num_updates=10790, lr=4.64635e-05, gnorm=0.424, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=69489
2023-01-11 19:22:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:07 - progress_bar.py[line:274] - INFO: epoch 001:  10815 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.23, wps=99.4, ups=0.6, wpb=109.9, bsz=40, num_updates=10800, lr=4.64583e-05, gnorm=0.644, clip=10, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=69506
2023-01-11 19:23:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:24 - progress_bar.py[line:274] - INFO: epoch 001:  10825 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2667, wps=97.7, ups=0.6, wpb=108.4, bsz=40, num_updates=10810, lr=4.64531e-05, gnorm=1.454, clip=50, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=69523
2023-01-11 19:23:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:41 - progress_bar.py[line:274] - INFO: epoch 001:  10835 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3043, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=10820, lr=4.64479e-05, gnorm=0.586, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69540
2023-01-11 19:23:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:23:57 - progress_bar.py[line:274] - INFO: epoch 001:  10845 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=103, ups=0.62, wpb=110.9, bsz=40, num_updates=10830, lr=4.64427e-05, gnorm=0.562, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69556
2023-01-11 19:23:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:14 - progress_bar.py[line:274] - INFO: epoch 001:  10855 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2524, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=10840, lr=4.64375e-05, gnorm=0.463, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69572
2023-01-11 19:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:31 - progress_bar.py[line:274] - INFO: epoch 001:  10865 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3229, wps=99.3, ups=0.6, wpb=109.6, bsz=40, num_updates=10850, lr=4.64323e-05, gnorm=1.489, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=69589
2023-01-11 19:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:47 - progress_bar.py[line:274] - INFO: epoch 001:  10875 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2745, wps=101.9, ups=0.62, wpb=109.1, bsz=40, num_updates=10860, lr=4.64271e-05, gnorm=1.009, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69606
2023-01-11 19:24:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:05 - progress_bar.py[line:274] - INFO: epoch 001:  10885 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2247, wps=101.6, ups=0.61, wpb=110.7, bsz=40, num_updates=10870, lr=4.64219e-05, gnorm=0.574, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69623
2023-01-11 19:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:22 - progress_bar.py[line:274] - INFO: epoch 001:  10895 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3333, wps=100.7, ups=0.62, wpb=109.1, bsz=40, num_updates=10880, lr=4.64167e-05, gnorm=0.634, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=69640
2023-01-11 19:25:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:39 - progress_bar.py[line:274] - INFO: epoch 001:  10905 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1881, wps=98.8, ups=0.61, wpb=108.5, bsz=40, num_updates=10890, lr=4.64115e-05, gnorm=1.001, clip=40, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=69657
2023-01-11 19:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:56 - progress_bar.py[line:274] - INFO: epoch 001:  10915 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.28, wps=100.3, ups=0.61, wpb=109.7, bsz=40, num_updates=10900, lr=4.64062e-05, gnorm=0.577, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69674
2023-01-11 19:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:13 - progress_bar.py[line:274] - INFO: epoch 001:  10925 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2747, wps=104.1, ups=0.63, wpb=110.7, bsz=40, num_updates=10910, lr=4.6401e-05, gnorm=1.825, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69691
2023-01-11 19:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:30 - progress_bar.py[line:274] - INFO: epoch 001:  10935 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.297, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=10920, lr=4.63958e-05, gnorm=0.829, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69708
2023-01-11 19:26:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:47 - progress_bar.py[line:274] - INFO: epoch 001:  10945 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3271, wps=99.1, ups=0.6, wpb=109.2, bsz=40, num_updates=10930, lr=4.63906e-05, gnorm=1.553, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=69725
2023-01-11 19:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:04 - progress_bar.py[line:274] - INFO: epoch 001:  10955 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2718, wps=97.8, ups=0.6, wpb=108.1, bsz=40, num_updates=10940, lr=4.63854e-05, gnorm=0.749, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=69742
2023-01-11 19:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:21 - progress_bar.py[line:274] - INFO: epoch 001:  10965 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3043, wps=101.7, ups=0.61, wpb=111.6, bsz=40, num_updates=10950, lr=4.63802e-05, gnorm=0.821, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69760
2023-01-11 19:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:38 - progress_bar.py[line:274] - INFO: epoch 001:  10975 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3011, wps=102.2, ups=0.61, wpb=110.9, bsz=40, num_updates=10960, lr=4.6375e-05, gnorm=0.924, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69777
2023-01-11 19:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:55 - progress_bar.py[line:274] - INFO: epoch 001:  10985 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1771, wps=102.7, ups=0.63, wpb=109.1, bsz=40, num_updates=10970, lr=4.63698e-05, gnorm=1.115, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=69793
2023-01-11 19:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:27:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:12 - progress_bar.py[line:274] - INFO: epoch 001:  10995 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3238, wps=98, ups=0.6, wpb=108.5, bsz=40, num_updates=10980, lr=4.63646e-05, gnorm=1.21, clip=50, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=69811
2023-01-11 19:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:29 - progress_bar.py[line:274] - INFO: epoch 001:  11005 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3796, wps=105.8, ups=0.64, wpb=110.6, bsz=40, num_updates=10990, lr=4.63594e-05, gnorm=0.807, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=69827
2023-01-11 19:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 19:28:45 - progress_bar.py[line:274] - INFO: epoch 001:  11015 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2895, wps=105.2, ups=0.64, wpb=109.1, bsz=40, num_updates=11000, lr=4.63542e-05, gnorm=0.894, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=69843
2023-01-11 19:28:45 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 19:28:46 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 19:28:46 - train.py[line:551] - INFO: load:0.91 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 19:30:48 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 19:30:48 - train.py[line:551] - INFO: load:0.94 valid_run:122.06 task_valid:119.02 collect_output:1.87
2023-01-11 19:32:49 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 19:32:49 - train.py[line:551] - INFO: load:0.96 valid_run:242.12 task_valid:234.71 collect_output:5.13
2023-01-11 19:34:51 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 19:34:51 - train.py[line:551] - INFO: load:0.99 valid_run:364.03 task_valid:350.91 collect_output:9.77
2023-01-11 19:36:52 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 19:36:52 - train.py[line:551] - INFO: load:1.01 valid_run:485.79 task_valid:464.27 collect_output:17.11
2023-01-11 19:38:53 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 19:38:53 - train.py[line:551] - INFO: load:1.04 valid_run:606.49 task_valid:581.50 collect_output:19.47
2023-01-11 19:40:57 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 19:40:57 - train.py[line:551] - INFO: load:1.06 valid_run:729.82 task_valid:700.23 collect_output:22.96
2023-01-11 19:42:59 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 19:42:59 - train.py[line:551] - INFO: load:1.08 valid_run:852.44 task_valid:817.82 collect_output:26.98
2023-01-11 19:45:01 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 19:45:01 - train.py[line:551] - INFO: load:1.11 valid_run:974.41 task_valid:934.13 collect_output:31.57
2023-01-11 19:47:05 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 19:47:05 - train.py[line:551] - INFO: load:1.13 valid_run:1098.24 task_valid:1051.08 collect_output:37.35
2023-01-11 19:49:07 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 19:49:07 - train.py[line:551] - INFO: load:1.16 valid_run:1220.06 task_valid:1163.90 collect_output:45.23
2023-01-11 19:51:07 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 19:51:08 - train.py[line:551] - INFO: load:1.18 valid_run:1340.21 task_valid:1279.44 collect_output:48.77
2023-01-11 19:53:09 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 19:53:09 - train.py[line:551] - INFO: load:1.21 valid_run:1461.58 task_valid:1396.28 collect_output:52.20
2023-01-11 19:55:08 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 19:55:08 - train.py[line:551] - INFO: load:1.23 valid_run:1580.58 task_valid:1509.94 collect_output:56.39
2023-01-11 19:57:10 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 19:57:10 - train.py[line:551] - INFO: load:1.26 valid_run:1702.31 task_valid:1627.53 collect_output:59.43
2023-01-11 19:59:11 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 19:59:11 - train.py[line:551] - INFO: load:1.28 valid_run:1823.06 task_valid:1743.28 collect_output:63.38
2023-01-11 20:01:12 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 20:01:12 - train.py[line:551] - INFO: load:1.31 valid_run:1944.31 task_valid:1856.97 collect_output:69.88
2023-01-11 20:03:13 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 20:03:13 - train.py[line:551] - INFO: load:1.33 valid_run:2065.62 task_valid:1972.75 collect_output:74.35
2023-01-11 20:05:14 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 20:05:14 - train.py[line:551] - INFO: load:1.36 valid_run:2186.00 task_valid:2090.21 collect_output:76.20
2023-01-11 20:07:15 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 20:07:15 - train.py[line:551] - INFO: load:1.38 valid_run:2307.34 task_valid:2206.92 collect_output:79.76
2023-01-11 20:09:15 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 20:09:15 - train.py[line:551] - INFO: load:1.41 valid_run:2427.29 task_valid:2322.97 collect_output:82.63
2023-01-11 20:11:17 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 20:11:17 - train.py[line:551] - INFO: load:1.44 valid_run:2549.16 task_valid:2439.61 collect_output:86.76
2023-01-11 20:13:20 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 20:13:20 - train.py[line:551] - INFO: load:1.46 valid_run:2671.77 task_valid:2558.99 collect_output:88.80
2023-01-11 20:15:21 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 20:15:21 - train.py[line:551] - INFO: load:1.49 valid_run:2792.35 task_valid:2673.45 collect_output:93.82
2023-01-11 20:17:21 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 20:17:21 - train.py[line:551] - INFO: load:1.51 valid_run:2913.06 task_valid:2790.12 collect_output:96.76
2023-01-11 20:19:23 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 20:19:23 - train.py[line:551] - INFO: load:1.54 valid_run:3034.87 task_valid:2906.05 collect_output:101.51
2023-01-11 20:21:26 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 20:21:26 - train.py[line:551] - INFO: load:1.56 valid_run:3157.23 task_valid:3021.52 collect_output:107.39
2023-01-11 20:23:26 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 20:23:26 - train.py[line:551] - INFO: load:1.59 valid_run:3276.91 task_valid:3135.33 collect_output:112.17
2023-01-11 20:25:27 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 20:25:27 - train.py[line:551] - INFO: load:1.61 valid_run:3398.66 task_valid:3254.33 collect_output:113.84
2023-01-11 20:27:29 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 20:27:29 - train.py[line:551] - INFO: load:1.64 valid_run:3520.27 task_valid:3369.65 collect_output:119.02
2023-01-11 20:29:31 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 20:29:31 - train.py[line:551] - INFO: load:1.66 valid_run:3641.86 task_valid:3487.66 collect_output:121.56
2023-01-11 20:31:32 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 20:31:32 - train.py[line:551] - INFO: load:1.69 valid_run:3762.70 task_valid:3605.69 collect_output:123.30

====================================================================================================
SGG eval:     R @ 50: 0.5985;     R @ 100: 0.6570;     R @ 500: 0.6985;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3855;    mR @ 100: 0.4342;    mR @ 500: 0.4719;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7902) (covered in:0.8750) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9075) (says:0.0000) (sitting on:0.7727) (standing on:0.3143) (using:0.6500) (walking in:0.0000) (walking on:0.6757) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5985;     R @ 100: 0.6570;     R @ 500: 0.6985;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3855;    mR @ 100: 0.4342;    mR @ 500: 0.4719;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7902) (covered in:0.8750) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9075) (says:0.0000) (sitting on:0.7727) (standing on:0.3143) (using:0.6500) (walking in:0.0000) (walking on:0.6757) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 20:32:03 - train.py[line:487] - INFO: 0.6569815126050421
2023-01-11 20:32:03 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 20:32:03 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.322 | loss_v1 0 | loss_v2 0 | nll_loss 0.164 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.656982 | ppl 1.12 | vqa_score 0.5766 | wps 118.2 | wpb 72 | bsz 24 | num_updates 11000 | best_R@100 0.697584
2023-01-11 20:32:03 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 11000 updates
2023-01-11 20:32:03 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_11000.pt
2023-01-11 20:32:49 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_11000.pt
2023-01-11 20:34:25 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_11000.pt (epoch 1 @ 11000 updates, score 0.6569815126050421) (writing took 141.0427796561271 seconds)
2023-01-11 20:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:41 - progress_bar.py[line:274] - INFO: epoch 001:  11025 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3125, wps=0.4, ups=0, wpb=108.5, bsz=40, num_updates=11010, lr=4.6349e-05, gnorm=1.077, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73800
2023-01-11 20:34:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:34:58 - progress_bar.py[line:274] - INFO: epoch 001:  11035 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2887, wps=100.6, ups=0.6, wpb=111.2, bsz=40, num_updates=11020, lr=4.63438e-05, gnorm=1.099, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=73817
2023-01-11 20:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:15 - progress_bar.py[line:274] - INFO: epoch 001:  11045 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2653, wps=103.3, ups=0.62, wpb=111.2, bsz=40, num_updates=11030, lr=4.63385e-05, gnorm=0.473, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=73833
2023-01-11 20:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:32 - progress_bar.py[line:274] - INFO: epoch 001:  11055 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2581, wps=102.1, ups=0.61, wpb=111.3, bsz=40, num_updates=11040, lr=4.63333e-05, gnorm=0.603, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=73850
2023-01-11 20:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:49 - progress_bar.py[line:274] - INFO: epoch 001:  11065 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.287, wps=98, ups=0.59, wpb=110.2, bsz=40, num_updates=11050, lr=4.63281e-05, gnorm=0.872, clip=20, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=73867
2023-01-11 20:35:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:35:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:05 - progress_bar.py[line:274] - INFO: epoch 001:  11075 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2371, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=11060, lr=4.63229e-05, gnorm=0.394, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73884
2023-01-11 20:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:22 - progress_bar.py[line:274] - INFO: epoch 001:  11085 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2473, wps=99.5, ups=0.6, wpb=110.2, bsz=40, num_updates=11070, lr=4.63177e-05, gnorm=0.729, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=73901
2023-01-11 20:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:39 - progress_bar.py[line:274] - INFO: epoch 001:  11095 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2947, wps=98, ups=0.6, wpb=109.3, bsz=40, num_updates=11080, lr=4.63125e-05, gnorm=2.955, clip=80, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=73918
2023-01-11 20:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:56 - progress_bar.py[line:274] - INFO: epoch 001:  11105 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3604, wps=98.9, ups=0.61, wpb=108.7, bsz=40, num_updates=11090, lr=4.63073e-05, gnorm=0.995, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73935
2023-01-11 20:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:12 - progress_bar.py[line:274] - INFO: epoch 001:  11115 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.233, wps=104.9, ups=0.63, wpb=110.7, bsz=40, num_updates=11100, lr=4.63021e-05, gnorm=0.989, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73951
2023-01-11 20:37:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:29 - progress_bar.py[line:274] - INFO: epoch 001:  11125 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2692, wps=100.2, ups=0.6, wpb=110.5, bsz=40, num_updates=11110, lr=4.62969e-05, gnorm=0.592, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=73968
2023-01-11 20:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:46 - progress_bar.py[line:274] - INFO: epoch 001:  11135 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2788, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=11120, lr=4.62917e-05, gnorm=1.213, clip=50, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=73984
2023-01-11 20:37:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:37:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:02 - progress_bar.py[line:274] - INFO: epoch 001:  11145 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3426, wps=101.1, ups=0.62, wpb=108.9, bsz=40, num_updates=11130, lr=4.62865e-05, gnorm=0.729, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74001
2023-01-11 20:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:18 - progress_bar.py[line:274] - INFO: epoch 001:  11155 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.375, wps=103.6, ups=0.62, wpb=111.4, bsz=40, num_updates=11140, lr=4.62813e-05, gnorm=0.831, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74017
2023-01-11 20:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:35 - progress_bar.py[line:274] - INFO: epoch 001:  11165 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3028, wps=98.3, ups=0.6, wpb=109.4, bsz=40, num_updates=11150, lr=4.6276e-05, gnorm=1.264, clip=50, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=74034
2023-01-11 20:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:52 - progress_bar.py[line:274] - INFO: epoch 001:  11175 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3429, wps=101.1, ups=0.61, wpb=111.3, bsz=40, num_updates=11160, lr=4.62708e-05, gnorm=0.552, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74051
2023-01-11 20:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:09 - progress_bar.py[line:274] - INFO: epoch 001:  11185 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2887, wps=98.8, ups=0.6, wpb=110.3, bsz=40, num_updates=11170, lr=4.62656e-05, gnorm=0.588, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=74068
2023-01-11 20:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:27 - progress_bar.py[line:274] - INFO: epoch 001:  11195 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2692, wps=98.1, ups=0.6, wpb=109.9, bsz=40, num_updates=11180, lr=4.62604e-05, gnorm=1.549, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=74085
2023-01-11 20:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:43 - progress_bar.py[line:274] - INFO: epoch 001:  11205 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2667, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=11190, lr=4.62552e-05, gnorm=1.296, clip=50, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=74102
2023-01-11 20:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:00 - progress_bar.py[line:274] - INFO: epoch 001:  11215 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2292, wps=101.2, ups=0.61, wpb=110.9, bsz=40, num_updates=11200, lr=4.625e-05, gnorm=0.813, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74119
2023-01-11 20:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:17 - progress_bar.py[line:274] - INFO: epoch 001:  11225 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2947, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=11210, lr=4.62448e-05, gnorm=0.563, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74136
2023-01-11 20:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:34 - progress_bar.py[line:274] - INFO: epoch 001:  11235 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.25, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=11220, lr=4.62396e-05, gnorm=1.076, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74152
2023-01-11 20:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:51 - progress_bar.py[line:274] - INFO: epoch 001:  11245 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2432, wps=98.9, ups=0.61, wpb=108.9, bsz=40, num_updates=11230, lr=4.62344e-05, gnorm=1.531, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74169
2023-01-11 20:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:07 - progress_bar.py[line:274] - INFO: epoch 001:  11255 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2088, wps=101.5, ups=0.61, wpb=110.4, bsz=40, num_updates=11240, lr=4.62292e-05, gnorm=0.852, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=74186
2023-01-11 20:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:24 - progress_bar.py[line:274] - INFO: epoch 001:  11265 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2294, wps=97.5, ups=0.6, wpb=108.3, bsz=40, num_updates=11250, lr=4.6224e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=74203
2023-01-11 20:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:41 - progress_bar.py[line:274] - INFO: epoch 001:  11275 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2796, wps=99.4, ups=0.6, wpb=110.7, bsz=40, num_updates=11260, lr=4.62188e-05, gnorm=0.922, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=74220
2023-01-11 20:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:41:58 - progress_bar.py[line:274] - INFO: epoch 001:  11285 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2973, wps=102.1, ups=0.62, wpb=109.2, bsz=40, num_updates=11270, lr=4.62135e-05, gnorm=0.907, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74236
2023-01-11 20:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:14 - progress_bar.py[line:274] - INFO: epoch 001:  11295 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2897, wps=98.6, ups=0.61, wpb=108.3, bsz=40, num_updates=11280, lr=4.62083e-05, gnorm=0.897, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74253
2023-01-11 20:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:31 - progress_bar.py[line:274] - INFO: epoch 001:  11305 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2245, wps=99.8, ups=0.61, wpb=109.3, bsz=40, num_updates=11290, lr=4.62031e-05, gnorm=0.593, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74270
2023-01-11 20:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:47 - progress_bar.py[line:274] - INFO: epoch 001:  11315 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2299, wps=104.1, ups=0.63, wpb=110.9, bsz=40, num_updates=11300, lr=4.61979e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74286
2023-01-11 20:42:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:03 - progress_bar.py[line:274] - INFO: epoch 001:  11325 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3861, wps=106, ups=0.64, wpb=110.4, bsz=40, num_updates=11310, lr=4.61927e-05, gnorm=0.652, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74302
2023-01-11 20:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:20 - progress_bar.py[line:274] - INFO: epoch 001:  11335 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3069, wps=100.4, ups=0.61, wpb=109, bsz=40, num_updates=11320, lr=4.61875e-05, gnorm=1.58, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74319
2023-01-11 20:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:37 - progress_bar.py[line:274] - INFO: epoch 001:  11345 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.398, wps=98.3, ups=0.59, wpb=110.3, bsz=40, num_updates=11330, lr=4.61823e-05, gnorm=0.624, clip=20, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=74336
2023-01-11 20:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:54 - progress_bar.py[line:274] - INFO: epoch 001:  11355 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2717, wps=104.7, ups=0.63, wpb=111.3, bsz=40, num_updates=11340, lr=4.61771e-05, gnorm=0.652, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74352
2023-01-11 20:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:10 - progress_bar.py[line:274] - INFO: epoch 001:  11365 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3372, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=11350, lr=4.61719e-05, gnorm=0.692, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74369
2023-01-11 20:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:14 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 20:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:29 - progress_bar.py[line:274] - INFO: epoch 001:  11376 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.188, nsentences=40, sample_size=109.188, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3058, wps=94.8, ups=0.54, wpb=109.2, bsz=40, num_updates=11360, lr=4.61667e-05, gnorm=0.734, clip=20, loss_scale=256, train_wall=18, gb_free=10.1, ema_decay=0.9999, wall=74388
2023-01-11 20:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:46 - progress_bar.py[line:274] - INFO: epoch 001:  11386 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2414, wps=101.1, ups=0.61, wpb=110.1, bsz=40, num_updates=11370, lr=4.61615e-05, gnorm=0.571, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74404
2023-01-11 20:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:44:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:03 - progress_bar.py[line:274] - INFO: epoch 001:  11396 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2947, wps=98.5, ups=0.6, wpb=109.7, bsz=40, num_updates=11380, lr=4.61563e-05, gnorm=1.059, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=74421
2023-01-11 20:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:20 - progress_bar.py[line:274] - INFO: epoch 001:  11406 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3061, wps=98.4, ups=0.6, wpb=108.5, bsz=40, num_updates=11390, lr=4.6151e-05, gnorm=0.644, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74438
2023-01-11 20:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:37 - progress_bar.py[line:274] - INFO: epoch 001:  11416 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3, wps=97.4, ups=0.59, wpb=109.2, bsz=40, num_updates=11400, lr=4.61458e-05, gnorm=0.479, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=74455
2023-01-11 20:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:53 - progress_bar.py[line:274] - INFO: epoch 001:  11426 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3608, wps=103, ups=0.61, wpb=111.7, bsz=40, num_updates=11410, lr=4.61406e-05, gnorm=0.709, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74472
2023-01-11 20:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:10 - progress_bar.py[line:274] - INFO: epoch 001:  11436 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3226, wps=98.9, ups=0.61, wpb=108.7, bsz=40, num_updates=11420, lr=4.61354e-05, gnorm=0.954, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74489
2023-01-11 20:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:27 - progress_bar.py[line:274] - INFO: epoch 001:  11446 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3519, wps=103.7, ups=0.62, wpb=111, bsz=40, num_updates=11430, lr=4.61302e-05, gnorm=0.829, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74505
2023-01-11 20:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:43 - progress_bar.py[line:274] - INFO: epoch 001:  11456 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2824, wps=102.5, ups=0.62, wpb=110.9, bsz=40, num_updates=11440, lr=4.6125e-05, gnorm=0.845, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74522
2023-01-11 20:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:00 - progress_bar.py[line:274] - INFO: epoch 001:  11466 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3191, wps=101, ups=0.61, wpb=110.9, bsz=40, num_updates=11450, lr=4.61198e-05, gnorm=0.704, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74538
2023-01-11 20:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:16 - progress_bar.py[line:274] - INFO: epoch 001:  11476 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2277, wps=102.6, ups=0.62, wpb=110.5, bsz=40, num_updates=11460, lr=4.61146e-05, gnorm=0.648, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74555
2023-01-11 20:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:33 - progress_bar.py[line:274] - INFO: epoch 001:  11486 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.268, wps=98.8, ups=0.6, wpb=110.5, bsz=40, num_updates=11470, lr=4.61094e-05, gnorm=0.813, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=74572
2023-01-11 20:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:50 - progress_bar.py[line:274] - INFO: epoch 001:  11496 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2524, wps=100, ups=0.62, wpb=108.3, bsz=40, num_updates=11480, lr=4.61042e-05, gnorm=0.921, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74589
2023-01-11 20:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:06 - progress_bar.py[line:274] - INFO: epoch 001:  11506 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2569, wps=100.6, ups=0.62, wpb=108.6, bsz=40, num_updates=11490, lr=4.6099e-05, gnorm=0.805, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74605
2023-01-11 20:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:23 - progress_bar.py[line:274] - INFO: epoch 001:  11516 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.268, wps=100.8, ups=0.6, wpb=111.1, bsz=40, num_updates=11500, lr=4.60937e-05, gnorm=0.539, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74622
2023-01-11 20:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:40 - progress_bar.py[line:274] - INFO: epoch 001:  11526 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3646, wps=100, ups=0.61, wpb=109.7, bsz=40, num_updates=11510, lr=4.60885e-05, gnorm=0.572, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=74639
2023-01-11 20:48:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:56 - progress_bar.py[line:274] - INFO: epoch 001:  11536 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3333, wps=101.5, ups=0.62, wpb=109.1, bsz=40, num_updates=11520, lr=4.60833e-05, gnorm=0.739, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74655
2023-01-11 20:48:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:13 - progress_bar.py[line:274] - INFO: epoch 001:  11546 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2526, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=11530, lr=4.60781e-05, gnorm=0.429, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74672
2023-01-11 20:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:30 - progress_bar.py[line:274] - INFO: epoch 001:  11556 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3304, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=11540, lr=4.60729e-05, gnorm=1.291, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74688
2023-01-11 20:49:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:47 - progress_bar.py[line:274] - INFO: epoch 001:  11566 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3444, wps=101.1, ups=0.61, wpb=111.2, bsz=40, num_updates=11550, lr=4.60677e-05, gnorm=0.697, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74705
2023-01-11 20:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:49:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:03 - progress_bar.py[line:274] - INFO: epoch 001:  11576 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2936, wps=101, ups=0.62, wpb=109.2, bsz=40, num_updates=11560, lr=4.60625e-05, gnorm=0.998, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74722
2023-01-11 20:50:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:20 - progress_bar.py[line:274] - INFO: epoch 001:  11586 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2115, wps=98.9, ups=0.61, wpb=108.7, bsz=40, num_updates=11570, lr=4.60573e-05, gnorm=0.592, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74739
2023-01-11 20:50:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:37 - progress_bar.py[line:274] - INFO: epoch 001:  11596 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3125, wps=100.1, ups=0.61, wpb=110.2, bsz=40, num_updates=11580, lr=4.60521e-05, gnorm=0.824, clip=40, loss_scale=256, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=74755
2023-01-11 20:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:53 - progress_bar.py[line:274] - INFO: epoch 001:  11606 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3579, wps=102.7, ups=0.62, wpb=110.9, bsz=40, num_updates=11590, lr=4.60469e-05, gnorm=0.526, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74772
2023-01-11 20:50:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:10 - progress_bar.py[line:274] - INFO: epoch 001:  11616 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2857, wps=101.8, ups=0.62, wpb=109.1, bsz=40, num_updates=11600, lr=4.60417e-05, gnorm=1.031, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74788
2023-01-11 20:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:27 - progress_bar.py[line:274] - INFO: epoch 001:  11626 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2174, wps=101, ups=0.61, wpb=110.3, bsz=40, num_updates=11610, lr=4.60365e-05, gnorm=0.879, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74806
2023-01-11 20:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:44 - progress_bar.py[line:274] - INFO: epoch 001:  11636 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3404, wps=98.2, ups=0.59, wpb=110.1, bsz=40, num_updates=11620, lr=4.60313e-05, gnorm=0.967, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=74823
2023-01-11 20:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:01 - progress_bar.py[line:274] - INFO: epoch 001:  11646 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3107, wps=100.8, ups=0.62, wpb=109.1, bsz=40, num_updates=11630, lr=4.6026e-05, gnorm=0.603, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74839
2023-01-11 20:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:18 - progress_bar.py[line:274] - INFO: epoch 001:  11656 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2947, wps=101, ups=0.61, wpb=110.1, bsz=40, num_updates=11640, lr=4.60208e-05, gnorm=1.355, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74856
2023-01-11 20:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:34 - progress_bar.py[line:274] - INFO: epoch 001:  11666 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3619, wps=99, ups=0.61, wpb=108.3, bsz=40, num_updates=11650, lr=4.60156e-05, gnorm=0.727, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74873
2023-01-11 20:52:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:51 - progress_bar.py[line:274] - INFO: epoch 001:  11676 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3838, wps=99.3, ups=0.61, wpb=108.7, bsz=40, num_updates=11660, lr=4.60104e-05, gnorm=0.551, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=74890
2023-01-11 20:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:08 - progress_bar.py[line:274] - INFO: epoch 001:  11686 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2062, wps=99.9, ups=0.6, wpb=110.3, bsz=40, num_updates=11670, lr=4.60052e-05, gnorm=0.369, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=74907
2023-01-11 20:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:24 - progress_bar.py[line:274] - INFO: epoch 001:  11696 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3438, wps=103.2, ups=0.62, wpb=110.7, bsz=40, num_updates=11680, lr=4.6e-05, gnorm=1.182, clip=30, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=74923
2023-01-11 20:53:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:41 - progress_bar.py[line:274] - INFO: epoch 001:  11706 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3148, wps=99.1, ups=0.61, wpb=108.5, bsz=40, num_updates=11690, lr=4.59948e-05, gnorm=0.812, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74940
2023-01-11 20:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:53:58 - progress_bar.py[line:274] - INFO: epoch 001:  11716 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2442, wps=101.9, ups=0.61, wpb=110.5, bsz=40, num_updates=11700, lr=4.59896e-05, gnorm=0.633, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74956
2023-01-11 20:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:14 - progress_bar.py[line:274] - INFO: epoch 001:  11726 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.29, wps=101.2, ups=0.62, wpb=109, bsz=40, num_updates=11710, lr=4.59844e-05, gnorm=1.078, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74973
2023-01-11 20:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:31 - progress_bar.py[line:274] - INFO: epoch 001:  11736 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2963, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=11720, lr=4.59792e-05, gnorm=0.847, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74990
2023-01-11 20:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:48 - progress_bar.py[line:274] - INFO: epoch 001:  11746 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2788, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=11730, lr=4.5974e-05, gnorm=0.81, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=75006
2023-01-11 20:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:54:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:04 - progress_bar.py[line:274] - INFO: epoch 001:  11756 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3019, wps=101.6, ups=0.62, wpb=109.1, bsz=40, num_updates=11740, lr=4.59688e-05, gnorm=1.184, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=75023
2023-01-11 20:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:21 - progress_bar.py[line:274] - INFO: epoch 001:  11766 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3048, wps=101.2, ups=0.62, wpb=108.6, bsz=40, num_updates=11750, lr=4.59635e-05, gnorm=0.621, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=75039
2023-01-11 20:55:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:38 - progress_bar.py[line:274] - INFO: epoch 001:  11776 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2809, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=11760, lr=4.59583e-05, gnorm=0.631, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=75056
2023-01-11 20:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:54 - progress_bar.py[line:274] - INFO: epoch 001:  11786 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2842, wps=100.1, ups=0.61, wpb=110.3, bsz=40, num_updates=11770, lr=4.59531e-05, gnorm=0.661, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=75073
2023-01-11 20:55:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:12 - progress_bar.py[line:274] - INFO: epoch 001:  11796 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3372, wps=97.9, ups=0.59, wpb=110.1, bsz=40, num_updates=11780, lr=4.59479e-05, gnorm=0.684, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=75090
2023-01-11 20:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:28 - progress_bar.py[line:274] - INFO: epoch 001:  11806 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3368, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=11790, lr=4.59427e-05, gnorm=2.095, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=75107
2023-01-11 20:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:45 - progress_bar.py[line:274] - INFO: epoch 001:  11816 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3708, wps=97.9, ups=0.6, wpb=109.5, bsz=40, num_updates=11800, lr=4.59375e-05, gnorm=1.144, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=75124
2023-01-11 20:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:03 - progress_bar.py[line:274] - INFO: epoch 001:  11826 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3645, wps=101.6, ups=0.61, wpb=110.9, bsz=40, num_updates=11810, lr=4.59323e-05, gnorm=0.483, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=75140
2023-01-11 20:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:20 - progress_bar.py[line:274] - INFO: epoch 001:  11836 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3173, wps=97.7, ups=0.6, wpb=108.4, bsz=40, num_updates=11820, lr=4.59271e-05, gnorm=0.911, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=75158
2023-01-11 20:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:36 - progress_bar.py[line:274] - INFO: epoch 001:  11846 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2766, wps=101.3, ups=0.61, wpb=110, bsz=40, num_updates=11830, lr=4.59219e-05, gnorm=0.572, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=75175
2023-01-11 20:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:53 - progress_bar.py[line:274] - INFO: epoch 001:  11856 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3483, wps=99.8, ups=0.6, wpb=111.5, bsz=40, num_updates=11840, lr=4.59167e-05, gnorm=0.755, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=75192
2023-01-11 20:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:57:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:10 - progress_bar.py[line:274] - INFO: epoch 001:  11866 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3868, wps=100.2, ups=0.61, wpb=109.4, bsz=40, num_updates=11850, lr=4.59115e-05, gnorm=0.859, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=75209
2023-01-11 20:58:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:27 - progress_bar.py[line:274] - INFO: epoch 001:  11876 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4021, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=11860, lr=4.59063e-05, gnorm=0.93, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=75225
2023-01-11 20:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:43 - progress_bar.py[line:274] - INFO: epoch 001:  11886 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2653, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=11870, lr=4.5901e-05, gnorm=0.726, clip=20, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=75242
2023-01-11 20:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:00 - progress_bar.py[line:274] - INFO: epoch 001:  11896 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2857, wps=100.3, ups=0.62, wpb=108.4, bsz=40, num_updates=11880, lr=4.58958e-05, gnorm=1.635, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=75258
2023-01-11 20:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:17 - progress_bar.py[line:274] - INFO: epoch 001:  11906 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3398, wps=97.9, ups=0.6, wpb=108.3, bsz=40, num_updates=11890, lr=4.58906e-05, gnorm=0.729, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=75276
2023-01-11 20:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:34 - progress_bar.py[line:274] - INFO: epoch 001:  11916 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3516, wps=101.4, ups=0.62, wpb=109.9, bsz=40, num_updates=11900, lr=4.58854e-05, gnorm=0.474, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=75292
2023-01-11 20:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:51 - progress_bar.py[line:274] - INFO: epoch 001:  11926 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3214, wps=102.1, ups=0.62, wpb=109, bsz=40, num_updates=11910, lr=4.58802e-05, gnorm=2.443, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=75309
2023-01-11 20:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 20:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:08 - progress_bar.py[line:274] - INFO: epoch 001:  11936 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3118, wps=96.8, ups=0.59, wpb=110.1, bsz=40, num_updates=11920, lr=4.5875e-05, gnorm=0.78, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=75327
2023-01-11 21:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:25 - progress_bar.py[line:274] - INFO: epoch 001:  11946 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.3137, wps=98.3, ups=0.6, wpb=109.5, bsz=40, num_updates=11930, lr=4.58698e-05, gnorm=1.134, clip=50, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=75343
2023-01-11 21:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:42 - progress_bar.py[line:274] - INFO: epoch 001:  11956 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3646, wps=101.9, ups=0.61, wpb=110.5, bsz=40, num_updates=11940, lr=4.58646e-05, gnorm=0.731, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=75360
2023-01-11 21:00:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:00:59 - progress_bar.py[line:274] - INFO: epoch 001:  11966 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.375, wps=103.2, ups=0.62, wpb=111.5, bsz=40, num_updates=11950, lr=4.58594e-05, gnorm=0.583, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=75377
2023-01-11 21:00:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:15 - progress_bar.py[line:274] - INFO: epoch 001:  11976 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2842, wps=102.1, ups=0.61, wpb=111.2, bsz=40, num_updates=11960, lr=4.58542e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=75394
2023-01-11 21:01:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:32 - progress_bar.py[line:274] - INFO: epoch 001:  11986 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3535, wps=99.9, ups=0.6, wpb=111, bsz=40, num_updates=11970, lr=4.5849e-05, gnorm=0.508, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=75411
2023-01-11 21:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:49 - progress_bar.py[line:274] - INFO: epoch 001:  11996 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2897, wps=101.9, ups=0.62, wpb=108.9, bsz=40, num_updates=11980, lr=4.58437e-05, gnorm=1.345, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=75427
2023-01-11 21:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:02:05 - progress_bar.py[line:274] - INFO: epoch 001:  12006 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3608, wps=99.1, ups=0.6, wpb=109.5, bsz=40, num_updates=11990, lr=4.58385e-05, gnorm=0.73, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=75444
2023-01-11 21:02:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 21:02:22 - progress_bar.py[line:274] - INFO: epoch 001:  12016 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3667, wps=102.7, ups=0.62, wpb=110.6, bsz=40, num_updates=12000, lr=4.58333e-05, gnorm=0.534, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=75461
2023-01-11 21:02:22 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 21:02:24 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 21:02:24 - train.py[line:551] - INFO: load:1.30 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 21:04:25 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 21:04:25 - train.py[line:551] - INFO: load:1.32 valid_run:121.62 task_valid:118.74 collect_output:1.82
2023-01-11 21:06:25 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 21:06:25 - train.py[line:551] - INFO: load:1.34 valid_run:241.55 task_valid:234.12 collect_output:5.34
2023-01-11 21:08:27 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 21:08:27 - train.py[line:551] - INFO: load:1.37 valid_run:363.55 task_valid:350.28 collect_output:10.14
2023-01-11 21:10:30 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 21:10:30 - train.py[line:551] - INFO: load:1.39 valid_run:485.64 task_valid:463.74 collect_output:17.71
2023-01-11 21:12:30 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 21:12:30 - train.py[line:551] - INFO: load:1.42 valid_run:606.12 task_valid:580.88 collect_output:20.00
2023-01-11 21:14:33 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 21:14:33 - train.py[line:551] - INFO: load:1.44 valid_run:728.95 task_valid:699.36 collect_output:23.30
2023-01-11 21:16:36 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 21:16:36 - train.py[line:551] - INFO: load:1.47 valid_run:851.97 task_valid:816.96 collect_output:27.70
2023-01-11 21:18:38 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 21:18:38 - train.py[line:551] - INFO: load:1.49 valid_run:973.76 task_valid:933.16 collect_output:32.25
2023-01-11 21:20:42 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 21:20:42 - train.py[line:551] - INFO: load:1.52 valid_run:1097.51 task_valid:1049.94 collect_output:38.16
2023-01-11 21:22:44 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 21:22:44 - train.py[line:551] - INFO: load:1.54 valid_run:1219.32 task_valid:1162.24 collect_output:46.68
2023-01-11 21:24:44 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 21:24:44 - train.py[line:551] - INFO: load:1.56 valid_run:1339.55 task_valid:1277.68 collect_output:50.44
2023-01-11 21:26:45 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 21:26:45 - train.py[line:551] - INFO: load:1.59 valid_run:1460.86 task_valid:1394.22 collect_output:54.16
2023-01-11 21:28:44 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 21:28:44 - train.py[line:551] - INFO: load:1.61 valid_run:1579.75 task_valid:1507.75 collect_output:58.46
2023-01-11 21:30:45 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 21:30:45 - train.py[line:551] - INFO: load:1.64 valid_run:1700.34 task_valid:1625.05 collect_output:60.70
2023-01-11 21:32:47 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 21:32:47 - train.py[line:551] - INFO: load:1.66 valid_run:1821.75 task_valid:1741.12 collect_output:64.93
2023-01-11 21:34:48 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 21:34:48 - train.py[line:551] - INFO: load:1.69 valid_run:1942.77 task_valid:1854.79 collect_output:71.22
2023-01-11 21:36:49 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 21:36:49 - train.py[line:551] - INFO: load:1.71 valid_run:2063.77 task_valid:1970.45 collect_output:75.54
2023-01-11 21:38:50 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 21:38:50 - train.py[line:551] - INFO: load:1.74 valid_run:2184.41 task_valid:2088.04 collect_output:77.56
2023-01-11 21:40:52 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 21:40:52 - train.py[line:551] - INFO: load:1.76 valid_run:2306.15 task_valid:2205.17 collect_output:81.06
2023-01-11 21:42:52 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 21:42:52 - train.py[line:551] - INFO: load:1.79 valid_run:2426.09 task_valid:2321.25 collect_output:83.90
2023-01-11 21:44:53 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 21:44:54 - train.py[line:551] - INFO: load:1.81 valid_run:2547.90 task_valid:2437.67 collect_output:88.23
2023-01-11 21:46:55 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 21:46:55 - train.py[line:551] - INFO: load:1.84 valid_run:2669.72 task_valid:2556.21 collect_output:90.45
2023-01-11 21:48:56 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 21:48:56 - train.py[line:551] - INFO: load:1.86 valid_run:2789.90 task_valid:2670.16 collect_output:95.63
2023-01-11 21:50:56 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 21:50:56 - train.py[line:551] - INFO: load:1.88 valid_run:2909.97 task_valid:2786.01 collect_output:98.81
2023-01-11 21:52:57 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 21:52:57 - train.py[line:551] - INFO: load:1.91 valid_run:3031.31 task_valid:2901.67 collect_output:103.45
2023-01-11 21:55:00 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 21:55:00 - train.py[line:551] - INFO: load:1.93 valid_run:3154.27 task_valid:3017.23 collect_output:109.83
2023-01-11 21:57:00 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 21:57:00 - train.py[line:551] - INFO: load:1.96 valid_run:3273.83 task_valid:3130.87 collect_output:114.68
2023-01-11 21:59:01 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 21:59:01 - train.py[line:551] - INFO: load:1.98 valid_run:3395.19 task_valid:3249.76 collect_output:116.12
2023-01-11 22:01:03 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 22:01:03 - train.py[line:551] - INFO: load:2.01 valid_run:3516.76 task_valid:3364.73 collect_output:121.69
2023-01-11 22:03:05 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 22:03:05 - train.py[line:551] - INFO: load:2.04 valid_run:3638.31 task_valid:3482.72 collect_output:124.22
2023-01-11 22:05:06 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 22:05:06 - train.py[line:551] - INFO: load:2.06 valid_run:3759.10 task_valid:3600.66 collect_output:126.05

====================================================================================================
SGG eval:     R @ 50: 0.5832;     R @ 100: 0.6453;     R @ 500: 0.6908;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3656;    mR @ 100: 0.4206;    mR @ 500: 0.4618;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7854) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.1500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.8938) (says:0.0000) (sitting on:0.7625) (standing on:0.3293) (using:0.6500) (walking in:0.0000) (walking on:0.6396) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 22:05:36 - train.py[line:487] - INFO: 0.645281512605042

====================================================================================================
SGG eval:     R @ 50: 0.5832;     R @ 100: 0.6453;     R @ 500: 0.6908;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3656;    mR @ 100: 0.4206;    mR @ 500: 0.4618;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7854) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.1500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.8938) (says:0.0000) (sitting on:0.7625) (standing on:0.3293) (using:0.6500) (walking in:0.0000) (walking on:0.6396) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-11 22:05:36 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 22:05:36 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.291 | loss_v1 0 | loss_v2 0 | nll_loss 0.128 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.645282 | ppl 1.09 | vqa_score 0.5721 | wps 118.3 | wpb 72 | bsz 24 | num_updates 12000 | best_R@100 0.697584
2023-01-11 22:05:36 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 12000 updates
2023-01-11 22:05:37 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-11 22:06:16 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-11 22:07:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 0.645281512605042) (writing took 131.8445185162127 seconds)
2023-01-11 22:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:05 - progress_bar.py[line:274] - INFO: epoch 001:  12026 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=0.4, ups=0, wpb=109.5, bsz=40, num_updates=12010, lr=4.58281e-05, gnorm=1.305, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79404
2023-01-11 22:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:22 - progress_bar.py[line:274] - INFO: epoch 001:  12036 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4141, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=12020, lr=4.58229e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79420
2023-01-11 22:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:38 - progress_bar.py[line:274] - INFO: epoch 001:  12046 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.271, wps=99.7, ups=0.61, wpb=109.1, bsz=40, num_updates=12030, lr=4.58177e-05, gnorm=0.706, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79437
2023-01-11 22:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:55 - progress_bar.py[line:274] - INFO: epoch 001:  12056 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2857, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=12040, lr=4.58125e-05, gnorm=0.557, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79453
2023-01-11 22:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:12 - progress_bar.py[line:274] - INFO: epoch 001:  12066 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2841, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=12050, lr=4.58073e-05, gnorm=0.696, clip=20, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=79470
2023-01-11 22:09:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:28 - progress_bar.py[line:274] - INFO: epoch 001:  12076 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3061, wps=101.4, ups=0.62, wpb=109.4, bsz=40, num_updates=12060, lr=4.58021e-05, gnorm=1.004, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79487
2023-01-11 22:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:45 - progress_bar.py[line:274] - INFO: epoch 001:  12086 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2946, wps=99.7, ups=0.61, wpb=108.8, bsz=40, num_updates=12070, lr=4.57969e-05, gnorm=1.714, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79503
2023-01-11 22:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:01 - progress_bar.py[line:274] - INFO: epoch 001:  12096 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.375, wps=102.8, ups=0.62, wpb=111.3, bsz=40, num_updates=12080, lr=4.57917e-05, gnorm=0.912, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=79520
2023-01-11 22:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:18 - progress_bar.py[line:274] - INFO: epoch 001:  12106 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3529, wps=97.9, ups=0.6, wpb=109.7, bsz=40, num_updates=12090, lr=4.57865e-05, gnorm=0.784, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79537
2023-01-11 22:10:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 22:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:37 - progress_bar.py[line:274] - INFO: epoch 001:  12117 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.562, nsentences=40, sample_size=108.562, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3415, wps=95.3, ups=0.55, wpb=108.6, bsz=40, num_updates=12100, lr=4.57813e-05, gnorm=0.615, clip=10, loss_scale=256, train_wall=18, gb_free=9.9, ema_decay=0.9999, wall=79555
2023-01-11 22:10:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:53 - progress_bar.py[line:274] - INFO: epoch 001:  12127 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2447, wps=100.1, ups=0.61, wpb=109.3, bsz=40, num_updates=12110, lr=4.5776e-05, gnorm=0.772, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79572
2023-01-11 22:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:10:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:10 - progress_bar.py[line:274] - INFO: epoch 001:  12137 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3226, wps=103.2, ups=0.62, wpb=110.4, bsz=40, num_updates=12120, lr=4.57708e-05, gnorm=1.374, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79588
2023-01-11 22:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:26 - progress_bar.py[line:274] - INFO: epoch 001:  12147 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3671, wps=101.9, ups=0.61, wpb=111.6, bsz=40, num_updates=12130, lr=4.57656e-05, gnorm=0.847, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79605
2023-01-11 22:11:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:43 - progress_bar.py[line:274] - INFO: epoch 001:  12157 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4018, wps=99.7, ups=0.62, wpb=108, bsz=40, num_updates=12140, lr=4.57604e-05, gnorm=0.672, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79622
2023-01-11 22:11:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:11:59 - progress_bar.py[line:274] - INFO: epoch 001:  12167 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2333, wps=102.2, ups=0.62, wpb=109.5, bsz=40, num_updates=12150, lr=4.57552e-05, gnorm=0.568, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79638
2023-01-11 22:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:16 - progress_bar.py[line:274] - INFO: epoch 001:  12177 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3958, wps=101.6, ups=0.62, wpb=109.2, bsz=40, num_updates=12160, lr=4.575e-05, gnorm=0.801, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79654
2023-01-11 22:12:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:32 - progress_bar.py[line:274] - INFO: epoch 001:  12187 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.3365, wps=100.7, ups=0.61, wpb=110, bsz=40, num_updates=12170, lr=4.57448e-05, gnorm=2.499, clip=60, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79671
2023-01-11 22:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:49 - progress_bar.py[line:274] - INFO: epoch 001:  12197 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2885, wps=97.5, ups=0.59, wpb=109.5, bsz=40, num_updates=12180, lr=4.57396e-05, gnorm=1.256, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=79688
2023-01-11 22:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:06 - progress_bar.py[line:274] - INFO: epoch 001:  12207 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2885, wps=99.5, ups=0.6, wpb=110.3, bsz=40, num_updates=12190, lr=4.57344e-05, gnorm=0.811, clip=30, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=79705
2023-01-11 22:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:23 - progress_bar.py[line:274] - INFO: epoch 001:  12217 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3838, wps=100.5, ups=0.61, wpb=110.3, bsz=40, num_updates=12200, lr=4.57292e-05, gnorm=0.919, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79722
2023-01-11 22:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:39 - progress_bar.py[line:274] - INFO: epoch 001:  12227 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2424, wps=102.3, ups=0.62, wpb=110.3, bsz=40, num_updates=12210, lr=4.5724e-05, gnorm=0.53, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=79738
2023-01-11 22:13:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:56 - progress_bar.py[line:274] - INFO: epoch 001:  12237 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3125, wps=100.1, ups=0.62, wpb=108.2, bsz=40, num_updates=12220, lr=4.57187e-05, gnorm=0.699, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79754
2023-01-11 22:13:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:13:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:12 - progress_bar.py[line:274] - INFO: epoch 001:  12247 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3158, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=12230, lr=4.57135e-05, gnorm=1.565, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79771
2023-01-11 22:14:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:29 - progress_bar.py[line:274] - INFO: epoch 001:  12257 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3107, wps=99.5, ups=0.61, wpb=108.7, bsz=40, num_updates=12240, lr=4.57083e-05, gnorm=0.726, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=79788
2023-01-11 22:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:45 - progress_bar.py[line:274] - INFO: epoch 001:  12267 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3394, wps=104.6, ups=0.63, wpb=110.3, bsz=40, num_updates=12250, lr=4.57031e-05, gnorm=0.516, clip=10, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=79804
2023-01-11 22:14:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:02 - progress_bar.py[line:274] - INFO: epoch 001:  12277 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3556, wps=102.8, ups=0.62, wpb=111, bsz=40, num_updates=12260, lr=4.56979e-05, gnorm=0.598, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=79821
2023-01-11 22:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:19 - progress_bar.py[line:274] - INFO: epoch 001:  12287 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3187, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=12270, lr=4.56927e-05, gnorm=0.668, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=79837
2023-01-11 22:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:35 - progress_bar.py[line:274] - INFO: epoch 001:  12297 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3191, wps=101.7, ups=0.62, wpb=109.1, bsz=40, num_updates=12280, lr=4.56875e-05, gnorm=0.861, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79854
2023-01-11 22:15:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:52 - progress_bar.py[line:274] - INFO: epoch 001:  12307 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3069, wps=98.7, ups=0.6, wpb=109.4, bsz=40, num_updates=12290, lr=4.56823e-05, gnorm=0.643, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=79871
2023-01-11 22:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:09 - progress_bar.py[line:274] - INFO: epoch 001:  12317 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4, wps=101.7, ups=0.62, wpb=110, bsz=40, num_updates=12300, lr=4.56771e-05, gnorm=0.567, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79887
2023-01-11 22:16:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:25 - progress_bar.py[line:274] - INFO: epoch 001:  12327 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.37, wps=101.7, ups=0.61, wpb=110.3, bsz=40, num_updates=12310, lr=4.56719e-05, gnorm=0.53, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79904
2023-01-11 22:16:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:42 - progress_bar.py[line:274] - INFO: epoch 001:  12337 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2347, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=12320, lr=4.56667e-05, gnorm=0.448, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79920
2023-01-11 22:16:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:16:58 - progress_bar.py[line:274] - INFO: epoch 001:  12347 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3545, wps=100.2, ups=0.61, wpb=108.8, bsz=40, num_updates=12330, lr=4.56615e-05, gnorm=1.06, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=79937
2023-01-11 22:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:15 - progress_bar.py[line:274] - INFO: epoch 001:  12357 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3535, wps=100.8, ups=0.61, wpb=109.7, bsz=40, num_updates=12340, lr=4.56563e-05, gnorm=0.477, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79953
2023-01-11 22:17:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:32 - progress_bar.py[line:274] - INFO: epoch 001:  12367 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3226, wps=99, ups=0.6, wpb=109.5, bsz=40, num_updates=12350, lr=4.5651e-05, gnorm=0.814, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79970
2023-01-11 22:17:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:48 - progress_bar.py[line:274] - INFO: epoch 001:  12377 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3673, wps=104.3, ups=0.62, wpb=111.8, bsz=40, num_updates=12360, lr=4.56458e-05, gnorm=0.78, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79987
2023-01-11 22:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:17:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:05 - progress_bar.py[line:274] - INFO: epoch 001:  12387 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3696, wps=102.2, ups=0.61, wpb=110.8, bsz=40, num_updates=12370, lr=4.56406e-05, gnorm=1.062, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=80003
2023-01-11 22:18:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:21 - progress_bar.py[line:274] - INFO: epoch 001:  12397 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3137, wps=100.8, ups=0.61, wpb=111, bsz=40, num_updates=12380, lr=4.56354e-05, gnorm=1.077, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80020
2023-01-11 22:18:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:38 - progress_bar.py[line:274] - INFO: epoch 001:  12407 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3438, wps=102.1, ups=0.62, wpb=110.5, bsz=40, num_updates=12390, lr=4.56302e-05, gnorm=0.598, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80037
2023-01-11 22:18:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:54 - progress_bar.py[line:274] - INFO: epoch 001:  12417 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3364, wps=100.1, ups=0.61, wpb=109, bsz=40, num_updates=12400, lr=4.5625e-05, gnorm=0.605, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80053
2023-01-11 22:18:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:11 - progress_bar.py[line:274] - INFO: epoch 001:  12427 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2818, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=12410, lr=4.56198e-05, gnorm=0.738, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80070
2023-01-11 22:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:28 - progress_bar.py[line:274] - INFO: epoch 001:  12437 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3125, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=12420, lr=4.56146e-05, gnorm=0.67, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80086
2023-01-11 22:19:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:45 - progress_bar.py[line:274] - INFO: epoch 001:  12447 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.4, wps=98.7, ups=0.6, wpb=109.1, bsz=40, num_updates=12430, lr=4.56094e-05, gnorm=0.492, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80103
2023-01-11 22:19:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:19:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:01 - progress_bar.py[line:274] - INFO: epoch 001:  12457 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2857, wps=104, ups=0.63, wpb=109.7, bsz=40, num_updates=12440, lr=4.56042e-05, gnorm=0.808, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80119
2023-01-11 22:20:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:18 - progress_bar.py[line:274] - INFO: epoch 001:  12467 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.4259, wps=100.9, ups=0.62, wpb=109.3, bsz=40, num_updates=12450, lr=4.5599e-05, gnorm=0.56, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80136
2023-01-11 22:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:35 - progress_bar.py[line:274] - INFO: epoch 001:  12477 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3412, wps=101.5, ups=0.61, wpb=111.1, bsz=40, num_updates=12460, lr=4.55938e-05, gnorm=0.879, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80154
2023-01-11 22:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:51 - progress_bar.py[line:274] - INFO: epoch 001:  12487 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.433, wps=102.9, ups=0.63, wpb=109.7, bsz=40, num_updates=12470, lr=4.55885e-05, gnorm=0.614, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80170
2023-01-11 22:20:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:20:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:08 - progress_bar.py[line:274] - INFO: epoch 001:  12497 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3368, wps=101.9, ups=0.62, wpb=110.3, bsz=40, num_updates=12480, lr=4.55833e-05, gnorm=0.984, clip=30, loss_scale=256, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=80186
2023-01-11 22:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:24 - progress_bar.py[line:274] - INFO: epoch 001:  12507 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2826, wps=104.4, ups=0.63, wpb=110.1, bsz=40, num_updates=12490, lr=4.55781e-05, gnorm=0.585, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=80202
2023-01-11 22:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:40 - progress_bar.py[line:274] - INFO: epoch 001:  12517 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3981, wps=103.9, ups=0.63, wpb=110.3, bsz=40, num_updates=12500, lr=4.55729e-05, gnorm=1.023, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80219
2023-01-11 22:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:57 - progress_bar.py[line:274] - INFO: epoch 001:  12527 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4091, wps=100.8, ups=0.6, wpb=111.9, bsz=40, num_updates=12510, lr=4.55677e-05, gnorm=1.051, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80236
2023-01-11 22:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:21:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:14 - progress_bar.py[line:274] - INFO: epoch 001:  12537 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.267, nsentences=40, sample_size=107.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2547, wps=96.9, ups=0.6, wpb=107.3, bsz=40, num_updates=12520, lr=4.55625e-05, gnorm=1.991, clip=50, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80253
2023-01-11 22:22:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:31 - progress_bar.py[line:274] - INFO: epoch 001:  12547 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3942, wps=102, ups=0.62, wpb=110, bsz=40, num_updates=12530, lr=4.55573e-05, gnorm=0.826, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80269
2023-01-11 22:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:47 - progress_bar.py[line:274] - INFO: epoch 001:  12557 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3084, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=12540, lr=4.55521e-05, gnorm=0.842, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80286
2023-01-11 22:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:04 - progress_bar.py[line:274] - INFO: epoch 001:  12567 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3451, wps=98.5, ups=0.61, wpb=108.3, bsz=40, num_updates=12550, lr=4.55469e-05, gnorm=0.958, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80303
2023-01-11 22:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:21 - progress_bar.py[line:274] - INFO: epoch 001:  12577 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2708, wps=99.7, ups=0.61, wpb=109.1, bsz=40, num_updates=12560, lr=4.55417e-05, gnorm=0.768, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80320
2023-01-11 22:23:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:38 - progress_bar.py[line:274] - INFO: epoch 001:  12587 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4022, wps=101.2, ups=0.61, wpb=110.8, bsz=40, num_updates=12570, lr=4.55365e-05, gnorm=1.199, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80336
2023-01-11 22:23:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:55 - progress_bar.py[line:274] - INFO: epoch 001:  12597 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3725, wps=99.8, ups=0.6, wpb=111.5, bsz=40, num_updates=12580, lr=4.55313e-05, gnorm=1.738, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80354
2023-01-11 22:23:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:11 - progress_bar.py[line:274] - INFO: epoch 001:  12607 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4222, wps=103.9, ups=0.63, wpb=110.3, bsz=40, num_updates=12590, lr=4.5526e-05, gnorm=0.743, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80370
2023-01-11 22:24:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:29 - progress_bar.py[line:274] - INFO: epoch 001:  12617 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.34, wps=97.3, ups=0.59, wpb=109.5, bsz=40, num_updates=12600, lr=4.55208e-05, gnorm=0.563, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80387
2023-01-11 22:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:45 - progress_bar.py[line:274] - INFO: epoch 001:  12627 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4333, wps=101.7, ups=0.61, wpb=111.1, bsz=40, num_updates=12610, lr=4.55156e-05, gnorm=1.022, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=80404
2023-01-11 22:24:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:02 - progress_bar.py[line:274] - INFO: epoch 001:  12637 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3636, wps=104.9, ups=0.63, wpb=111.5, bsz=40, num_updates=12620, lr=4.55104e-05, gnorm=1.045, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80420
2023-01-11 22:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:18 - progress_bar.py[line:274] - INFO: epoch 001:  12647 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3564, wps=104.3, ups=0.63, wpb=110.7, bsz=40, num_updates=12630, lr=4.55052e-05, gnorm=1.396, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80437
2023-01-11 22:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:35 - progress_bar.py[line:274] - INFO: epoch 001:  12657 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3434, wps=99, ups=0.6, wpb=109.6, bsz=40, num_updates=12640, lr=4.55e-05, gnorm=0.609, clip=20, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=80454
2023-01-11 22:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:52 - progress_bar.py[line:274] - INFO: epoch 001:  12667 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3, wps=102.2, ups=0.62, wpb=110.7, bsz=40, num_updates=12650, lr=4.54948e-05, gnorm=0.924, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80470
2023-01-11 22:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:09 - progress_bar.py[line:274] - INFO: epoch 001:  12677 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3146, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=12660, lr=4.54896e-05, gnorm=0.668, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80487
2023-01-11 22:26:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:25 - progress_bar.py[line:274] - INFO: epoch 001:  12687 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3204, wps=101.9, ups=0.63, wpb=108.7, bsz=40, num_updates=12670, lr=4.54844e-05, gnorm=0.969, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=80504
2023-01-11 22:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:42 - progress_bar.py[line:274] - INFO: epoch 001:  12697 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2967, wps=104.5, ups=0.63, wpb=111.3, bsz=40, num_updates=12680, lr=4.54792e-05, gnorm=0.366, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80520
2023-01-11 22:26:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:26:58 - progress_bar.py[line:274] - INFO: epoch 001:  12707 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3774, wps=100.6, ups=0.62, wpb=108.8, bsz=40, num_updates=12690, lr=4.5474e-05, gnorm=0.588, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=80537
2023-01-11 22:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:15 - progress_bar.py[line:274] - INFO: epoch 001:  12717 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.301, wps=99.7, ups=0.61, wpb=108.5, bsz=40, num_updates=12700, lr=4.54688e-05, gnorm=0.595, clip=10, loss_scale=512, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=80553
2023-01-11 22:27:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:32 - progress_bar.py[line:274] - INFO: epoch 001:  12727 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.35, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=12710, lr=4.54635e-05, gnorm=0.761, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80570
2023-01-11 22:27:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:48 - progress_bar.py[line:274] - INFO: epoch 001:  12737 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4167, wps=102.5, ups=0.62, wpb=110.7, bsz=40, num_updates=12720, lr=4.54583e-05, gnorm=0.84, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80587
2023-01-11 22:27:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:06 - progress_bar.py[line:274] - INFO: epoch 001:  12747 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=96, ups=0.58, wpb=110.7, bsz=40, num_updates=12730, lr=4.54531e-05, gnorm=1.292, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80604
2023-01-11 22:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:23 - progress_bar.py[line:274] - INFO: epoch 001:  12757 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3469, wps=98.6, ups=0.6, wpb=110, bsz=40, num_updates=12740, lr=4.54479e-05, gnorm=1.832, clip=50, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=80621
2023-01-11 22:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:39 - progress_bar.py[line:274] - INFO: epoch 001:  12767 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3187, wps=101.2, ups=0.61, wpb=110.8, bsz=40, num_updates=12750, lr=4.54427e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80638
2023-01-11 22:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:56 - progress_bar.py[line:274] - INFO: epoch 001:  12777 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3178, wps=99.3, ups=0.61, wpb=108.7, bsz=40, num_updates=12760, lr=4.54375e-05, gnorm=1.114, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80655
2023-01-11 22:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:13 - progress_bar.py[line:274] - INFO: epoch 001:  12787 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3529, wps=101.6, ups=0.61, wpb=111.1, bsz=40, num_updates=12770, lr=4.54323e-05, gnorm=0.742, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80672
2023-01-11 22:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:30 - progress_bar.py[line:274] - INFO: epoch 001:  12797 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2737, wps=99, ups=0.6, wpb=110.3, bsz=40, num_updates=12780, lr=4.54271e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80689
2023-01-11 22:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:47 - progress_bar.py[line:274] - INFO: epoch 001:  12807 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2913, wps=99.7, ups=0.6, wpb=110.6, bsz=40, num_updates=12790, lr=4.54219e-05, gnorm=0.848, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80706
2023-01-11 22:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:04 - progress_bar.py[line:274] - INFO: epoch 001:  12817 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3611, wps=98.4, ups=0.6, wpb=109, bsz=40, num_updates=12800, lr=4.54167e-05, gnorm=0.957, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80722
2023-01-11 22:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:20 - progress_bar.py[line:274] - INFO: epoch 001:  12827 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3034, wps=102.3, ups=0.62, wpb=110.7, bsz=40, num_updates=12810, lr=4.54115e-05, gnorm=0.748, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80739
2023-01-11 22:30:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:37 - progress_bar.py[line:274] - INFO: epoch 001:  12837 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=107.333, nsentences=40, sample_size=107.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2667, wps=95.8, ups=0.59, wpb=107.3, bsz=40, num_updates=12820, lr=4.54063e-05, gnorm=0.55, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80756
2023-01-11 22:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:54 - progress_bar.py[line:274] - INFO: epoch 001:  12847 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3148, wps=101.8, ups=0.62, wpb=110.1, bsz=40, num_updates=12830, lr=4.5401e-05, gnorm=0.565, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80772
2023-01-11 22:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:10 - progress_bar.py[line:274] - INFO: epoch 001:  12857 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2955, wps=104, ups=0.63, wpb=110.7, bsz=40, num_updates=12840, lr=4.53958e-05, gnorm=0.41, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80789
2023-01-11 22:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:27 - progress_bar.py[line:274] - INFO: epoch 001:  12867 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3696, wps=100, ups=0.6, wpb=110.8, bsz=40, num_updates=12850, lr=4.53906e-05, gnorm=0.879, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=80805
2023-01-11 22:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:43 - progress_bar.py[line:274] - INFO: epoch 001:  12877 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3579, wps=101.8, ups=0.62, wpb=109.8, bsz=40, num_updates=12860, lr=4.53854e-05, gnorm=1.606, clip=40, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=80822
2023-01-11 22:31:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:31:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:00 - progress_bar.py[line:274] - INFO: epoch 001:  12887 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2738, wps=101.8, ups=0.61, wpb=111.5, bsz=40, num_updates=12870, lr=4.53802e-05, gnorm=0.873, clip=40, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=80839
2023-01-11 22:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:16 - progress_bar.py[line:274] - INFO: epoch 001:  12897 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2609, wps=100.9, ups=0.62, wpb=109.2, bsz=40, num_updates=12880, lr=4.5375e-05, gnorm=1.12, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80855
2023-01-11 22:32:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:33 - progress_bar.py[line:274] - INFO: epoch 001:  12907 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2895, wps=97.9, ups=0.6, wpb=108.3, bsz=40, num_updates=12890, lr=4.53698e-05, gnorm=0.967, clip=30, loss_scale=512, train_wall=17, gb_free=9.2, ema_decay=0.9999, wall=80872
2023-01-11 22:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:50 - progress_bar.py[line:274] - INFO: epoch 001:  12917 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.29, wps=101.9, ups=0.61, wpb=110.5, bsz=40, num_updates=12900, lr=4.53646e-05, gnorm=0.994, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80888
2023-01-11 22:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:06 - progress_bar.py[line:274] - INFO: epoch 001:  12927 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3679, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=12910, lr=4.53594e-05, gnorm=0.557, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=80905
2023-01-11 22:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:23 - progress_bar.py[line:274] - INFO: epoch 001:  12937 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3173, wps=101, ups=0.61, wpb=110.3, bsz=40, num_updates=12920, lr=4.53542e-05, gnorm=0.842, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80921
2023-01-11 22:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:40 - progress_bar.py[line:274] - INFO: epoch 001:  12947 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3069, wps=100.3, ups=0.6, wpb=111.1, bsz=40, num_updates=12930, lr=4.5349e-05, gnorm=1.21, clip=20, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=80938
2023-01-11 22:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:56 - progress_bar.py[line:274] - INFO: epoch 001:  12957 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2959, wps=102.5, ups=0.61, wpb=111.3, bsz=40, num_updates=12940, lr=4.53438e-05, gnorm=0.683, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80955
2023-01-11 22:33:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:13 - progress_bar.py[line:274] - INFO: epoch 001:  12967 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2857, wps=100.1, ups=0.62, wpb=108.5, bsz=40, num_updates=12950, lr=4.53385e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=80971
2023-01-11 22:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:29 - progress_bar.py[line:274] - INFO: epoch 001:  12977 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4554, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=12960, lr=4.53333e-05, gnorm=0.682, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=80988
2023-01-11 22:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:46 - progress_bar.py[line:274] - INFO: epoch 001:  12987 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2062, wps=100.6, ups=0.61, wpb=110.1, bsz=40, num_updates=12970, lr=4.53281e-05, gnorm=0.64, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=81004
2023-01-11 22:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:03 - progress_bar.py[line:274] - INFO: epoch 001:  12997 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3711, wps=98.2, ups=0.6, wpb=108.5, bsz=40, num_updates=12980, lr=4.53229e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=81021
2023-01-11 22:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:19 - progress_bar.py[line:274] - INFO: epoch 001:  13007 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3942, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=12990, lr=4.53177e-05, gnorm=0.602, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=81038
2023-01-11 22:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 22:35:36 - progress_bar.py[line:274] - INFO: epoch 001:  13017 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3039, wps=100.9, ups=0.61, wpb=110, bsz=40, num_updates=13000, lr=4.53125e-05, gnorm=0.292, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=81055
2023-01-11 22:35:36 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 22:35:37 - train.py[line:549] - INFO: 0 / 6234
2023-01-11 22:35:37 - train.py[line:551] - INFO: load:0.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 22:37:39 - train.py[line:549] - INFO: 200 / 6234
2023-01-11 22:37:39 - train.py[line:551] - INFO: load:0.90 valid_run:121.70 task_valid:118.79 collect_output:1.82
2023-01-11 22:39:39 - train.py[line:549] - INFO: 400 / 6234
2023-01-11 22:39:39 - train.py[line:551] - INFO: load:0.93 valid_run:241.80 task_valid:234.42 collect_output:5.21
2023-01-11 22:41:41 - train.py[line:549] - INFO: 600 / 6234
2023-01-11 22:41:41 - train.py[line:551] - INFO: load:0.95 valid_run:363.87 task_valid:350.74 collect_output:9.87
2023-01-11 22:43:43 - train.py[line:549] - INFO: 800 / 6234
2023-01-11 22:43:43 - train.py[line:551] - INFO: load:0.97 valid_run:485.48 task_valid:463.94 collect_output:17.26
2023-01-11 22:45:43 - train.py[line:549] - INFO: 1000 / 6234
2023-01-11 22:45:43 - train.py[line:551] - INFO: load:1.00 valid_run:605.85 task_valid:580.89 collect_output:19.64
2023-01-11 22:47:46 - train.py[line:549] - INFO: 1200 / 6234
2023-01-11 22:47:46 - train.py[line:551] - INFO: load:1.02 valid_run:728.39 task_valid:699.11 collect_output:22.92
2023-01-11 22:49:49 - train.py[line:549] - INFO: 1400 / 6234
2023-01-11 22:49:49 - train.py[line:551] - INFO: load:1.04 valid_run:851.41 task_valid:816.83 collect_output:27.18
2023-01-11 22:51:51 - train.py[line:549] - INFO: 1600 / 6234
2023-01-11 22:51:51 - train.py[line:551] - INFO: load:1.07 valid_run:972.99 task_valid:932.78 collect_output:31.80
2023-01-11 22:53:55 - train.py[line:549] - INFO: 1800 / 6234
2023-01-11 22:53:55 - train.py[line:551] - INFO: load:1.09 valid_run:1096.74 task_valid:1049.56 collect_output:37.73
2023-01-11 22:55:56 - train.py[line:549] - INFO: 2000 / 6234
2023-01-11 22:55:56 - train.py[line:551] - INFO: load:1.12 valid_run:1218.25 task_valid:1161.67 collect_output:46.14
2023-01-11 22:57:56 - train.py[line:549] - INFO: 2200 / 6234
2023-01-11 22:57:56 - train.py[line:551] - INFO: load:1.14 valid_run:1337.84 task_valid:1276.60 collect_output:49.82
2023-01-11 22:59:57 - train.py[line:549] - INFO: 2400 / 6234
2023-01-11 22:59:57 - train.py[line:551] - INFO: load:1.16 valid_run:1459.02 task_valid:1392.84 collect_output:53.77
2023-01-11 23:01:56 - train.py[line:549] - INFO: 2600 / 6234
2023-01-11 23:01:56 - train.py[line:551] - INFO: load:1.19 valid_run:1577.45 task_valid:1506.10 collect_output:57.93
2023-01-11 23:03:56 - train.py[line:549] - INFO: 2800 / 6234
2023-01-11 23:03:56 - train.py[line:551] - INFO: load:1.21 valid_run:1697.99 task_valid:1623.38 collect_output:60.17
2023-01-11 23:05:57 - train.py[line:549] - INFO: 3000 / 6234
2023-01-11 23:05:57 - train.py[line:551] - INFO: load:1.24 valid_run:1818.47 task_valid:1738.85 collect_output:64.18
2023-01-11 23:07:57 - train.py[line:549] - INFO: 3200 / 6234
2023-01-11 23:07:57 - train.py[line:551] - INFO: load:1.26 valid_run:1938.97 task_valid:1852.23 collect_output:70.28
2023-01-11 23:09:58 - train.py[line:549] - INFO: 3400 / 6234
2023-01-11 23:09:58 - train.py[line:551] - INFO: load:1.29 valid_run:2059.76 task_valid:1967.75 collect_output:74.55
2023-01-11 23:11:58 - train.py[line:549] - INFO: 3600 / 6234
2023-01-11 23:11:58 - train.py[line:551] - INFO: load:1.31 valid_run:2179.83 task_valid:2085.00 collect_output:76.34
2023-01-11 23:13:59 - train.py[line:549] - INFO: 3800 / 6234
2023-01-11 23:13:59 - train.py[line:551] - INFO: load:1.34 valid_run:2300.43 task_valid:2201.20 collect_output:79.73
2023-01-11 23:15:59 - train.py[line:549] - INFO: 4000 / 6234
2023-01-11 23:15:59 - train.py[line:551] - INFO: load:1.36 valid_run:2420.22 task_valid:2316.98 collect_output:82.73
2023-01-11 23:18:00 - train.py[line:549] - INFO: 4200 / 6234
2023-01-11 23:18:00 - train.py[line:551] - INFO: load:1.38 valid_run:2541.36 task_valid:2432.74 collect_output:87.11
2023-01-11 23:20:02 - train.py[line:549] - INFO: 4400 / 6234
2023-01-11 23:20:02 - train.py[line:551] - INFO: load:1.41 valid_run:2662.78 task_valid:2550.95 collect_output:89.30
2023-01-11 23:22:02 - train.py[line:549] - INFO: 4600 / 6234
2023-01-11 23:22:02 - train.py[line:551] - INFO: load:1.43 valid_run:2782.65 task_valid:2664.75 collect_output:94.36
2023-01-11 23:24:01 - train.py[line:549] - INFO: 4800 / 6234
2023-01-11 23:24:01 - train.py[line:551] - INFO: load:1.46 valid_run:2901.74 task_valid:2780.16 collect_output:97.03
2023-01-11 23:26:02 - train.py[line:549] - INFO: 5000 / 6234
2023-01-11 23:26:02 - train.py[line:551] - INFO: load:1.48 valid_run:3022.83 task_valid:2895.74 collect_output:101.49
2023-01-11 23:28:04 - train.py[line:549] - INFO: 5200 / 6234
2023-01-11 23:28:04 - train.py[line:551] - INFO: load:1.51 valid_run:3145.19 task_valid:3011.22 collect_output:107.35
2023-01-11 23:30:03 - train.py[line:549] - INFO: 5400 / 6234
2023-01-11 23:30:03 - train.py[line:551] - INFO: load:1.53 valid_run:3264.15 task_valid:3124.63 collect_output:111.91
2023-01-11 23:32:05 - train.py[line:549] - INFO: 5600 / 6234
2023-01-11 23:32:05 - train.py[line:551] - INFO: load:1.56 valid_run:3385.39 task_valid:3243.43 collect_output:113.35
2023-01-11 23:34:06 - train.py[line:549] - INFO: 5800 / 6234
2023-01-11 23:34:06 - train.py[line:551] - INFO: load:1.58 valid_run:3506.51 task_valid:3358.23 collect_output:118.65
2023-01-11 23:36:07 - train.py[line:549] - INFO: 6000 / 6234
2023-01-11 23:36:07 - train.py[line:551] - INFO: load:1.61 valid_run:3627.77 task_valid:3476.02 collect_output:121.10
2023-01-11 23:38:08 - train.py[line:549] - INFO: 6200 / 6234
2023-01-11 23:38:08 - train.py[line:551] - INFO: load:1.63 valid_run:3748.03 task_valid:3593.60 collect_output:122.77

====================================================================================================
SGG eval:     R @ 50: 0.5669;     R @ 100: 0.6425;     R @ 500: 0.6885;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3580;    mR @ 100: 0.4144;    mR @ 500: 0.4635;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.1500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9036) (says:0.0000) (sitting on:0.7523) (standing on:0.3293) (using:0.6000) (walking in:0.0000) (walking on:0.6126) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-01-11 23:38:38 - train.py[line:487] - INFO: 0.6425481792717087

====================================================================================================
SGG eval:     R @ 50: 0.5669;     R @ 100: 0.6425;     R @ 500: 0.6885;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3580;    mR @ 100: 0.4144;    mR @ 500: 0.4635;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4194) (lying on:0.1500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9036) (says:0.0000) (sitting on:0.7523) (standing on:0.3293) (using:0.6000) (walking in:0.0000) (walking on:0.6126) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-01-11 23:38:39 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 23:38:39 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.345 | loss_v1 0 | loss_v2 0 | nll_loss 0.191 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.642548 | ppl 1.14 | vqa_score 0.5653 | wps 118.6 | wpb 72 | bsz 24 | num_updates 13000 | best_R@100 0.697584
2023-01-11 23:38:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 13000 updates
2023-01-11 23:38:39 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_13000.pt
2023-01-11 23:39:23 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_13000.pt
2023-01-11 23:40:52 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_13000.pt (epoch 1 @ 13000 updates, score 0.6425481792717087) (writing took 133.27601375617087 seconds)
2023-01-11 23:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:08 - progress_bar.py[line:274] - INFO: epoch 001:  13027 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3158, wps=0.4, ups=0, wpb=107.7, bsz=40, num_updates=13010, lr=4.53073e-05, gnorm=0.778, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84987
2023-01-11 23:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:25 - progress_bar.py[line:274] - INFO: epoch 001:  13037 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.419, wps=98.7, ups=0.61, wpb=108.5, bsz=40, num_updates=13020, lr=4.53021e-05, gnorm=1.706, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85004
2023-01-11 23:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:41 - progress_bar.py[line:274] - INFO: epoch 001:  13047 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3265, wps=101.1, ups=0.62, wpb=109.1, bsz=40, num_updates=13030, lr=4.52969e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85020
2023-01-11 23:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:41:58 - progress_bar.py[line:274] - INFO: epoch 001:  13057 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3301, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=13040, lr=4.52917e-05, gnorm=0.964, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85037
2023-01-11 23:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:15 - progress_bar.py[line:274] - INFO: epoch 001:  13067 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3158, wps=99.7, ups=0.6, wpb=110.1, bsz=40, num_updates=13050, lr=4.52865e-05, gnorm=1.434, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=85053
2023-01-11 23:42:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:32 - progress_bar.py[line:274] - INFO: epoch 001:  13077 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3838, wps=99.4, ups=0.6, wpb=111.1, bsz=40, num_updates=13060, lr=4.52812e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=85070
2023-01-11 23:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:49 - progress_bar.py[line:274] - INFO: epoch 001:  13087 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3684, wps=97.5, ups=0.59, wpb=109.3, bsz=40, num_updates=13070, lr=4.5276e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=85087
2023-01-11 23:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:06 - progress_bar.py[line:274] - INFO: epoch 001:  13097 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2897, wps=96.8, ups=0.6, wpb=108.2, bsz=40, num_updates=13080, lr=4.52708e-05, gnorm=0.513, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=85105
2023-01-11 23:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:23 - progress_bar.py[line:274] - INFO: epoch 001:  13107 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3398, wps=98.8, ups=0.6, wpb=109.1, bsz=40, num_updates=13090, lr=4.52656e-05, gnorm=0.571, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=85121
2023-01-11 23:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:39 - progress_bar.py[line:274] - INFO: epoch 001:  13117 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3269, wps=104, ups=0.63, wpb=110.9, bsz=40, num_updates=13100, lr=4.52604e-05, gnorm=0.562, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=85138
2023-01-11 23:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:56 - progress_bar.py[line:274] - INFO: epoch 001:  13127 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3922, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=13110, lr=4.52552e-05, gnorm=0.627, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85154
2023-01-11 23:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:12 - progress_bar.py[line:274] - INFO: epoch 001:  13137 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3364, wps=100.1, ups=0.61, wpb=109.7, bsz=40, num_updates=13120, lr=4.525e-05, gnorm=1.212, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85171
2023-01-11 23:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:29 - progress_bar.py[line:274] - INFO: epoch 001:  13147 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2991, wps=98.7, ups=0.6, wpb=109.3, bsz=40, num_updates=13130, lr=4.52448e-05, gnorm=0.986, clip=20, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=85188
2023-01-11 23:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:46 - progress_bar.py[line:274] - INFO: epoch 001:  13157 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3333, wps=100.1, ups=0.62, wpb=108.3, bsz=40, num_updates=13140, lr=4.52396e-05, gnorm=0.808, clip=30, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85204
2023-01-11 23:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:02 - progress_bar.py[line:274] - INFO: epoch 001:  13167 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3617, wps=102.9, ups=0.63, wpb=108.9, bsz=40, num_updates=13150, lr=4.52344e-05, gnorm=0.507, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85220
2023-01-11 23:45:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:19 - progress_bar.py[line:274] - INFO: epoch 001:  13177 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3824, wps=100.3, ups=0.6, wpb=110.9, bsz=40, num_updates=13160, lr=4.52292e-05, gnorm=0.632, clip=0, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=85237
2023-01-11 23:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:35 - progress_bar.py[line:274] - INFO: epoch 001:  13187 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.48, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=13170, lr=4.5224e-05, gnorm=0.7, clip=30, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85254
2023-01-11 23:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:52 - progress_bar.py[line:274] - INFO: epoch 001:  13197 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3774, wps=101.1, ups=0.61, wpb=110, bsz=40, num_updates=13180, lr=4.52188e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=85271
2023-01-11 23:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:08 - progress_bar.py[line:274] - INFO: epoch 001:  13207 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2551, wps=101.6, ups=0.61, wpb=110.5, bsz=40, num_updates=13190, lr=4.52135e-05, gnorm=1.037, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85287
2023-01-11 23:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:25 - progress_bar.py[line:274] - INFO: epoch 001:  13217 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3936, wps=98.5, ups=0.6, wpb=109.6, bsz=40, num_updates=13200, lr=4.52083e-05, gnorm=0.738, clip=10, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=85304
2023-01-11 23:46:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:32 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 23:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:44 - progress_bar.py[line:274] - INFO: epoch 001:  13228 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.625, nsentences=40, sample_size=109.625, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3419, wps=93.1, ups=0.53, wpb=109.6, bsz=40, num_updates=13210, lr=4.52031e-05, gnorm=0.827, clip=30, loss_scale=512, train_wall=19, gb_free=10.1, ema_decay=0.9999, wall=85323
2023-01-11 23:46:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:01 - progress_bar.py[line:274] - INFO: epoch 001:  13238 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3299, wps=102.4, ups=0.62, wpb=110.3, bsz=40, num_updates=13220, lr=4.51979e-05, gnorm=0.778, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85339
2023-01-11 23:47:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:17 - progress_bar.py[line:274] - INFO: epoch 001:  13248 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2475, wps=101.5, ups=0.61, wpb=110.6, bsz=40, num_updates=13230, lr=4.51927e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=85356
2023-01-11 23:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:34 - progress_bar.py[line:274] - INFO: epoch 001:  13258 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4848, wps=99.9, ups=0.61, wpb=109.1, bsz=40, num_updates=13240, lr=4.51875e-05, gnorm=1.37, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85373
2023-01-11 23:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:50 - progress_bar.py[line:274] - INFO: epoch 001:  13268 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3, wps=100.4, ups=0.62, wpb=108.6, bsz=40, num_updates=13250, lr=4.51823e-05, gnorm=0.903, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85389
2023-01-11 23:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:07 - progress_bar.py[line:274] - INFO: epoch 001:  13278 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3793, wps=100.3, ups=0.6, wpb=111.4, bsz=40, num_updates=13260, lr=4.51771e-05, gnorm=0.551, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=85406
2023-01-11 23:48:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:24 - progress_bar.py[line:274] - INFO: epoch 001:  13288 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3273, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=13270, lr=4.51719e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=85423
2023-01-11 23:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:41 - progress_bar.py[line:274] - INFO: epoch 001:  13298 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.4141, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=13280, lr=4.51667e-05, gnorm=0.928, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85440
2023-01-11 23:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:48:58 - progress_bar.py[line:274] - INFO: epoch 001:  13308 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2857, wps=101.8, ups=0.62, wpb=110.2, bsz=40, num_updates=13290, lr=4.51615e-05, gnorm=1.092, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85457
2023-01-11 23:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:15 - progress_bar.py[line:274] - INFO: epoch 001:  13318 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3, wps=102.1, ups=0.61, wpb=111, bsz=40, num_updates=13300, lr=4.51563e-05, gnorm=0.746, clip=20, loss_scale=512, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=85474
2023-01-11 23:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:33 - progress_bar.py[line:274] - INFO: epoch 001:  13328 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4128, wps=99, ups=0.61, wpb=108.7, bsz=40, num_updates=13310, lr=4.5151e-05, gnorm=0.496, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85491
2023-01-11 23:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:50 - progress_bar.py[line:274] - INFO: epoch 001:  13338 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4242, wps=99.5, ups=0.61, wpb=108.9, bsz=40, num_updates=13320, lr=4.51458e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85508
2023-01-11 23:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:07 - progress_bar.py[line:274] - INFO: epoch 001:  13348 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3737, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=13330, lr=4.51406e-05, gnorm=1.035, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85525
2023-01-11 23:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:08 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 23:50:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:26 - progress_bar.py[line:274] - INFO: epoch 001:  13359 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.125, nsentences=40, sample_size=108.125, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3206, wps=93.9, ups=0.54, wpb=108.1, bsz=40, num_updates=13340, lr=4.51354e-05, gnorm=0.551, clip=10, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=85544
2023-01-11 23:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:43 - progress_bar.py[line:274] - INFO: epoch 001:  13369 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3495, wps=101.1, ups=0.61, wpb=109.9, bsz=40, num_updates=13350, lr=4.51302e-05, gnorm=0.712, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85561
2023-01-11 23:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:50:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:00 - progress_bar.py[line:274] - INFO: epoch 001:  13379 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4091, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=13360, lr=4.5125e-05, gnorm=0.607, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85578
2023-01-11 23:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:17 - progress_bar.py[line:274] - INFO: epoch 001:  13389 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2549, wps=99.2, ups=0.61, wpb=108.9, bsz=40, num_updates=13370, lr=4.51198e-05, gnorm=0.624, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85595
2023-01-11 23:51:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:34 - progress_bar.py[line:274] - INFO: epoch 001:  13399 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.43, wps=99.1, ups=0.6, wpb=110.6, bsz=40, num_updates=13380, lr=4.51146e-05, gnorm=0.385, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=85612
2023-01-11 23:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:51 - progress_bar.py[line:274] - INFO: epoch 001:  13409 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3796, wps=102.2, ups=0.62, wpb=110.2, bsz=40, num_updates=13390, lr=4.51094e-05, gnorm=0.809, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85629
2023-01-11 23:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:08 - progress_bar.py[line:274] - INFO: epoch 001:  13419 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.466, wps=99.5, ups=0.61, wpb=109.2, bsz=40, num_updates=13400, lr=4.51042e-05, gnorm=0.494, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85646
2023-01-11 23:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:25 - progress_bar.py[line:274] - INFO: epoch 001:  13429 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.31, wps=101.3, ups=0.62, wpb=108.5, bsz=40, num_updates=13410, lr=4.5099e-05, gnorm=1.356, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85663
2023-01-11 23:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:42 - progress_bar.py[line:274] - INFO: epoch 001:  13439 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3396, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=13420, lr=4.50938e-05, gnorm=0.825, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85680
2023-01-11 23:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:52:59 - progress_bar.py[line:274] - INFO: epoch 001:  13449 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3514, wps=101.2, ups=0.62, wpb=108.3, bsz=40, num_updates=13430, lr=4.50885e-05, gnorm=0.843, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85697
2023-01-11 23:52:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:15 - progress_bar.py[line:274] - INFO: epoch 001:  13459 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4433, wps=102.6, ups=0.62, wpb=109.5, bsz=40, num_updates=13440, lr=4.50833e-05, gnorm=0.987, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85713
2023-01-11 23:53:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:32 - progress_bar.py[line:274] - INFO: epoch 001:  13469 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.4245, wps=99.8, ups=0.61, wpb=108.9, bsz=40, num_updates=13450, lr=4.50781e-05, gnorm=0.692, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85730
2023-01-11 23:53:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:49 - progress_bar.py[line:274] - INFO: epoch 001:  13479 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=113.067, nsentences=40, sample_size=113.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3409, wps=104.7, ups=0.62, wpb=113.1, bsz=40, num_updates=13460, lr=4.50729e-05, gnorm=0.682, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85747
2023-01-11 23:53:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:53:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:05 - progress_bar.py[line:274] - INFO: epoch 001:  13489 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3333, wps=102.1, ups=0.62, wpb=109.4, bsz=40, num_updates=13470, lr=4.50677e-05, gnorm=0.641, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85764
2023-01-11 23:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:22 - progress_bar.py[line:274] - INFO: epoch 001:  13499 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3333, wps=102.8, ups=0.63, wpb=109.1, bsz=40, num_updates=13480, lr=4.50625e-05, gnorm=0.645, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85780
2023-01-11 23:54:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:39 - progress_bar.py[line:274] - INFO: epoch 001:  13509 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.38, wps=101.8, ups=0.61, wpb=111.3, bsz=40, num_updates=13490, lr=4.50573e-05, gnorm=2.086, clip=50, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85797
2023-01-11 23:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:54:56 - progress_bar.py[line:274] - INFO: epoch 001:  13519 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.301, wps=101.8, ups=0.61, wpb=110.5, bsz=40, num_updates=13500, lr=4.50521e-05, gnorm=1.458, clip=50, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85814
2023-01-11 23:54:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:13 - progress_bar.py[line:274] - INFO: epoch 001:  13529 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4222, wps=100.3, ups=0.6, wpb=110.8, bsz=40, num_updates=13510, lr=4.50469e-05, gnorm=1.497, clip=40, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=85831
2023-01-11 23:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:30 - progress_bar.py[line:274] - INFO: epoch 001:  13539 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4059, wps=99.1, ups=0.6, wpb=109.3, bsz=40, num_updates=13520, lr=4.50417e-05, gnorm=0.895, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85848
2023-01-11 23:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:47 - progress_bar.py[line:274] - INFO: epoch 001:  13549 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2766, wps=101.8, ups=0.61, wpb=111, bsz=40, num_updates=13530, lr=4.50365e-05, gnorm=0.433, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85865
2023-01-11 23:55:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:05 - progress_bar.py[line:274] - INFO: epoch 001:  13559 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3295, wps=101.7, ups=0.61, wpb=111.1, bsz=40, num_updates=13540, lr=4.50313e-05, gnorm=0.935, clip=30, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=85883
2023-01-11 23:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:23 - progress_bar.py[line:274] - INFO: epoch 001:  13569 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.398, wps=96.8, ups=0.59, wpb=109.7, bsz=40, num_updates=13550, lr=4.5026e-05, gnorm=0.88, clip=30, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=85901
2023-01-11 23:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:40 - progress_bar.py[line:274] - INFO: epoch 001:  13579 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.451, wps=100.3, ups=0.61, wpb=109.1, bsz=40, num_updates=13560, lr=4.50208e-05, gnorm=1.557, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85918
2023-01-11 23:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:56:57 - progress_bar.py[line:274] - INFO: epoch 001:  13589 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3426, wps=101.1, ups=0.61, wpb=109.9, bsz=40, num_updates=13570, lr=4.50156e-05, gnorm=0.631, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85935
2023-01-11 23:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:14 - progress_bar.py[line:274] - INFO: epoch 001:  13599 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3737, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=13580, lr=4.50104e-05, gnorm=0.571, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85952
2023-01-11 23:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:31 - progress_bar.py[line:274] - INFO: epoch 001:  13609 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3173, wps=102.9, ups=0.63, wpb=109.5, bsz=40, num_updates=13590, lr=4.50052e-05, gnorm=1.65, clip=30, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=85969
2023-01-11 23:57:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:48 - progress_bar.py[line:274] - INFO: epoch 001:  13619 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2644, wps=100.7, ups=0.6, wpb=111.1, bsz=40, num_updates=13600, lr=4.5e-05, gnorm=0.892, clip=20, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=85986
2023-01-11 23:57:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:57:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:05 - progress_bar.py[line:274] - INFO: epoch 001:  13629 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3273, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=13610, lr=4.49948e-05, gnorm=1.033, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=86003
2023-01-11 23:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:22 - progress_bar.py[line:274] - INFO: epoch 001:  13639 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3524, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=13620, lr=4.49896e-05, gnorm=0.467, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=86020
2023-01-11 23:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:39 - progress_bar.py[line:274] - INFO: epoch 001:  13649 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3366, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=13630, lr=4.49844e-05, gnorm=0.497, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=86037
2023-01-11 23:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:58:56 - progress_bar.py[line:274] - INFO: epoch 001:  13659 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3564, wps=101.6, ups=0.61, wpb=110.4, bsz=40, num_updates=13640, lr=4.49792e-05, gnorm=0.808, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=86054
2023-01-11 23:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:13 - progress_bar.py[line:274] - INFO: epoch 001:  13669 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3786, wps=98.9, ups=0.6, wpb=109.5, bsz=40, num_updates=13650, lr=4.4974e-05, gnorm=0.483, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=86071
2023-01-11 23:59:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:30 - progress_bar.py[line:274] - INFO: epoch 001:  13679 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3333, wps=98.8, ups=0.61, wpb=107.7, bsz=40, num_updates=13660, lr=4.49688e-05, gnorm=0.893, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86088
2023-01-11 23:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:47 - progress_bar.py[line:274] - INFO: epoch 001:  13689 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4286, wps=99, ups=0.6, wpb=109.4, bsz=40, num_updates=13670, lr=4.49635e-05, gnorm=0.765, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=86106
2023-01-11 23:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-11 23:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:04 - progress_bar.py[line:274] - INFO: epoch 001:  13699 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3918, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=13680, lr=4.49583e-05, gnorm=0.871, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=86122
2023-01-12 00:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:21 - progress_bar.py[line:274] - INFO: epoch 001:  13709 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3368, wps=103.3, ups=0.63, wpb=110.2, bsz=40, num_updates=13690, lr=4.49531e-05, gnorm=0.513, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=86139
2023-01-12 00:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:38 - progress_bar.py[line:274] - INFO: epoch 001:  13719 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3333, wps=101.9, ups=0.62, wpb=110.1, bsz=40, num_updates=13700, lr=4.49479e-05, gnorm=0.56, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=86156
2023-01-12 00:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:00:54 - progress_bar.py[line:274] - INFO: epoch 001:  13729 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3578, wps=100.4, ups=0.62, wpb=107.6, bsz=40, num_updates=13710, lr=4.49427e-05, gnorm=0.605, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=86173
2023-01-12 00:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:11 - progress_bar.py[line:274] - INFO: epoch 001:  13739 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3762, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=13720, lr=4.49375e-05, gnorm=1.132, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86190
2023-01-12 00:01:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:28 - progress_bar.py[line:274] - INFO: epoch 001:  13749 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4286, wps=101, ups=0.62, wpb=109.5, bsz=40, num_updates=13730, lr=4.49323e-05, gnorm=1.292, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86206
2023-01-12 00:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:45 - progress_bar.py[line:274] - INFO: epoch 001:  13759 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3711, wps=98.5, ups=0.6, wpb=109.2, bsz=40, num_updates=13740, lr=4.49271e-05, gnorm=0.989, clip=30, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=86224
2023-01-12 00:01:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:02 - progress_bar.py[line:274] - INFO: epoch 001:  13769 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4184, wps=103.4, ups=0.63, wpb=109.1, bsz=40, num_updates=13750, lr=4.49219e-05, gnorm=1.845, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=86240
2023-01-12 00:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:19 - progress_bar.py[line:274] - INFO: epoch 001:  13779 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2981, wps=99.1, ups=0.61, wpb=108.4, bsz=40, num_updates=13760, lr=4.49167e-05, gnorm=1.096, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86257
2023-01-12 00:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:36 - progress_bar.py[line:274] - INFO: epoch 001:  13789 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3469, wps=98.9, ups=0.6, wpb=109.1, bsz=40, num_updates=13770, lr=4.49115e-05, gnorm=1.693, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=86274
2023-01-12 00:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:02:53 - progress_bar.py[line:274] - INFO: epoch 001:  13799 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.42, wps=103.5, ups=0.62, wpb=111.5, bsz=40, num_updates=13780, lr=4.49063e-05, gnorm=0.492, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86291
2023-01-12 00:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:10 - progress_bar.py[line:274] - INFO: epoch 001:  13809 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3469, wps=98.6, ups=0.6, wpb=110.1, bsz=40, num_updates=13790, lr=4.4901e-05, gnorm=1.165, clip=30, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=86308
2023-01-12 00:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:27 - progress_bar.py[line:274] - INFO: epoch 001:  13819 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3564, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=13800, lr=4.48958e-05, gnorm=0.688, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=86326
2023-01-12 00:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:45 - progress_bar.py[line:274] - INFO: epoch 001:  13829 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3868, wps=99.6, ups=0.6, wpb=110.1, bsz=40, num_updates=13810, lr=4.48906e-05, gnorm=0.446, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=86343
2023-01-12 00:03:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:01 - progress_bar.py[line:274] - INFO: epoch 001:  13839 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3519, wps=101.4, ups=0.62, wpb=109.6, bsz=40, num_updates=13820, lr=4.48854e-05, gnorm=0.44, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=86360
2023-01-12 00:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:19 - progress_bar.py[line:274] - INFO: epoch 001:  13849 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2929, wps=99.7, ups=0.6, wpb=110, bsz=40, num_updates=13830, lr=4.48802e-05, gnorm=0.427, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=86377
2023-01-12 00:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:21 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-12 00:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:37 - progress_bar.py[line:274] - INFO: epoch 001:  13860 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.312, nsentences=40, sample_size=109.312, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3972, wps=94.7, ups=0.54, wpb=109.3, bsz=40, num_updates=13840, lr=4.4875e-05, gnorm=0.553, clip=10, loss_scale=128, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=86396
2023-01-12 00:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:04:54 - progress_bar.py[line:274] - INFO: epoch 001:  13870 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4476, wps=102.4, ups=0.62, wpb=110.5, bsz=40, num_updates=13850, lr=4.48698e-05, gnorm=0.505, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86413
2023-01-12 00:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:11 - progress_bar.py[line:274] - INFO: epoch 001:  13880 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.367, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=13860, lr=4.48646e-05, gnorm=0.825, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86429
2023-01-12 00:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:28 - progress_bar.py[line:274] - INFO: epoch 001:  13890 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3097, wps=100.4, ups=0.61, wpb=109, bsz=40, num_updates=13870, lr=4.48594e-05, gnorm=0.596, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86446
2023-01-12 00:05:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:45 - progress_bar.py[line:274] - INFO: epoch 001:  13900 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4316, wps=102.3, ups=0.62, wpb=110.1, bsz=40, num_updates=13880, lr=4.48542e-05, gnorm=0.837, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=86463
2023-01-12 00:05:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:02 - progress_bar.py[line:274] - INFO: epoch 001:  13910 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3826, wps=99.4, ups=0.61, wpb=107.9, bsz=40, num_updates=13890, lr=4.4849e-05, gnorm=0.505, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=86480
2023-01-12 00:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:19 - progress_bar.py[line:274] - INFO: epoch 001:  13920 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3696, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=13900, lr=4.48438e-05, gnorm=2.143, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86497
2023-01-12 00:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:36 - progress_bar.py[line:274] - INFO: epoch 001:  13930 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4018, wps=100.1, ups=0.61, wpb=109.7, bsz=40, num_updates=13910, lr=4.48385e-05, gnorm=0.696, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86514
2023-01-12 00:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:06:53 - progress_bar.py[line:274] - INFO: epoch 001:  13940 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3494, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=13920, lr=4.48333e-05, gnorm=0.559, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=86531
2023-01-12 00:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:10 - progress_bar.py[line:274] - INFO: epoch 001:  13950 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.33, wps=100.4, ups=0.61, wpb=108.9, bsz=40, num_updates=13930, lr=4.48281e-05, gnorm=0.797, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=86548
2023-01-12 00:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:27 - progress_bar.py[line:274] - INFO: epoch 001:  13960 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3462, wps=100.7, ups=0.61, wpb=109.2, bsz=40, num_updates=13940, lr=4.48229e-05, gnorm=0.487, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86565
2023-01-12 00:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:43 - progress_bar.py[line:274] - INFO: epoch 001:  13970 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3962, wps=103.7, ups=0.62, wpb=111.3, bsz=40, num_updates=13950, lr=4.48177e-05, gnorm=0.579, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=86582
2023-01-12 00:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:00 - progress_bar.py[line:274] - INFO: epoch 001:  13980 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3725, wps=99.2, ups=0.6, wpb=109.7, bsz=40, num_updates=13960, lr=4.48125e-05, gnorm=1.913, clip=30, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=86598
2023-01-12 00:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:16 - progress_bar.py[line:274] - INFO: epoch 001:  13990 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3158, wps=100.5, ups=0.61, wpb=109.1, bsz=40, num_updates=13970, lr=4.48073e-05, gnorm=1.65, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86615
2023-01-12 00:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:33 - progress_bar.py[line:274] - INFO: epoch 001:  14000 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.33, wps=100.4, ups=0.6, wpb=111.5, bsz=40, num_updates=13980, lr=4.48021e-05, gnorm=0.517, clip=0, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=86632
2023-01-12 00:08:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:50 - progress_bar.py[line:274] - INFO: epoch 001:  14010 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3636, wps=99.8, ups=0.61, wpb=109.6, bsz=40, num_updates=13990, lr=4.47969e-05, gnorm=1.298, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=86649
2023-01-12 00:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:08:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:09:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:09:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 00:09:07 - progress_bar.py[line:274] - INFO: epoch 001:  14020 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4062, wps=98.4, ups=0.6, wpb=108.9, bsz=40, num_updates=14000, lr=4.47917e-05, gnorm=0.728, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=86665
2023-01-12 00:09:07 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 00:09:08 - train.py[line:549] - INFO: 0 / 6234
2023-01-12 00:09:08 - train.py[line:551] - INFO: load:1.11 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 00:11:10 - train.py[line:549] - INFO: 200 / 6234
2023-01-12 00:11:10 - train.py[line:551] - INFO: load:1.13 valid_run:121.49 task_valid:118.38 collect_output:2.06
2023-01-12 00:13:09 - train.py[line:549] - INFO: 400 / 6234
2023-01-12 00:13:09 - train.py[line:551] - INFO: load:1.16 valid_run:241.02 task_valid:233.63 collect_output:5.32
2023-01-12 00:15:11 - train.py[line:549] - INFO: 600 / 6234
2023-01-12 00:15:11 - train.py[line:551] - INFO: load:1.18 valid_run:362.78 task_valid:349.59 collect_output:10.11
2023-01-12 00:17:13 - train.py[line:549] - INFO: 800 / 6234
2023-01-12 00:17:13 - train.py[line:551] - INFO: load:1.20 valid_run:484.59 task_valid:462.83 collect_output:17.65
2023-01-12 00:19:13 - train.py[line:549] - INFO: 1000 / 6234
2023-01-12 00:19:13 - train.py[line:551] - INFO: load:1.23 valid_run:604.66 task_valid:579.65 collect_output:19.91
2023-01-12 00:21:16 - train.py[line:549] - INFO: 1200 / 6234
2023-01-12 00:21:16 - train.py[line:551] - INFO: load:1.25 valid_run:727.17 task_valid:697.79 collect_output:23.27
2023-01-12 00:23:18 - train.py[line:549] - INFO: 1400 / 6234
2023-01-12 00:23:18 - train.py[line:551] - INFO: load:1.28 valid_run:849.83 task_valid:815.51 collect_output:27.20
2023-01-12 00:25:20 - train.py[line:549] - INFO: 1600 / 6234
2023-01-12 00:25:20 - train.py[line:551] - INFO: load:1.30 valid_run:971.45 task_valid:931.68 collect_output:31.62
2023-01-12 00:27:24 - train.py[line:549] - INFO: 1800 / 6234
2023-01-12 00:27:24 - train.py[line:551] - INFO: load:1.33 valid_run:1094.83 task_valid:1048.42 collect_output:37.27
2023-01-12 00:29:25 - train.py[line:549] - INFO: 2000 / 6234
2023-01-12 00:29:25 - train.py[line:551] - INFO: load:1.35 valid_run:1216.19 task_valid:1160.60 collect_output:45.44
2023-01-12 00:31:25 - train.py[line:549] - INFO: 2200 / 6234
2023-01-12 00:31:25 - train.py[line:551] - INFO: load:1.38 valid_run:1335.91 task_valid:1275.62 collect_output:49.14
2023-01-12 00:33:26 - train.py[line:549] - INFO: 2400 / 6234
2023-01-12 00:33:26 - train.py[line:551] - INFO: load:1.40 valid_run:1457.10 task_valid:1391.94 collect_output:53.02
2023-01-12 00:35:25 - train.py[line:549] - INFO: 2600 / 6234
2023-01-12 00:35:25 - train.py[line:551] - INFO: load:1.43 valid_run:1575.63 task_valid:1505.21 collect_output:57.27
2023-01-12 00:37:25 - train.py[line:549] - INFO: 2800 / 6234
2023-01-12 00:37:25 - train.py[line:551] - INFO: load:1.45 valid_run:1696.05 task_valid:1622.25 collect_output:59.65
2023-01-12 00:39:26 - train.py[line:549] - INFO: 3000 / 6234
2023-01-12 00:39:26 - train.py[line:551] - INFO: load:1.48 valid_run:1816.68 task_valid:1737.88 collect_output:63.62
2023-01-12 00:41:27 - train.py[line:549] - INFO: 3200 / 6234
2023-01-12 00:41:27 - train.py[line:551] - INFO: load:1.50 valid_run:1937.32 task_valid:1851.40 collect_output:69.72
2023-01-12 00:43:28 - train.py[line:549] - INFO: 3400 / 6234
2023-01-12 00:43:28 - train.py[line:551] - INFO: load:1.52 valid_run:2058.28 task_valid:1966.99 collect_output:74.09
2023-01-12 00:45:28 - train.py[line:549] - INFO: 3600 / 6234
2023-01-12 00:45:28 - train.py[line:551] - INFO: load:1.55 valid_run:2178.37 task_valid:2084.20 collect_output:75.96
2023-01-12 00:47:29 - train.py[line:549] - INFO: 3800 / 6234
2023-01-12 00:47:29 - train.py[line:551] - INFO: load:1.57 valid_run:2299.04 task_valid:2200.47 collect_output:79.33
2023-01-12 00:49:28 - train.py[line:549] - INFO: 4000 / 6234
2023-01-12 00:49:28 - train.py[line:551] - INFO: load:1.60 valid_run:2418.78 task_valid:2316.39 collect_output:82.14
2023-01-12 00:51:30 - train.py[line:549] - INFO: 4200 / 6234
2023-01-12 00:51:30 - train.py[line:551] - INFO: load:1.63 valid_run:2540.00 task_valid:2432.36 collect_output:86.37
2023-01-12 00:53:31 - train.py[line:549] - INFO: 4400 / 6234
2023-01-12 00:53:31 - train.py[line:551] - INFO: load:1.65 valid_run:2661.58 task_valid:2550.80 collect_output:88.47
2023-01-12 00:55:31 - train.py[line:549] - INFO: 4600 / 6234
2023-01-12 00:55:31 - train.py[line:551] - INFO: load:1.68 valid_run:2781.47 task_valid:2664.65 collect_output:93.48
2023-01-12 00:57:31 - train.py[line:549] - INFO: 4800 / 6234
2023-01-12 00:57:31 - train.py[line:551] - INFO: load:1.70 valid_run:2900.70 task_valid:2780.17 collect_output:96.17
2023-01-12 00:59:32 - train.py[line:549] - INFO: 5000 / 6234
2023-01-12 00:59:32 - train.py[line:551] - INFO: load:1.73 valid_run:3021.80 task_valid:2895.87 collect_output:100.55
2023-01-12 01:01:34 - train.py[line:549] - INFO: 5200 / 6234
2023-01-12 01:01:34 - train.py[line:551] - INFO: load:1.76 valid_run:3144.27 task_valid:3011.28 collect_output:106.61
2023-01-12 01:03:34 - train.py[line:549] - INFO: 5400 / 6234
2023-01-12 01:03:34 - train.py[line:551] - INFO: load:1.78 valid_run:3263.32 task_valid:3124.84 collect_output:111.07
2023-01-12 01:05:35 - train.py[line:549] - INFO: 5600 / 6234
2023-01-12 01:05:35 - train.py[line:551] - INFO: load:1.81 valid_run:3384.64 task_valid:3243.70 collect_output:112.50
2023-01-12 01:07:36 - train.py[line:549] - INFO: 5800 / 6234
2023-01-12 01:07:36 - train.py[line:551] - INFO: load:1.83 valid_run:3505.68 task_valid:3358.57 collect_output:117.67
2023-01-12 01:09:38 - train.py[line:549] - INFO: 6000 / 6234
2023-01-12 01:09:38 - train.py[line:551] - INFO: load:1.86 valid_run:3627.04 task_valid:3476.33 collect_output:120.23
2023-01-12 01:11:38 - train.py[line:549] - INFO: 6200 / 6234
2023-01-12 01:11:38 - train.py[line:551] - INFO: load:1.89 valid_run:3747.77 task_valid:3594.30 collect_output:121.98

====================================================================================================
SGG eval:     R @ 50: 0.5522;     R @ 100: 0.6331;     R @ 500: 0.6784;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3504;    mR @ 100: 0.4067;    mR @ 500: 0.4562;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7756) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4065) (lying on:0.1500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.8938) (says:0.0000) (sitting on:0.7514) (standing on:0.3143) (using:0.6000) (walking in:0.0000) (walking on:0.5856) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5522;     R @ 100: 0.6331;     R @ 500: 0.6784;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3504;    mR @ 100: 0.4067;    mR @ 500: 0.4562;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7756) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4065) (lying on:0.1500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.8938) (says:0.0000) (sitting on:0.7514) (standing on:0.3143) (using:0.6000) (walking in:0.0000) (walking on:0.5856) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-12 01:12:09 - train.py[line:487] - INFO: 0.6331481792717086
2023-01-12 01:12:09 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 01:12:10 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.337 | loss_v1 0 | loss_v2 0 | nll_loss 0.184 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.633148 | ppl 1.14 | vqa_score 0.5709 | wps 118.6 | wpb 72 | bsz 24 | num_updates 14000 | best_R@100 0.697584
2023-01-12 01:12:10 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 14000 updates
2023-01-12 01:12:10 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-12 01:12:53 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-12 01:14:25 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 0.6331481792717086) (writing took 135.28359371609986 seconds)
2023-01-12 01:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:42 - progress_bar.py[line:274] - INFO: epoch 001:  14030 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4144, wps=0.4, ups=0, wpb=107.6, bsz=40, num_updates=14010, lr=4.47865e-05, gnorm=1.881, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90600
2023-01-12 01:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:14:59 - progress_bar.py[line:274] - INFO: epoch 001:  14040 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3368, wps=99.2, ups=0.61, wpb=109.2, bsz=40, num_updates=14020, lr=4.47813e-05, gnorm=0.854, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90617
2023-01-12 01:15:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:15 - progress_bar.py[line:274] - INFO: epoch 001:  14050 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3505, wps=101.3, ups=0.61, wpb=110.8, bsz=40, num_updates=14030, lr=4.4776e-05, gnorm=0.615, clip=0, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=90634
2023-01-12 01:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:32 - progress_bar.py[line:274] - INFO: epoch 001:  14060 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3529, wps=102.1, ups=0.61, wpb=110.8, bsz=40, num_updates=14040, lr=4.47708e-05, gnorm=1.016, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90650
2023-01-12 01:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:48 - progress_bar.py[line:274] - INFO: epoch 001:  14070 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3585, wps=98.3, ups=0.6, wpb=108.3, bsz=40, num_updates=14050, lr=4.47656e-05, gnorm=2.438, clip=30, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=90667
2023-01-12 01:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:15:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:04 - progress_bar.py[line:274] - INFO: epoch 001:  14080 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3333, wps=106.1, ups=0.64, wpb=110.5, bsz=40, num_updates=14060, lr=4.47604e-05, gnorm=0.944, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90683
2023-01-12 01:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:21 - progress_bar.py[line:274] - INFO: epoch 001:  14090 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3883, wps=99, ups=0.6, wpb=109.4, bsz=40, num_updates=14070, lr=4.47552e-05, gnorm=2.096, clip=50, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=90700
2023-01-12 01:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:38 - progress_bar.py[line:274] - INFO: epoch 001:  14100 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.48, wps=98.7, ups=0.61, wpb=108.7, bsz=40, num_updates=14080, lr=4.475e-05, gnorm=0.532, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90717
2023-01-12 01:16:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:16:54 - progress_bar.py[line:274] - INFO: epoch 001:  14110 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3367, wps=101.5, ups=0.61, wpb=110.1, bsz=40, num_updates=14090, lr=4.47448e-05, gnorm=0.902, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90733
2023-01-12 01:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:10 - progress_bar.py[line:274] - INFO: epoch 001:  14120 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4, wps=103.7, ups=0.63, wpb=109.9, bsz=40, num_updates=14100, lr=4.47396e-05, gnorm=1.095, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90749
2023-01-12 01:17:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:27 - progress_bar.py[line:274] - INFO: epoch 001:  14130 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4022, wps=101.1, ups=0.61, wpb=110.7, bsz=40, num_updates=14110, lr=4.47344e-05, gnorm=1.036, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90766
2023-01-12 01:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:44 - progress_bar.py[line:274] - INFO: epoch 001:  14140 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2963, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=14120, lr=4.47292e-05, gnorm=0.59, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90782
2023-01-12 01:17:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:00 - progress_bar.py[line:274] - INFO: epoch 001:  14150 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.46, wps=103.9, ups=0.62, wpb=111, bsz=40, num_updates=14130, lr=4.4724e-05, gnorm=1.181, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90799
2023-01-12 01:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:17 - progress_bar.py[line:274] - INFO: epoch 001:  14160 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4157, wps=102.3, ups=0.61, wpb=111.1, bsz=40, num_updates=14140, lr=4.47188e-05, gnorm=1.766, clip=30, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90815
2023-01-12 01:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:33 - progress_bar.py[line:274] - INFO: epoch 001:  14170 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3564, wps=100.5, ups=0.62, wpb=108.8, bsz=40, num_updates=14150, lr=4.47135e-05, gnorm=0.56, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90832
2023-01-12 01:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:50 - progress_bar.py[line:274] - INFO: epoch 001:  14180 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4273, wps=99.3, ups=0.61, wpb=108.3, bsz=40, num_updates=14160, lr=4.47083e-05, gnorm=0.748, clip=40, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90848
2023-01-12 01:18:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:18:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:06 - progress_bar.py[line:274] - INFO: epoch 001:  14190 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3409, wps=103.4, ups=0.62, wpb=110.3, bsz=40, num_updates=14170, lr=4.47031e-05, gnorm=1.151, clip=50, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=90865
2023-01-12 01:19:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:22 - progress_bar.py[line:274] - INFO: epoch 001:  14200 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3118, wps=100.6, ups=0.61, wpb=109.8, bsz=40, num_updates=14180, lr=4.46979e-05, gnorm=1.16, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90881
2023-01-12 01:19:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:39 - progress_bar.py[line:274] - INFO: epoch 001:  14210 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3398, wps=98.4, ups=0.6, wpb=108.7, bsz=40, num_updates=14190, lr=4.46927e-05, gnorm=0.907, clip=40, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=90898
2023-01-12 01:19:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:19:56 - progress_bar.py[line:274] - INFO: epoch 001:  14220 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.398, wps=102.2, ups=0.61, wpb=111, bsz=40, num_updates=14200, lr=4.46875e-05, gnorm=0.664, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90915
2023-01-12 01:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:12 - progress_bar.py[line:274] - INFO: epoch 001:  14230 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3235, wps=102.4, ups=0.62, wpb=109.6, bsz=40, num_updates=14210, lr=4.46823e-05, gnorm=0.598, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90931
2023-01-12 01:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:29 - progress_bar.py[line:274] - INFO: epoch 001:  14240 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3529, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=14220, lr=4.46771e-05, gnorm=1.521, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90947
2023-01-12 01:20:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:45 - progress_bar.py[line:274] - INFO: epoch 001:  14250 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3364, wps=103.9, ups=0.63, wpb=110, bsz=40, num_updates=14230, lr=4.46719e-05, gnorm=1.26, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90963
2023-01-12 01:20:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:01 - progress_bar.py[line:274] - INFO: epoch 001:  14260 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2941, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=14240, lr=4.46667e-05, gnorm=0.546, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90980
2023-01-12 01:21:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:18 - progress_bar.py[line:274] - INFO: epoch 001:  14270 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4019, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=14250, lr=4.46615e-05, gnorm=0.561, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90996
2023-01-12 01:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:34 - progress_bar.py[line:274] - INFO: epoch 001:  14280 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3711, wps=104.4, ups=0.64, wpb=108.9, bsz=40, num_updates=14260, lr=4.46562e-05, gnorm=0.945, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91012
2023-01-12 01:21:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:50 - progress_bar.py[line:274] - INFO: epoch 001:  14290 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.4019, wps=103.2, ups=0.63, wpb=109.7, bsz=40, num_updates=14270, lr=4.4651e-05, gnorm=0.607, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91029
2023-01-12 01:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:21:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:07 - progress_bar.py[line:274] - INFO: epoch 001:  14300 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3178, wps=99.6, ups=0.61, wpb=109.5, bsz=40, num_updates=14280, lr=4.46458e-05, gnorm=0.967, clip=30, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91045
2023-01-12 01:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:23 - progress_bar.py[line:274] - INFO: epoch 001:  14310 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3093, wps=98.2, ups=0.6, wpb=109.7, bsz=40, num_updates=14290, lr=4.46406e-05, gnorm=0.909, clip=30, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91062
2023-01-12 01:22:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:40 - progress_bar.py[line:274] - INFO: epoch 001:  14320 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.402, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=14300, lr=4.46354e-05, gnorm=0.833, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91079
2023-01-12 01:22:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:22:57 - progress_bar.py[line:274] - INFO: epoch 001:  14330 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3265, wps=101.3, ups=0.61, wpb=111.2, bsz=40, num_updates=14310, lr=4.46302e-05, gnorm=1.567, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91096
2023-01-12 01:23:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:14 - progress_bar.py[line:274] - INFO: epoch 001:  14340 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3523, wps=100.2, ups=0.6, wpb=110.9, bsz=40, num_updates=14320, lr=4.4625e-05, gnorm=1.39, clip=30, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91113
2023-01-12 01:23:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:30 - progress_bar.py[line:274] - INFO: epoch 001:  14350 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3444, wps=102.4, ups=0.62, wpb=109.8, bsz=40, num_updates=14330, lr=4.46198e-05, gnorm=1.21, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91129
2023-01-12 01:23:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:47 - progress_bar.py[line:274] - INFO: epoch 001:  14360 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2941, wps=98.6, ups=0.6, wpb=109.9, bsz=40, num_updates=14340, lr=4.46146e-05, gnorm=0.544, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=91146
2023-01-12 01:23:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:03 - progress_bar.py[line:274] - INFO: epoch 001:  14370 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3295, wps=104.7, ups=0.63, wpb=110.6, bsz=40, num_updates=14350, lr=4.46094e-05, gnorm=0.719, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91162
2023-01-12 01:24:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:20 - progress_bar.py[line:274] - INFO: epoch 001:  14380 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4188, wps=100.6, ups=0.61, wpb=109.6, bsz=40, num_updates=14360, lr=4.46042e-05, gnorm=1.952, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91178
2023-01-12 01:24:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:36 - progress_bar.py[line:274] - INFO: epoch 001:  14390 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3761, wps=101.6, ups=0.62, wpb=109.3, bsz=40, num_updates=14370, lr=4.4599e-05, gnorm=0.645, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91195
2023-01-12 01:24:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:24:53 - progress_bar.py[line:274] - INFO: epoch 001:  14400 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4757, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=14380, lr=4.45938e-05, gnorm=2.19, clip=50, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=91211
2023-01-12 01:24:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:09 - progress_bar.py[line:274] - INFO: epoch 001:  14410 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4019, wps=100.2, ups=0.61, wpb=110, bsz=40, num_updates=14390, lr=4.45885e-05, gnorm=0.576, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=91228
2023-01-12 01:25:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:26 - progress_bar.py[line:274] - INFO: epoch 001:  14420 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3431, wps=102.3, ups=0.62, wpb=109.6, bsz=40, num_updates=14400, lr=4.45833e-05, gnorm=0.599, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91244
2023-01-12 01:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:42 - progress_bar.py[line:274] - INFO: epoch 001:  14430 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4021, wps=100, ups=0.61, wpb=109.7, bsz=40, num_updates=14410, lr=4.45781e-05, gnorm=0.587, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91261
2023-01-12 01:25:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:25:59 - progress_bar.py[line:274] - INFO: epoch 001:  14440 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.301, wps=102.4, ups=0.62, wpb=110.5, bsz=40, num_updates=14420, lr=4.45729e-05, gnorm=1.351, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91277
2023-01-12 01:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:15 - progress_bar.py[line:274] - INFO: epoch 001:  14450 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3367, wps=104, ups=0.63, wpb=109.8, bsz=40, num_updates=14430, lr=4.45677e-05, gnorm=0.579, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91293
2023-01-12 01:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:31 - progress_bar.py[line:274] - INFO: epoch 001:  14460 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.45, wps=103.2, ups=0.63, wpb=109.8, bsz=40, num_updates=14440, lr=4.45625e-05, gnorm=0.73, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91310
2023-01-12 01:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:48 - progress_bar.py[line:274] - INFO: epoch 001:  14470 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3846, wps=101.7, ups=0.61, wpb=110.3, bsz=40, num_updates=14450, lr=4.45573e-05, gnorm=1.323, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=91326
2023-01-12 01:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:26:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:04 - progress_bar.py[line:274] - INFO: epoch 001:  14480 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3714, wps=98.2, ups=0.61, wpb=107.5, bsz=40, num_updates=14460, lr=4.45521e-05, gnorm=0.637, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91343
2023-01-12 01:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:21 - progress_bar.py[line:274] - INFO: epoch 001:  14490 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3173, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=14470, lr=4.45469e-05, gnorm=0.639, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91360
2023-01-12 01:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:37 - progress_bar.py[line:274] - INFO: epoch 001:  14500 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3434, wps=100.5, ups=0.61, wpb=109.1, bsz=40, num_updates=14480, lr=4.45417e-05, gnorm=1.013, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91376
2023-01-12 01:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:27:54 - progress_bar.py[line:274] - INFO: epoch 001:  14510 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3939, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=14490, lr=4.45365e-05, gnorm=0.46, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91393
2023-01-12 01:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:10 - progress_bar.py[line:274] - INFO: epoch 001:  14520 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3945, wps=103.1, ups=0.63, wpb=109.7, bsz=40, num_updates=14500, lr=4.45313e-05, gnorm=0.644, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91409
2023-01-12 01:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:27 - progress_bar.py[line:274] - INFO: epoch 001:  14530 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3398, wps=100.3, ups=0.61, wpb=109, bsz=40, num_updates=14510, lr=4.4526e-05, gnorm=0.568, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91425
2023-01-12 01:28:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:43 - progress_bar.py[line:274] - INFO: epoch 001:  14540 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4078, wps=99.6, ups=0.61, wpb=109, bsz=40, num_updates=14520, lr=4.45208e-05, gnorm=0.692, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91442
2023-01-12 01:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:28:59 - progress_bar.py[line:274] - INFO: epoch 001:  14550 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4811, wps=103.1, ups=0.63, wpb=109.7, bsz=40, num_updates=14530, lr=4.45156e-05, gnorm=0.697, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91458
2023-01-12 01:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:16 - progress_bar.py[line:274] - INFO: epoch 001:  14560 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4419, wps=101.8, ups=0.61, wpb=110.3, bsz=40, num_updates=14540, lr=4.45104e-05, gnorm=1.264, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=91475
2023-01-12 01:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:33 - progress_bar.py[line:274] - INFO: epoch 001:  14570 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4216, wps=100.4, ups=0.6, wpb=111.5, bsz=40, num_updates=14550, lr=4.45052e-05, gnorm=1.949, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91492
2023-01-12 01:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:49 - progress_bar.py[line:274] - INFO: epoch 001:  14580 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3398, wps=102.5, ups=0.62, wpb=110, bsz=40, num_updates=14560, lr=4.45e-05, gnorm=1.871, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91508
2023-01-12 01:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:06 - progress_bar.py[line:274] - INFO: epoch 001:  14590 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3333, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=14570, lr=4.44948e-05, gnorm=0.626, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91524
2023-01-12 01:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:23 - progress_bar.py[line:274] - INFO: epoch 001:  14600 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4792, wps=97.3, ups=0.6, wpb=108.7, bsz=40, num_updates=14580, lr=4.44896e-05, gnorm=0.817, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91541
2023-01-12 01:30:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:39 - progress_bar.py[line:274] - INFO: epoch 001:  14610 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.422, wps=101, ups=0.61, wpb=111.1, bsz=40, num_updates=14590, lr=4.44844e-05, gnorm=1.198, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91558
2023-01-12 01:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:30:56 - progress_bar.py[line:274] - INFO: epoch 001:  14620 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3204, wps=103.4, ups=0.63, wpb=109.9, bsz=40, num_updates=14600, lr=4.44792e-05, gnorm=0.401, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91574
2023-01-12 01:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:13 - progress_bar.py[line:274] - INFO: epoch 001:  14630 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.39, wps=99.1, ups=0.6, wpb=110.3, bsz=40, num_updates=14610, lr=4.4474e-05, gnorm=1.58, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91591
2023-01-12 01:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:29 - progress_bar.py[line:274] - INFO: epoch 001:  14640 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3444, wps=101.9, ups=0.61, wpb=112, bsz=40, num_updates=14620, lr=4.44688e-05, gnorm=0.419, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91608
2023-01-12 01:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:46 - progress_bar.py[line:274] - INFO: epoch 001:  14650 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3636, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=14630, lr=4.44635e-05, gnorm=1.043, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91625
2023-01-12 01:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:03 - progress_bar.py[line:274] - INFO: epoch 001:  14660 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3684, wps=97.8, ups=0.6, wpb=109.4, bsz=40, num_updates=14640, lr=4.44583e-05, gnorm=0.993, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=91642
2023-01-12 01:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:20 - progress_bar.py[line:274] - INFO: epoch 001:  14670 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3431, wps=99.4, ups=0.61, wpb=109.1, bsz=40, num_updates=14650, lr=4.44531e-05, gnorm=0.852, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91658
2023-01-12 01:32:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:36 - progress_bar.py[line:274] - INFO: epoch 001:  14680 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=107.133, nsentences=40, sample_size=107.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3853, wps=101.1, ups=0.63, wpb=107.1, bsz=40, num_updates=14660, lr=4.44479e-05, gnorm=0.461, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91675
2023-01-12 01:32:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:32:53 - progress_bar.py[line:274] - INFO: epoch 001:  14690 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4587, wps=101.6, ups=0.61, wpb=110.9, bsz=40, num_updates=14670, lr=4.44427e-05, gnorm=0.779, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=91691
2023-01-12 01:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:09 - progress_bar.py[line:274] - INFO: epoch 001:  14700 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3173, wps=103.2, ups=0.63, wpb=109.5, bsz=40, num_updates=14680, lr=4.44375e-05, gnorm=0.778, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=91707
2023-01-12 01:33:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:25 - progress_bar.py[line:274] - INFO: epoch 001:  14710 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.396, wps=100.6, ups=0.62, wpb=108.3, bsz=40, num_updates=14690, lr=4.44323e-05, gnorm=0.875, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=91724
2023-01-12 01:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:41 - progress_bar.py[line:274] - INFO: epoch 001:  14720 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3265, wps=103.6, ups=0.62, wpb=111.3, bsz=40, num_updates=14700, lr=4.44271e-05, gnorm=1.045, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91740
2023-01-12 01:33:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:33:58 - progress_bar.py[line:274] - INFO: epoch 001:  14730 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4272, wps=104, ups=0.63, wpb=110.1, bsz=40, num_updates=14710, lr=4.44219e-05, gnorm=1.875, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91756
2023-01-12 01:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:14 - progress_bar.py[line:274] - INFO: epoch 001:  14740 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3084, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=14720, lr=4.44167e-05, gnorm=0.82, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91773
2023-01-12 01:34:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:31 - progress_bar.py[line:274] - INFO: epoch 001:  14750 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3838, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=14730, lr=4.44115e-05, gnorm=0.533, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=91789
2023-01-12 01:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:47 - progress_bar.py[line:274] - INFO: epoch 001:  14760 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4286, wps=99.5, ups=0.61, wpb=108.5, bsz=40, num_updates=14740, lr=4.44063e-05, gnorm=0.728, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91806
2023-01-12 01:34:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:03 - progress_bar.py[line:274] - INFO: epoch 001:  14770 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3958, wps=100.7, ups=0.62, wpb=108.5, bsz=40, num_updates=14750, lr=4.4401e-05, gnorm=1.124, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91822
2023-01-12 01:35:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:20 - progress_bar.py[line:274] - INFO: epoch 001:  14780 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2432, wps=101.8, ups=0.63, wpb=108.3, bsz=40, num_updates=14760, lr=4.43958e-05, gnorm=0.602, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=91838
2023-01-12 01:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:36 - progress_bar.py[line:274] - INFO: epoch 001:  14790 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3846, wps=101.9, ups=0.62, wpb=108.8, bsz=40, num_updates=14770, lr=4.43906e-05, gnorm=1.682, clip=40, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=91855
2023-01-12 01:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:35:52 - progress_bar.py[line:274] - INFO: epoch 001:  14800 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4071, wps=102.9, ups=0.63, wpb=109.1, bsz=40, num_updates=14780, lr=4.43854e-05, gnorm=0.549, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91871
2023-01-12 01:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:09 - progress_bar.py[line:274] - INFO: epoch 001:  14810 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3854, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=14790, lr=4.43802e-05, gnorm=1.007, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91887
2023-01-12 01:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:25 - progress_bar.py[line:274] - INFO: epoch 001:  14820 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3469, wps=101.8, ups=0.61, wpb=110.9, bsz=40, num_updates=14800, lr=4.4375e-05, gnorm=0.729, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=91904
2023-01-12 01:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:42 - progress_bar.py[line:274] - INFO: epoch 001:  14830 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3832, wps=99.6, ups=0.6, wpb=110.2, bsz=40, num_updates=14810, lr=4.43698e-05, gnorm=0.546, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=91921
2023-01-12 01:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:36:59 - progress_bar.py[line:274] - INFO: epoch 001:  14840 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.42, wps=101, ups=0.6, wpb=111.7, bsz=40, num_updates=14820, lr=4.43646e-05, gnorm=0.809, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=91938
2023-01-12 01:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:16 - progress_bar.py[line:274] - INFO: epoch 001:  14850 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3297, wps=100.4, ups=0.61, wpb=110.4, bsz=40, num_updates=14830, lr=4.43594e-05, gnorm=0.678, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=91954
2023-01-12 01:37:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:33 - progress_bar.py[line:274] - INFO: epoch 001:  14860 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4078, wps=97.9, ups=0.6, wpb=108.5, bsz=40, num_updates=14840, lr=4.43542e-05, gnorm=1.132, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=91971
2023-01-12 01:37:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:49 - progress_bar.py[line:274] - INFO: epoch 001:  14870 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3981, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=14850, lr=4.4349e-05, gnorm=1.689, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=91988
2023-01-12 01:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:05 - progress_bar.py[line:274] - INFO: epoch 001:  14880 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3895, wps=103.5, ups=0.62, wpb=110.5, bsz=40, num_updates=14860, lr=4.43438e-05, gnorm=0.756, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=92004
2023-01-12 01:38:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:22 - progress_bar.py[line:274] - INFO: epoch 001:  14890 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4217, wps=100.8, ups=0.61, wpb=110.9, bsz=40, num_updates=14870, lr=4.43385e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=92021
2023-01-12 01:38:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:39 - progress_bar.py[line:274] - INFO: epoch 001:  14900 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4494, wps=102.2, ups=0.62, wpb=110.4, bsz=40, num_updates=14880, lr=4.43333e-05, gnorm=0.548, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=92037
2023-01-12 01:38:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:38:55 - progress_bar.py[line:274] - INFO: epoch 001:  14910 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3474, wps=104.7, ups=0.63, wpb=110.4, bsz=40, num_updates=14890, lr=4.43281e-05, gnorm=0.542, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=92053
2023-01-12 01:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:11 - progress_bar.py[line:274] - INFO: epoch 001:  14920 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3723, wps=100.6, ups=0.61, wpb=110.3, bsz=40, num_updates=14900, lr=4.43229e-05, gnorm=0.416, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=92070
2023-01-12 01:39:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:28 - progress_bar.py[line:274] - INFO: epoch 001:  14930 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3571, wps=98.3, ups=0.6, wpb=108.5, bsz=40, num_updates=14910, lr=4.43177e-05, gnorm=0.436, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=92087
2023-01-12 01:39:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:44 - progress_bar.py[line:274] - INFO: epoch 001:  14940 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3922, wps=102.7, ups=0.62, wpb=109.7, bsz=40, num_updates=14920, lr=4.43125e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=92103
2023-01-12 01:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:01 - progress_bar.py[line:274] - INFO: epoch 001:  14950 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3545, wps=99.8, ups=0.61, wpb=109.1, bsz=40, num_updates=14930, lr=4.43073e-05, gnorm=0.582, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=92120
2023-01-12 01:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:18 - progress_bar.py[line:274] - INFO: epoch 001:  14960 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3925, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=14940, lr=4.43021e-05, gnorm=1.59, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=92136
2023-01-12 01:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:35 - progress_bar.py[line:274] - INFO: epoch 001:  14970 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4059, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=14950, lr=4.42969e-05, gnorm=0.69, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=92153
2023-01-12 01:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:50 - progress_bar.py[line:274] - INFO: epoch 001:  14980 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3482, wps=103.5, ups=0.64, wpb=108.2, bsz=40, num_updates=14960, lr=4.42917e-05, gnorm=0.789, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=92169
2023-01-12 01:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:07 - progress_bar.py[line:274] - INFO: epoch 001:  14990 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3168, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=14970, lr=4.42865e-05, gnorm=0.722, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=92186
2023-01-12 01:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:21 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-12 01:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:25 - progress_bar.py[line:274] - INFO: epoch 001:  15001 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3711, wps=94.4, ups=0.57, wpb=111.2, bsz=40, num_updates=14980, lr=4.42813e-05, gnorm=0.762, clip=10, loss_scale=256, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=92204
2023-01-12 01:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:41 - progress_bar.py[line:274] - INFO: epoch 001:  15011 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3211, wps=99, ups=0.61, wpb=108.3, bsz=40, num_updates=14990, lr=4.4276e-05, gnorm=0.945, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=92220
2023-01-12 01:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 01:41:58 - progress_bar.py[line:274] - INFO: epoch 001:  15021 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4388, wps=100.8, ups=0.61, wpb=110.6, bsz=40, num_updates=15000, lr=4.42708e-05, gnorm=0.474, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=92237
2023-01-12 01:41:58 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 01:41:59 - train.py[line:549] - INFO: 0 / 6234
2023-01-12 01:41:59 - train.py[line:551] - INFO: load:0.84 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 01:44:01 - train.py[line:549] - INFO: 200 / 6234
2023-01-12 01:44:02 - train.py[line:551] - INFO: load:0.87 valid_run:122.17 task_valid:118.88 collect_output:2.20
2023-01-12 01:46:01 - train.py[line:549] - INFO: 400 / 6234
2023-01-12 01:46:01 - train.py[line:551] - INFO: load:0.89 valid_run:241.71 task_valid:234.26 collect_output:5.30
2023-01-12 01:48:03 - train.py[line:549] - INFO: 600 / 6234
2023-01-12 01:48:03 - train.py[line:551] - INFO: load:0.92 valid_run:363.42 task_valid:350.30 collect_output:9.93
2023-01-12 01:50:05 - train.py[line:549] - INFO: 800 / 6234
2023-01-12 01:50:05 - train.py[line:551] - INFO: load:0.94 valid_run:485.03 task_valid:463.58 collect_output:17.22
2023-01-12 01:52:05 - train.py[line:549] - INFO: 1000 / 6234
2023-01-12 01:52:05 - train.py[line:551] - INFO: load:0.97 valid_run:605.13 task_valid:580.55 collect_output:19.32
2023-01-12 01:54:07 - train.py[line:549] - INFO: 1200 / 6234
2023-01-12 01:54:07 - train.py[line:551] - INFO: load:0.99 valid_run:727.61 task_valid:698.70 collect_output:22.60
2023-01-12 01:56:10 - train.py[line:549] - INFO: 1400 / 6234
2023-01-12 01:56:10 - train.py[line:551] - INFO: load:1.02 valid_run:850.10 task_valid:816.14 collect_output:26.64
2023-01-12 01:58:11 - train.py[line:549] - INFO: 1600 / 6234
2023-01-12 01:58:11 - train.py[line:551] - INFO: load:1.05 valid_run:971.60 task_valid:932.18 collect_output:31.05
2023-01-12 02:00:15 - train.py[line:549] - INFO: 1800 / 6234
2023-01-12 02:00:15 - train.py[line:551] - INFO: load:1.07 valid_run:1094.85 task_valid:1048.72 collect_output:36.71
2023-01-12 02:02:16 - train.py[line:549] - INFO: 2000 / 6234
2023-01-12 02:02:16 - train.py[line:551] - INFO: load:1.10 valid_run:1216.09 task_valid:1160.96 collect_output:44.69
2023-01-12 02:04:16 - train.py[line:549] - INFO: 2200 / 6234
2023-01-12 02:04:16 - train.py[line:551] - INFO: load:1.12 valid_run:1335.77 task_valid:1276.08 collect_output:48.21
2023-01-12 02:06:17 - train.py[line:549] - INFO: 2400 / 6234
2023-01-12 02:06:17 - train.py[line:551] - INFO: load:1.15 valid_run:1456.88 task_valid:1392.40 collect_output:51.99
2023-01-12 02:08:16 - train.py[line:549] - INFO: 2600 / 6234
2023-01-12 02:08:16 - train.py[line:551] - INFO: load:1.18 valid_run:1575.34 task_valid:1505.79 collect_output:56.05
2023-01-12 02:10:16 - train.py[line:549] - INFO: 2800 / 6234
2023-01-12 02:10:16 - train.py[line:551] - INFO: load:1.20 valid_run:1695.80 task_valid:1622.88 collect_output:58.41
2023-01-12 02:12:17 - train.py[line:549] - INFO: 3000 / 6234
2023-01-12 02:12:17 - train.py[line:551] - INFO: load:1.23 valid_run:1816.16 task_valid:1738.25 collect_output:62.38
2023-01-12 02:14:17 - train.py[line:549] - INFO: 3200 / 6234
2023-01-12 02:14:17 - train.py[line:551] - INFO: load:1.25 valid_run:1936.77 task_valid:1851.63 collect_output:68.61
2023-01-12 02:16:18 - train.py[line:549] - INFO: 3400 / 6234
2023-01-12 02:16:18 - train.py[line:551] - INFO: load:1.28 valid_run:2057.67 task_valid:1967.23 collect_output:72.88
2023-01-12 02:18:18 - train.py[line:549] - INFO: 3600 / 6234
2023-01-12 02:18:18 - train.py[line:551] - INFO: load:1.30 valid_run:2177.72 task_valid:2084.50 collect_output:74.63
2023-01-12 02:20:19 - train.py[line:549] - INFO: 3800 / 6234
2023-01-12 02:20:19 - train.py[line:551] - INFO: load:1.33 valid_run:2298.35 task_valid:2200.77 collect_output:77.97
2023-01-12 02:22:19 - train.py[line:549] - INFO: 4000 / 6234
2023-01-12 02:22:19 - train.py[line:551] - INFO: load:1.36 valid_run:2418.06 task_valid:2316.51 collect_output:80.93
2023-01-12 02:24:20 - train.py[line:549] - INFO: 4200 / 6234
2023-01-12 02:24:20 - train.py[line:551] - INFO: load:1.38 valid_run:2539.29 task_valid:2432.37 collect_output:85.30
2023-01-12 02:26:22 - train.py[line:549] - INFO: 4400 / 6234
2023-01-12 02:26:22 - train.py[line:551] - INFO: load:1.41 valid_run:2660.62 task_valid:2550.48 collect_output:87.51
2023-01-12 02:28:22 - train.py[line:549] - INFO: 4600 / 6234
2023-01-12 02:28:22 - train.py[line:551] - INFO: load:1.44 valid_run:2780.46 task_valid:2664.25 collect_output:92.57
2023-01-12 02:30:21 - train.py[line:549] - INFO: 4800 / 6234
2023-01-12 02:30:21 - train.py[line:551] - INFO: load:1.46 valid_run:2899.51 task_valid:2779.64 collect_output:95.22
2023-01-12 02:32:22 - train.py[line:549] - INFO: 5000 / 6234
2023-01-12 02:32:22 - train.py[line:551] - INFO: load:1.49 valid_run:3020.54 task_valid:2895.03 collect_output:99.85
2023-01-12 02:34:24 - train.py[line:549] - INFO: 5200 / 6234
2023-01-12 02:34:24 - train.py[line:551] - INFO: load:1.51 valid_run:3142.85 task_valid:3010.31 collect_output:105.83
2023-01-12 02:36:23 - train.py[line:549] - INFO: 5400 / 6234
2023-01-12 02:36:23 - train.py[line:551] - INFO: load:1.54 valid_run:3261.65 task_valid:3123.52 collect_output:110.40
2023-01-12 02:38:24 - train.py[line:549] - INFO: 5600 / 6234
2023-01-12 02:38:24 - train.py[line:551] - INFO: load:1.57 valid_run:3382.89 task_valid:3242.07 collect_output:112.07
2023-01-12 02:40:26 - train.py[line:549] - INFO: 5800 / 6234
2023-01-12 02:40:26 - train.py[line:551] - INFO: load:1.59 valid_run:3504.04 task_valid:3356.94 collect_output:117.34
2023-01-12 02:42:27 - train.py[line:549] - INFO: 6000 / 6234
2023-01-12 02:42:27 - train.py[line:551] - INFO: load:1.62 valid_run:3625.42 task_valid:3474.83 collect_output:119.80
2023-01-12 02:44:28 - train.py[line:549] - INFO: 6200 / 6234
2023-01-12 02:44:28 - train.py[line:551] - INFO: load:1.65 valid_run:3745.84 task_valid:3592.52 collect_output:121.51

====================================================================================================
SGG eval:     R @ 50: 0.5350;     R @ 100: 0.6231;     R @ 500: 0.6690;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3370;    mR @ 100: 0.3936;    mR @ 500: 0.4443;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7756) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3871) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.8987) (says:0.0000) (sitting on:0.7429) (standing on:0.3043) (using:0.6000) (walking in:0.0000) (walking on:0.5225) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5350;     R @ 100: 0.6231;     R @ 500: 0.6690;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3370;    mR @ 100: 0.3936;    mR @ 500: 0.4443;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7756) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3871) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.8987) (says:0.0000) (sitting on:0.7429) (standing on:0.3043) (using:0.6000) (walking in:0.0000) (walking on:0.5225) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-12 02:44:58 - train.py[line:487] - INFO: 0.623081512605042
2023-01-12 02:44:58 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 02:44:59 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.338 | loss_v1 0 | loss_v2 0 | nll_loss 0.18 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.623082 | ppl 1.13 | vqa_score 0.5608 | wps 118.7 | wpb 72 | bsz 24 | num_updates 15000 | best_R@100 0.697584
2023-01-12 02:44:59 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 15000 updates
2023-01-12 02:44:59 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_15000.pt
2023-01-12 02:45:40 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_15000.pt
2023-01-12 02:47:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_15000.pt (epoch 1 @ 15000 updates, score 0.623081512605042) (writing took 134.54567354917526 seconds)
2023-01-12 02:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:30 - progress_bar.py[line:274] - INFO: epoch 001:  15031 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4245, wps=0.4, ups=0, wpb=109.1, bsz=40, num_updates=15010, lr=4.42656e-05, gnorm=0.573, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96168
2023-01-12 02:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:47 - progress_bar.py[line:274] - INFO: epoch 001:  15041 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3505, wps=98.9, ups=0.6, wpb=110.8, bsz=40, num_updates=15020, lr=4.42604e-05, gnorm=1.194, clip=50, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=96185
2023-01-12 02:47:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:47:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:04 - progress_bar.py[line:274] - INFO: epoch 001:  15051 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.367, wps=98, ups=0.6, wpb=108.6, bsz=40, num_updates=15030, lr=4.42552e-05, gnorm=0.845, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=96202
2023-01-12 02:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:20 - progress_bar.py[line:274] - INFO: epoch 001:  15061 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4194, wps=99.8, ups=0.61, wpb=108.9, bsz=40, num_updates=15040, lr=4.425e-05, gnorm=0.759, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96219
2023-01-12 02:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:37 - progress_bar.py[line:274] - INFO: epoch 001:  15071 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3061, wps=98.9, ups=0.6, wpb=110.6, bsz=40, num_updates=15050, lr=4.42448e-05, gnorm=0.408, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=96236
2023-01-12 02:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:48:54 - progress_bar.py[line:274] - INFO: epoch 001:  15081 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3737, wps=103, ups=0.62, wpb=109.9, bsz=40, num_updates=15060, lr=4.42396e-05, gnorm=1.356, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96252
2023-01-12 02:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:10 - progress_bar.py[line:274] - INFO: epoch 001:  15091 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.396, wps=100.7, ups=0.61, wpb=110.4, bsz=40, num_updates=15070, lr=4.42344e-05, gnorm=0.539, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96269
2023-01-12 02:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:27 - progress_bar.py[line:274] - INFO: epoch 001:  15101 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.433, wps=99.6, ups=0.6, wpb=110.2, bsz=40, num_updates=15080, lr=4.42292e-05, gnorm=0.446, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=96286
2023-01-12 02:49:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:43 - progress_bar.py[line:274] - INFO: epoch 001:  15111 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.34, wps=103.3, ups=0.63, wpb=109.9, bsz=40, num_updates=15090, lr=4.4224e-05, gnorm=0.77, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96302
2023-01-12 02:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:49:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:00 - progress_bar.py[line:274] - INFO: epoch 001:  15121 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3391, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=15100, lr=4.42188e-05, gnorm=0.571, clip=10, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=96318
2023-01-12 02:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:16 - progress_bar.py[line:274] - INFO: epoch 001:  15131 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3137, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=15110, lr=4.42135e-05, gnorm=2.015, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96335
2023-01-12 02:50:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:32 - progress_bar.py[line:274] - INFO: epoch 001:  15141 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4528, wps=101.7, ups=0.62, wpb=109.4, bsz=40, num_updates=15120, lr=4.42083e-05, gnorm=0.774, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96351
2023-01-12 02:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:49 - progress_bar.py[line:274] - INFO: epoch 001:  15151 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3846, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=15130, lr=4.42031e-05, gnorm=0.73, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96368
2023-01-12 02:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:06 - progress_bar.py[line:274] - INFO: epoch 001:  15161 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3678, wps=100.6, ups=0.61, wpb=110.3, bsz=40, num_updates=15140, lr=4.41979e-05, gnorm=0.635, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96384
2023-01-12 02:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:22 - progress_bar.py[line:274] - INFO: epoch 001:  15171 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3273, wps=100.9, ups=0.62, wpb=109.3, bsz=40, num_updates=15150, lr=4.41927e-05, gnorm=1.233, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96401
2023-01-12 02:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:39 - progress_bar.py[line:274] - INFO: epoch 001:  15181 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.48, wps=102.5, ups=0.62, wpb=110.6, bsz=40, num_updates=15160, lr=4.41875e-05, gnorm=1.179, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96417
2023-01-12 02:51:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:51:55 - progress_bar.py[line:274] - INFO: epoch 001:  15191 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3725, wps=100.6, ups=0.6, wpb=111, bsz=40, num_updates=15170, lr=4.41823e-05, gnorm=0.51, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=96434
2023-01-12 02:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:12 - progress_bar.py[line:274] - INFO: epoch 001:  15201 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3711, wps=99.7, ups=0.6, wpb=109.9, bsz=40, num_updates=15180, lr=4.41771e-05, gnorm=1.021, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96451
2023-01-12 02:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:29 - progress_bar.py[line:274] - INFO: epoch 001:  15211 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4037, wps=100.4, ups=0.61, wpb=109.3, bsz=40, num_updates=15190, lr=4.41719e-05, gnorm=1.781, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96467
2023-01-12 02:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:45 - progress_bar.py[line:274] - INFO: epoch 001:  15221 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3396, wps=102.3, ups=0.63, wpb=108.6, bsz=40, num_updates=15200, lr=4.41667e-05, gnorm=1.306, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96484
2023-01-12 02:52:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:01 - progress_bar.py[line:274] - INFO: epoch 001:  15231 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4486, wps=102.2, ups=0.62, wpb=109.4, bsz=40, num_updates=15210, lr=4.41615e-05, gnorm=1.422, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96500
2023-01-12 02:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:18 - progress_bar.py[line:274] - INFO: epoch 001:  15241 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4245, wps=101.2, ups=0.62, wpb=108.9, bsz=40, num_updates=15220, lr=4.41562e-05, gnorm=0.396, clip=10, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=96516
2023-01-12 02:53:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:34 - progress_bar.py[line:274] - INFO: epoch 001:  15251 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4216, wps=102.3, ups=0.62, wpb=109.5, bsz=40, num_updates=15230, lr=4.4151e-05, gnorm=2.479, clip=30, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=96533
2023-01-12 02:53:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:50 - progress_bar.py[line:274] - INFO: epoch 001:  15261 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3958, wps=102.4, ups=0.61, wpb=111.2, bsz=40, num_updates=15240, lr=4.41458e-05, gnorm=0.78, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96549
2023-01-12 02:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:53:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:07 - progress_bar.py[line:274] - INFO: epoch 001:  15271 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3232, wps=100, ups=0.61, wpb=109.2, bsz=40, num_updates=15250, lr=4.41406e-05, gnorm=1.066, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96566
2023-01-12 02:54:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:24 - progress_bar.py[line:274] - INFO: epoch 001:  15281 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.733, nsentences=40, sample_size=107.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3394, wps=97.7, ups=0.6, wpb=107.7, bsz=40, num_updates=15260, lr=4.41354e-05, gnorm=0.973, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96582
2023-01-12 02:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:40 - progress_bar.py[line:274] - INFO: epoch 001:  15291 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4324, wps=102.2, ups=0.62, wpb=109.7, bsz=40, num_updates=15270, lr=4.41302e-05, gnorm=1.368, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96599
2023-01-12 02:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:54:57 - progress_bar.py[line:274] - INFO: epoch 001:  15301 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2449, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=15280, lr=4.4125e-05, gnorm=0.709, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96615
2023-01-12 02:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:13 - progress_bar.py[line:274] - INFO: epoch 001:  15311 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4271, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=15290, lr=4.41198e-05, gnorm=0.44, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96632
2023-01-12 02:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:29 - progress_bar.py[line:274] - INFO: epoch 001:  15321 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.37, wps=102.4, ups=0.62, wpb=109.5, bsz=40, num_updates=15300, lr=4.41146e-05, gnorm=1.125, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96648
2023-01-12 02:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:46 - progress_bar.py[line:274] - INFO: epoch 001:  15331 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3407, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=15310, lr=4.41094e-05, gnorm=0.474, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=96665
2023-01-12 02:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:03 - progress_bar.py[line:274] - INFO: epoch 001:  15341 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3407, wps=98.3, ups=0.6, wpb=110, bsz=40, num_updates=15320, lr=4.41042e-05, gnorm=0.511, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=96682
2023-01-12 02:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:20 - progress_bar.py[line:274] - INFO: epoch 001:  15351 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2842, wps=100, ups=0.61, wpb=109.7, bsz=40, num_updates=15330, lr=4.4099e-05, gnorm=0.491, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96698
2023-01-12 02:56:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:36 - progress_bar.py[line:274] - INFO: epoch 001:  15361 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4574, wps=103.6, ups=0.63, wpb=109.7, bsz=40, num_updates=15340, lr=4.40937e-05, gnorm=0.659, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96714
2023-01-12 02:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:52 - progress_bar.py[line:274] - INFO: epoch 001:  15371 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3895, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=15350, lr=4.40885e-05, gnorm=0.453, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96731
2023-01-12 02:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:09 - progress_bar.py[line:274] - INFO: epoch 001:  15381 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3905, wps=100.6, ups=0.62, wpb=108.2, bsz=40, num_updates=15360, lr=4.40833e-05, gnorm=0.523, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96747
2023-01-12 02:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:25 - progress_bar.py[line:274] - INFO: epoch 001:  15391 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4362, wps=103.9, ups=0.62, wpb=111.3, bsz=40, num_updates=15370, lr=4.40781e-05, gnorm=0.833, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96764
2023-01-12 02:57:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:41 - progress_bar.py[line:274] - INFO: epoch 001:  15401 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.359, wps=98.4, ups=0.61, wpb=107.5, bsz=40, num_updates=15380, lr=4.40729e-05, gnorm=0.267, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96780
2023-01-12 02:57:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:57:58 - progress_bar.py[line:274] - INFO: epoch 001:  15411 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4792, wps=102.7, ups=0.61, wpb=111.5, bsz=40, num_updates=15390, lr=4.40677e-05, gnorm=0.707, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96797
2023-01-12 02:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:14 - progress_bar.py[line:274] - INFO: epoch 001:  15421 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4216, wps=102.1, ups=0.62, wpb=110.5, bsz=40, num_updates=15400, lr=4.40625e-05, gnorm=0.478, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=96813
2023-01-12 02:58:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:31 - progress_bar.py[line:274] - INFO: epoch 001:  15431 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3708, wps=102.7, ups=0.62, wpb=111.1, bsz=40, num_updates=15410, lr=4.40573e-05, gnorm=0.769, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96830
2023-01-12 02:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:47 - progress_bar.py[line:274] - INFO: epoch 001:  15441 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.375, wps=101.6, ups=0.62, wpb=108.9, bsz=40, num_updates=15420, lr=4.40521e-05, gnorm=1.645, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96846
2023-01-12 02:58:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:04 - progress_bar.py[line:274] - INFO: epoch 001:  15451 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4062, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=15430, lr=4.40469e-05, gnorm=1.51, clip=50, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96862
2023-01-12 02:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:20 - progress_bar.py[line:274] - INFO: epoch 001:  15461 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5377, wps=102.3, ups=0.62, wpb=109.5, bsz=40, num_updates=15440, lr=4.40417e-05, gnorm=0.625, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96879
2023-01-12 02:59:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:37 - progress_bar.py[line:274] - INFO: epoch 001:  15471 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4158, wps=100.5, ups=0.61, wpb=109.6, bsz=40, num_updates=15450, lr=4.40365e-05, gnorm=0.768, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=96895
2023-01-12 02:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 02:59:53 - progress_bar.py[line:274] - INFO: epoch 001:  15481 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.404, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=15460, lr=4.40313e-05, gnorm=0.695, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96912
2023-01-12 02:59:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:10 - progress_bar.py[line:274] - INFO: epoch 001:  15491 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4, wps=100.8, ups=0.61, wpb=109.8, bsz=40, num_updates=15470, lr=4.4026e-05, gnorm=0.528, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96928
2023-01-12 03:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:26 - progress_bar.py[line:274] - INFO: epoch 001:  15501 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.398, wps=103, ups=0.62, wpb=110.3, bsz=40, num_updates=15480, lr=4.40208e-05, gnorm=1.278, clip=50, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=96945
2023-01-12 03:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:43 - progress_bar.py[line:274] - INFO: epoch 001:  15511 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4175, wps=100.7, ups=0.61, wpb=110.3, bsz=40, num_updates=15490, lr=4.40156e-05, gnorm=1.379, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96961
2023-01-12 03:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:00:59 - progress_bar.py[line:274] - INFO: epoch 001:  15521 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4348, wps=98.1, ups=0.6, wpb=108.5, bsz=40, num_updates=15500, lr=4.40104e-05, gnorm=1.635, clip=50, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=96978
2023-01-12 03:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:16 - progress_bar.py[line:274] - INFO: epoch 001:  15531 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3627, wps=103.1, ups=0.63, wpb=109.9, bsz=40, num_updates=15510, lr=4.40052e-05, gnorm=1.017, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96994
2023-01-12 03:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:32 - progress_bar.py[line:274] - INFO: epoch 001:  15541 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3333, wps=103.7, ups=0.63, wpb=108.9, bsz=40, num_updates=15520, lr=4.4e-05, gnorm=0.606, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97010
2023-01-12 03:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:48 - progress_bar.py[line:274] - INFO: epoch 001:  15551 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3191, wps=102.2, ups=0.62, wpb=109.5, bsz=40, num_updates=15530, lr=4.39948e-05, gnorm=0.51, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=97027
2023-01-12 03:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:04 - progress_bar.py[line:274] - INFO: epoch 001:  15561 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4138, wps=103.3, ups=0.62, wpb=111.7, bsz=40, num_updates=15540, lr=4.39896e-05, gnorm=0.747, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=97043
2023-01-12 03:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:21 - progress_bar.py[line:274] - INFO: epoch 001:  15571 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3725, wps=98.7, ups=0.6, wpb=110.3, bsz=40, num_updates=15550, lr=4.39844e-05, gnorm=0.984, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=97060
2023-01-12 03:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:38 - progress_bar.py[line:274] - INFO: epoch 001:  15581 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4038, wps=96.6, ups=0.59, wpb=108.3, bsz=40, num_updates=15560, lr=4.39792e-05, gnorm=0.535, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=97077
2023-01-12 03:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:02:55 - progress_bar.py[line:274] - INFO: epoch 001:  15591 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4227, wps=99.3, ups=0.61, wpb=109.3, bsz=40, num_updates=15570, lr=4.3974e-05, gnorm=1.124, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97094
2023-01-12 03:03:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:11 - progress_bar.py[line:274] - INFO: epoch 001:  15601 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.42, wps=102.1, ups=0.62, wpb=109.7, bsz=40, num_updates=15580, lr=4.39688e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=97110
2023-01-12 03:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:28 - progress_bar.py[line:274] - INFO: epoch 001:  15611 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2796, wps=100.9, ups=0.6, wpb=111.6, bsz=40, num_updates=15590, lr=4.39635e-05, gnorm=0.932, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=97127
2023-01-12 03:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:44 - progress_bar.py[line:274] - INFO: epoch 001:  15621 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4286, wps=102.3, ups=0.62, wpb=109.1, bsz=40, num_updates=15600, lr=4.39583e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=97143
2023-01-12 03:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:01 - progress_bar.py[line:274] - INFO: epoch 001:  15631 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4273, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=15610, lr=4.39531e-05, gnorm=0.46, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=97160
2023-01-12 03:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:18 - progress_bar.py[line:274] - INFO: epoch 001:  15641 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.301, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=15620, lr=4.39479e-05, gnorm=0.492, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97176
2023-01-12 03:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:34 - progress_bar.py[line:274] - INFO: epoch 001:  15651 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3883, wps=103.3, ups=0.62, wpb=110.2, bsz=40, num_updates=15630, lr=4.39427e-05, gnorm=0.907, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97193
2023-01-12 03:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:51 - progress_bar.py[line:274] - INFO: epoch 001:  15661 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3964, wps=99.5, ups=0.61, wpb=108.8, bsz=40, num_updates=15640, lr=4.39375e-05, gnorm=0.33, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=97209
2023-01-12 03:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:07 - progress_bar.py[line:274] - INFO: epoch 001:  15671 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4135, wps=100.8, ups=0.61, wpb=109.5, bsz=40, num_updates=15650, lr=4.39323e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=97226
2023-01-12 03:05:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:24 - progress_bar.py[line:274] - INFO: epoch 001:  15681 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3878, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=15660, lr=4.39271e-05, gnorm=0.6, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97243
2023-01-12 03:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:40 - progress_bar.py[line:274] - INFO: epoch 001:  15691 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.667, nsentences=40, sample_size=106.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4587, wps=97.6, ups=0.61, wpb=106.7, bsz=40, num_updates=15670, lr=4.39219e-05, gnorm=0.668, clip=30, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=97259
2023-01-12 03:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:05:57 - progress_bar.py[line:274] - INFO: epoch 001:  15701 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4412, wps=102.4, ups=0.62, wpb=110.9, bsz=40, num_updates=15680, lr=4.39167e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=97276
2023-01-12 03:06:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:14 - progress_bar.py[line:274] - INFO: epoch 001:  15711 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3367, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=15690, lr=4.39115e-05, gnorm=0.513, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=97292
2023-01-12 03:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:30 - progress_bar.py[line:274] - INFO: epoch 001:  15721 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4337, wps=106.2, ups=0.63, wpb=112, bsz=40, num_updates=15700, lr=4.39063e-05, gnorm=1.997, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97309
2023-01-12 03:06:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:46 - progress_bar.py[line:274] - INFO: epoch 001:  15731 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4019, wps=103.3, ups=0.63, wpb=109, bsz=40, num_updates=15710, lr=4.3901e-05, gnorm=0.529, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97325
2023-01-12 03:06:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:03 - progress_bar.py[line:274] - INFO: epoch 001:  15741 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3878, wps=98.7, ups=0.6, wpb=109.2, bsz=40, num_updates=15720, lr=4.38958e-05, gnorm=1.106, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=97341
2023-01-12 03:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:19 - progress_bar.py[line:274] - INFO: epoch 001:  15751 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4043, wps=101.6, ups=0.61, wpb=110.9, bsz=40, num_updates=15730, lr=4.38906e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=97358
2023-01-12 03:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:36 - progress_bar.py[line:274] - INFO: epoch 001:  15761 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4537, wps=102.9, ups=0.62, wpb=111.1, bsz=40, num_updates=15740, lr=4.38854e-05, gnorm=0.683, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97374
2023-01-12 03:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:52 - progress_bar.py[line:274] - INFO: epoch 001:  15771 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4167, wps=102.5, ups=0.62, wpb=110.5, bsz=40, num_updates=15750, lr=4.38802e-05, gnorm=1.047, clip=30, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=97391
2023-01-12 03:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:09 - progress_bar.py[line:274] - INFO: epoch 001:  15781 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4095, wps=99.5, ups=0.6, wpb=109.7, bsz=40, num_updates=15760, lr=4.3875e-05, gnorm=0.667, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97408
2023-01-12 03:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:26 - progress_bar.py[line:274] - INFO: epoch 001:  15791 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3398, wps=99.8, ups=0.6, wpb=110.4, bsz=40, num_updates=15770, lr=4.38698e-05, gnorm=0.492, clip=10, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=97424
2023-01-12 03:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:43 - progress_bar.py[line:274] - INFO: epoch 001:  15801 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4476, wps=98.7, ups=0.61, wpb=108, bsz=40, num_updates=15780, lr=4.38646e-05, gnorm=0.525, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97441
2023-01-12 03:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:08:59 - progress_bar.py[line:274] - INFO: epoch 001:  15811 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.37, wps=101.7, ups=0.62, wpb=109.7, bsz=40, num_updates=15790, lr=4.38594e-05, gnorm=0.628, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97458
2023-01-12 03:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:16 - progress_bar.py[line:274] - INFO: epoch 001:  15821 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3084, wps=99.6, ups=0.61, wpb=109.5, bsz=40, num_updates=15800, lr=4.38542e-05, gnorm=0.65, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97474
2023-01-12 03:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:32 - progress_bar.py[line:274] - INFO: epoch 001:  15831 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3368, wps=102.3, ups=0.61, wpb=110.9, bsz=40, num_updates=15810, lr=4.3849e-05, gnorm=0.542, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97491
2023-01-12 03:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:49 - progress_bar.py[line:274] - INFO: epoch 001:  15841 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4255, wps=101, ups=0.61, wpb=111.1, bsz=40, num_updates=15820, lr=4.38437e-05, gnorm=1.233, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=97508
2023-01-12 03:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:05 - progress_bar.py[line:274] - INFO: epoch 001:  15851 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2718, wps=100.5, ups=0.61, wpb=109.2, bsz=40, num_updates=15830, lr=4.38385e-05, gnorm=0.688, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=97524
2023-01-12 03:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:22 - progress_bar.py[line:274] - INFO: epoch 001:  15861 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4261, wps=98.9, ups=0.61, wpb=108.3, bsz=40, num_updates=15840, lr=4.38333e-05, gnorm=1.591, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97541
2023-01-12 03:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:38 - progress_bar.py[line:274] - INFO: epoch 001:  15871 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4271, wps=103.1, ups=0.62, wpb=110.7, bsz=40, num_updates=15850, lr=4.38281e-05, gnorm=0.49, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=97557
2023-01-12 03:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:10:54 - progress_bar.py[line:274] - INFO: epoch 001:  15881 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3261, wps=106.6, ups=0.64, wpb=111.7, bsz=40, num_updates=15860, lr=4.38229e-05, gnorm=0.653, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=97573
2023-01-12 03:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:11 - progress_bar.py[line:274] - INFO: epoch 001:  15891 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4239, wps=103, ups=0.62, wpb=110.2, bsz=40, num_updates=15870, lr=4.38177e-05, gnorm=0.827, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97589
2023-01-12 03:11:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:27 - progress_bar.py[line:274] - INFO: epoch 001:  15901 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.41, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=15880, lr=4.38125e-05, gnorm=0.842, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97606
2023-01-12 03:11:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:43 - progress_bar.py[line:274] - INFO: epoch 001:  15911 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4167, wps=104.7, ups=0.63, wpb=111.1, bsz=40, num_updates=15890, lr=4.38073e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97622
2023-01-12 03:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:00 - progress_bar.py[line:274] - INFO: epoch 001:  15921 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4151, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=15900, lr=4.38021e-05, gnorm=0.884, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97638
2023-01-12 03:12:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:16 - progress_bar.py[line:274] - INFO: epoch 001:  15931 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3333, wps=101.6, ups=0.62, wpb=109.9, bsz=40, num_updates=15910, lr=4.37969e-05, gnorm=0.566, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97655
2023-01-12 03:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:33 - progress_bar.py[line:274] - INFO: epoch 001:  15941 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4455, wps=102, ups=0.62, wpb=110.5, bsz=40, num_updates=15920, lr=4.37917e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=97671
2023-01-12 03:12:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:50 - progress_bar.py[line:274] - INFO: epoch 001:  15951 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.451, wps=98.5, ups=0.6, wpb=109.3, bsz=40, num_updates=15930, lr=4.37865e-05, gnorm=1.078, clip=20, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=97688
2023-01-12 03:12:51 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-12 03:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:07 - progress_bar.py[line:274] - INFO: epoch 001:  15962 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4245, wps=96.5, ups=0.59, wpb=109.8, bsz=40, num_updates=15940, lr=4.37813e-05, gnorm=1.165, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=97706
2023-01-12 03:13:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:24 - progress_bar.py[line:274] - INFO: epoch 001:  15972 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3617, wps=100.5, ups=0.61, wpb=110.5, bsz=40, num_updates=15950, lr=4.3776e-05, gnorm=0.838, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97722
2023-01-12 03:13:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:40 - progress_bar.py[line:274] - INFO: epoch 001:  15982 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4382, wps=103.9, ups=0.62, wpb=111.5, bsz=40, num_updates=15960, lr=4.37708e-05, gnorm=0.636, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97739
2023-01-12 03:13:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:13:57 - progress_bar.py[line:274] - INFO: epoch 001:  15992 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3684, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=15970, lr=4.37656e-05, gnorm=0.805, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97755
2023-01-12 03:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:13 - progress_bar.py[line:274] - INFO: epoch 001:  16002 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3587, wps=103.5, ups=0.62, wpb=111, bsz=40, num_updates=15980, lr=4.37604e-05, gnorm=0.959, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=97772
2023-01-12 03:14:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:30 - progress_bar.py[line:274] - INFO: epoch 001:  16012 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3235, wps=99.1, ups=0.61, wpb=108.7, bsz=40, num_updates=15990, lr=4.37552e-05, gnorm=1.133, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=97788
2023-01-12 03:14:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 03:14:46 - progress_bar.py[line:274] - INFO: epoch 001:  16022 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3238, wps=104.5, ups=0.63, wpb=110.2, bsz=40, num_updates=16000, lr=4.375e-05, gnorm=1.063, clip=60, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=97804
2023-01-12 03:14:46 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 03:14:47 - train.py[line:549] - INFO: 0 / 6234
2023-01-12 03:14:47 - train.py[line:551] - INFO: load:0.98 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 03:16:48 - train.py[line:549] - INFO: 200 / 6234
2023-01-12 03:16:48 - train.py[line:551] - INFO: load:1.00 valid_run:121.29 task_valid:118.52 collect_output:1.74
2023-01-12 03:18:48 - train.py[line:549] - INFO: 400 / 6234
2023-01-12 03:18:48 - train.py[line:551] - INFO: load:1.03 valid_run:240.73 task_valid:233.76 collect_output:4.96
2023-01-12 03:20:49 - train.py[line:549] - INFO: 600 / 6234
2023-01-12 03:20:49 - train.py[line:551] - INFO: load:1.05 valid_run:362.26 task_valid:349.74 collect_output:9.49
2023-01-12 03:22:51 - train.py[line:549] - INFO: 800 / 6234
2023-01-12 03:22:51 - train.py[line:551] - INFO: load:1.07 valid_run:483.75 task_valid:462.86 collect_output:16.88
2023-01-12 03:24:51 - train.py[line:549] - INFO: 1000 / 6234
2023-01-12 03:24:51 - train.py[line:551] - INFO: load:1.10 valid_run:603.72 task_valid:579.67 collect_output:19.03
2023-01-12 03:26:53 - train.py[line:549] - INFO: 1200 / 6234
2023-01-12 03:26:53 - train.py[line:551] - INFO: load:1.12 valid_run:726.12 task_valid:697.91 collect_output:22.20
2023-01-12 03:28:56 - train.py[line:549] - INFO: 1400 / 6234
2023-01-12 03:28:56 - train.py[line:551] - INFO: load:1.14 valid_run:848.73 task_valid:815.61 collect_output:26.13
2023-01-12 03:30:58 - train.py[line:549] - INFO: 1600 / 6234
2023-01-12 03:30:58 - train.py[line:551] - INFO: load:1.17 valid_run:970.26 task_valid:931.71 collect_output:30.57
2023-01-12 03:33:01 - train.py[line:549] - INFO: 1800 / 6234
2023-01-12 03:33:01 - train.py[line:551] - INFO: load:1.19 valid_run:1093.59 task_valid:1048.25 collect_output:36.37
2023-01-12 03:35:02 - train.py[line:549] - INFO: 2000 / 6234
2023-01-12 03:35:02 - train.py[line:551] - INFO: load:1.21 valid_run:1214.84 task_valid:1160.37 collect_output:44.51
2023-01-12 03:37:02 - train.py[line:549] - INFO: 2200 / 6234
2023-01-12 03:37:02 - train.py[line:551] - INFO: load:1.24 valid_run:1334.53 task_valid:1275.54 collect_output:48.01
2023-01-12 03:39:03 - train.py[line:549] - INFO: 2400 / 6234
2023-01-12 03:39:03 - train.py[line:551] - INFO: load:1.26 valid_run:1455.64 task_valid:1391.81 collect_output:51.86
2023-01-12 03:41:02 - train.py[line:549] - INFO: 2600 / 6234
2023-01-12 03:41:02 - train.py[line:551] - INFO: load:1.28 valid_run:1574.03 task_valid:1505.15 collect_output:55.90
2023-01-12 03:43:02 - train.py[line:549] - INFO: 2800 / 6234
2023-01-12 03:43:02 - train.py[line:551] - INFO: load:1.31 valid_run:1694.28 task_valid:1622.26 collect_output:58.05
2023-01-12 03:45:03 - train.py[line:549] - INFO: 3000 / 6234
2023-01-12 03:45:03 - train.py[line:551] - INFO: load:1.33 valid_run:1814.71 task_valid:1737.89 collect_output:61.85
2023-01-12 03:47:03 - train.py[line:549] - INFO: 3200 / 6234
2023-01-12 03:47:03 - train.py[line:551] - INFO: load:1.36 valid_run:1935.27 task_valid:1851.30 collect_output:68.01
2023-01-12 03:49:04 - train.py[line:549] - INFO: 3400 / 6234
2023-01-12 03:49:04 - train.py[line:551] - INFO: load:1.38 valid_run:2056.00 task_valid:1966.86 collect_output:72.20
2023-01-12 03:51:04 - train.py[line:549] - INFO: 3600 / 6234
2023-01-12 03:51:04 - train.py[line:551] - INFO: load:1.41 valid_run:2176.12 task_valid:2084.22 collect_output:73.94
2023-01-12 03:53:05 - train.py[line:549] - INFO: 3800 / 6234
2023-01-12 03:53:05 - train.py[line:551] - INFO: load:1.43 valid_run:2296.87 task_valid:2200.77 collect_output:77.14
2023-01-12 03:55:05 - train.py[line:549] - INFO: 4000 / 6234
2023-01-12 03:55:05 - train.py[line:551] - INFO: load:1.46 valid_run:2416.72 task_valid:2316.71 collect_output:80.06
2023-01-12 03:57:06 - train.py[line:549] - INFO: 4200 / 6234
2023-01-12 03:57:06 - train.py[line:551] - INFO: load:1.48 valid_run:2537.83 task_valid:2432.46 collect_output:84.43
2023-01-12 03:59:08 - train.py[line:549] - INFO: 4400 / 6234
2023-01-12 03:59:08 - train.py[line:551] - INFO: load:1.51 valid_run:2659.46 task_valid:2550.91 collect_output:86.60
2023-01-12 04:01:08 - train.py[line:549] - INFO: 4600 / 6234
2023-01-12 04:01:08 - train.py[line:551] - INFO: load:1.53 valid_run:2779.18 task_valid:2664.61 collect_output:91.63
2023-01-12 04:03:07 - train.py[line:549] - INFO: 4800 / 6234
2023-01-12 04:03:07 - train.py[line:551] - INFO: load:1.55 valid_run:2898.34 task_valid:2780.14 collect_output:94.26
2023-01-12 04:05:08 - train.py[line:549] - INFO: 5000 / 6234
2023-01-12 04:05:08 - train.py[line:551] - INFO: load:1.58 valid_run:3019.31 task_valid:2895.63 collect_output:98.75
2023-01-12 04:07:11 - train.py[line:549] - INFO: 5200 / 6234
2023-01-12 04:07:11 - train.py[line:551] - INFO: load:1.60 valid_run:3141.63 task_valid:3010.99 collect_output:104.71
2023-01-12 04:09:09 - train.py[line:549] - INFO: 5400 / 6234
2023-01-12 04:09:09 - train.py[line:551] - INFO: load:1.63 valid_run:3260.48 task_valid:3124.49 collect_output:109.06
2023-01-12 04:11:11 - train.py[line:549] - INFO: 5600 / 6234
2023-01-12 04:11:11 - train.py[line:551] - INFO: load:1.65 valid_run:3381.65 task_valid:3243.24 collect_output:110.48
2023-01-12 04:13:12 - train.py[line:549] - INFO: 5800 / 6234
2023-01-12 04:13:12 - train.py[line:551] - INFO: load:1.68 valid_run:3502.71 task_valid:3358.10 collect_output:115.69
2023-01-12 04:15:13 - train.py[line:549] - INFO: 6000 / 6234
2023-01-12 04:15:13 - train.py[line:551] - INFO: load:1.70 valid_run:3624.05 task_valid:3475.98 collect_output:118.13
2023-01-12 04:17:14 - train.py[line:549] - INFO: 6200 / 6234
2023-01-12 04:17:14 - train.py[line:551] - INFO: load:1.73 valid_run:3744.47 task_valid:3593.76 collect_output:119.77

====================================================================================================
SGG eval:     R @ 50: 0.5162;     R @ 100: 0.6107;     R @ 500: 0.6555;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3270;    mR @ 100: 0.3874;    mR @ 500: 0.4362;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7756) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3871) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.8889) (says:0.0000) (sitting on:0.7259) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.4865) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5162;     R @ 100: 0.6107;     R @ 500: 0.6555;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3270;    mR @ 100: 0.3874;    mR @ 500: 0.4362;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7756) (covered in:0.7500) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3871) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.8889) (says:0.0000) (sitting on:0.7259) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.4865) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-01-12 04:17:44 - train.py[line:487] - INFO: 0.6107481792717087
2023-01-12 04:17:45 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 04:17:45 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.33 | loss_v1 0 | loss_v2 0 | nll_loss 0.175 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.610748 | ppl 1.13 | vqa_score 0.5507 | wps 118.7 | wpb 72 | bsz 24 | num_updates 16000 | best_R@100 0.697584
2023-01-12 04:17:45 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 16000 updates
2023-01-12 04:17:45 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-12 04:18:23 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-12 04:19:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt (epoch 1 @ 16000 updates, score 0.6107481792717087) (writing took 122.52032830193639 seconds)
2023-01-12 04:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:05 - progress_bar.py[line:274] - INFO: epoch 001:  16032 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.36, wps=0.4, ups=0, wpb=110.7, bsz=40, num_updates=16010, lr=4.37448e-05, gnorm=0.501, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101723
2023-01-12 04:20:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:21 - progress_bar.py[line:274] - INFO: epoch 001:  16042 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3818, wps=101.9, ups=0.61, wpb=110.5, bsz=40, num_updates=16020, lr=4.37396e-05, gnorm=0.391, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101740
2023-01-12 04:20:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:38 - progress_bar.py[line:274] - INFO: epoch 001:  16052 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.402, wps=101, ups=0.61, wpb=109.7, bsz=40, num_updates=16030, lr=4.37344e-05, gnorm=0.536, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101757
2023-01-12 04:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:20:54 - progress_bar.py[line:274] - INFO: epoch 001:  16062 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3429, wps=103.4, ups=0.62, wpb=111.1, bsz=40, num_updates=16040, lr=4.37292e-05, gnorm=0.765, clip=20, loss_scale=256, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=101773
2023-01-12 04:20:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:11 - progress_bar.py[line:274] - INFO: epoch 001:  16072 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3034, wps=101.1, ups=0.61, wpb=111, bsz=40, num_updates=16050, lr=4.3724e-05, gnorm=1.058, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=101790
2023-01-12 04:21:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:28 - progress_bar.py[line:274] - INFO: epoch 001:  16082 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3558, wps=98.5, ups=0.6, wpb=109.1, bsz=40, num_updates=16060, lr=4.37188e-05, gnorm=0.665, clip=30, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=101807
2023-01-12 04:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:45 - progress_bar.py[line:274] - INFO: epoch 001:  16092 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3725, wps=98.9, ups=0.6, wpb=109.5, bsz=40, num_updates=16070, lr=4.37135e-05, gnorm=0.548, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=101824
2023-01-12 04:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:01 - progress_bar.py[line:274] - INFO: epoch 001:  16102 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4242, wps=103.7, ups=0.62, wpb=111.1, bsz=40, num_updates=16080, lr=4.37083e-05, gnorm=0.49, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101840
2023-01-12 04:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  16112 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.398, wps=102.2, ups=0.62, wpb=109.3, bsz=40, num_updates=16090, lr=4.37031e-05, gnorm=0.855, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=101856
2023-01-12 04:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:34 - progress_bar.py[line:274] - INFO: epoch 001:  16122 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3535, wps=103.1, ups=0.62, wpb=110.9, bsz=40, num_updates=16100, lr=4.36979e-05, gnorm=1.015, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=101872
2023-01-12 04:22:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:50 - progress_bar.py[line:274] - INFO: epoch 001:  16132 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3855, wps=102.6, ups=0.62, wpb=110.7, bsz=40, num_updates=16110, lr=4.36927e-05, gnorm=0.534, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=101889
2023-01-12 04:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:22:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:07 - progress_bar.py[line:274] - INFO: epoch 001:  16142 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3053, wps=99.4, ups=0.61, wpb=108.8, bsz=40, num_updates=16120, lr=4.36875e-05, gnorm=0.423, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101906
2023-01-12 04:23:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:23 - progress_bar.py[line:274] - INFO: epoch 001:  16152 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3725, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=16130, lr=4.36823e-05, gnorm=0.485, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101922
2023-01-12 04:23:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:40 - progress_bar.py[line:274] - INFO: epoch 001:  16162 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4364, wps=102.1, ups=0.62, wpb=109.3, bsz=40, num_updates=16140, lr=4.36771e-05, gnorm=0.596, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=101939
2023-01-12 04:23:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:23:56 - progress_bar.py[line:274] - INFO: epoch 001:  16172 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4167, wps=100, ups=0.62, wpb=108.3, bsz=40, num_updates=16150, lr=4.36719e-05, gnorm=0.634, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101955
2023-01-12 04:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:13 - progress_bar.py[line:274] - INFO: epoch 001:  16182 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4314, wps=99.9, ups=0.61, wpb=109.6, bsz=40, num_updates=16160, lr=4.36667e-05, gnorm=0.722, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101972
2023-01-12 04:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:29 - progress_bar.py[line:274] - INFO: epoch 001:  16192 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3684, wps=104.2, ups=0.63, wpb=110.6, bsz=40, num_updates=16170, lr=4.36615e-05, gnorm=1.286, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=101988
2023-01-12 04:24:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:45 - progress_bar.py[line:274] - INFO: epoch 001:  16202 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3565, wps=102.1, ups=0.62, wpb=109.5, bsz=40, num_updates=16180, lr=4.36563e-05, gnorm=0.646, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102004
2023-01-12 04:24:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:24:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:02 - progress_bar.py[line:274] - INFO: epoch 001:  16212 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4433, wps=100.5, ups=0.61, wpb=110, bsz=40, num_updates=16190, lr=4.3651e-05, gnorm=0.616, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=102021
2023-01-12 04:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:19 - progress_bar.py[line:274] - INFO: epoch 001:  16222 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3204, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=16200, lr=4.36458e-05, gnorm=0.512, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102037
2023-01-12 04:25:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:35 - progress_bar.py[line:274] - INFO: epoch 001:  16232 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3978, wps=103.2, ups=0.62, wpb=111.1, bsz=40, num_updates=16210, lr=4.36406e-05, gnorm=0.448, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102054
2023-01-12 04:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:52 - progress_bar.py[line:274] - INFO: epoch 001:  16242 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3524, wps=96.3, ups=0.6, wpb=107.7, bsz=40, num_updates=16220, lr=4.36354e-05, gnorm=0.419, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=102071
2023-01-12 04:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:08 - progress_bar.py[line:274] - INFO: epoch 001:  16252 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4245, wps=100.7, ups=0.62, wpb=108.7, bsz=40, num_updates=16230, lr=4.36302e-05, gnorm=0.727, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102087
2023-01-12 04:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:25 - progress_bar.py[line:274] - INFO: epoch 001:  16262 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.37, wps=105.5, ups=0.63, wpb=111.1, bsz=40, num_updates=16240, lr=4.3625e-05, gnorm=0.594, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=102103
2023-01-12 04:26:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:41 - progress_bar.py[line:274] - INFO: epoch 001:  16272 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3964, wps=99.8, ups=0.6, wpb=110.3, bsz=40, num_updates=16250, lr=4.36198e-05, gnorm=0.495, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=102120
2023-01-12 04:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:26:58 - progress_bar.py[line:274] - INFO: epoch 001:  16282 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4082, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=16260, lr=4.36146e-05, gnorm=1.21, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=102137
2023-01-12 04:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:15 - progress_bar.py[line:274] - INFO: epoch 001:  16292 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.404, wps=99.2, ups=0.61, wpb=108.8, bsz=40, num_updates=16270, lr=4.36094e-05, gnorm=1.226, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102153
2023-01-12 04:27:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:32 - progress_bar.py[line:274] - INFO: epoch 001:  16302 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4348, wps=100.6, ups=0.6, wpb=111.5, bsz=40, num_updates=16280, lr=4.36042e-05, gnorm=0.362, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=102170
2023-01-12 04:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:48 - progress_bar.py[line:274] - INFO: epoch 001:  16312 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4216, wps=100.6, ups=0.61, wpb=110, bsz=40, num_updates=16290, lr=4.3599e-05, gnorm=0.501, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102187
2023-01-12 04:27:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:05 - progress_bar.py[line:274] - INFO: epoch 001:  16322 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3738, wps=101.1, ups=0.62, wpb=108.9, bsz=40, num_updates=16300, lr=4.35937e-05, gnorm=0.502, clip=0, loss_scale=256, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=102203
2023-01-12 04:28:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:21 - progress_bar.py[line:274] - INFO: epoch 001:  16332 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3939, wps=102.3, ups=0.61, wpb=111, bsz=40, num_updates=16310, lr=4.35885e-05, gnorm=1.954, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102220
2023-01-12 04:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:38 - progress_bar.py[line:274] - INFO: epoch 001:  16342 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3854, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=16320, lr=4.35833e-05, gnorm=0.565, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102236
2023-01-12 04:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:28:54 - progress_bar.py[line:274] - INFO: epoch 001:  16352 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4433, wps=102, ups=0.61, wpb=111, bsz=40, num_updates=16330, lr=4.35781e-05, gnorm=0.463, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102253
2023-01-12 04:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:11 - progress_bar.py[line:274] - INFO: epoch 001:  16362 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.396, wps=101.3, ups=0.61, wpb=110.3, bsz=40, num_updates=16340, lr=4.35729e-05, gnorm=0.315, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=102269
2023-01-12 04:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:27 - progress_bar.py[line:274] - INFO: epoch 001:  16372 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5181, wps=101.9, ups=0.61, wpb=110.5, bsz=40, num_updates=16350, lr=4.35677e-05, gnorm=1.864, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102286
2023-01-12 04:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:43 - progress_bar.py[line:274] - INFO: epoch 001:  16382 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4771, wps=102.4, ups=0.63, wpb=109.1, bsz=40, num_updates=16360, lr=4.35625e-05, gnorm=0.714, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102302
2023-01-12 04:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:00 - progress_bar.py[line:274] - INFO: epoch 001:  16392 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4141, wps=103.8, ups=0.62, wpb=110.8, bsz=40, num_updates=16370, lr=4.35573e-05, gnorm=0.753, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102318
2023-01-12 04:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:16 - progress_bar.py[line:274] - INFO: epoch 001:  16402 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4021, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=16380, lr=4.35521e-05, gnorm=0.738, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=102335
2023-01-12 04:30:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:33 - progress_bar.py[line:274] - INFO: epoch 001:  16412 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3627, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=16390, lr=4.35469e-05, gnorm=0.518, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102352
2023-01-12 04:30:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:49 - progress_bar.py[line:274] - INFO: epoch 001:  16422 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3611, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=16400, lr=4.35417e-05, gnorm=0.615, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102368
2023-01-12 04:30:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:06 - progress_bar.py[line:274] - INFO: epoch 001:  16432 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3429, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=16410, lr=4.35365e-05, gnorm=0.635, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102385
2023-01-12 04:31:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:22 - progress_bar.py[line:274] - INFO: epoch 001:  16442 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4038, wps=99.7, ups=0.62, wpb=108, bsz=40, num_updates=16420, lr=4.35313e-05, gnorm=1.11, clip=30, loss_scale=256, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=102401
2023-01-12 04:31:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:39 - progress_bar.py[line:274] - INFO: epoch 001:  16452 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4234, wps=100.8, ups=0.61, wpb=110.2, bsz=40, num_updates=16430, lr=4.3526e-05, gnorm=0.973, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102418
2023-01-12 04:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:31:56 - progress_bar.py[line:274] - INFO: epoch 001:  16462 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3714, wps=100.8, ups=0.6, wpb=111.1, bsz=40, num_updates=16440, lr=4.35208e-05, gnorm=0.446, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102435
2023-01-12 04:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:12 - progress_bar.py[line:274] - INFO: epoch 001:  16472 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3929, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=16450, lr=4.35156e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102451
2023-01-12 04:32:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:29 - progress_bar.py[line:274] - INFO: epoch 001:  16482 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4045, wps=102.3, ups=0.62, wpb=110, bsz=40, num_updates=16460, lr=4.35104e-05, gnorm=0.741, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102467
2023-01-12 04:32:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:46 - progress_bar.py[line:274] - INFO: epoch 001:  16492 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3689, wps=98.4, ups=0.61, wpb=108.3, bsz=40, num_updates=16470, lr=4.35052e-05, gnorm=2.435, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102484
2023-01-12 04:32:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:02 - progress_bar.py[line:274] - INFO: epoch 001:  16502 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4048, wps=102.4, ups=0.62, wpb=110.3, bsz=40, num_updates=16480, lr=4.35e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102501
2023-01-12 04:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:19 - progress_bar.py[line:274] - INFO: epoch 001:  16512 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5306, wps=102, ups=0.61, wpb=111.1, bsz=40, num_updates=16490, lr=4.34948e-05, gnorm=0.893, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102517
2023-01-12 04:33:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:35 - progress_bar.py[line:274] - INFO: epoch 001:  16522 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4175, wps=99.6, ups=0.6, wpb=110.1, bsz=40, num_updates=16500, lr=4.34896e-05, gnorm=1.274, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=102534
2023-01-12 04:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:52 - progress_bar.py[line:274] - INFO: epoch 001:  16532 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.381, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=16510, lr=4.34844e-05, gnorm=1.779, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102551
2023-01-12 04:33:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:09 - progress_bar.py[line:274] - INFO: epoch 001:  16542 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.375, wps=99.2, ups=0.6, wpb=110.7, bsz=40, num_updates=16520, lr=4.34792e-05, gnorm=0.624, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=102568
2023-01-12 04:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:25 - progress_bar.py[line:274] - INFO: epoch 001:  16552 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.2911, wps=103.8, ups=0.62, wpb=110.7, bsz=40, num_updates=16530, lr=4.3474e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102584
2023-01-12 04:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:42 - progress_bar.py[line:274] - INFO: epoch 001:  16562 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3763, wps=101.2, ups=0.62, wpb=109.7, bsz=40, num_updates=16540, lr=4.34688e-05, gnorm=0.562, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102601
2023-01-12 04:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:34:58 - progress_bar.py[line:274] - INFO: epoch 001:  16572 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.34, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=16550, lr=4.34635e-05, gnorm=0.487, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102617
2023-01-12 04:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:15 - progress_bar.py[line:274] - INFO: epoch 001:  16582 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4118, wps=98.9, ups=0.61, wpb=108.5, bsz=40, num_updates=16560, lr=4.34583e-05, gnorm=0.587, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102634
2023-01-12 04:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:32 - progress_bar.py[line:274] - INFO: epoch 001:  16592 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3423, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=16570, lr=4.34531e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102651
2023-01-12 04:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:48 - progress_bar.py[line:274] - INFO: epoch 001:  16602 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4455, wps=101.5, ups=0.62, wpb=109.5, bsz=40, num_updates=16580, lr=4.34479e-05, gnorm=0.461, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102667
2023-01-12 04:35:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:35:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:01 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-12 04:36:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:07 - progress_bar.py[line:274] - INFO: epoch 001:  16613 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4486, wps=92.1, ups=0.56, wpb=109.5, bsz=40, num_updates=16590, lr=4.34427e-05, gnorm=1.008, clip=20, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=102685
2023-01-12 04:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:23 - progress_bar.py[line:274] - INFO: epoch 001:  16623 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4706, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=16600, lr=4.34375e-05, gnorm=0.382, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=102702
2023-01-12 04:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:40 - progress_bar.py[line:274] - INFO: epoch 001:  16633 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3846, wps=101.4, ups=0.61, wpb=110.2, bsz=40, num_updates=16610, lr=4.34323e-05, gnorm=0.573, clip=10, loss_scale=256, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=102719
2023-01-12 04:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:36:57 - progress_bar.py[line:274] - INFO: epoch 001:  16643 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.46, wps=100.2, ups=0.61, wpb=109.7, bsz=40, num_updates=16620, lr=4.34271e-05, gnorm=0.516, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102735
2023-01-12 04:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:13 - progress_bar.py[line:274] - INFO: epoch 001:  16653 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4271, wps=104.1, ups=0.63, wpb=110.7, bsz=40, num_updates=16630, lr=4.34219e-05, gnorm=1.418, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102751
2023-01-12 04:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:29 - progress_bar.py[line:274] - INFO: epoch 001:  16663 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4059, wps=100.1, ups=0.61, wpb=109.9, bsz=40, num_updates=16640, lr=4.34167e-05, gnorm=0.318, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102768
2023-01-12 04:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:46 - progress_bar.py[line:274] - INFO: epoch 001:  16673 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4135, wps=104.4, ups=0.63, wpb=111, bsz=40, num_updates=16650, lr=4.34115e-05, gnorm=0.701, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102784
2023-01-12 04:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:02 - progress_bar.py[line:274] - INFO: epoch 001:  16683 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4, wps=104.8, ups=0.63, wpb=110.8, bsz=40, num_updates=16660, lr=4.34063e-05, gnorm=0.734, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102800
2023-01-12 04:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:18 - progress_bar.py[line:274] - INFO: epoch 001:  16693 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.381, wps=102.3, ups=0.62, wpb=110, bsz=40, num_updates=16670, lr=4.3401e-05, gnorm=1.223, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=102817
2023-01-12 04:38:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:35 - progress_bar.py[line:274] - INFO: epoch 001:  16703 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3043, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=16680, lr=4.33958e-05, gnorm=1.493, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=102833
2023-01-12 04:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:51 - progress_bar.py[line:274] - INFO: epoch 001:  16713 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4074, wps=100.5, ups=0.61, wpb=109.8, bsz=40, num_updates=16690, lr=4.33906e-05, gnorm=0.88, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102850
2023-01-12 04:38:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:38:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:07 - progress_bar.py[line:274] - INFO: epoch 001:  16723 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4, wps=103.2, ups=0.63, wpb=110.1, bsz=40, num_updates=16700, lr=4.33854e-05, gnorm=1.314, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102866
2023-01-12 04:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:23 - progress_bar.py[line:274] - INFO: epoch 001:  16733 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3942, wps=103.9, ups=0.63, wpb=109.1, bsz=40, num_updates=16710, lr=4.33802e-05, gnorm=0.557, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102882
2023-01-12 04:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:40 - progress_bar.py[line:274] - INFO: epoch 001:  16743 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3299, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=16720, lr=4.3375e-05, gnorm=1.275, clip=30, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=102899
2023-01-12 04:39:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:39:56 - progress_bar.py[line:274] - INFO: epoch 001:  16753 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3558, wps=104.2, ups=0.63, wpb=110.1, bsz=40, num_updates=16730, lr=4.33698e-05, gnorm=0.736, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102915
2023-01-12 04:39:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:12 - progress_bar.py[line:274] - INFO: epoch 001:  16763 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2626, wps=103, ups=0.62, wpb=110.1, bsz=40, num_updates=16740, lr=4.33646e-05, gnorm=0.516, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102931
2023-01-12 04:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:29 - progress_bar.py[line:274] - INFO: epoch 001:  16773 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.133, nsentences=40, sample_size=107.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3889, wps=98.9, ups=0.62, wpb=107.1, bsz=40, num_updates=16750, lr=4.33594e-05, gnorm=0.584, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=102947
2023-01-12 04:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:45 - progress_bar.py[line:274] - INFO: epoch 001:  16783 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3423, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=16760, lr=4.33542e-05, gnorm=0.53, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102964
2023-01-12 04:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:02 - progress_bar.py[line:274] - INFO: epoch 001:  16793 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3438, wps=102, ups=0.61, wpb=110.6, bsz=40, num_updates=16770, lr=4.3349e-05, gnorm=1.179, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=102981
2023-01-12 04:41:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:18 - progress_bar.py[line:274] - INFO: epoch 001:  16803 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4444, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=16780, lr=4.33438e-05, gnorm=0.473, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102997
2023-01-12 04:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:35 - progress_bar.py[line:274] - INFO: epoch 001:  16813 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3173, wps=100.8, ups=0.62, wpb=108.6, bsz=40, num_updates=16790, lr=4.33385e-05, gnorm=0.673, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=103014
2023-01-12 04:41:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:51 - progress_bar.py[line:274] - INFO: epoch 001:  16823 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3918, wps=101.6, ups=0.61, wpb=110.3, bsz=40, num_updates=16800, lr=4.33333e-05, gnorm=0.566, clip=30, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=103030
2023-01-12 04:41:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:08 - progress_bar.py[line:274] - INFO: epoch 001:  16833 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3776, wps=103.7, ups=0.63, wpb=110, bsz=40, num_updates=16810, lr=4.33281e-05, gnorm=0.355, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=103046
2023-01-12 04:42:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:24 - progress_bar.py[line:274] - INFO: epoch 001:  16843 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3889, wps=102.6, ups=0.63, wpb=108.6, bsz=40, num_updates=16820, lr=4.33229e-05, gnorm=0.889, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=103062
2023-01-12 04:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:40 - progress_bar.py[line:274] - INFO: epoch 001:  16853 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4112, wps=105.2, ups=0.64, wpb=110.2, bsz=40, num_updates=16830, lr=4.33177e-05, gnorm=1.563, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=103078
2023-01-12 04:42:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:42:56 - progress_bar.py[line:274] - INFO: epoch 001:  16863 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4234, wps=99.5, ups=0.61, wpb=109.3, bsz=40, num_updates=16840, lr=4.33125e-05, gnorm=0.974, clip=40, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=103095
2023-01-12 04:42:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:13 - progress_bar.py[line:274] - INFO: epoch 001:  16873 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3039, wps=100.1, ups=0.6, wpb=110.6, bsz=40, num_updates=16850, lr=4.33073e-05, gnorm=0.671, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=103112
2023-01-12 04:43:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:30 - progress_bar.py[line:274] - INFO: epoch 001:  16883 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.433, wps=99.8, ups=0.61, wpb=109.3, bsz=40, num_updates=16860, lr=4.33021e-05, gnorm=0.432, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=103128
2023-01-12 04:43:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:46 - progress_bar.py[line:274] - INFO: epoch 001:  16893 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=16870, lr=4.32969e-05, gnorm=0.797, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=103145
2023-01-12 04:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:02 - progress_bar.py[line:274] - INFO: epoch 001:  16903 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3761, wps=103.7, ups=0.63, wpb=109.9, bsz=40, num_updates=16880, lr=4.32917e-05, gnorm=0.556, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=103161
2023-01-12 04:44:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:19 - progress_bar.py[line:274] - INFO: epoch 001:  16913 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3763, wps=102.1, ups=0.61, wpb=111, bsz=40, num_updates=16890, lr=4.32865e-05, gnorm=0.583, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=103178
2023-01-12 04:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:36 - progress_bar.py[line:274] - INFO: epoch 001:  16923 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4091, wps=99.1, ups=0.6, wpb=109.8, bsz=40, num_updates=16900, lr=4.32812e-05, gnorm=1.536, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=103194
2023-01-12 04:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:52 - progress_bar.py[line:274] - INFO: epoch 001:  16933 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4231, wps=101, ups=0.61, wpb=109.7, bsz=40, num_updates=16910, lr=4.3276e-05, gnorm=0.631, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=103211
2023-01-12 04:44:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:09 - progress_bar.py[line:274] - INFO: epoch 001:  16943 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4318, wps=99.8, ups=0.6, wpb=110.3, bsz=40, num_updates=16920, lr=4.32708e-05, gnorm=0.749, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=103228
2023-01-12 04:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:25 - progress_bar.py[line:274] - INFO: epoch 001:  16953 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.433, wps=101.4, ups=0.62, wpb=109.4, bsz=40, num_updates=16930, lr=4.32656e-05, gnorm=0.765, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=103244
2023-01-12 04:45:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:42 - progress_bar.py[line:274] - INFO: epoch 001:  16963 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3608, wps=100.1, ups=0.62, wpb=108.3, bsz=40, num_updates=16940, lr=4.32604e-05, gnorm=0.544, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=103261
2023-01-12 04:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:45:59 - progress_bar.py[line:274] - INFO: epoch 001:  16973 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2783, wps=97.6, ups=0.6, wpb=108.2, bsz=40, num_updates=16950, lr=4.32552e-05, gnorm=0.513, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=103277
2023-01-12 04:46:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:15 - progress_bar.py[line:274] - INFO: epoch 001:  16983 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4455, wps=102.6, ups=0.62, wpb=109.7, bsz=40, num_updates=16960, lr=4.325e-05, gnorm=0.698, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=103294
2023-01-12 04:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:32 - progress_bar.py[line:274] - INFO: epoch 001:  16993 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4141, wps=99.9, ups=0.61, wpb=108.4, bsz=40, num_updates=16970, lr=4.32448e-05, gnorm=0.905, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=103310
2023-01-12 04:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:48 - progress_bar.py[line:274] - INFO: epoch 001:  17003 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4045, wps=102.9, ups=0.62, wpb=111.2, bsz=40, num_updates=16980, lr=4.32396e-05, gnorm=0.723, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=103327
2023-01-12 04:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:47:05 - progress_bar.py[line:274] - INFO: epoch 001:  17013 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4167, wps=99.3, ups=0.61, wpb=108.6, bsz=40, num_updates=16990, lr=4.32344e-05, gnorm=0.88, clip=10, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=103343
2023-01-12 04:47:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:47:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:47:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.95 alpha 1.0
2023-01-12 04:47:21 - progress_bar.py[line:274] - INFO: epoch 001:  17023 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4421, wps=102.2, ups=0.61, wpb=112.6, bsz=40, num_updates=17000, lr=4.32292e-05, gnorm=2.072, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=103360
2023-01-12 04:47:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 04:47:23 - train.py[line:549] - INFO: 0 / 6234
2023-01-12 04:47:23 - train.py[line:551] - INFO: load:0.99 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 04:49:24 - train.py[line:549] - INFO: 200 / 6234
2023-01-12 04:49:24 - train.py[line:551] - INFO: load:1.02 valid_run:121.59 task_valid:118.51 collect_output:2.02
2023-01-12 04:51:24 - train.py[line:549] - INFO: 400 / 6234
2023-01-12 04:51:24 - train.py[line:551] - INFO: load:1.04 valid_run:241.17 task_valid:233.83 collect_output:5.23
2023-01-12 04:53:26 - train.py[line:549] - INFO: 600 / 6234
2023-01-12 04:53:26 - train.py[line:551] - INFO: load:1.07 valid_run:362.72 task_valid:349.73 collect_output:9.86
2023-01-12 04:55:27 - train.py[line:549] - INFO: 800 / 6234
2023-01-12 04:55:27 - train.py[line:551] - INFO: load:1.09 valid_run:484.38 task_valid:463.05 collect_output:17.19
2023-01-12 04:57:28 - train.py[line:549] - INFO: 1000 / 6234
2023-01-12 04:57:28 - train.py[line:551] - INFO: load:1.12 valid_run:604.47 task_valid:579.91 collect_output:19.40
2023-01-12 04:59:30 - train.py[line:549] - INFO: 1200 / 6234
2023-01-12 04:59:30 - train.py[line:551] - INFO: load:1.14 valid_run:727.01 task_valid:698.17 collect_output:22.64
2023-01-12 05:01:33 - train.py[line:549] - INFO: 1400 / 6234
2023-01-12 05:01:33 - train.py[line:551] - INFO: load:1.17 valid_run:849.55 task_valid:815.75 collect_output:26.59
2023-01-12 05:03:34 - train.py[line:549] - INFO: 1600 / 6234
2023-01-12 05:03:34 - train.py[line:551] - INFO: load:1.19 valid_run:970.98 task_valid:931.64 collect_output:31.11
2023-01-12 05:05:38 - train.py[line:549] - INFO: 1800 / 6234
2023-01-12 05:05:38 - train.py[line:551] - INFO: load:1.22 valid_run:1094.30 task_valid:1048.23 collect_output:36.83
2023-01-12 05:07:39 - train.py[line:549] - INFO: 2000 / 6234
2023-01-12 05:07:39 - train.py[line:551] - INFO: load:1.24 valid_run:1215.59 task_valid:1160.37 collect_output:44.96
2023-01-12 05:09:39 - train.py[line:549] - INFO: 2200 / 6234
2023-01-12 05:09:39 - train.py[line:551] - INFO: load:1.27 valid_run:1335.63 task_valid:1275.62 collect_output:48.74
2023-01-12 05:11:40 - train.py[line:549] - INFO: 2400 / 6234
2023-01-12 05:11:40 - train.py[line:551] - INFO: load:1.29 valid_run:1456.74 task_valid:1391.93 collect_output:52.53
2023-01-12 05:13:39 - train.py[line:549] - INFO: 2600 / 6234
2023-01-12 05:13:39 - train.py[line:551] - INFO: load:1.32 valid_run:1575.21 task_valid:1505.18 collect_output:56.69
2023-01-12 05:15:39 - train.py[line:549] - INFO: 2800 / 6234
2023-01-12 05:15:40 - train.py[line:551] - INFO: load:1.35 valid_run:1695.68 task_valid:1622.51 collect_output:58.82
2023-01-12 05:17:40 - train.py[line:549] - INFO: 3000 / 6234
2023-01-12 05:17:40 - train.py[line:551] - INFO: load:1.37 valid_run:1816.02 task_valid:1737.96 collect_output:62.69
2023-01-12 05:19:41 - train.py[line:549] - INFO: 3200 / 6234
2023-01-12 05:19:41 - train.py[line:551] - INFO: load:1.40 valid_run:1936.67 task_valid:1851.33 collect_output:68.95
2023-01-12 05:21:42 - train.py[line:549] - INFO: 3400 / 6234
2023-01-12 05:21:42 - train.py[line:551] - INFO: load:1.42 valid_run:2057.47 task_valid:1966.99 collect_output:73.09
2023-01-12 05:23:42 - train.py[line:549] - INFO: 3600 / 6234
2023-01-12 05:23:42 - train.py[line:551] - INFO: load:1.45 valid_run:2177.64 task_valid:2084.40 collect_output:74.82
2023-01-12 05:25:43 - train.py[line:549] - INFO: 3800 / 6234
2023-01-12 05:25:43 - train.py[line:551] - INFO: load:1.47 valid_run:2298.41 task_valid:2200.87 collect_output:78.13
2023-01-12 05:27:43 - train.py[line:549] - INFO: 4000 / 6234
2023-01-12 05:27:43 - train.py[line:551] - INFO: load:1.50 valid_run:2418.32 task_valid:2316.89 collect_output:81.01
2023-01-12 05:29:44 - train.py[line:549] - INFO: 4200 / 6234
2023-01-12 05:29:44 - train.py[line:551] - INFO: load:1.52 valid_run:2539.56 task_valid:2432.88 collect_output:85.26
2023-01-12 05:31:46 - train.py[line:549] - INFO: 4400 / 6234
2023-01-12 05:31:46 - train.py[line:551] - INFO: load:1.55 valid_run:2661.13 task_valid:2551.38 collect_output:87.34
2023-01-12 05:33:46 - train.py[line:549] - INFO: 4600 / 6234
2023-01-12 05:33:46 - train.py[line:551] - INFO: load:1.57 valid_run:2781.07 task_valid:2665.22 collect_output:92.43
2023-01-12 05:35:45 - train.py[line:549] - INFO: 4800 / 6234
2023-01-12 05:35:45 - train.py[line:551] - INFO: load:1.60 valid_run:2900.48 task_valid:2780.83 collect_output:95.25
2023-01-12 05:37:46 - train.py[line:549] - INFO: 5000 / 6234
2023-01-12 05:37:46 - train.py[line:551] - INFO: load:1.62 valid_run:3021.51 task_valid:2896.23 collect_output:99.88
2023-01-12 05:39:49 - train.py[line:549] - INFO: 5200 / 6234
2023-01-12 05:39:49 - train.py[line:551] - INFO: load:1.65 valid_run:3143.94 task_valid:3011.57 collect_output:105.96
2023-01-12 05:41:48 - train.py[line:549] - INFO: 5400 / 6234
2023-01-12 05:41:48 - train.py[line:551] - INFO: load:1.67 valid_run:3262.97 task_valid:3125.11 collect_output:110.45
2023-01-12 05:43:49 - train.py[line:549] - INFO: 5600 / 6234
2023-01-12 05:43:49 - train.py[line:551] - INFO: load:1.70 valid_run:3384.39 task_valid:3244.13 collect_output:111.83
2023-01-12 05:45:51 - train.py[line:549] - INFO: 5800 / 6234
2023-01-12 05:45:51 - train.py[line:551] - INFO: load:1.72 valid_run:3505.63 task_valid:3359.11 collect_output:117.11
2023-01-12 05:47:52 - train.py[line:549] - INFO: 6000 / 6234
2023-01-12 05:47:52 - train.py[line:551] - INFO: load:1.75 valid_run:3626.94 task_valid:3476.93 collect_output:119.60
2023-01-12 05:49:53 - train.py[line:549] - INFO: 6200 / 6234
2023-01-12 05:49:53 - train.py[line:551] - INFO: load:1.77 valid_run:3747.31 task_valid:3594.64 collect_output:121.25

====================================================================================================
SGG eval:     R @ 50: 0.5139;     R @ 100: 0.5995;     R @ 500: 0.6448;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3215;    mR @ 100: 0.3750;    mR @ 500: 0.4241;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7878) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3613) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8542) (playing:0.0000) (riding:0.8791) (says:0.0000) (sitting on:0.7225) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.4324) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5139;     R @ 100: 0.5995;     R @ 500: 0.6448;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3215;    mR @ 100: 0.3750;    mR @ 500: 0.4241;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7878) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3613) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8542) (playing:0.0000) (riding:0.8791) (says:0.0000) (sitting on:0.7225) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.4324) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-12 05:50:23 - train.py[line:487] - INFO: 0.599481512605042
2023-01-12 05:50:23 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 05:50:24 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.347 | loss_v1 0 | loss_v2 0 | nll_loss 0.191 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.599482 | ppl 1.14 | vqa_score 0.545 | wps 118.7 | wpb 72 | bsz 24 | num_updates 17000 | best_R@100 0.697584
2023-01-12 05:50:24 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 17000 updates
2023-01-12 05:50:24 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.95_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_17000.pt
