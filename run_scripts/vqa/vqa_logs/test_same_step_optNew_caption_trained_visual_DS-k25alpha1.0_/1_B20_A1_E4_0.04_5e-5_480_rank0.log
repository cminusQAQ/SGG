2023-02-16 15:49:05 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-16 15:49:05 - utils.py[line:261] - INFO: Start init
2023-02-16 15:49:05 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-16 15:49:05 - utils.py[line:261] - INFO: Start init
2023-02-16 15:49:06 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-16 15:49:06 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-16 15:49:06 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-16 15:49:06 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-16 15:49:10 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 4, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=4, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-16 15:49:10 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-16 15:49:10 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-16 15:49:15 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-16 15:49:15 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-16 15:49:15 - train.py[line:119] - INFO: model: OFAModel
2023-02-16 15:49:15 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-16 15:49:15 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-16 15:49:15 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-16 15:49:15 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-16 15:49:15 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-16 15:49:16 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-16 15:49:16 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-16 15:49:16 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-16 15:49:16 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-16 15:49:17 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-16 15:49:17 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-16 15:49:17 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-02-16 15:49:24 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-16 15:49:25 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-16 15:49:25 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-16 15:49:25 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-16 15:49:25 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-16 15:49:25 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-16 15:49:25 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 0 row count 578200 total row count 1156400
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k25alpha1.0_train_NA1_E0.tsv slice_id 1 row count 578200 total row count 1156400
2023-02-16 15:49:28 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
2023-02-16 15:49:29 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-16 15:49:29 - train.py[line:312] - INFO: Start iterating over samples
2023-02-16 15:49:45 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 28910 loss=0.927, loss_v1=0, loss_v2=0, nll_loss=0.783, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=96.9, ups=0.88, wpb=110.8, bsz=40, num_updates=10, lr=1.08108e-07, gnorm=10.213, clip=100, loss_scale=128, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=29
2023-02-16 15:49:57 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 28910 loss=1.104, loss_v1=0, loss_v2=0, nll_loss=0.966, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.95, wps=95.5, ups=0.87, wpb=109.6, bsz=40, num_updates=20, lr=2.16216e-07, gnorm=13.388, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=41
2023-02-16 15:50:09 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 28910 loss=1.022, loss_v1=0, loss_v2=0, nll_loss=0.886, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.85, wps=92.5, ups=0.83, wpb=111.1, bsz=40, num_updates=30, lr=3.24324e-07, gnorm=11.336, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=53
2023-02-16 15:50:20 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 28910 loss=1.018, loss_v1=0, loss_v2=0, nll_loss=0.879, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.84, wps=99.1, ups=0.89, wpb=111.6, bsz=40, num_updates=40, lr=4.32432e-07, gnorm=11.429, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64
2023-02-16 15:50:32 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 28910 loss=0.95, loss_v1=0, loss_v2=0, nll_loss=0.817, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=94.5, ups=0.86, wpb=109.4, bsz=40, num_updates=50, lr=5.40541e-07, gnorm=9.819, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=76
2023-02-16 15:50:43 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 28910 loss=0.924, loss_v1=0, loss_v2=0, nll_loss=0.795, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=96, ups=0.86, wpb=111.1, bsz=40, num_updates=60, lr=6.48649e-07, gnorm=10.216, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=87
2023-02-16 15:50:55 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 28910 loss=0.851, loss_v1=0, loss_v2=0, nll_loss=0.725, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=95, ups=0.87, wpb=109.8, bsz=40, num_updates=70, lr=7.56757e-07, gnorm=9.697, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=99
2023-02-16 15:51:06 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 28910 loss=0.853, loss_v1=0, loss_v2=0, nll_loss=0.737, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=100, ups=0.91, wpb=110, bsz=40, num_updates=80, lr=8.64865e-07, gnorm=8.354, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=110
2023-02-16 15:51:17 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 28910 loss=0.771, loss_v1=0, loss_v2=0, nll_loss=0.648, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.2, ups=0.88, wpb=111, bsz=40, num_updates=90, lr=9.72973e-07, gnorm=7.935, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=121
2023-02-16 15:51:29 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 28910 loss=0.778, loss_v1=0, loss_v2=0, nll_loss=0.67, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=95, ups=0.86, wpb=110.1, bsz=40, num_updates=100, lr=1.08108e-06, gnorm=6.223, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=133
2023-02-16 15:51:40 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 28910 loss=0.75, loss_v1=0, loss_v2=0, nll_loss=0.642, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=97.6, ups=0.89, wpb=110, bsz=40, num_updates=110, lr=1.18919e-06, gnorm=5.979, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=144
2023-02-16 15:51:52 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 28910 loss=0.692, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=95.7, ups=0.86, wpb=111.2, bsz=40, num_updates=120, lr=1.2973e-06, gnorm=5.294, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=156
2023-02-16 15:52:03 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 28910 loss=0.725, loss_v1=0, loss_v2=0, nll_loss=0.625, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=130, lr=1.40541e-06, gnorm=4.625, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=167
2023-02-16 15:52:14 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 28910 loss=0.685, loss_v1=0, loss_v2=0, nll_loss=0.583, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=98.6, ups=0.89, wpb=110.9, bsz=40, num_updates=140, lr=1.51351e-06, gnorm=5.035, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=179
2023-02-16 15:52:26 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 28910 loss=0.684, loss_v1=0, loss_v2=0, nll_loss=0.584, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=150, lr=1.62162e-06, gnorm=4.52, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=190
2023-02-16 15:52:37 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 28910 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.56, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=102.1, ups=0.92, wpb=110.5, bsz=40, num_updates=160, lr=1.72973e-06, gnorm=4.231, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=201
2023-02-16 15:52:48 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 28910 loss=0.708, loss_v1=0, loss_v2=0, nll_loss=0.618, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=170, lr=1.83784e-06, gnorm=4.03, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=212
2023-02-16 15:52:59 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 28910 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=180, lr=1.94595e-06, gnorm=3.768, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=223
2023-02-16 15:53:10 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 28910 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=101.8, ups=0.91, wpb=111.8, bsz=40, num_updates=190, lr=2.05405e-06, gnorm=3.273, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=234
2023-02-16 15:53:21 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 28910 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.578, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=200, lr=2.16216e-06, gnorm=3.41, clip=100, loss_scale=128, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=246
2023-02-16 15:53:33 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 28910 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.516, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=210, lr=2.27027e-06, gnorm=3.189, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=257
2023-02-16 15:53:44 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 28910 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.7, ups=0.9, wpb=109.8, bsz=40, num_updates=220, lr=2.37838e-06, gnorm=3.132, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=268
2023-02-16 15:53:55 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 28910 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.516, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=97.5, ups=0.88, wpb=110.9, bsz=40, num_updates=230, lr=2.48649e-06, gnorm=3.046, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=279
2023-02-16 15:54:06 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 28910 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=240, lr=2.59459e-06, gnorm=2.62, clip=100, loss_scale=128, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=291
2023-02-16 15:54:18 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 28910 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.518, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=250, lr=2.7027e-06, gnorm=2.948, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=302
2023-02-16 15:54:28 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 28910 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=103, ups=0.93, wpb=110.5, bsz=40, num_updates=260, lr=2.81081e-06, gnorm=2.924, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=312
2023-02-16 15:54:39 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 28910 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.567, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=97.7, ups=0.9, wpb=108.9, bsz=40, num_updates=270, lr=2.91892e-06, gnorm=3.195, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=324
2023-02-16 15:54:51 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 28910 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=97.3, ups=0.89, wpb=109.6, bsz=40, num_updates=280, lr=3.02703e-06, gnorm=2.582, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=335
2023-02-16 15:55:03 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 28910 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=91.1, ups=0.83, wpb=110, bsz=40, num_updates=290, lr=3.13514e-06, gnorm=2.787, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=347
2023-02-16 15:55:14 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 28910 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=300, lr=3.24324e-06, gnorm=2.468, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=358
2023-02-16 15:55:25 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 28910 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.7, ups=0.88, wpb=110.2, bsz=40, num_updates=310, lr=3.35135e-06, gnorm=2.546, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=370
2023-02-16 15:55:36 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 28910 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.5, ups=0.91, wpb=111.3, bsz=40, num_updates=320, lr=3.45946e-06, gnorm=2.427, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=381
2023-02-16 15:55:48 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 28910 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=330, lr=3.56757e-06, gnorm=2.706, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=392
2023-02-16 15:55:58 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 28910 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=104.4, ups=0.94, wpb=111.2, bsz=40, num_updates=340, lr=3.67568e-06, gnorm=2.664, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=403
2023-02-16 15:56:10 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 28910 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.1, ups=0.89, wpb=111.8, bsz=40, num_updates=350, lr=3.78378e-06, gnorm=2.32, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=414
2023-02-16 15:56:21 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 28910 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98.1, ups=0.89, wpb=110.2, bsz=40, num_updates=360, lr=3.89189e-06, gnorm=2.297, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=425
2023-02-16 15:56:32 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 28910 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=370, lr=4e-06, gnorm=2.415, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=436
2023-02-16 15:56:43 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 28910 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.5, ups=0.91, wpb=109.9, bsz=40, num_updates=380, lr=4.10811e-06, gnorm=2.327, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=447
2023-02-16 15:56:54 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 28910 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.9, wpb=111, bsz=40, num_updates=390, lr=4.21622e-06, gnorm=2.292, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=458
2023-02-16 15:57:05 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 28910 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=400, lr=4.32432e-06, gnorm=2.387, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=469
2023-02-16 15:57:16 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 28910 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.3, ups=0.88, wpb=109.5, bsz=40, num_updates=410, lr=4.43243e-06, gnorm=2.319, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=481
2023-02-16 15:57:27 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 28910 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.1, ups=0.92, wpb=110.4, bsz=40, num_updates=420, lr=4.54054e-06, gnorm=2.278, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=492
2023-02-16 15:57:38 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 28910 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.3, ups=0.9, wpb=111.3, bsz=40, num_updates=430, lr=4.64865e-06, gnorm=2.413, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=503
2023-02-16 15:57:50 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 28910 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98.7, ups=0.9, wpb=109.6, bsz=40, num_updates=440, lr=4.75676e-06, gnorm=2.613, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=514
2023-02-16 15:58:01 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 28910 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=450, lr=4.86486e-06, gnorm=2.222, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=525
2023-02-16 15:58:12 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 28910 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=95.6, ups=0.87, wpb=109.6, bsz=40, num_updates=460, lr=4.97297e-06, gnorm=2.416, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=537
2023-02-16 15:58:24 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 28910 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=470, lr=5.08108e-06, gnorm=2.38, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=548
2023-02-16 15:58:35 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 28910 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.9, ups=0.91, wpb=109.2, bsz=40, num_updates=480, lr=5.18919e-06, gnorm=2.254, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=559
2023-02-16 15:58:46 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 28910 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.1, ups=0.88, wpb=110.4, bsz=40, num_updates=490, lr=5.2973e-06, gnorm=2.188, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=570
2023-02-16 15:58:57 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 28910 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=500, lr=5.40541e-06, gnorm=2.248, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=582
2023-02-16 15:59:09 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 28910 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=510, lr=5.51351e-06, gnorm=2.135, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=593
2023-02-16 15:59:20 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 28910 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.5, ups=0.9, wpb=111.5, bsz=40, num_updates=520, lr=5.62162e-06, gnorm=2.027, clip=100, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=604
2023-02-16 15:59:30 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 28910 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=103.3, ups=0.93, wpb=111, bsz=40, num_updates=530, lr=5.72973e-06, gnorm=2.173, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=615
2023-02-16 15:59:42 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 28910 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=540, lr=5.83784e-06, gnorm=2.159, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=626
2023-02-16 15:59:53 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 28910 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.5, ups=0.91, wpb=110.9, bsz=40, num_updates=550, lr=5.94595e-06, gnorm=2.25, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=637
2023-02-16 16:00:03 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 28910 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.5, ups=0.93, wpb=110.1, bsz=40, num_updates=560, lr=6.05405e-06, gnorm=2.185, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=648
2023-02-16 16:00:15 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 28910 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=570, lr=6.16216e-06, gnorm=2.062, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=659
2023-02-16 16:00:26 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 28910 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.3, ups=0.88, wpb=111.1, bsz=40, num_updates=580, lr=6.27027e-06, gnorm=2.321, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=670
2023-02-16 16:00:37 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 28910 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.5, ups=0.9, wpb=112.1, bsz=40, num_updates=590, lr=6.37838e-06, gnorm=2.45, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=681
2023-02-16 16:00:49 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 28910 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.1, ups=0.87, wpb=110.4, bsz=40, num_updates=600, lr=6.48649e-06, gnorm=2.199, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=693
2023-02-16 16:01:00 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 28910 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=610, lr=6.59459e-06, gnorm=2.119, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=704
2023-02-16 16:01:11 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 28910 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.2, ups=0.91, wpb=109.6, bsz=40, num_updates=620, lr=6.7027e-06, gnorm=2.055, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=715
2023-02-16 16:01:22 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 28910 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=630, lr=6.81081e-06, gnorm=2.064, clip=100, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=727
2023-02-16 16:01:34 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 28910 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=640, lr=6.91892e-06, gnorm=2.474, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=738
2023-02-16 16:01:45 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 28910 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=650, lr=7.02703e-06, gnorm=2.205, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=749
2023-02-16 16:01:56 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 28910 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.2, ups=0.87, wpb=110.6, bsz=40, num_updates=660, lr=7.13514e-06, gnorm=2.099, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=761
2023-02-16 16:02:08 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 28910 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=670, lr=7.24324e-06, gnorm=2.061, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=772
2023-02-16 16:02:19 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 28910 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=680, lr=7.35135e-06, gnorm=2.228, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=783
2023-02-16 16:02:30 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 28910 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.8, ups=0.93, wpb=109.9, bsz=40, num_updates=690, lr=7.45946e-06, gnorm=2.154, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=794
2023-02-16 16:02:40 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 28910 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=104.8, ups=0.93, wpb=112.4, bsz=40, num_updates=700, lr=7.56757e-06, gnorm=2.095, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=805
2023-02-16 16:02:52 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 28910 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=710, lr=7.67568e-06, gnorm=2.152, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=816
2023-02-16 16:03:03 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 28910 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=720, lr=7.78378e-06, gnorm=2.14, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=827
2023-02-16 16:03:14 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 28910 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=730, lr=7.89189e-06, gnorm=2.202, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=838
2023-02-16 16:03:25 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 28910 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=740, lr=8e-06, gnorm=2.275, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=849
2023-02-16 16:03:36 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 28910 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=750, lr=8.10811e-06, gnorm=1.928, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=861
2023-02-16 16:03:47 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 28910 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.5, ups=0.92, wpb=109.4, bsz=40, num_updates=760, lr=8.21622e-06, gnorm=2.281, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=872
2023-02-16 16:03:59 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 28910 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.3, ups=0.87, wpb=111.8, bsz=40, num_updates=770, lr=8.32432e-06, gnorm=2.102, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=883
2023-02-16 16:04:10 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=780, lr=8.43243e-06, gnorm=1.972, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=894
2023-02-16 16:04:21 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 28910 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=790, lr=8.54054e-06, gnorm=2.129, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=905
2023-02-16 16:04:32 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 28910 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.2, ups=0.9, wpb=108.7, bsz=40, num_updates=800, lr=8.64865e-06, gnorm=2.099, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=917
2023-02-16 16:04:43 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 28910 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=810, lr=8.75676e-06, gnorm=1.94, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=928
2023-02-16 16:04:54 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 28910 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.7, ups=0.92, wpb=110.3, bsz=40, num_updates=820, lr=8.86486e-06, gnorm=1.846, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=938
2023-02-16 16:05:05 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 28910 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.5, ups=0.89, wpb=111.5, bsz=40, num_updates=830, lr=8.97297e-06, gnorm=2.126, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=950
2023-02-16 16:05:16 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 28910 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=840, lr=9.08108e-06, gnorm=2.048, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=961
2023-02-16 16:05:28 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 28910 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=850, lr=9.18919e-06, gnorm=2.04, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=972
2023-02-16 16:05:39 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 28910 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.5, ups=0.9, wpb=111, bsz=40, num_updates=860, lr=9.2973e-06, gnorm=1.9, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=983
2023-02-16 16:05:50 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 28910 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=870, lr=9.40541e-06, gnorm=1.959, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=994
2023-02-16 16:06:01 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 28910 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=880, lr=9.51351e-06, gnorm=1.981, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1006
2023-02-16 16:06:13 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.88, wpb=112.1, bsz=40, num_updates=890, lr=9.62162e-06, gnorm=1.912, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1017
2023-02-16 16:06:24 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 28910 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=900, lr=9.72973e-06, gnorm=2.048, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1028
2023-02-16 16:06:35 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 28910 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=910, lr=9.83784e-06, gnorm=2.022, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1039
2023-02-16 16:06:47 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 28910 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.88, wpb=111.7, bsz=40, num_updates=920, lr=9.94595e-06, gnorm=1.91, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1051
2023-02-16 16:06:58 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.87, wpb=111.3, bsz=40, num_updates=930, lr=1.00541e-05, gnorm=2.058, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1062
2023-02-16 16:07:09 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 28910 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.8, ups=0.92, wpb=110.7, bsz=40, num_updates=940, lr=1.01622e-05, gnorm=2.14, clip=100, loss_scale=256, train_wall=11, gb_free=10, ema_decay=0.9999, wall=1073
2023-02-16 16:07:20 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 28910 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.7, ups=0.92, wpb=109.8, bsz=40, num_updates=950, lr=1.02703e-05, gnorm=2.281, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1084
2023-02-16 16:07:31 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 28910 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.4, ups=0.89, wpb=109, bsz=40, num_updates=960, lr=1.03784e-05, gnorm=2.116, clip=100, loss_scale=256, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=1095
2023-02-16 16:07:42 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 28910 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=970, lr=1.04865e-05, gnorm=1.963, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1107
2023-02-16 16:07:54 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 28910 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.5, ups=0.87, wpb=109.6, bsz=40, num_updates=980, lr=1.05946e-05, gnorm=1.813, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1118
2023-02-16 16:08:05 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 28910 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.4, ups=0.93, wpb=111.3, bsz=40, num_updates=990, lr=1.07027e-05, gnorm=2.028, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1129
2023-02-16 16:08:16 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 28910 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.1, ups=0.9, wpb=112.1, bsz=40, num_updates=1000, lr=1.08108e-05, gnorm=1.968, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1140
2023-02-16 16:08:16 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 16:08:16 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-16 16:08:17 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 16:08:17 - train.py[line:551] - INFO: load:0.94 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 16:10:21 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 16:10:21 - train.py[line:551] - INFO: load:0.97 valid_run:123.71 task_valid:120.43 collect_output:2.24
2023-02-16 16:12:21 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 16:12:21 - train.py[line:551] - INFO: load:0.99 valid_run:243.87 task_valid:236.34 collect_output:5.47
2023-02-16 16:14:24 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 16:14:24 - train.py[line:551] - INFO: load:1.02 valid_run:366.47 task_valid:352.71 collect_output:10.69
2023-02-16 16:16:26 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 16:16:26 - train.py[line:551] - INFO: load:1.04 valid_run:488.28 task_valid:466.31 collect_output:17.91
2023-02-16 16:18:26 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 16:18:26 - train.py[line:551] - INFO: load:1.07 valid_run:608.66 task_valid:583.47 collect_output:20.12
2023-02-16 16:20:29 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 16:20:29 - train.py[line:551] - INFO: load:1.10 valid_run:731.44 task_valid:702.01 collect_output:23.36
2023-02-16 16:22:32 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 16:22:32 - train.py[line:551] - INFO: load:1.12 valid_run:854.41 task_valid:819.86 collect_output:27.47
2023-02-16 16:24:34 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 16:24:34 - train.py[line:551] - INFO: load:1.15 valid_run:976.33 task_valid:936.38 collect_output:31.85
2023-02-16 16:26:38 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 16:26:38 - train.py[line:551] - INFO: load:1.17 valid_run:1100.01 task_valid:1053.46 collect_output:37.45
2023-02-16 16:28:39 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 16:28:39 - train.py[line:551] - INFO: load:1.20 valid_run:1221.52 task_valid:1166.04 collect_output:45.35
2023-02-16 16:30:39 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 16:30:39 - train.py[line:551] - INFO: load:1.22 valid_run:1341.54 task_valid:1281.54 collect_output:48.88
2023-02-16 16:32:41 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 16:32:41 - train.py[line:551] - INFO: load:1.25 valid_run:1463.20 task_valid:1398.39 collect_output:52.68
2023-02-16 16:34:40 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 16:34:40 - train.py[line:551] - INFO: load:1.28 valid_run:1582.38 task_valid:1512.15 collect_output:57.08
2023-02-16 16:36:41 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 16:36:41 - train.py[line:551] - INFO: load:1.30 valid_run:1703.31 task_valid:1629.65 collect_output:59.50
2023-02-16 16:38:42 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 16:38:42 - train.py[line:551] - INFO: load:1.33 valid_run:1824.20 task_valid:1745.56 collect_output:63.48
2023-02-16 16:40:43 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 16:40:43 - train.py[line:551] - INFO: load:1.35 valid_run:1945.22 task_valid:1859.38 collect_output:69.67
2023-02-16 16:42:45 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 16:42:45 - train.py[line:551] - INFO: load:1.38 valid_run:2066.50 task_valid:1975.41 collect_output:73.92
2023-02-16 16:44:45 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 16:44:45 - train.py[line:551] - INFO: load:1.41 valid_run:2187.03 task_valid:2093.06 collect_output:75.78
2023-02-16 16:46:46 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 16:46:46 - train.py[line:551] - INFO: load:1.43 valid_run:2308.25 task_valid:2209.92 collect_output:79.13
2023-02-16 16:48:47 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 16:48:47 - train.py[line:551] - INFO: load:1.46 valid_run:2428.57 task_valid:2326.29 collect_output:82.07
2023-02-16 16:50:49 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 16:50:49 - train.py[line:551] - INFO: load:1.49 valid_run:2550.21 task_valid:2442.66 collect_output:86.34
2023-02-16 16:52:50 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 16:52:50 - train.py[line:551] - INFO: load:1.51 valid_run:2672.03 task_valid:2561.24 collect_output:88.56
2023-02-16 16:54:51 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 16:54:51 - train.py[line:551] - INFO: load:1.54 valid_run:2792.49 task_valid:2675.41 collect_output:93.85
2023-02-16 16:56:51 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 16:56:51 - train.py[line:551] - INFO: load:1.56 valid_run:2912.28 task_valid:2791.31 collect_output:96.72
2023-02-16 16:58:52 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 16:58:52 - train.py[line:551] - INFO: load:1.59 valid_run:3033.81 task_valid:2907.32 collect_output:101.21
2023-02-16 17:00:55 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 17:00:55 - train.py[line:551] - INFO: load:1.62 valid_run:3156.65 task_valid:3023.18 collect_output:107.18
2023-02-16 17:02:55 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 17:02:55 - train.py[line:551] - INFO: load:1.64 valid_run:3276.14 task_valid:3137.04 collect_output:111.80
2023-02-16 17:04:57 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 17:04:57 - train.py[line:551] - INFO: load:1.67 valid_run:3397.78 task_valid:3256.09 collect_output:113.37
2023-02-16 17:06:58 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 17:06:58 - train.py[line:551] - INFO: load:1.70 valid_run:3519.56 task_valid:3371.63 collect_output:118.60
2023-02-16 17:09:00 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 17:09:00 - train.py[line:551] - INFO: load:1.72 valid_run:3641.21 task_valid:3489.67 collect_output:121.17
2023-02-16 17:11:01 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 17:11:01 - train.py[line:551] - INFO: load:1.75 valid_run:3761.96 task_valid:3607.62 collect_output:122.96

====================================================================================================
SGG eval:     R @ 50: 0.1715;     R @ 100: 0.2502;     R @ 500: 0.3382;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0806;    mR @ 100: 0.1306;    mR @ 500: 0.1838;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.0000) (eating:0.2941) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3613) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.1250) (playing:0.0000) (riding:0.2069) (says:0.0000) (sitting on:0.2959) (standing on:0.4700) (using:0.1500) (walking in:0.0000) (walking on:0.0541) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.1715;     R @ 100: 0.2502;     R @ 500: 0.3382;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0806;    mR @ 100: 0.1306;    mR @ 500: 0.1838;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.0000) (eating:0.2941) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3613) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.1250) (playing:0.0000) (riding:0.2069) (says:0.0000) (sitting on:0.2959) (standing on:0.4700) (using:0.1500) (walking in:0.0000) (walking on:0.0541) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-16 17:11:31 - train.py[line:487] - INFO: 0.25016666666666665
2023-02-16 17:11:31 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 17:11:31 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.376 | loss_v1 0 | loss_v2 0 | nll_loss 0.225 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.250167 | ppl 1.17 | vqa_score 0.1734 | wps 118.3 | wpb 72 | bsz 24 | num_updates 1000
2023-02-16 17:11:31 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1000 updates
2023-02-16 17:11:31 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_1000.pt
2023-02-16 17:11:37 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_1000.pt
2023-02-16 17:11:42 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 0.25016666666666665) (writing took 10.633734902366996 seconds)
2023-02-16 17:11:53 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 28910 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=0.3, ups=0, wpb=109.3, bsz=40, num_updates=1010, lr=1.09189e-05, gnorm=2.13, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4957
2023-02-16 17:12:04 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 28910 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=1020, lr=1.1027e-05, gnorm=1.974, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=4968
2023-02-16 17:12:16 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 28910 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.88, wpb=111.2, bsz=40, num_updates=1030, lr=1.11351e-05, gnorm=1.973, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=4980
2023-02-16 17:12:27 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=1040, lr=1.12432e-05, gnorm=1.746, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4991
2023-02-16 17:12:38 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 28910 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=1050, lr=1.13514e-05, gnorm=1.934, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5002
2023-02-16 17:12:49 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.2, ups=0.9, wpb=108.6, bsz=40, num_updates=1060, lr=1.14595e-05, gnorm=1.833, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5014
2023-02-16 17:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.8, ups=0.87, wpb=111, bsz=40, num_updates=1070, lr=1.15676e-05, gnorm=1.964, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5025
2023-02-16 17:13:12 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=1080, lr=1.16757e-05, gnorm=1.845, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5036
2023-02-16 17:13:23 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.91, wpb=109.7, bsz=40, num_updates=1090, lr=1.17838e-05, gnorm=1.832, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5047
2023-02-16 17:13:34 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 28910 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=1100, lr=1.18919e-05, gnorm=1.805, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5059
2023-02-16 17:13:46 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 28910 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=1110, lr=1.2e-05, gnorm=1.956, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5070
2023-02-16 17:13:57 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 28910 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=1120, lr=1.21081e-05, gnorm=1.938, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5081
2023-02-16 17:14:08 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 28910 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.9, ups=0.93, wpb=110.7, bsz=40, num_updates=1130, lr=1.22162e-05, gnorm=1.882, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5092
2023-02-16 17:14:19 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=1140, lr=1.23243e-05, gnorm=1.793, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5103
2023-02-16 17:14:30 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=1150, lr=1.24324e-05, gnorm=1.837, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5114
2023-02-16 17:14:41 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.2, ups=0.93, wpb=110, bsz=40, num_updates=1160, lr=1.25405e-05, gnorm=1.922, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5125
2023-02-16 17:14:52 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 28910 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=1170, lr=1.26486e-05, gnorm=1.726, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5136
2023-02-16 17:15:03 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=1180, lr=1.27568e-05, gnorm=1.843, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5147
2023-02-16 17:15:14 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 28910 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.1, ups=0.88, wpb=108.9, bsz=40, num_updates=1190, lr=1.28649e-05, gnorm=2.132, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5159
2023-02-16 17:15:26 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 28910 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.9, wpb=109.8, bsz=40, num_updates=1200, lr=1.2973e-05, gnorm=2.062, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5170
2023-02-16 17:15:37 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 28910 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.2, ups=0.89, wpb=109.1, bsz=40, num_updates=1210, lr=1.30811e-05, gnorm=1.764, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5181
2023-02-16 17:15:48 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 28910 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=1220, lr=1.31892e-05, gnorm=1.692, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5192
2023-02-16 17:15:59 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 28910 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=1230, lr=1.32973e-05, gnorm=1.872, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5204
2023-02-16 17:16:11 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 28910 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97, ups=0.89, wpb=108.6, bsz=40, num_updates=1240, lr=1.34054e-05, gnorm=2.219, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5215
2023-02-16 17:16:22 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=94.7, ups=0.87, wpb=108.5, bsz=40, num_updates=1250, lr=1.35135e-05, gnorm=1.81, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5226
2023-02-16 17:16:34 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 28910 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.4, ups=0.87, wpb=109.5, bsz=40, num_updates=1260, lr=1.36216e-05, gnorm=1.952, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5238
2023-02-16 17:16:44 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=1270, lr=1.37297e-05, gnorm=1.745, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5249
2023-02-16 17:16:56 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=1280, lr=1.38378e-05, gnorm=1.755, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5260
2023-02-16 17:17:07 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.7, ups=0.88, wpb=109.4, bsz=40, num_updates=1290, lr=1.39459e-05, gnorm=1.684, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5271
2023-02-16 17:17:19 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.87, wpb=111.9, bsz=40, num_updates=1300, lr=1.40541e-05, gnorm=1.748, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5283
2023-02-16 17:17:30 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=1310, lr=1.41622e-05, gnorm=1.719, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5294
2023-02-16 17:17:41 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=1320, lr=1.42703e-05, gnorm=1.693, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5305
2023-02-16 17:17:52 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 28910 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=1330, lr=1.43784e-05, gnorm=1.484, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5316
2023-02-16 17:18:03 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.1, ups=0.89, wpb=112.1, bsz=40, num_updates=1340, lr=1.44865e-05, gnorm=1.605, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5327
2023-02-16 17:18:14 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=1350, lr=1.45946e-05, gnorm=1.817, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5339
2023-02-16 17:18:25 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103.1, ups=0.93, wpb=111, bsz=40, num_updates=1360, lr=1.47027e-05, gnorm=1.722, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5349
2023-02-16 17:18:36 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 28910 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=1370, lr=1.48108e-05, gnorm=1.754, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5361
2023-02-16 17:18:47 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 28910 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=103.9, ups=0.94, wpb=110.2, bsz=40, num_updates=1380, lr=1.49189e-05, gnorm=1.73, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5371
2023-02-16 17:18:58 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 28910 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=1390, lr=1.5027e-05, gnorm=1.515, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5382
2023-02-16 17:19:10 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 28910 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=1400, lr=1.51351e-05, gnorm=1.631, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5394
2023-02-16 17:19:21 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=1410, lr=1.52432e-05, gnorm=1.686, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5405
2023-02-16 17:19:32 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 28910 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.4, ups=0.9, wpb=110.2, bsz=40, num_updates=1420, lr=1.53514e-05, gnorm=1.717, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5416
2023-02-16 17:19:43 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 28910 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=102, ups=0.93, wpb=109.8, bsz=40, num_updates=1430, lr=1.54595e-05, gnorm=1.742, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5427
2023-02-16 17:19:54 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 28910 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.91, wpb=108.6, bsz=40, num_updates=1440, lr=1.55676e-05, gnorm=1.763, clip=100, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=5438
2023-02-16 17:20:05 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=1450, lr=1.56757e-05, gnorm=1.548, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5449
2023-02-16 17:20:16 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 28910 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.3, ups=0.89, wpb=109.1, bsz=40, num_updates=1460, lr=1.57838e-05, gnorm=1.697, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5460
2023-02-16 17:20:27 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 28910 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=1470, lr=1.58919e-05, gnorm=1.599, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5471
2023-02-16 17:20:38 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 28910 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99, ups=0.9, wpb=110.5, bsz=40, num_updates=1480, lr=1.6e-05, gnorm=1.514, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5483
2023-02-16 17:20:49 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.9, ups=0.93, wpb=110.8, bsz=40, num_updates=1490, lr=1.61081e-05, gnorm=1.487, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5493
2023-02-16 17:21:00 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.9, ups=0.88, wpb=111.8, bsz=40, num_updates=1500, lr=1.62162e-05, gnorm=1.503, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5505
2023-02-16 17:21:12 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=1510, lr=1.63243e-05, gnorm=1.726, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5516
2023-02-16 17:21:23 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=1520, lr=1.64324e-05, gnorm=1.533, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5527
2023-02-16 17:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 28910 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103.8, ups=0.93, wpb=111.6, bsz=40, num_updates=1530, lr=1.65405e-05, gnorm=1.647, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5538
2023-02-16 17:21:43 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 17:21:46 - progress_bar.py[line:274] - INFO: epoch 001:   1541 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=88.1, ups=0.8, wpb=110.5, bsz=40, num_updates=1540, lr=1.66486e-05, gnorm=1.483, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5550
2023-02-16 17:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   1551 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.6, ups=0.92, wpb=110.9, bsz=40, num_updates=1550, lr=1.67568e-05, gnorm=1.515, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5561
2023-02-16 17:22:08 - progress_bar.py[line:274] - INFO: epoch 001:   1561 / 28910 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.5, ups=0.92, wpb=109.5, bsz=40, num_updates=1560, lr=1.68649e-05, gnorm=1.716, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5572
2023-02-16 17:22:19 - progress_bar.py[line:274] - INFO: epoch 001:   1571 / 28910 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.91, wpb=109.6, bsz=40, num_updates=1570, lr=1.6973e-05, gnorm=1.645, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5583
2023-02-16 17:22:30 - progress_bar.py[line:274] - INFO: epoch 001:   1581 / 28910 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=1580, lr=1.70811e-05, gnorm=1.61, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5594
2023-02-16 17:22:41 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.7, ups=0.9, wpb=111.4, bsz=40, num_updates=1590, lr=1.71892e-05, gnorm=1.462, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5605
2023-02-16 17:22:53 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.6, ups=0.86, wpb=111, bsz=40, num_updates=1600, lr=1.72973e-05, gnorm=1.556, clip=100, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=5617
2023-02-16 17:23:04 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=1610, lr=1.74054e-05, gnorm=1.353, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5628
2023-02-16 17:23:15 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=1620, lr=1.75135e-05, gnorm=1.659, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5639
2023-02-16 17:23:26 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=1630, lr=1.76216e-05, gnorm=1.581, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5651
2023-02-16 17:23:37 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=1640, lr=1.77297e-05, gnorm=1.541, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5662
2023-02-16 17:23:48 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.4, ups=0.92, wpb=110.5, bsz=40, num_updates=1650, lr=1.78378e-05, gnorm=1.504, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5673
2023-02-16 17:24:00 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=1660, lr=1.79459e-05, gnorm=1.333, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5684
2023-02-16 17:24:11 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.2, ups=0.9, wpb=110.6, bsz=40, num_updates=1670, lr=1.80541e-05, gnorm=1.344, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5695
2023-02-16 17:24:22 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=1680, lr=1.81622e-05, gnorm=1.521, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5706
2023-02-16 17:24:34 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 28910 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.8, ups=0.87, wpb=110.2, bsz=40, num_updates=1690, lr=1.82703e-05, gnorm=1.557, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5718
2023-02-16 17:24:44 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 28910 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.8, ups=0.92, wpb=109.2, bsz=40, num_updates=1700, lr=1.83784e-05, gnorm=1.614, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5729
2023-02-16 17:24:55 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 28910 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.5, ups=0.91, wpb=109.8, bsz=40, num_updates=1710, lr=1.84865e-05, gnorm=1.768, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5740
2023-02-16 17:25:06 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 28910 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103.4, ups=0.93, wpb=110.6, bsz=40, num_updates=1720, lr=1.85946e-05, gnorm=1.586, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5750
2023-02-16 17:25:17 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=1730, lr=1.87027e-05, gnorm=1.487, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5762
2023-02-16 17:25:28 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 28910 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.9, ups=0.91, wpb=110.1, bsz=40, num_updates=1740, lr=1.88108e-05, gnorm=1.42, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5773
2023-02-16 17:25:40 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.9, ups=0.89, wpb=110.6, bsz=40, num_updates=1750, lr=1.89189e-05, gnorm=1.666, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5784
2023-02-16 17:25:51 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.1, ups=0.88, wpb=111.2, bsz=40, num_updates=1760, lr=1.9027e-05, gnorm=1.36, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5795
2023-02-16 17:26:02 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 28910 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101, ups=0.92, wpb=110.3, bsz=40, num_updates=1770, lr=1.91351e-05, gnorm=1.326, clip=100, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=5806
2023-02-16 17:26:14 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96, ups=0.87, wpb=110.6, bsz=40, num_updates=1780, lr=1.92432e-05, gnorm=1.473, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5818
2023-02-16 17:26:25 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 28910 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.1, ups=0.88, wpb=111.2, bsz=40, num_updates=1790, lr=1.93514e-05, gnorm=1.496, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5829
2023-02-16 17:26:36 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 28910 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.7, ups=0.88, wpb=109.6, bsz=40, num_updates=1800, lr=1.94595e-05, gnorm=1.465, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5840
2023-02-16 17:26:48 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 28910 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=95.5, ups=0.88, wpb=108.4, bsz=40, num_updates=1810, lr=1.95676e-05, gnorm=1.663, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5852
2023-02-16 17:26:59 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 28910 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=1820, lr=1.96757e-05, gnorm=1.395, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5863
2023-02-16 17:27:10 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.8, ups=0.89, wpb=108.3, bsz=40, num_updates=1830, lr=1.97838e-05, gnorm=1.42, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5874
2023-02-16 17:27:21 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 28910 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=1840, lr=1.98919e-05, gnorm=1.338, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5885
2023-02-16 17:27:32 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=104.1, ups=0.93, wpb=111.9, bsz=40, num_updates=1850, lr=2e-05, gnorm=1.34, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5896
2023-02-16 17:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 28910 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.3, ups=0.92, wpb=109, bsz=40, num_updates=1860, lr=2.01081e-05, gnorm=1.455, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5907
2023-02-16 17:27:54 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 28910 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.9, ups=0.88, wpb=111.8, bsz=40, num_updates=1870, lr=2.02162e-05, gnorm=1.311, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5918
2023-02-16 17:28:05 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 28910 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.3, ups=0.92, wpb=110.4, bsz=40, num_updates=1880, lr=2.03243e-05, gnorm=1.374, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5929
2023-02-16 17:28:16 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 28910 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=1890, lr=2.04324e-05, gnorm=1.498, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5941
2023-02-16 17:28:28 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=1900, lr=2.05405e-05, gnorm=1.601, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5952
2023-02-16 17:28:39 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=1910, lr=2.06486e-05, gnorm=1.65, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5963
2023-02-16 17:28:50 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=1920, lr=2.07568e-05, gnorm=1.313, clip=100, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5974
2023-02-16 17:29:01 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=1930, lr=2.08649e-05, gnorm=1.478, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5985
2023-02-16 17:29:12 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.4, ups=0.87, wpb=110.4, bsz=40, num_updates=1940, lr=2.0973e-05, gnorm=1.405, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5997
2023-02-16 17:29:23 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 28910 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.1, ups=0.92, wpb=110.2, bsz=40, num_updates=1950, lr=2.10811e-05, gnorm=1.628, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6008
2023-02-16 17:29:34 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 28910 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101, ups=0.93, wpb=108.9, bsz=40, num_updates=1960, lr=2.11892e-05, gnorm=1.51, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6018
2023-02-16 17:29:45 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 28910 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=1970, lr=2.12973e-05, gnorm=1.359, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6030
2023-02-16 17:29:57 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 28910 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.7, ups=0.88, wpb=111.8, bsz=40, num_updates=1980, lr=2.14054e-05, gnorm=1.398, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6041
2023-02-16 17:30:07 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 28910 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=104.7, ups=0.94, wpb=111.2, bsz=40, num_updates=1990, lr=2.15135e-05, gnorm=1.398, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6052
2023-02-16 17:30:19 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 28910 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=2000, lr=2.16216e-05, gnorm=1.549, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6063
2023-02-16 17:30:19 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 17:30:20 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 17:30:20 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 17:32:23 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 17:32:23 - train.py[line:551] - INFO: load:1.07 valid_run:122.90 task_valid:119.66 collect_output:2.19
2023-02-16 17:34:23 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 17:34:23 - train.py[line:551] - INFO: load:1.09 valid_run:242.79 task_valid:235.35 collect_output:5.38
2023-02-16 17:36:26 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 17:36:26 - train.py[line:551] - INFO: load:1.12 valid_run:365.40 task_valid:351.71 collect_output:10.62
2023-02-16 17:38:28 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 17:38:28 - train.py[line:551] - INFO: load:1.14 valid_run:487.40 task_valid:465.42 collect_output:17.90
2023-02-16 17:40:28 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 17:40:28 - train.py[line:551] - INFO: load:1.17 valid_run:607.78 task_valid:582.58 collect_output:20.11
2023-02-16 17:42:31 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 17:42:31 - train.py[line:551] - INFO: load:1.20 valid_run:730.57 task_valid:701.11 collect_output:23.39
2023-02-16 17:44:34 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 17:44:34 - train.py[line:551] - INFO: load:1.22 valid_run:853.61 task_valid:818.92 collect_output:27.62
2023-02-16 17:46:36 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 17:46:36 - train.py[line:551] - INFO: load:1.25 valid_run:975.51 task_valid:935.28 collect_output:32.17
2023-02-16 17:48:40 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 17:48:40 - train.py[line:551] - INFO: load:1.27 valid_run:1099.23 task_valid:1052.27 collect_output:37.91
2023-02-16 17:50:41 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 17:50:41 - train.py[line:551] - INFO: load:1.30 valid_run:1220.92 task_valid:1164.91 collect_output:45.97
2023-02-16 17:52:42 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 17:52:42 - train.py[line:551] - INFO: load:1.32 valid_run:1341.08 task_valid:1280.42 collect_output:49.63
2023-02-16 17:54:43 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 17:54:43 - train.py[line:551] - INFO: load:1.35 valid_run:1462.61 task_valid:1397.09 collect_output:53.49
2023-02-16 17:56:42 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 17:56:42 - train.py[line:551] - INFO: load:1.37 valid_run:1581.52 task_valid:1510.73 collect_output:57.78
2023-02-16 17:58:43 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 17:58:43 - train.py[line:551] - INFO: load:1.40 valid_run:1702.29 task_valid:1628.12 collect_output:60.15
2023-02-16 18:00:44 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 18:00:44 - train.py[line:551] - INFO: load:1.42 valid_run:1823.21 task_valid:1744.13 collect_output:64.05
2023-02-16 18:02:45 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 18:02:45 - train.py[line:551] - INFO: load:1.45 valid_run:1944.42 task_valid:1858.10 collect_output:70.30
2023-02-16 18:04:47 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 18:04:47 - train.py[line:551] - INFO: load:1.47 valid_run:2065.69 task_valid:1974.15 collect_output:74.51
2023-02-16 18:06:47 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 18:06:47 - train.py[line:551] - INFO: load:1.50 valid_run:2186.19 task_valid:2091.78 collect_output:76.36
2023-02-16 18:08:48 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 18:08:48 - train.py[line:551] - INFO: load:1.52 valid_run:2307.34 task_valid:2208.62 collect_output:79.67
2023-02-16 18:10:49 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 18:10:49 - train.py[line:551] - INFO: load:1.55 valid_run:2427.58 task_valid:2325.02 collect_output:82.50
2023-02-16 18:12:50 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 18:12:50 - train.py[line:551] - INFO: load:1.57 valid_run:2549.26 task_valid:2441.44 collect_output:86.77
2023-02-16 18:14:52 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 18:14:52 - train.py[line:551] - INFO: load:1.60 valid_run:2671.08 task_valid:2560.01 collect_output:89.02
2023-02-16 18:16:53 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 18:16:53 - train.py[line:551] - INFO: load:1.62 valid_run:2791.35 task_valid:2674.25 collect_output:94.05
2023-02-16 18:18:52 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 18:18:52 - train.py[line:551] - INFO: load:1.65 valid_run:2911.07 task_valid:2790.29 collect_output:96.73
2023-02-16 18:20:54 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 18:20:54 - train.py[line:551] - INFO: load:1.68 valid_run:3032.62 task_valid:2906.20 collect_output:101.35
2023-02-16 18:22:57 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 18:22:57 - train.py[line:551] - INFO: load:1.70 valid_run:3155.46 task_valid:3022.00 collect_output:107.39
2023-02-16 18:24:56 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 18:24:56 - train.py[line:551] - INFO: load:1.73 valid_run:3274.89 task_valid:3135.92 collect_output:111.89
2023-02-16 18:26:58 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 18:26:58 - train.py[line:551] - INFO: load:1.76 valid_run:3396.57 task_valid:3255.10 collect_output:113.39
2023-02-16 18:29:00 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 18:29:00 - train.py[line:551] - INFO: load:1.78 valid_run:3518.21 task_valid:3370.58 collect_output:118.55
2023-02-16 18:31:01 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 18:31:01 - train.py[line:551] - INFO: load:1.81 valid_run:3639.67 task_valid:3488.40 collect_output:121.19
2023-02-16 18:33:02 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 18:33:02 - train.py[line:551] - INFO: load:1.84 valid_run:3760.35 task_valid:3606.35 collect_output:122.94

====================================================================================================
SGG eval:     R @ 50: 0.3295;     R @ 100: 0.3820;     R @ 500: 0.4449;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1582;    mR @ 100: 0.2099;    mR @ 500: 0.2468;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0976) (covered in:0.0000) (covering:0.1429) (eating:0.5000) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5484) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.2500) (playing:0.0000) (riding:0.5078) (says:0.0000) (sitting on:0.4541) (standing on:0.4483) (using:0.2000) (walking in:0.0000) (walking on:0.2568) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.3295;     R @ 100: 0.3820;     R @ 500: 0.4449;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1582;    mR @ 100: 0.2099;    mR @ 500: 0.2468;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0976) (covered in:0.0000) (covering:0.1429) (eating:0.5000) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5484) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.2500) (playing:0.0000) (riding:0.5078) (says:0.0000) (sitting on:0.4541) (standing on:0.4483) (using:0.2000) (walking in:0.0000) (walking on:0.2568) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-16 18:33:32 - train.py[line:487] - INFO: 0.38203333333333334
2023-02-16 18:33:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 18:33:32 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.34 | loss_v1 0 | loss_v2 0 | nll_loss 0.17 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.382033 | ppl 1.12 | vqa_score 0.2162 | wps 118.3 | wpb 72 | bsz 24 | num_updates 2000 | best_R@100 0.382033
2023-02-16 18:33:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-02-16 18:33:32 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_2000.pt
2023-02-16 18:33:38 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_2000.pt
2023-02-16 18:33:43 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.38203333333333334) (writing took 10.327033931389451 seconds)
2023-02-16 18:33:54 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=0.3, ups=0, wpb=109.9, bsz=40, num_updates=2010, lr=2.17297e-05, gnorm=1.425, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=9878
2023-02-16 18:34:05 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 28910 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.2, ups=0.88, wpb=109.3, bsz=40, num_updates=2020, lr=2.18378e-05, gnorm=1.326, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=9890
2023-02-16 18:34:16 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 28910 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=2030, lr=2.19459e-05, gnorm=1.393, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9901
2023-02-16 18:34:27 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 28910 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=2040, lr=2.20541e-05, gnorm=1.388, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9912
2023-02-16 18:34:39 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.4, ups=0.88, wpb=109.7, bsz=40, num_updates=2050, lr=2.21622e-05, gnorm=1.431, clip=100, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=9923
2023-02-16 18:34:50 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 28910 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=2060, lr=2.22703e-05, gnorm=1.28, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9934
2023-02-16 18:35:01 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.2, ups=0.9, wpb=108.5, bsz=40, num_updates=2070, lr=2.23784e-05, gnorm=1.284, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9945
2023-02-16 18:35:12 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=2080, lr=2.24865e-05, gnorm=1.238, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9956
2023-02-16 18:35:23 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 28910 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=2090, lr=2.25946e-05, gnorm=1.229, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9968
2023-02-16 18:35:24 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 18:35:36 - progress_bar.py[line:274] - INFO: epoch 001:   2102 / 28910 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=89.9, ups=0.81, wpb=111.4, bsz=40, num_updates=2100, lr=2.27027e-05, gnorm=1.398, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=9980
2023-02-16 18:35:47 - progress_bar.py[line:274] - INFO: epoch 001:   2112 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.5, ups=0.89, wpb=109.4, bsz=40, num_updates=2110, lr=2.28108e-05, gnorm=1.35, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9991
2023-02-16 18:35:58 - progress_bar.py[line:274] - INFO: epoch 001:   2122 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.1, ups=0.89, wpb=108.9, bsz=40, num_updates=2120, lr=2.29189e-05, gnorm=1.248, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10002
2023-02-16 18:36:10 - progress_bar.py[line:274] - INFO: epoch 001:   2132 / 28910 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=95.9, ups=0.88, wpb=108.7, bsz=40, num_updates=2130, lr=2.3027e-05, gnorm=1.483, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10014
2023-02-16 18:36:21 - progress_bar.py[line:274] - INFO: epoch 001:   2142 / 28910 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=95.5, ups=0.88, wpb=108.3, bsz=40, num_updates=2140, lr=2.31351e-05, gnorm=1.373, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10025
2023-02-16 18:36:32 - progress_bar.py[line:274] - INFO: epoch 001:   2152 / 28910 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.9, ups=0.89, wpb=110.1, bsz=40, num_updates=2150, lr=2.32432e-05, gnorm=1.391, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10036
2023-02-16 18:36:43 - progress_bar.py[line:274] - INFO: epoch 001:   2162 / 28910 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=2160, lr=2.33514e-05, gnorm=1.323, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10048
2023-02-16 18:36:54 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 28910 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100, ups=0.92, wpb=109.2, bsz=40, num_updates=2170, lr=2.34595e-05, gnorm=1.524, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10059
2023-02-16 18:37:05 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 28910 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=2180, lr=2.35676e-05, gnorm=1.367, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10070
2023-02-16 18:37:17 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 28910 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=2190, lr=2.36757e-05, gnorm=1.276, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10081
2023-02-16 18:37:28 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.2, ups=0.92, wpb=109.4, bsz=40, num_updates=2200, lr=2.37838e-05, gnorm=1.432, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10092
2023-02-16 18:37:39 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 28910 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.8, ups=0.87, wpb=111.2, bsz=40, num_updates=2210, lr=2.38919e-05, gnorm=1.278, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10103
2023-02-16 18:37:50 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=2220, lr=2.4e-05, gnorm=1.314, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10115
2023-02-16 18:38:02 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 28910 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.1, ups=0.87, wpb=110.5, bsz=40, num_updates=2230, lr=2.41081e-05, gnorm=1.64, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10126
2023-02-16 18:38:13 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 28910 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=94.5, ups=0.87, wpb=109.2, bsz=40, num_updates=2240, lr=2.42162e-05, gnorm=1.398, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=10138
2023-02-16 18:38:25 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 28910 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.8, ups=0.89, wpb=110.4, bsz=40, num_updates=2250, lr=2.43243e-05, gnorm=1.334, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10149
2023-02-16 18:38:36 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 28910 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=2260, lr=2.44324e-05, gnorm=1.12, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10160
2023-02-16 18:38:47 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.6, ups=0.91, wpb=111.1, bsz=40, num_updates=2270, lr=2.45405e-05, gnorm=1.302, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10171
2023-02-16 18:38:58 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 28910 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=2280, lr=2.46486e-05, gnorm=1.147, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10182
2023-02-16 18:39:09 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.1, ups=0.92, wpb=110.8, bsz=40, num_updates=2290, lr=2.47568e-05, gnorm=1.267, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10193
2023-02-16 18:39:21 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 28910 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=95.1, ups=0.87, wpb=109.4, bsz=40, num_updates=2300, lr=2.48649e-05, gnorm=1.28, clip=100, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=10205
2023-02-16 18:39:32 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 28910 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.4, ups=0.92, wpb=110.3, bsz=40, num_updates=2310, lr=2.4973e-05, gnorm=1.236, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10216
2023-02-16 18:39:43 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.2, ups=0.89, wpb=108.9, bsz=40, num_updates=2320, lr=2.50811e-05, gnorm=1.256, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10227
2023-02-16 18:39:54 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 28910 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.4, ups=0.89, wpb=109.3, bsz=40, num_updates=2330, lr=2.51892e-05, gnorm=1.434, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10238
2023-02-16 18:40:05 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 28910 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.3, ups=0.92, wpb=110.4, bsz=40, num_updates=2340, lr=2.52973e-05, gnorm=1.174, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10249
2023-02-16 18:40:16 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=95.8, ups=0.87, wpb=110.2, bsz=40, num_updates=2350, lr=2.54054e-05, gnorm=1.471, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10261
2023-02-16 18:40:28 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 28910 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=2360, lr=2.55135e-05, gnorm=1.264, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10272
2023-02-16 18:40:39 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 28910 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=2370, lr=2.56216e-05, gnorm=1.093, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10283
2023-02-16 18:40:50 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 28910 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=2380, lr=2.57297e-05, gnorm=1.354, clip=90, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10294
2023-02-16 18:41:01 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=2390, lr=2.58378e-05, gnorm=1.413, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10306
2023-02-16 18:41:13 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 28910 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=2400, lr=2.59459e-05, gnorm=1.211, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10317
2023-02-16 18:41:24 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 28910 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.9, ups=0.91, wpb=110.2, bsz=40, num_updates=2410, lr=2.60541e-05, gnorm=1.142, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10328
2023-02-16 18:41:34 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 28910 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.4, ups=0.93, wpb=110.2, bsz=40, num_updates=2420, lr=2.61622e-05, gnorm=1.304, clip=80, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10339
2023-02-16 18:41:45 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 28910 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.5, ups=0.91, wpb=110.9, bsz=40, num_updates=2430, lr=2.62703e-05, gnorm=1.157, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10349
2023-02-16 18:41:56 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=2440, lr=2.63784e-05, gnorm=1.291, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10361
2023-02-16 18:42:08 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 28910 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=2450, lr=2.64865e-05, gnorm=1.289, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10372
2023-02-16 18:42:18 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 28910 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.2, ups=0.93, wpb=110.9, bsz=40, num_updates=2460, lr=2.65946e-05, gnorm=1.375, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10383
2023-02-16 18:42:30 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=2470, lr=2.67027e-05, gnorm=1.208, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10394
2023-02-16 18:42:41 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 28910 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=2480, lr=2.68108e-05, gnorm=1.11, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10405
2023-02-16 18:42:53 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.2, ups=0.87, wpb=111.5, bsz=40, num_updates=2490, lr=2.69189e-05, gnorm=1.357, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10417
2023-02-16 18:43:04 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 28910 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.3, ups=0.88, wpb=110.8, bsz=40, num_updates=2500, lr=2.7027e-05, gnorm=1.218, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10428
2023-02-16 18:43:15 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=2510, lr=2.71351e-05, gnorm=1.185, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10439
2023-02-16 18:43:26 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 28910 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=2520, lr=2.72432e-05, gnorm=1.253, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10451
2023-02-16 18:43:37 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 28910 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=2530, lr=2.73514e-05, gnorm=1.209, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10462
2023-02-16 18:43:48 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 28910 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=103.2, ups=0.93, wpb=111, bsz=40, num_updates=2540, lr=2.74595e-05, gnorm=1.032, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10472
2023-02-16 18:43:59 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 28910 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.92, wpb=108.7, bsz=40, num_updates=2550, lr=2.75676e-05, gnorm=1.249, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10483
2023-02-16 18:44:10 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 28910 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.2, ups=0.9, wpb=111.7, bsz=40, num_updates=2560, lr=2.76757e-05, gnorm=1.423, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10495
2023-02-16 18:44:22 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 28910 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=2570, lr=2.77838e-05, gnorm=1.196, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10506
2023-02-16 18:44:33 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 28910 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96, ups=0.87, wpb=110, bsz=40, num_updates=2580, lr=2.78919e-05, gnorm=1.08, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10517
2023-02-16 18:44:44 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 28910 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=2590, lr=2.8e-05, gnorm=1.05, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10529
2023-02-16 18:44:56 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.6, ups=0.87, wpb=111.2, bsz=40, num_updates=2600, lr=2.81081e-05, gnorm=1.188, clip=80, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=10540
2023-02-16 18:45:07 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.2, ups=0.87, wpb=111.2, bsz=40, num_updates=2610, lr=2.82162e-05, gnorm=1.16, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10552
2023-02-16 18:45:19 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.6, ups=0.88, wpb=111.9, bsz=40, num_updates=2620, lr=2.83243e-05, gnorm=1.138, clip=80, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10563
2023-02-16 18:45:25 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 18:45:31 - progress_bar.py[line:274] - INFO: epoch 001:   2633 / 28910 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=89.1, ups=0.81, wpb=110.2, bsz=40, num_updates=2630, lr=2.84324e-05, gnorm=1.182, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=10575
2023-02-16 18:45:42 - progress_bar.py[line:274] - INFO: epoch 001:   2643 / 28910 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.1, ups=0.88, wpb=111.1, bsz=40, num_updates=2640, lr=2.85405e-05, gnorm=1.087, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10587
2023-02-16 18:45:54 - progress_bar.py[line:274] - INFO: epoch 001:   2653 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.1, ups=0.89, wpb=110.2, bsz=40, num_updates=2650, lr=2.86486e-05, gnorm=1.09, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10598
2023-02-16 18:46:04 - progress_bar.py[line:274] - INFO: epoch 001:   2663 / 28910 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.2, ups=0.92, wpb=109.9, bsz=40, num_updates=2660, lr=2.87568e-05, gnorm=1.402, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10609
2023-02-16 18:46:15 - progress_bar.py[line:274] - INFO: epoch 001:   2673 / 28910 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.5, ups=0.92, wpb=107.3, bsz=40, num_updates=2670, lr=2.88649e-05, gnorm=1.26, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10620
2023-02-16 18:46:27 - progress_bar.py[line:274] - INFO: epoch 001:   2683 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.3, ups=0.87, wpb=110.6, bsz=40, num_updates=2680, lr=2.8973e-05, gnorm=0.995, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10631
2023-02-16 18:46:38 - progress_bar.py[line:274] - INFO: epoch 001:   2693 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.3, ups=0.88, wpb=109.9, bsz=40, num_updates=2690, lr=2.90811e-05, gnorm=1.217, clip=70, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=10643
2023-02-16 18:46:50 - progress_bar.py[line:274] - INFO: epoch 001:   2703 / 28910 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.1, ups=0.87, wpb=111.1, bsz=40, num_updates=2700, lr=2.91892e-05, gnorm=1.185, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10654
2023-02-16 18:47:01 - progress_bar.py[line:274] - INFO: epoch 001:   2713 / 28910 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=2710, lr=2.92973e-05, gnorm=1.173, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10665
2023-02-16 18:47:12 - progress_bar.py[line:274] - INFO: epoch 001:   2723 / 28910 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.4, ups=0.92, wpb=110.5, bsz=40, num_updates=2720, lr=2.94054e-05, gnorm=1.176, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10676
2023-02-16 18:47:23 - progress_bar.py[line:274] - INFO: epoch 001:   2733 / 28910 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=93.4, ups=0.86, wpb=108.2, bsz=40, num_updates=2730, lr=2.95135e-05, gnorm=1.11, clip=60, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=10688
2023-02-16 18:47:35 - progress_bar.py[line:274] - INFO: epoch 001:   2743 / 28910 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=2740, lr=2.96216e-05, gnorm=1.157, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10699
2023-02-16 18:47:46 - progress_bar.py[line:274] - INFO: epoch 001:   2753 / 28910 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.7, ups=0.91, wpb=110.1, bsz=40, num_updates=2750, lr=2.97297e-05, gnorm=1.266, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10710
2023-02-16 18:47:57 - progress_bar.py[line:274] - INFO: epoch 001:   2763 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=2760, lr=2.98378e-05, gnorm=1.112, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10721
2023-02-16 18:48:08 - progress_bar.py[line:274] - INFO: epoch 001:   2773 / 28910 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.1, ups=0.92, wpb=110.2, bsz=40, num_updates=2770, lr=2.99459e-05, gnorm=1.199, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10732
2023-02-16 18:48:19 - progress_bar.py[line:274] - INFO: epoch 001:   2783 / 28910 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=2780, lr=3.00541e-05, gnorm=1.189, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10743
2023-02-16 18:48:30 - progress_bar.py[line:274] - INFO: epoch 001:   2793 / 28910 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101, ups=0.9, wpb=111.8, bsz=40, num_updates=2790, lr=3.01622e-05, gnorm=1.005, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10754
2023-02-16 18:48:41 - progress_bar.py[line:274] - INFO: epoch 001:   2803 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.4, ups=0.88, wpb=111.5, bsz=40, num_updates=2800, lr=3.02703e-05, gnorm=1.148, clip=80, loss_scale=512, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=10766
2023-02-16 18:48:53 - progress_bar.py[line:274] - INFO: epoch 001:   2813 / 28910 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.9, ups=0.89, wpb=111.7, bsz=40, num_updates=2810, lr=3.03784e-05, gnorm=1.285, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10777
2023-02-16 18:49:04 - progress_bar.py[line:274] - INFO: epoch 001:   2823 / 28910 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=2820, lr=3.04865e-05, gnorm=1.21, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10788
2023-02-16 18:49:14 - progress_bar.py[line:274] - INFO: epoch 001:   2833 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=2830, lr=3.05946e-05, gnorm=1.118, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10799
2023-02-16 18:49:25 - progress_bar.py[line:274] - INFO: epoch 001:   2843 / 28910 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102.8, ups=0.92, wpb=111.7, bsz=40, num_updates=2840, lr=3.07027e-05, gnorm=1.217, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10810
2023-02-16 18:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   2853 / 28910 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=103, ups=0.93, wpb=110.4, bsz=40, num_updates=2850, lr=3.08108e-05, gnorm=1.074, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10820
2023-02-16 18:49:47 - progress_bar.py[line:274] - INFO: epoch 001:   2863 / 28910 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.4, ups=0.89, wpb=109, bsz=40, num_updates=2860, lr=3.09189e-05, gnorm=1.05, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10831
2023-02-16 18:49:58 - progress_bar.py[line:274] - INFO: epoch 001:   2873 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=2870, lr=3.1027e-05, gnorm=0.938, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10843
2023-02-16 18:50:10 - progress_bar.py[line:274] - INFO: epoch 001:   2883 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.5, ups=0.87, wpb=110.6, bsz=40, num_updates=2880, lr=3.11351e-05, gnorm=1.075, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10854
2023-02-16 18:50:21 - progress_bar.py[line:274] - INFO: epoch 001:   2893 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.7, ups=0.88, wpb=109.6, bsz=40, num_updates=2890, lr=3.12432e-05, gnorm=1.188, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10865
2023-02-16 18:50:32 - progress_bar.py[line:274] - INFO: epoch 001:   2903 / 28910 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.9, ups=0.9, wpb=111.7, bsz=40, num_updates=2900, lr=3.13514e-05, gnorm=1.214, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10877
2023-02-16 18:50:44 - progress_bar.py[line:274] - INFO: epoch 001:   2913 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.5, ups=0.87, wpb=110.9, bsz=40, num_updates=2910, lr=3.14595e-05, gnorm=1.14, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10888
2023-02-16 18:50:55 - progress_bar.py[line:274] - INFO: epoch 001:   2923 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=2920, lr=3.15676e-05, gnorm=0.894, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10899
2023-02-16 18:51:00 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-02-16 18:51:07 - progress_bar.py[line:274] - INFO: epoch 001:   2934 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=91.2, ups=0.83, wpb=109.9, bsz=40, num_updates=2930, lr=3.16757e-05, gnorm=1.077, clip=60, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=10911
2023-02-16 18:51:18 - progress_bar.py[line:274] - INFO: epoch 001:   2944 / 28910 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.5, ups=0.91, wpb=110.8, bsz=40, num_updates=2940, lr=3.17838e-05, gnorm=1.095, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10922
2023-02-16 18:51:29 - progress_bar.py[line:274] - INFO: epoch 001:   2954 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=2950, lr=3.18919e-05, gnorm=0.995, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10933
2023-02-16 18:51:41 - progress_bar.py[line:274] - INFO: epoch 001:   2964 / 28910 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=97.3, ups=0.88, wpb=109.9, bsz=40, num_updates=2960, lr=3.2e-05, gnorm=1.263, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10945
2023-02-16 18:51:52 - progress_bar.py[line:274] - INFO: epoch 001:   2974 / 28910 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=2970, lr=3.21081e-05, gnorm=1.139, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10956
2023-02-16 18:52:03 - progress_bar.py[line:274] - INFO: epoch 001:   2984 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=2980, lr=3.22162e-05, gnorm=0.975, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10967
2023-02-16 18:52:14 - progress_bar.py[line:274] - INFO: epoch 001:   2994 / 28910 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=2990, lr=3.23243e-05, gnorm=1.047, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10979
2023-02-16 18:52:25 - progress_bar.py[line:274] - INFO: epoch 001:   3004 / 28910 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.9, ups=0.91, wpb=110.4, bsz=40, num_updates=3000, lr=3.24324e-05, gnorm=1.111, clip=60, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10990
2023-02-16 18:52:25 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 18:52:27 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 18:52:27 - train.py[line:551] - INFO: load:0.89 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 18:54:29 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 18:54:29 - train.py[line:551] - INFO: load:0.92 valid_run:122.86 task_valid:119.62 collect_output:2.04
2023-02-16 18:56:29 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 18:56:29 - train.py[line:551] - INFO: load:0.95 valid_run:242.86 task_valid:235.58 collect_output:5.02
2023-02-16 18:58:32 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 18:58:32 - train.py[line:551] - INFO: load:0.97 valid_run:365.75 task_valid:352.34 collect_output:10.08
2023-02-16 19:00:35 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 19:00:35 - train.py[line:551] - INFO: load:1.00 valid_run:487.76 task_valid:466.06 collect_output:17.32
2023-02-16 19:02:35 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 19:02:35 - train.py[line:551] - INFO: load:1.03 valid_run:608.32 task_valid:583.30 collect_output:19.59
2023-02-16 19:04:38 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 19:04:38 - train.py[line:551] - INFO: load:1.05 valid_run:731.27 task_valid:701.90 collect_output:22.90
2023-02-16 19:06:41 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 19:06:41 - train.py[line:551] - INFO: load:1.08 valid_run:854.27 task_valid:819.92 collect_output:26.83
2023-02-16 19:08:43 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 19:08:43 - train.py[line:551] - INFO: load:1.11 valid_run:976.11 task_valid:936.46 collect_output:31.10
2023-02-16 19:10:47 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 19:10:47 - train.py[line:551] - INFO: load:1.13 valid_run:1099.86 task_valid:1053.63 collect_output:36.63
2023-02-16 19:12:49 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 19:12:49 - train.py[line:551] - INFO: load:1.16 valid_run:1221.61 task_valid:1166.38 collect_output:44.58
2023-02-16 19:14:49 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 19:14:49 - train.py[line:551] - INFO: load:1.19 valid_run:1341.76 task_valid:1281.84 collect_output:48.25
2023-02-16 19:16:51 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 19:16:51 - train.py[line:551] - INFO: load:1.21 valid_run:1463.27 task_valid:1398.49 collect_output:52.10
2023-02-16 19:18:50 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 19:18:50 - train.py[line:551] - INFO: load:1.24 valid_run:1582.27 task_valid:1512.17 collect_output:56.41
2023-02-16 19:20:50 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 19:20:50 - train.py[line:551] - INFO: load:1.26 valid_run:1703.00 task_valid:1629.52 collect_output:58.78
2023-02-16 19:22:51 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 19:22:51 - train.py[line:551] - INFO: load:1.29 valid_run:1823.75 task_valid:1745.34 collect_output:62.70
2023-02-16 19:24:52 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 19:24:52 - train.py[line:551] - INFO: load:1.31 valid_run:1944.89 task_valid:1859.16 collect_output:69.02
2023-02-16 19:26:54 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 19:26:54 - train.py[line:551] - INFO: load:1.34 valid_run:2066.20 task_valid:1975.16 collect_output:73.32
2023-02-16 19:28:54 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 19:28:54 - train.py[line:551] - INFO: load:1.36 valid_run:2186.61 task_valid:2092.71 collect_output:75.17
2023-02-16 19:30:55 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 19:30:55 - train.py[line:551] - INFO: load:1.39 valid_run:2307.83 task_valid:2209.56 collect_output:78.54
2023-02-16 19:32:56 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 19:32:56 - train.py[line:551] - INFO: load:1.42 valid_run:2428.12 task_valid:2325.91 collect_output:81.46
2023-02-16 19:34:58 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 19:34:58 - train.py[line:551] - INFO: load:1.44 valid_run:2549.80 task_valid:2442.19 collect_output:85.87
2023-02-16 19:37:00 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 19:37:00 - train.py[line:551] - INFO: load:1.47 valid_run:2671.71 task_valid:2560.86 collect_output:88.08
2023-02-16 19:39:00 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 19:39:00 - train.py[line:551] - INFO: load:1.49 valid_run:2792.05 task_valid:2675.14 collect_output:93.09
2023-02-16 19:41:00 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 19:41:00 - train.py[line:551] - INFO: load:1.52 valid_run:2911.80 task_valid:2791.16 collect_output:95.78
2023-02-16 19:43:01 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 19:43:01 - train.py[line:551] - INFO: load:1.54 valid_run:3033.38 task_valid:2907.10 collect_output:100.42
2023-02-16 19:45:04 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 19:45:04 - train.py[line:551] - INFO: load:1.57 valid_run:3156.30 task_valid:3022.87 collect_output:106.56
2023-02-16 19:47:04 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 19:47:04 - train.py[line:551] - INFO: load:1.60 valid_run:3275.66 task_valid:3136.64 collect_output:111.13
2023-02-16 19:49:05 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 19:49:05 - train.py[line:551] - INFO: load:1.62 valid_run:3397.24 task_valid:3255.70 collect_output:112.64
2023-02-16 19:51:07 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 19:51:07 - train.py[line:551] - INFO: load:1.65 valid_run:3518.89 task_valid:3371.05 collect_output:117.95
2023-02-16 19:53:09 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 19:53:09 - train.py[line:551] - INFO: load:1.67 valid_run:3640.63 task_valid:3489.20 collect_output:120.53
2023-02-16 19:55:10 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 19:55:10 - train.py[line:551] - INFO: load:1.70 valid_run:3761.42 task_valid:3607.17 collect_output:122.34

====================================================================================================
SGG eval:     R @ 50: 0.4568;     R @ 100: 0.5148;     R @ 500: 0.5858;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2515;    mR @ 100: 0.3222;    mR @ 500: 0.4027;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.4171) (covered in:0.0625) (covering:0.4000) (eating:0.6471) (flying in:0.6364) (growing on:0.3750) (hanging from:0.4355) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.5000) (playing:0.0000) (riding:0.7696) (says:0.0000) (sitting on:0.5692) (standing on:0.4183) (using:0.3500) (walking in:0.0000) (walking on:0.5135) (watching:0.0833) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4568;     R @ 100: 0.5148;     R @ 500: 0.5858;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2515;    mR @ 100: 0.3222;    mR @ 500: 0.4027;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.4171) (covered in:0.0625) (covering:0.4000) (eating:0.6471) (flying in:0.6364) (growing on:0.3750) (hanging from:0.4355) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.5000) (playing:0.0000) (riding:0.7696) (says:0.0000) (sitting on:0.5692) (standing on:0.4183) (using:0.3500) (walking in:0.0000) (walking on:0.5135) (watching:0.0833) 
--------------------------------------------------------
====================================================================================================

2023-02-16 19:55:40 - train.py[line:487] - INFO: 0.5147883116883116
2023-02-16 19:55:40 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 19:55:41 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.382 | loss_v1 0 | loss_v2 0 | nll_loss 0.228 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.514788 | ppl 1.17 | vqa_score 0.3164 | wps 118.2 | wpb 72 | bsz 24 | num_updates 3000 | best_R@100 0.514788
2023-02-16 19:55:41 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3000 updates
2023-02-16 19:55:41 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_3000.pt
2023-02-16 19:55:46 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_3000.pt
2023-02-16 19:55:51 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 0.5147883116883116) (writing took 10.48582823574543 seconds)
2023-02-16 19:56:02 - progress_bar.py[line:274] - INFO: epoch 001:   3014 / 28910 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=0.3, ups=0, wpb=110.1, bsz=40, num_updates=3010, lr=3.25405e-05, gnorm=1.161, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14806
2023-02-16 19:56:13 - progress_bar.py[line:274] - INFO: epoch 001:   3024 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.9, wpb=109.8, bsz=40, num_updates=3020, lr=3.26486e-05, gnorm=0.968, clip=40, loss_scale=256, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=14817
2023-02-16 19:56:24 - progress_bar.py[line:274] - INFO: epoch 001:   3034 / 28910 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.1, ups=0.91, wpb=111.8, bsz=40, num_updates=3030, lr=3.27568e-05, gnorm=1.26, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14828
2023-02-16 19:56:35 - progress_bar.py[line:274] - INFO: epoch 001:   3044 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98, ups=0.9, wpb=108.6, bsz=40, num_updates=3040, lr=3.28649e-05, gnorm=1.024, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14840
2023-02-16 19:56:46 - progress_bar.py[line:274] - INFO: epoch 001:   3054 / 28910 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=103.5, ups=0.93, wpb=111.2, bsz=40, num_updates=3050, lr=3.2973e-05, gnorm=1.11, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14850
2023-02-16 19:56:58 - progress_bar.py[line:274] - INFO: epoch 001:   3064 / 28910 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.8, ups=0.87, wpb=111.1, bsz=40, num_updates=3060, lr=3.30811e-05, gnorm=1.168, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14862
2023-02-16 19:57:08 - progress_bar.py[line:274] - INFO: epoch 001:   3074 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=3070, lr=3.31892e-05, gnorm=0.998, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14873
2023-02-16 19:57:20 - progress_bar.py[line:274] - INFO: epoch 001:   3084 / 28910 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=3080, lr=3.32973e-05, gnorm=0.949, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14884
2023-02-16 19:57:31 - progress_bar.py[line:274] - INFO: epoch 001:   3094 / 28910 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=3090, lr=3.34054e-05, gnorm=1.324, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14895
2023-02-16 19:57:42 - progress_bar.py[line:274] - INFO: epoch 001:   3104 / 28910 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.9, ups=0.9, wpb=109.8, bsz=40, num_updates=3100, lr=3.35135e-05, gnorm=1.175, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14906
2023-02-16 19:57:53 - progress_bar.py[line:274] - INFO: epoch 001:   3114 / 28910 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=3110, lr=3.36216e-05, gnorm=0.96, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14917
2023-02-16 19:58:05 - progress_bar.py[line:274] - INFO: epoch 001:   3124 / 28910 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.9, ups=0.88, wpb=111.4, bsz=40, num_updates=3120, lr=3.37297e-05, gnorm=1.041, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14929
2023-02-16 19:58:15 - progress_bar.py[line:274] - INFO: epoch 001:   3134 / 28910 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.7, ups=0.92, wpb=111.4, bsz=40, num_updates=3130, lr=3.38378e-05, gnorm=0.962, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14940
2023-02-16 19:58:27 - progress_bar.py[line:274] - INFO: epoch 001:   3144 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.2, ups=0.87, wpb=109.7, bsz=40, num_updates=3140, lr=3.39459e-05, gnorm=0.89, clip=30, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14951
2023-02-16 19:58:38 - progress_bar.py[line:274] - INFO: epoch 001:   3154 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.91, wpb=109.8, bsz=40, num_updates=3150, lr=3.40541e-05, gnorm=1.098, clip=50, loss_scale=256, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=14962
2023-02-16 19:58:49 - progress_bar.py[line:274] - INFO: epoch 001:   3164 / 28910 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=94.9, ups=0.87, wpb=109.5, bsz=40, num_updates=3160, lr=3.41622e-05, gnorm=1.111, clip=80, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14974
2023-02-16 19:59:01 - progress_bar.py[line:274] - INFO: epoch 001:   3174 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.9, wpb=109.8, bsz=40, num_updates=3170, lr=3.42703e-05, gnorm=1.044, clip=50, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14985
2023-02-16 19:59:12 - progress_bar.py[line:274] - INFO: epoch 001:   3184 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.7, ups=0.88, wpb=110.9, bsz=40, num_updates=3180, lr=3.43784e-05, gnorm=0.893, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14996
2023-02-16 19:59:23 - progress_bar.py[line:274] - INFO: epoch 001:   3194 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.3, ups=0.93, wpb=109.1, bsz=40, num_updates=3190, lr=3.44865e-05, gnorm=1.171, clip=70, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15007
2023-02-16 19:59:34 - progress_bar.py[line:274] - INFO: epoch 001:   3204 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.91, wpb=109.4, bsz=40, num_updates=3200, lr=3.45946e-05, gnorm=1.091, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15018
2023-02-16 19:59:45 - progress_bar.py[line:274] - INFO: epoch 001:   3214 / 28910 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=3210, lr=3.47027e-05, gnorm=1.095, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15029
2023-02-16 19:59:56 - progress_bar.py[line:274] - INFO: epoch 001:   3224 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.7, ups=0.89, wpb=108.5, bsz=40, num_updates=3220, lr=3.48108e-05, gnorm=1.152, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15040
2023-02-16 20:00:07 - progress_bar.py[line:274] - INFO: epoch 001:   3234 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.3, ups=0.92, wpb=111.8, bsz=40, num_updates=3230, lr=3.49189e-05, gnorm=0.947, clip=30, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15051
2023-02-16 20:00:18 - progress_bar.py[line:274] - INFO: epoch 001:   3244 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.1, ups=0.88, wpb=109.1, bsz=40, num_updates=3240, lr=3.5027e-05, gnorm=1.172, clip=60, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15063
2023-02-16 20:00:30 - progress_bar.py[line:274] - INFO: epoch 001:   3254 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=3250, lr=3.51351e-05, gnorm=1.071, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15074
2023-02-16 20:00:41 - progress_bar.py[line:274] - INFO: epoch 001:   3264 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=3260, lr=3.52432e-05, gnorm=1.324, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15085
2023-02-16 20:00:52 - progress_bar.py[line:274] - INFO: epoch 001:   3274 / 28910 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=3270, lr=3.53514e-05, gnorm=1.301, clip=80, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15096
2023-02-16 20:01:03 - progress_bar.py[line:274] - INFO: epoch 001:   3284 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=3280, lr=3.54595e-05, gnorm=1.171, clip=70, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15107
2023-02-16 20:01:14 - progress_bar.py[line:274] - INFO: epoch 001:   3294 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.6, ups=0.93, wpb=109.2, bsz=40, num_updates=3290, lr=3.55676e-05, gnorm=1.032, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15118
2023-02-16 20:01:25 - progress_bar.py[line:274] - INFO: epoch 001:   3304 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=3300, lr=3.56757e-05, gnorm=1.035, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15129
2023-02-16 20:01:36 - progress_bar.py[line:274] - INFO: epoch 001:   3314 / 28910 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=3310, lr=3.57838e-05, gnorm=1.066, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15140
2023-02-16 20:01:47 - progress_bar.py[line:274] - INFO: epoch 001:   3324 / 28910 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.3, ups=0.88, wpb=110.5, bsz=40, num_updates=3320, lr=3.58919e-05, gnorm=1.016, clip=60, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15152
2023-02-16 20:01:59 - progress_bar.py[line:274] - INFO: epoch 001:   3334 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=3330, lr=3.6e-05, gnorm=0.871, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15163
2023-02-16 20:02:10 - progress_bar.py[line:274] - INFO: epoch 001:   3344 / 28910 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=96.4, ups=0.88, wpb=109.5, bsz=40, num_updates=3340, lr=3.61081e-05, gnorm=0.991, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15174
2023-02-16 20:02:21 - progress_bar.py[line:274] - INFO: epoch 001:   3354 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.6, ups=0.89, wpb=111.6, bsz=40, num_updates=3350, lr=3.62162e-05, gnorm=0.883, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15185
2023-02-16 20:02:32 - progress_bar.py[line:274] - INFO: epoch 001:   3364 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=3360, lr=3.63243e-05, gnorm=0.962, clip=30, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15197
2023-02-16 20:02:43 - progress_bar.py[line:274] - INFO: epoch 001:   3374 / 28910 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.6, ups=0.91, wpb=110, bsz=40, num_updates=3370, lr=3.64324e-05, gnorm=0.954, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15208
2023-02-16 20:02:55 - progress_bar.py[line:274] - INFO: epoch 001:   3384 / 28910 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.9, ups=0.89, wpb=109, bsz=40, num_updates=3380, lr=3.65405e-05, gnorm=0.946, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15219
2023-02-16 20:03:06 - progress_bar.py[line:274] - INFO: epoch 001:   3394 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=3390, lr=3.66486e-05, gnorm=0.844, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15230
2023-02-16 20:03:17 - progress_bar.py[line:274] - INFO: epoch 001:   3404 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=3400, lr=3.67568e-05, gnorm=0.964, clip=50, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15241
2023-02-16 20:03:28 - progress_bar.py[line:274] - INFO: epoch 001:   3414 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.4, ups=0.92, wpb=111.7, bsz=40, num_updates=3410, lr=3.68649e-05, gnorm=1.009, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15252
2023-02-16 20:03:39 - progress_bar.py[line:274] - INFO: epoch 001:   3424 / 28910 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=3420, lr=3.6973e-05, gnorm=0.974, clip=40, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15263
2023-02-16 20:03:50 - progress_bar.py[line:274] - INFO: epoch 001:   3434 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=3430, lr=3.70811e-05, gnorm=0.891, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15275
2023-02-16 20:04:01 - progress_bar.py[line:274] - INFO: epoch 001:   3444 / 28910 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=3440, lr=3.71892e-05, gnorm=0.966, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15286
2023-02-16 20:04:13 - progress_bar.py[line:274] - INFO: epoch 001:   3454 / 28910 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=3450, lr=3.72973e-05, gnorm=0.917, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15297
2023-02-16 20:04:23 - progress_bar.py[line:274] - INFO: epoch 001:   3464 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.1, ups=0.93, wpb=109.8, bsz=40, num_updates=3460, lr=3.74054e-05, gnorm=0.862, clip=20, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=15308
2023-02-16 20:04:34 - progress_bar.py[line:274] - INFO: epoch 001:   3474 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.5, ups=0.91, wpb=110.9, bsz=40, num_updates=3470, lr=3.75135e-05, gnorm=1.045, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15319
2023-02-16 20:04:46 - progress_bar.py[line:274] - INFO: epoch 001:   3484 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=3480, lr=3.76216e-05, gnorm=1.006, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15330
2023-02-16 20:04:57 - progress_bar.py[line:274] - INFO: epoch 001:   3494 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=95.9, ups=0.86, wpb=111.2, bsz=40, num_updates=3490, lr=3.77297e-05, gnorm=0.961, clip=50, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=15342
2023-02-16 20:05:08 - progress_bar.py[line:274] - INFO: epoch 001:   3504 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.9, wpb=109.2, bsz=40, num_updates=3500, lr=3.78378e-05, gnorm=0.942, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15353
2023-02-16 20:05:20 - progress_bar.py[line:274] - INFO: epoch 001:   3514 / 28910 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=3510, lr=3.79459e-05, gnorm=1.114, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15364
2023-02-16 20:05:31 - progress_bar.py[line:274] - INFO: epoch 001:   3524 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.9, ups=0.9, wpb=108.7, bsz=40, num_updates=3520, lr=3.80541e-05, gnorm=0.998, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15375
2023-02-16 20:05:42 - progress_bar.py[line:274] - INFO: epoch 001:   3534 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.91, wpb=110.1, bsz=40, num_updates=3530, lr=3.81622e-05, gnorm=0.944, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15386
2023-02-16 20:05:53 - progress_bar.py[line:274] - INFO: epoch 001:   3544 / 28910 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=3540, lr=3.82703e-05, gnorm=1.068, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15398
2023-02-16 20:06:04 - progress_bar.py[line:274] - INFO: epoch 001:   3554 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=3550, lr=3.83784e-05, gnorm=0.964, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15409
2023-02-16 20:06:16 - progress_bar.py[line:274] - INFO: epoch 001:   3564 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=3560, lr=3.84865e-05, gnorm=0.895, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15420
2023-02-16 20:06:27 - progress_bar.py[line:274] - INFO: epoch 001:   3574 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=3570, lr=3.85946e-05, gnorm=0.97, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15431
2023-02-16 20:06:38 - progress_bar.py[line:274] - INFO: epoch 001:   3584 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=3580, lr=3.87027e-05, gnorm=0.867, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15442
2023-02-16 20:06:49 - progress_bar.py[line:274] - INFO: epoch 001:   3594 / 28910 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=3590, lr=3.88108e-05, gnorm=0.911, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15454
2023-02-16 20:07:01 - progress_bar.py[line:274] - INFO: epoch 001:   3604 / 28910 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=3600, lr=3.89189e-05, gnorm=0.951, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15465
2023-02-16 20:07:12 - progress_bar.py[line:274] - INFO: epoch 001:   3614 / 28910 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=3610, lr=3.9027e-05, gnorm=0.912, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15476
2023-02-16 20:07:23 - progress_bar.py[line:274] - INFO: epoch 001:   3624 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=95.9, ups=0.88, wpb=109, bsz=40, num_updates=3620, lr=3.91351e-05, gnorm=1.059, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15487
2023-02-16 20:07:35 - progress_bar.py[line:274] - INFO: epoch 001:   3634 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.9, ups=0.87, wpb=111, bsz=40, num_updates=3630, lr=3.92432e-05, gnorm=0.912, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15499
2023-02-16 20:07:46 - progress_bar.py[line:274] - INFO: epoch 001:   3644 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.7, ups=0.87, wpb=112.2, bsz=40, num_updates=3640, lr=3.93514e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15510
2023-02-16 20:07:57 - progress_bar.py[line:274] - INFO: epoch 001:   3654 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=3650, lr=3.94595e-05, gnorm=0.917, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15522
2023-02-16 20:08:08 - progress_bar.py[line:274] - INFO: epoch 001:   3664 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=103.5, ups=0.93, wpb=111.4, bsz=40, num_updates=3660, lr=3.95676e-05, gnorm=0.958, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15532
2023-02-16 20:08:19 - progress_bar.py[line:274] - INFO: epoch 001:   3674 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=3670, lr=3.96757e-05, gnorm=0.95, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15544
2023-02-16 20:08:31 - progress_bar.py[line:274] - INFO: epoch 001:   3684 / 28910 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=3680, lr=3.97838e-05, gnorm=0.958, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15555
2023-02-16 20:08:42 - progress_bar.py[line:274] - INFO: epoch 001:   3694 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.9, wpb=111.2, bsz=40, num_updates=3690, lr=3.98919e-05, gnorm=0.838, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15566
2023-02-16 20:08:53 - progress_bar.py[line:274] - INFO: epoch 001:   3704 / 28910 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101, ups=0.91, wpb=110.6, bsz=40, num_updates=3700, lr=4e-05, gnorm=0.862, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15577
2023-02-16 20:09:04 - progress_bar.py[line:274] - INFO: epoch 001:   3714 / 28910 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=3710, lr=4.01081e-05, gnorm=0.868, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15588
2023-02-16 20:09:15 - progress_bar.py[line:274] - INFO: epoch 001:   3724 / 28910 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=3720, lr=4.02162e-05, gnorm=0.958, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15600
2023-02-16 20:09:27 - progress_bar.py[line:274] - INFO: epoch 001:   3734 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=3730, lr=4.03243e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15611
2023-02-16 20:09:38 - progress_bar.py[line:274] - INFO: epoch 001:   3744 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=3740, lr=4.04324e-05, gnorm=0.975, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15622
2023-02-16 20:09:49 - progress_bar.py[line:274] - INFO: epoch 001:   3754 / 28910 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.9, ups=0.88, wpb=110.7, bsz=40, num_updates=3750, lr=4.05405e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15633
2023-02-16 20:10:00 - progress_bar.py[line:274] - INFO: epoch 001:   3764 / 28910 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=3760, lr=4.06486e-05, gnorm=0.821, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15645
2023-02-16 20:10:12 - progress_bar.py[line:274] - INFO: epoch 001:   3774 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.89, wpb=111, bsz=40, num_updates=3770, lr=4.07568e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15656
2023-02-16 20:10:23 - progress_bar.py[line:274] - INFO: epoch 001:   3784 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=3780, lr=4.08649e-05, gnorm=0.879, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15667
2023-02-16 20:10:34 - progress_bar.py[line:274] - INFO: epoch 001:   3794 / 28910 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=96.6, ups=0.88, wpb=109.8, bsz=40, num_updates=3790, lr=4.0973e-05, gnorm=1.066, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15678
2023-02-16 20:10:45 - progress_bar.py[line:274] - INFO: epoch 001:   3804 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.2, ups=0.93, wpb=111.2, bsz=40, num_updates=3800, lr=4.10811e-05, gnorm=0.791, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15689
2023-02-16 20:10:56 - progress_bar.py[line:274] - INFO: epoch 001:   3814 / 28910 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=94.8, ups=0.87, wpb=109, bsz=40, num_updates=3810, lr=4.11892e-05, gnorm=0.852, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15701
2023-02-16 20:11:07 - progress_bar.py[line:274] - INFO: epoch 001:   3824 / 28910 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.7, ups=0.93, wpb=110.8, bsz=40, num_updates=3820, lr=4.12973e-05, gnorm=1.12, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15712
2023-02-16 20:11:19 - progress_bar.py[line:274] - INFO: epoch 001:   3834 / 28910 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=3830, lr=4.14054e-05, gnorm=0.813, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15723
2023-02-16 20:11:30 - progress_bar.py[line:274] - INFO: epoch 001:   3844 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=95, ups=0.86, wpb=110.2, bsz=40, num_updates=3840, lr=4.15135e-05, gnorm=0.884, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15734
2023-02-16 20:11:41 - progress_bar.py[line:274] - INFO: epoch 001:   3854 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.9, ups=0.88, wpb=109.7, bsz=40, num_updates=3850, lr=4.16216e-05, gnorm=0.779, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15746
2023-02-16 20:11:53 - progress_bar.py[line:274] - INFO: epoch 001:   3864 / 28910 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.5, ups=0.88, wpb=109.3, bsz=40, num_updates=3860, lr=4.17297e-05, gnorm=0.863, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15757
2023-02-16 20:12:04 - progress_bar.py[line:274] - INFO: epoch 001:   3874 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=3870, lr=4.18378e-05, gnorm=0.912, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=15768
2023-02-16 20:12:16 - progress_bar.py[line:274] - INFO: epoch 001:   3884 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97, ups=0.88, wpb=109.8, bsz=40, num_updates=3880, lr=4.19459e-05, gnorm=0.931, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15780
2023-02-16 20:12:27 - progress_bar.py[line:274] - INFO: epoch 001:   3894 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=3890, lr=4.20541e-05, gnorm=0.888, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15791
2023-02-16 20:12:38 - progress_bar.py[line:274] - INFO: epoch 001:   3904 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.91, wpb=110.1, bsz=40, num_updates=3900, lr=4.21622e-05, gnorm=0.892, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15802
2023-02-16 20:12:49 - progress_bar.py[line:274] - INFO: epoch 001:   3914 / 28910 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.91, wpb=109.5, bsz=40, num_updates=3910, lr=4.22703e-05, gnorm=0.93, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15813
2023-02-16 20:13:00 - progress_bar.py[line:274] - INFO: epoch 001:   3924 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.9, ups=0.93, wpb=110.9, bsz=40, num_updates=3920, lr=4.23784e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15824
2023-02-16 20:13:11 - progress_bar.py[line:274] - INFO: epoch 001:   3934 / 28910 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=3930, lr=4.24865e-05, gnorm=1.011, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15835
2023-02-16 20:13:22 - progress_bar.py[line:274] - INFO: epoch 001:   3944 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=3940, lr=4.25946e-05, gnorm=0.871, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15846
2023-02-16 20:13:33 - progress_bar.py[line:274] - INFO: epoch 001:   3954 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.91, wpb=110.9, bsz=40, num_updates=3950, lr=4.27027e-05, gnorm=0.866, clip=40, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15857
2023-02-16 20:13:37 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 20:13:45 - progress_bar.py[line:274] - INFO: epoch 001:   3965 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=94.1, ups=0.86, wpb=109.5, bsz=40, num_updates=3960, lr=4.28108e-05, gnorm=0.858, clip=40, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=15869
2023-02-16 20:13:56 - progress_bar.py[line:274] - INFO: epoch 001:   3975 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.89, wpb=110.2, bsz=40, num_updates=3970, lr=4.29189e-05, gnorm=0.79, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15880
2023-02-16 20:14:07 - progress_bar.py[line:274] - INFO: epoch 001:   3985 / 28910 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.8, ups=0.89, wpb=108.6, bsz=40, num_updates=3980, lr=4.3027e-05, gnorm=0.875, clip=20, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=15891
2023-02-16 20:14:19 - progress_bar.py[line:274] - INFO: epoch 001:   3995 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=3990, lr=4.31351e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15903
2023-02-16 20:14:29 - progress_bar.py[line:274] - INFO: epoch 001:   4005 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.93, wpb=109.7, bsz=40, num_updates=4000, lr=4.32432e-05, gnorm=0.822, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=15914
2023-02-16 20:14:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 20:14:31 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 20:14:31 - train.py[line:551] - INFO: load:0.95 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 20:16:33 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 20:16:33 - train.py[line:551] - INFO: load:0.98 valid_run:122.71 task_valid:119.72 collect_output:1.93
2023-02-16 20:18:33 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 20:18:33 - train.py[line:551] - INFO: load:1.01 valid_run:242.52 task_valid:235.27 collect_output:5.17
2023-02-16 20:20:35 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 20:20:35 - train.py[line:551] - INFO: load:1.03 valid_run:364.43 task_valid:351.73 collect_output:9.57
2023-02-16 20:22:37 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 20:22:37 - train.py[line:551] - INFO: load:1.06 valid_run:486.43 task_valid:465.45 collect_output:16.82
2023-02-16 20:24:38 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 20:24:38 - train.py[line:551] - INFO: load:1.08 valid_run:606.92 task_valid:582.64 collect_output:19.07
2023-02-16 20:26:41 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 20:26:41 - train.py[line:551] - INFO: load:1.11 valid_run:729.66 task_valid:701.05 collect_output:22.34
2023-02-16 20:28:44 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 20:28:44 - train.py[line:551] - INFO: load:1.14 valid_run:852.66 task_valid:818.93 collect_output:26.44
2023-02-16 20:30:45 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 20:30:45 - train.py[line:551] - INFO: load:1.16 valid_run:974.45 task_valid:935.26 collect_output:30.87
2023-02-16 20:32:49 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 20:32:49 - train.py[line:551] - INFO: load:1.19 valid_run:1098.10 task_valid:1052.20 collect_output:36.53
2023-02-16 20:34:51 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 20:34:51 - train.py[line:551] - INFO: load:1.22 valid_run:1219.71 task_valid:1164.75 collect_output:44.56
2023-02-16 20:36:51 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 20:36:51 - train.py[line:551] - INFO: load:1.25 valid_run:1339.87 task_valid:1280.17 collect_output:48.28
2023-02-16 20:38:53 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 20:38:53 - train.py[line:551] - INFO: load:1.27 valid_run:1461.26 task_valid:1396.69 collect_output:52.11
2023-02-16 20:40:51 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 20:40:51 - train.py[line:551] - INFO: load:1.30 valid_run:1580.11 task_valid:1510.20 collect_output:56.42
2023-02-16 20:42:52 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 20:42:52 - train.py[line:551] - INFO: load:1.33 valid_run:1700.88 task_valid:1627.59 collect_output:58.78
2023-02-16 20:44:53 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 20:44:53 - train.py[line:551] - INFO: load:1.36 valid_run:1821.89 task_valid:1743.67 collect_output:62.68
2023-02-16 20:46:54 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 20:46:54 - train.py[line:551] - INFO: load:1.38 valid_run:1942.96 task_valid:1857.57 collect_output:68.82
2023-02-16 20:48:56 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 20:48:56 - train.py[line:551] - INFO: load:1.41 valid_run:2064.06 task_valid:1973.29 collect_output:73.17
2023-02-16 20:50:56 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 20:50:56 - train.py[line:551] - INFO: load:1.44 valid_run:2184.49 task_valid:2090.84 collect_output:75.00
2023-02-16 20:52:57 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 20:52:57 - train.py[line:551] - INFO: load:1.46 valid_run:2305.52 task_valid:2207.48 collect_output:78.36
2023-02-16 20:54:57 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 20:54:57 - train.py[line:551] - INFO: load:1.49 valid_run:2425.69 task_valid:2323.62 collect_output:81.37
2023-02-16 20:56:59 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 20:56:59 - train.py[line:551] - INFO: load:1.52 valid_run:2547.28 task_valid:2439.64 collect_output:85.89
2023-02-16 20:59:01 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 20:59:01 - train.py[line:551] - INFO: load:1.55 valid_run:2668.94 task_valid:2558.04 collect_output:88.14
2023-02-16 21:01:01 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 21:01:01 - train.py[line:551] - INFO: load:1.57 valid_run:2789.16 task_valid:2672.10 collect_output:93.30
2023-02-16 21:03:01 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 21:03:01 - train.py[line:551] - INFO: load:1.60 valid_run:2908.77 task_valid:2787.99 collect_output:96.00
2023-02-16 21:05:02 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 21:05:02 - train.py[line:551] - INFO: load:1.63 valid_run:3030.35 task_valid:2904.11 collect_output:100.44
2023-02-16 21:07:05 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 21:07:05 - train.py[line:551] - INFO: load:1.65 valid_run:3153.09 task_valid:3019.70 collect_output:106.55
2023-02-16 21:09:05 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 21:09:05 - train.py[line:551] - INFO: load:1.68 valid_run:3272.44 task_valid:3133.49 collect_output:111.10
2023-02-16 21:11:06 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 21:11:06 - train.py[line:551] - INFO: load:1.71 valid_run:3394.10 task_valid:3252.49 collect_output:112.75
2023-02-16 21:13:08 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 21:13:08 - train.py[line:551] - INFO: load:1.74 valid_run:3515.69 task_valid:3367.77 collect_output:118.01
2023-02-16 21:15:09 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 21:15:09 - train.py[line:551] - INFO: load:1.76 valid_run:3637.17 task_valid:3485.73 collect_output:120.52
2023-02-16 21:17:10 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 21:17:10 - train.py[line:551] - INFO: load:1.79 valid_run:3757.85 task_valid:3603.56 collect_output:122.33

====================================================================================================
SGG eval:     R @ 50: 0.5502;     R @ 100: 0.5997;     R @ 500: 0.6467;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3430;    mR @ 100: 0.4270;    mR @ 500: 0.4906;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6659) (covered in:0.2500) (covering:0.5714) (eating:0.7647) (flying in:0.7273) (growing on:0.5000) (hanging from:0.4839) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8750) (playing:0.0000) (riding:0.9232) (says:0.0000) (sitting on:0.6372) (standing on:0.2950) (using:0.4000) (walking in:0.0000) (walking on:0.6486) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5502;     R @ 100: 0.5997;     R @ 500: 0.6467;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3430;    mR @ 100: 0.4270;    mR @ 500: 0.4906;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6659) (covered in:0.2500) (covering:0.5714) (eating:0.7647) (flying in:0.7273) (growing on:0.5000) (hanging from:0.4839) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.8750) (playing:0.0000) (riding:0.9232) (says:0.0000) (sitting on:0.6372) (standing on:0.2950) (using:0.4000) (walking in:0.0000) (walking on:0.6486) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-02-16 21:17:41 - train.py[line:487] - INFO: 0.5996567099567099
2023-02-16 21:17:41 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 21:17:41 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.358 | loss_v1 0 | loss_v2 0 | nll_loss 0.21 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.599657 | ppl 1.16 | vqa_score 0.4212 | wps 118.3 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.599657
2023-02-16 21:17:41 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-02-16 21:17:41 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_4000.pt
2023-02-16 21:17:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_4000.pt
2023-02-16 21:17:52 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.5996567099567099) (writing took 10.795121861621737 seconds)
2023-02-16 21:18:03 - progress_bar.py[line:274] - INFO: epoch 001:   4015 / 28910 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=0.3, ups=0, wpb=109.2, bsz=40, num_updates=4010, lr=4.33514e-05, gnorm=0.999, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19727
2023-02-16 21:18:14 - progress_bar.py[line:274] - INFO: epoch 001:   4025 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=4020, lr=4.34595e-05, gnorm=0.746, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19739
2023-02-16 21:18:25 - progress_bar.py[line:274] - INFO: epoch 001:   4035 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.3, ups=0.91, wpb=111.2, bsz=40, num_updates=4030, lr=4.35676e-05, gnorm=0.904, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=19750
2023-02-16 21:18:37 - progress_bar.py[line:274] - INFO: epoch 001:   4045 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.6, ups=0.88, wpb=111.1, bsz=40, num_updates=4040, lr=4.36757e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19761
2023-02-16 21:18:47 - progress_bar.py[line:274] - INFO: epoch 001:   4055 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.9, ups=0.94, wpb=110.7, bsz=40, num_updates=4050, lr=4.37838e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19772
2023-02-16 21:18:59 - progress_bar.py[line:274] - INFO: epoch 001:   4065 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.6, ups=0.9, wpb=110.2, bsz=40, num_updates=4060, lr=4.38919e-05, gnorm=0.859, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19783
2023-02-16 21:19:09 - progress_bar.py[line:274] - INFO: epoch 001:   4075 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.6, ups=0.93, wpb=110.6, bsz=40, num_updates=4070, lr=4.4e-05, gnorm=0.979, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19794
2023-02-16 21:19:20 - progress_bar.py[line:274] - INFO: epoch 001:   4085 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.92, wpb=110.1, bsz=40, num_updates=4080, lr=4.41081e-05, gnorm=0.819, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19804
2023-02-16 21:19:32 - progress_bar.py[line:274] - INFO: epoch 001:   4095 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=4090, lr=4.42162e-05, gnorm=0.797, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19816
2023-02-16 21:19:43 - progress_bar.py[line:274] - INFO: epoch 001:   4105 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.89, wpb=110.2, bsz=40, num_updates=4100, lr=4.43243e-05, gnorm=0.97, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19827
2023-02-16 21:19:54 - progress_bar.py[line:274] - INFO: epoch 001:   4115 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.7, ups=0.91, wpb=110.1, bsz=40, num_updates=4110, lr=4.44324e-05, gnorm=0.888, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19838
2023-02-16 21:20:05 - progress_bar.py[line:274] - INFO: epoch 001:   4125 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.1, ups=0.88, wpb=109.8, bsz=40, num_updates=4120, lr=4.45405e-05, gnorm=0.878, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19849
2023-02-16 21:20:16 - progress_bar.py[line:274] - INFO: epoch 001:   4135 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.1, ups=0.91, wpb=110.5, bsz=40, num_updates=4130, lr=4.46486e-05, gnorm=0.814, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19860
2023-02-16 21:20:27 - progress_bar.py[line:274] - INFO: epoch 001:   4145 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.5, ups=0.91, wpb=110, bsz=40, num_updates=4140, lr=4.47568e-05, gnorm=0.869, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19871
2023-02-16 21:20:38 - progress_bar.py[line:274] - INFO: epoch 001:   4155 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.8, ups=0.92, wpb=111, bsz=40, num_updates=4150, lr=4.48649e-05, gnorm=0.799, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19882
2023-02-16 21:20:49 - progress_bar.py[line:274] - INFO: epoch 001:   4165 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.6, ups=0.92, wpb=109.7, bsz=40, num_updates=4160, lr=4.4973e-05, gnorm=0.984, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19893
2023-02-16 21:21:00 - progress_bar.py[line:274] - INFO: epoch 001:   4175 / 28910 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.5, ups=0.89, wpb=109, bsz=40, num_updates=4170, lr=4.50811e-05, gnorm=0.888, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19904
2023-02-16 21:21:11 - progress_bar.py[line:274] - INFO: epoch 001:   4185 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.3, ups=0.89, wpb=111, bsz=40, num_updates=4180, lr=4.51892e-05, gnorm=0.754, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19916
2023-02-16 21:21:23 - progress_bar.py[line:274] - INFO: epoch 001:   4195 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=4190, lr=4.52973e-05, gnorm=0.813, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19927
2023-02-16 21:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   4205 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=4200, lr=4.54054e-05, gnorm=0.889, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19938
2023-02-16 21:21:45 - progress_bar.py[line:274] - INFO: epoch 001:   4215 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.9, wpb=109.6, bsz=40, num_updates=4210, lr=4.55135e-05, gnorm=0.786, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19949
2023-02-16 21:21:56 - progress_bar.py[line:274] - INFO: epoch 001:   4225 / 28910 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=4220, lr=4.56216e-05, gnorm=0.978, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19960
2023-02-16 21:22:07 - progress_bar.py[line:274] - INFO: epoch 001:   4235 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.89, wpb=110.3, bsz=40, num_updates=4230, lr=4.57297e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19971
2023-02-16 21:22:18 - progress_bar.py[line:274] - INFO: epoch 001:   4245 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.2, ups=0.9, wpb=111.3, bsz=40, num_updates=4240, lr=4.58378e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19983
2023-02-16 21:22:29 - progress_bar.py[line:274] - INFO: epoch 001:   4255 / 28910 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=4250, lr=4.59459e-05, gnorm=0.9, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19994
2023-02-16 21:22:41 - progress_bar.py[line:274] - INFO: epoch 001:   4265 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.88, wpb=112, bsz=40, num_updates=4260, lr=4.60541e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20005
2023-02-16 21:22:52 - progress_bar.py[line:274] - INFO: epoch 001:   4275 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.1, ups=0.91, wpb=109.3, bsz=40, num_updates=4270, lr=4.61622e-05, gnorm=0.843, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20016
2023-02-16 21:23:03 - progress_bar.py[line:274] - INFO: epoch 001:   4285 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.2, ups=0.91, wpb=108.4, bsz=40, num_updates=4280, lr=4.62703e-05, gnorm=0.784, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20027
2023-02-16 21:23:14 - progress_bar.py[line:274] - INFO: epoch 001:   4295 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.1, ups=0.93, wpb=110.9, bsz=40, num_updates=4290, lr=4.63784e-05, gnorm=0.855, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=20038
2023-02-16 21:23:25 - progress_bar.py[line:274] - INFO: epoch 001:   4305 / 28910 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=4300, lr=4.64865e-05, gnorm=0.737, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20049
2023-02-16 21:23:36 - progress_bar.py[line:274] - INFO: epoch 001:   4315 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=4310, lr=4.65946e-05, gnorm=0.715, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20060
2023-02-16 21:23:47 - progress_bar.py[line:274] - INFO: epoch 001:   4325 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97, ups=0.88, wpb=110.2, bsz=40, num_updates=4320, lr=4.67027e-05, gnorm=0.742, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20072
2023-02-16 21:23:59 - progress_bar.py[line:274] - INFO: epoch 001:   4335 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=95.4, ups=0.87, wpb=109, bsz=40, num_updates=4330, lr=4.68108e-05, gnorm=0.921, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20083
2023-02-16 21:24:10 - progress_bar.py[line:274] - INFO: epoch 001:   4345 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=4340, lr=4.69189e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20094
2023-02-16 21:24:21 - progress_bar.py[line:274] - INFO: epoch 001:   4355 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=4350, lr=4.7027e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20105
2023-02-16 21:24:32 - progress_bar.py[line:274] - INFO: epoch 001:   4365 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.5, ups=0.91, wpb=110.6, bsz=40, num_updates=4360, lr=4.71351e-05, gnorm=0.835, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20116
2023-02-16 21:24:43 - progress_bar.py[line:274] - INFO: epoch 001:   4375 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=4370, lr=4.72432e-05, gnorm=0.762, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20128
2023-02-16 21:24:55 - progress_bar.py[line:274] - INFO: epoch 001:   4385 / 28910 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=4380, lr=4.73514e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20139
2023-02-16 21:25:06 - progress_bar.py[line:274] - INFO: epoch 001:   4395 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=4390, lr=4.74595e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=20150
2023-02-16 21:25:17 - progress_bar.py[line:274] - INFO: epoch 001:   4405 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=105.1, ups=0.95, wpb=110.9, bsz=40, num_updates=4400, lr=4.75676e-05, gnorm=0.895, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20161
2023-02-16 21:25:28 - progress_bar.py[line:274] - INFO: epoch 001:   4415 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=4410, lr=4.76757e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20172
2023-02-16 21:25:39 - progress_bar.py[line:274] - INFO: epoch 001:   4425 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.3, ups=0.87, wpb=112, bsz=40, num_updates=4420, lr=4.77838e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=20184
2023-02-16 21:25:51 - progress_bar.py[line:274] - INFO: epoch 001:   4435 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.1, ups=0.89, wpb=110.6, bsz=40, num_updates=4430, lr=4.78919e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20195
2023-02-16 21:26:02 - progress_bar.py[line:274] - INFO: epoch 001:   4445 / 28910 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=4440, lr=4.8e-05, gnorm=0.879, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20206
2023-02-16 21:26:13 - progress_bar.py[line:274] - INFO: epoch 001:   4455 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=4450, lr=4.81081e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=20217
2023-02-16 21:26:24 - progress_bar.py[line:274] - INFO: epoch 001:   4465 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=4460, lr=4.82162e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20228
2023-02-16 21:26:35 - progress_bar.py[line:274] - INFO: epoch 001:   4475 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.92, wpb=109.6, bsz=40, num_updates=4470, lr=4.83243e-05, gnorm=0.884, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20239
2023-02-16 21:26:46 - progress_bar.py[line:274] - INFO: epoch 001:   4485 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=4480, lr=4.84324e-05, gnorm=0.822, clip=10, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=20251
2023-02-16 21:26:58 - progress_bar.py[line:274] - INFO: epoch 001:   4495 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.89, wpb=109.6, bsz=40, num_updates=4490, lr=4.85405e-05, gnorm=0.862, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20262
2023-02-16 21:27:09 - progress_bar.py[line:274] - INFO: epoch 001:   4505 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=4500, lr=4.86486e-05, gnorm=0.82, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20273
2023-02-16 21:27:15 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 21:27:21 - progress_bar.py[line:274] - INFO: epoch 001:   4516 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=89.7, ups=0.82, wpb=109.7, bsz=40, num_updates=4510, lr=4.87568e-05, gnorm=0.759, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=20285
2023-02-16 21:27:32 - progress_bar.py[line:274] - INFO: epoch 001:   4526 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=4520, lr=4.88649e-05, gnorm=0.742, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20297
2023-02-16 21:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   4536 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.91, wpb=108.6, bsz=40, num_updates=4530, lr=4.8973e-05, gnorm=1.032, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20308
2023-02-16 21:27:55 - progress_bar.py[line:274] - INFO: epoch 001:   4546 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95, ups=0.87, wpb=109.4, bsz=40, num_updates=4540, lr=4.90811e-05, gnorm=0.808, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20319
2023-02-16 21:28:06 - progress_bar.py[line:274] - INFO: epoch 001:   4556 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=4550, lr=4.91892e-05, gnorm=0.948, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20330
2023-02-16 21:28:18 - progress_bar.py[line:274] - INFO: epoch 001:   4566 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=4560, lr=4.92973e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20342
2023-02-16 21:28:29 - progress_bar.py[line:274] - INFO: epoch 001:   4576 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.88, wpb=110.5, bsz=40, num_updates=4570, lr=4.94054e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20353
2023-02-16 21:28:40 - progress_bar.py[line:274] - INFO: epoch 001:   4586 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.9, wpb=110.2, bsz=40, num_updates=4580, lr=4.95135e-05, gnorm=0.798, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20364
2023-02-16 21:28:51 - progress_bar.py[line:274] - INFO: epoch 001:   4596 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.4, ups=0.87, wpb=110.3, bsz=40, num_updates=4590, lr=4.96216e-05, gnorm=0.825, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20376
2023-02-16 21:29:02 - progress_bar.py[line:274] - INFO: epoch 001:   4606 / 28910 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.7, ups=0.91, wpb=108.4, bsz=40, num_updates=4600, lr=4.97297e-05, gnorm=0.911, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20387
2023-02-16 21:29:14 - progress_bar.py[line:274] - INFO: epoch 001:   4616 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=4610, lr=4.98378e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20398
2023-02-16 21:29:25 - progress_bar.py[line:274] - INFO: epoch 001:   4626 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.7, ups=0.89, wpb=108.3, bsz=40, num_updates=4620, lr=4.99459e-05, gnorm=0.833, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20409
2023-02-16 21:29:36 - progress_bar.py[line:274] - INFO: epoch 001:   4636 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=4630, lr=4.99977e-05, gnorm=0.826, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20420
2023-02-16 21:29:47 - progress_bar.py[line:274] - INFO: epoch 001:   4646 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.9, wpb=109.5, bsz=40, num_updates=4640, lr=4.99932e-05, gnorm=0.726, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20431
2023-02-16 21:29:58 - progress_bar.py[line:274] - INFO: epoch 001:   4656 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=4650, lr=4.99887e-05, gnorm=0.807, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20442
2023-02-16 21:30:10 - progress_bar.py[line:274] - INFO: epoch 001:   4666 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98, ups=0.87, wpb=112.5, bsz=40, num_updates=4660, lr=4.99842e-05, gnorm=0.881, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20454
2023-02-16 21:30:21 - progress_bar.py[line:274] - INFO: epoch 001:   4676 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=4670, lr=4.99797e-05, gnorm=0.783, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20465
2023-02-16 21:30:32 - progress_bar.py[line:274] - INFO: epoch 001:   4686 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.4, ups=0.9, wpb=110.8, bsz=40, num_updates=4680, lr=4.99752e-05, gnorm=0.757, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20476
2023-02-16 21:30:43 - progress_bar.py[line:274] - INFO: epoch 001:   4696 / 28910 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=4690, lr=4.99707e-05, gnorm=0.754, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20487
2023-02-16 21:30:54 - progress_bar.py[line:274] - INFO: epoch 001:   4706 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99, ups=0.9, wpb=110.5, bsz=40, num_updates=4700, lr=4.99662e-05, gnorm=0.735, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20498
2023-02-16 21:31:06 - progress_bar.py[line:274] - INFO: epoch 001:   4716 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.88, wpb=111.3, bsz=40, num_updates=4710, lr=4.99617e-05, gnorm=0.685, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20510
2023-02-16 21:31:17 - progress_bar.py[line:274] - INFO: epoch 001:   4726 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=4720, lr=4.99572e-05, gnorm=0.725, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20521
2023-02-16 21:31:28 - progress_bar.py[line:274] - INFO: epoch 001:   4736 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.89, wpb=110.1, bsz=40, num_updates=4730, lr=4.99527e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20532
2023-02-16 21:31:39 - progress_bar.py[line:274] - INFO: epoch 001:   4746 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.1, ups=0.88, wpb=111.3, bsz=40, num_updates=4740, lr=4.99482e-05, gnorm=0.825, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20544
2023-02-16 21:31:51 - progress_bar.py[line:274] - INFO: epoch 001:   4756 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=4750, lr=4.99437e-05, gnorm=0.882, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20555
2023-02-16 21:32:01 - progress_bar.py[line:274] - INFO: epoch 001:   4766 / 28910 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=4760, lr=4.99392e-05, gnorm=0.815, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20566
2023-02-16 21:32:13 - progress_bar.py[line:274] - INFO: epoch 001:   4776 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=4770, lr=4.99347e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=20577
2023-02-16 21:32:24 - progress_bar.py[line:274] - INFO: epoch 001:   4786 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97, ups=0.89, wpb=108.6, bsz=40, num_updates=4780, lr=4.99302e-05, gnorm=0.763, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20588
2023-02-16 21:32:35 - progress_bar.py[line:274] - INFO: epoch 001:   4796 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.89, wpb=112.4, bsz=40, num_updates=4790, lr=4.99257e-05, gnorm=0.789, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20599
2023-02-16 21:32:46 - progress_bar.py[line:274] - INFO: epoch 001:   4806 / 28910 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=4800, lr=4.99212e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20611
2023-02-16 21:32:57 - progress_bar.py[line:274] - INFO: epoch 001:   4816 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.91, wpb=110.5, bsz=40, num_updates=4810, lr=4.99167e-05, gnorm=0.708, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20622
2023-02-16 21:33:08 - progress_bar.py[line:274] - INFO: epoch 001:   4826 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.91, wpb=108.4, bsz=40, num_updates=4820, lr=4.99122e-05, gnorm=0.725, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20633
2023-02-16 21:33:20 - progress_bar.py[line:274] - INFO: epoch 001:   4836 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=4830, lr=4.99077e-05, gnorm=0.736, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20644
2023-02-16 21:33:31 - progress_bar.py[line:274] - INFO: epoch 001:   4846 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.7, ups=0.91, wpb=112.9, bsz=40, num_updates=4840, lr=4.99032e-05, gnorm=0.711, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20655
2023-02-16 21:33:41 - progress_bar.py[line:274] - INFO: epoch 001:   4856 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.9, ups=0.93, wpb=109.7, bsz=40, num_updates=4850, lr=4.98987e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20666
2023-02-16 21:33:52 - progress_bar.py[line:274] - INFO: epoch 001:   4866 / 28910 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102, ups=0.93, wpb=109.8, bsz=40, num_updates=4860, lr=4.98942e-05, gnorm=0.915, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20677
2023-02-16 21:34:04 - progress_bar.py[line:274] - INFO: epoch 001:   4876 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.3, ups=0.87, wpb=110.6, bsz=40, num_updates=4870, lr=4.98897e-05, gnorm=0.818, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20688
2023-02-16 21:34:15 - progress_bar.py[line:274] - INFO: epoch 001:   4886 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=4880, lr=4.98852e-05, gnorm=0.783, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20699
2023-02-16 21:34:26 - progress_bar.py[line:274] - INFO: epoch 001:   4896 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=4890, lr=4.98806e-05, gnorm=1.013, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20710
2023-02-16 21:34:37 - progress_bar.py[line:274] - INFO: epoch 001:   4906 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.5, ups=0.93, wpb=110.4, bsz=40, num_updates=4900, lr=4.98761e-05, gnorm=0.887, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20721
2023-02-16 21:34:48 - progress_bar.py[line:274] - INFO: epoch 001:   4916 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.4, ups=0.9, wpb=110.9, bsz=40, num_updates=4910, lr=4.98716e-05, gnorm=0.838, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20732
2023-02-16 21:34:59 - progress_bar.py[line:274] - INFO: epoch 001:   4926 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.9, ups=0.92, wpb=112.2, bsz=40, num_updates=4920, lr=4.98671e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20743
2023-02-16 21:35:10 - progress_bar.py[line:274] - INFO: epoch 001:   4936 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=4930, lr=4.98626e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20755
2023-02-16 21:35:21 - progress_bar.py[line:274] - INFO: epoch 001:   4946 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=4940, lr=4.98581e-05, gnorm=0.76, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20766
2023-02-16 21:35:33 - progress_bar.py[line:274] - INFO: epoch 001:   4956 / 28910 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=4950, lr=4.98536e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20777
2023-02-16 21:35:44 - progress_bar.py[line:274] - INFO: epoch 001:   4966 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=4960, lr=4.98491e-05, gnorm=0.844, clip=30, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=20788
2023-02-16 21:35:55 - progress_bar.py[line:274] - INFO: epoch 001:   4976 / 28910 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=95.7, ups=0.88, wpb=108.7, bsz=40, num_updates=4970, lr=4.98446e-05, gnorm=0.784, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20800
2023-02-16 21:36:07 - progress_bar.py[line:274] - INFO: epoch 001:   4986 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=4980, lr=4.98401e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20811
2023-02-16 21:36:17 - progress_bar.py[line:274] - INFO: epoch 001:   4996 / 28910 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.9, ups=0.92, wpb=109.3, bsz=40, num_updates=4990, lr=4.98356e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20822
2023-02-16 21:36:29 - progress_bar.py[line:274] - INFO: epoch 001:   5006 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=5000, lr=4.98311e-05, gnorm=0.759, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20833
2023-02-16 21:36:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 21:36:30 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 21:36:30 - train.py[line:551] - INFO: load:1.01 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 21:38:32 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 21:38:32 - train.py[line:551] - INFO: load:1.04 valid_run:122.08 task_valid:119.16 collect_output:1.83
2023-02-16 21:40:32 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 21:40:32 - train.py[line:551] - INFO: load:1.07 valid_run:242.16 task_valid:235.15 collect_output:4.82
2023-02-16 21:42:34 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 21:42:34 - train.py[line:551] - INFO: load:1.10 valid_run:364.37 task_valid:352.08 collect_output:9.01
2023-02-16 21:44:36 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 21:44:36 - train.py[line:551] - INFO: load:1.13 valid_run:486.43 task_valid:466.10 collect_output:15.99
2023-02-16 21:46:37 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 21:46:37 - train.py[line:551] - INFO: load:1.15 valid_run:607.10 task_valid:583.63 collect_output:18.04
2023-02-16 21:48:40 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 21:48:40 - train.py[line:551] - INFO: load:1.18 valid_run:730.09 task_valid:702.47 collect_output:21.13
2023-02-16 21:50:43 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 21:50:43 - train.py[line:551] - INFO: load:1.21 valid_run:853.12 task_valid:820.73 collect_output:24.83
2023-02-16 21:52:45 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 21:52:45 - train.py[line:551] - INFO: load:1.24 valid_run:974.99 task_valid:937.42 collect_output:28.96
2023-02-16 21:54:49 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 21:54:49 - train.py[line:551] - INFO: load:1.26 valid_run:1098.72 task_valid:1054.68 collect_output:34.35
2023-02-16 21:56:51 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 21:56:51 - train.py[line:551] - INFO: load:1.29 valid_run:1220.43 task_valid:1167.56 collect_output:42.13
2023-02-16 21:58:51 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 21:58:51 - train.py[line:551] - INFO: load:1.32 valid_run:1340.66 task_valid:1283.40 collect_output:45.48
2023-02-16 22:00:53 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 22:00:53 - train.py[line:551] - INFO: load:1.34 valid_run:1462.25 task_valid:1400.25 collect_output:49.19
2023-02-16 22:02:52 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 22:02:52 - train.py[line:551] - INFO: load:1.37 valid_run:1581.15 task_valid:1514.09 collect_output:53.22
2023-02-16 22:04:53 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 22:04:53 - train.py[line:551] - INFO: load:1.40 valid_run:1701.91 task_valid:1631.63 collect_output:55.41
2023-02-16 22:06:53 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 22:06:53 - train.py[line:551] - INFO: load:1.43 valid_run:1822.71 task_valid:1747.63 collect_output:59.19
2023-02-16 22:08:55 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 22:08:55 - train.py[line:551] - INFO: load:1.45 valid_run:1943.81 task_valid:1861.73 collect_output:65.16
2023-02-16 22:10:56 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 22:10:56 - train.py[line:551] - INFO: load:1.48 valid_run:2065.08 task_valid:1977.84 collect_output:69.27
2023-02-16 22:12:57 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 22:12:57 - train.py[line:551] - INFO: load:1.50 valid_run:2185.73 task_valid:2095.63 collect_output:71.11
2023-02-16 22:14:58 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 22:14:58 - train.py[line:551] - INFO: load:1.53 valid_run:2306.80 task_valid:2212.38 collect_output:74.40
2023-02-16 22:16:58 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 22:16:58 - train.py[line:551] - INFO: load:1.56 valid_run:2427.10 task_valid:2328.87 collect_output:77.15
2023-02-16 22:19:00 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 22:19:00 - train.py[line:551] - INFO: load:1.58 valid_run:2548.78 task_valid:2445.28 collect_output:81.37
2023-02-16 22:21:02 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 22:21:02 - train.py[line:551] - INFO: load:1.61 valid_run:2670.58 task_valid:2563.91 collect_output:83.53
2023-02-16 22:23:02 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 22:23:02 - train.py[line:551] - INFO: load:1.64 valid_run:2790.94 task_valid:2678.27 collect_output:88.49
2023-02-16 22:25:02 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 22:25:02 - train.py[line:551] - INFO: load:1.66 valid_run:2910.79 task_valid:2794.48 collect_output:91.08
2023-02-16 22:27:04 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 22:27:04 - train.py[line:551] - INFO: load:1.69 valid_run:3032.30 task_valid:2910.54 collect_output:95.48
2023-02-16 22:29:06 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 22:29:07 - train.py[line:551] - INFO: load:1.72 valid_run:3155.11 task_valid:3026.52 collect_output:101.29
2023-02-16 22:31:06 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 22:31:06 - train.py[line:551] - INFO: load:1.74 valid_run:3274.53 task_valid:3140.44 collect_output:105.77
2023-02-16 22:33:08 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 22:33:08 - train.py[line:551] - INFO: load:1.77 valid_run:3396.35 task_valid:3259.75 collect_output:107.23
2023-02-16 22:35:09 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 22:35:09 - train.py[line:551] - INFO: load:1.80 valid_run:3517.92 task_valid:3375.30 collect_output:112.21
2023-02-16 22:37:11 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 22:37:11 - train.py[line:551] - INFO: load:1.83 valid_run:3639.64 task_valid:3493.56 collect_output:114.62
2023-02-16 22:39:12 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 22:39:12 - train.py[line:551] - INFO: load:1.85 valid_run:3760.34 task_valid:3611.51 collect_output:116.37

====================================================================================================
SGG eval:     R @ 50: 0.5938;     R @ 100: 0.6298;     R @ 500: 0.6715;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4088;    mR @ 100: 0.4793;    mR @ 500: 0.5385;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7317) (covered in:0.6875) (covering:0.5429) (eating:0.8235) (flying in:0.7273) (growing on:0.5000) (hanging from:0.5161) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9575) (says:0.0000) (sitting on:0.6514) (standing on:0.2300) (using:0.5500) (walking in:0.0000) (walking on:0.7297) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5938;     R @ 100: 0.6298;     R @ 500: 0.6715;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4088;    mR @ 100: 0.4793;    mR @ 500: 0.5385;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7317) (covered in:0.6875) (covering:0.5429) (eating:0.8235) (flying in:0.7273) (growing on:0.5000) (hanging from:0.5161) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9575) (says:0.0000) (sitting on:0.6514) (standing on:0.2300) (using:0.5500) (walking in:0.0000) (walking on:0.7297) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-02-16 22:39:43 - train.py[line:487] - INFO: 0.6297709956709957
2023-02-16 22:39:43 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 22:39:43 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.331 | loss_v1 0 | loss_v2 0 | nll_loss 0.175 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.629771 | ppl 1.13 | vqa_score 0.4842 | wps 118.3 | wpb 72 | bsz 24 | num_updates 5000 | best_R@100 0.629771
2023-02-16 22:39:43 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5000 updates
2023-02-16 22:39:43 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_5000.pt
2023-02-16 22:39:49 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_5000.pt
2023-02-16 22:39:53 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 0.6297709956709957) (writing took 10.60729650221765 seconds)
2023-02-16 22:40:04 - progress_bar.py[line:274] - INFO: epoch 001:   5016 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=0.3, ups=0, wpb=109.8, bsz=40, num_updates=5010, lr=4.98266e-05, gnorm=0.763, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24649
2023-02-16 22:40:16 - progress_bar.py[line:274] - INFO: epoch 001:   5026 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.4, ups=0.87, wpb=110.5, bsz=40, num_updates=5020, lr=4.98221e-05, gnorm=0.715, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24660
2023-02-16 22:40:27 - progress_bar.py[line:274] - INFO: epoch 001:   5036 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.88, wpb=112.1, bsz=40, num_updates=5030, lr=4.98176e-05, gnorm=0.617, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24672
2023-02-16 22:40:39 - progress_bar.py[line:274] - INFO: epoch 001:   5046 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=5040, lr=4.98131e-05, gnorm=0.71, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24683
2023-02-16 22:40:50 - progress_bar.py[line:274] - INFO: epoch 001:   5056 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.89, wpb=110.8, bsz=40, num_updates=5050, lr=4.98086e-05, gnorm=0.809, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24694
2023-02-16 22:41:01 - progress_bar.py[line:274] - INFO: epoch 001:   5066 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.5, ups=0.92, wpb=111.3, bsz=40, num_updates=5060, lr=4.98041e-05, gnorm=0.805, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24705
2023-02-16 22:41:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 22:41:13 - progress_bar.py[line:274] - INFO: epoch 001:   5077 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=91.3, ups=0.82, wpb=111.6, bsz=40, num_updates=5070, lr=4.97996e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=24717
2023-02-16 22:41:24 - progress_bar.py[line:274] - INFO: epoch 001:   5087 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=5080, lr=4.97951e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24728
2023-02-16 22:41:35 - progress_bar.py[line:274] - INFO: epoch 001:   5097 / 28910 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.3, ups=0.93, wpb=110.1, bsz=40, num_updates=5090, lr=4.97906e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24739
2023-02-16 22:41:46 - progress_bar.py[line:274] - INFO: epoch 001:   5107 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.3, ups=0.92, wpb=110.5, bsz=40, num_updates=5100, lr=4.97861e-05, gnorm=0.764, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24750
2023-02-16 22:41:57 - progress_bar.py[line:274] - INFO: epoch 001:   5117 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=5110, lr=4.97816e-05, gnorm=0.736, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24761
2023-02-16 22:42:08 - progress_bar.py[line:274] - INFO: epoch 001:   5127 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.8, ups=0.91, wpb=110.3, bsz=40, num_updates=5120, lr=4.97771e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24772
2023-02-16 22:42:19 - progress_bar.py[line:274] - INFO: epoch 001:   5137 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=5130, lr=4.97726e-05, gnorm=0.773, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24784
2023-02-16 22:42:31 - progress_bar.py[line:274] - INFO: epoch 001:   5147 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.9, ups=0.88, wpb=109, bsz=40, num_updates=5140, lr=4.9768e-05, gnorm=0.766, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24795
2023-02-16 22:42:42 - progress_bar.py[line:274] - INFO: epoch 001:   5157 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=5150, lr=4.97635e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24806
2023-02-16 22:42:53 - progress_bar.py[line:274] - INFO: epoch 001:   5167 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=5160, lr=4.9759e-05, gnorm=0.811, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24817
2023-02-16 22:43:04 - progress_bar.py[line:274] - INFO: epoch 001:   5177 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.9, wpb=110.2, bsz=40, num_updates=5170, lr=4.97545e-05, gnorm=0.688, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24828
2023-02-16 22:43:15 - progress_bar.py[line:274] - INFO: epoch 001:   5187 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=5180, lr=4.975e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24839
2023-02-16 22:43:26 - progress_bar.py[line:274] - INFO: epoch 001:   5197 / 28910 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.7, ups=0.88, wpb=110.6, bsz=40, num_updates=5190, lr=4.97455e-05, gnorm=0.823, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24851
2023-02-16 22:43:37 - progress_bar.py[line:274] - INFO: epoch 001:   5207 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.9, ups=0.93, wpb=109.7, bsz=40, num_updates=5200, lr=4.9741e-05, gnorm=0.739, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24861
2023-02-16 22:43:48 - progress_bar.py[line:274] - INFO: epoch 001:   5217 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=5210, lr=4.97365e-05, gnorm=0.704, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24872
2023-02-16 22:43:59 - progress_bar.py[line:274] - INFO: epoch 001:   5227 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.2, ups=0.93, wpb=110.4, bsz=40, num_updates=5220, lr=4.9732e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24883
2023-02-16 22:44:11 - progress_bar.py[line:274] - INFO: epoch 001:   5237 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.7, ups=0.87, wpb=109.9, bsz=40, num_updates=5230, lr=4.97275e-05, gnorm=0.717, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24895
2023-02-16 22:44:22 - progress_bar.py[line:274] - INFO: epoch 001:   5247 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.9, ups=0.88, wpb=108.8, bsz=40, num_updates=5240, lr=4.9723e-05, gnorm=0.703, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24906
2023-02-16 22:44:33 - progress_bar.py[line:274] - INFO: epoch 001:   5257 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.6, ups=0.9, wpb=109.2, bsz=40, num_updates=5250, lr=4.97185e-05, gnorm=0.881, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24917
2023-02-16 22:44:44 - progress_bar.py[line:274] - INFO: epoch 001:   5267 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=5260, lr=4.9714e-05, gnorm=0.706, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24928
2023-02-16 22:44:56 - progress_bar.py[line:274] - INFO: epoch 001:   5277 / 28910 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.8, ups=0.88, wpb=109.9, bsz=40, num_updates=5270, lr=4.97095e-05, gnorm=0.776, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24940
2023-02-16 22:45:07 - progress_bar.py[line:274] - INFO: epoch 001:   5287 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.8, ups=0.88, wpb=108.5, bsz=40, num_updates=5280, lr=4.9705e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24951
2023-02-16 22:45:18 - progress_bar.py[line:274] - INFO: epoch 001:   5297 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.5, ups=0.93, wpb=110.5, bsz=40, num_updates=5290, lr=4.97005e-05, gnorm=0.797, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24962
2023-02-16 22:45:29 - progress_bar.py[line:274] - INFO: epoch 001:   5307 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.92, wpb=109.9, bsz=40, num_updates=5300, lr=4.9696e-05, gnorm=0.744, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24973
2023-02-16 22:45:40 - progress_bar.py[line:274] - INFO: epoch 001:   5317 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.91, wpb=108.8, bsz=40, num_updates=5310, lr=4.96915e-05, gnorm=0.689, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24984
2023-02-16 22:45:51 - progress_bar.py[line:274] - INFO: epoch 001:   5327 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=5320, lr=4.9687e-05, gnorm=0.79, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24995
2023-02-16 22:46:02 - progress_bar.py[line:274] - INFO: epoch 001:   5337 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.5, ups=0.92, wpb=110.7, bsz=40, num_updates=5330, lr=4.96825e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25006
2023-02-16 22:46:13 - progress_bar.py[line:274] - INFO: epoch 001:   5347 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.89, wpb=108.9, bsz=40, num_updates=5340, lr=4.9678e-05, gnorm=0.72, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25017
2023-02-16 22:46:24 - progress_bar.py[line:274] - INFO: epoch 001:   5357 / 28910 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.4, ups=0.89, wpb=112.4, bsz=40, num_updates=5350, lr=4.96735e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25028
2023-02-16 22:46:35 - progress_bar.py[line:274] - INFO: epoch 001:   5367 / 28910 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.2, ups=0.91, wpb=111.8, bsz=40, num_updates=5360, lr=4.9669e-05, gnorm=0.824, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25039
2023-02-16 22:46:46 - progress_bar.py[line:274] - INFO: epoch 001:   5377 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.89, wpb=109.4, bsz=40, num_updates=5370, lr=4.96645e-05, gnorm=0.777, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25051
2023-02-16 22:46:58 - progress_bar.py[line:274] - INFO: epoch 001:   5387 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.6, ups=0.87, wpb=111.2, bsz=40, num_updates=5380, lr=4.966e-05, gnorm=0.709, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25062
2023-02-16 22:47:09 - progress_bar.py[line:274] - INFO: epoch 001:   5397 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=5390, lr=4.96555e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25074
2023-02-16 22:47:21 - progress_bar.py[line:274] - INFO: epoch 001:   5407 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=5400, lr=4.96509e-05, gnorm=0.791, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25085
2023-02-16 22:47:32 - progress_bar.py[line:274] - INFO: epoch 001:   5417 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.2, ups=0.9, wpb=109.6, bsz=40, num_updates=5410, lr=4.96464e-05, gnorm=0.809, clip=30, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25096
2023-02-16 22:47:43 - progress_bar.py[line:274] - INFO: epoch 001:   5427 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.7, ups=0.9, wpb=111.4, bsz=40, num_updates=5420, lr=4.96419e-05, gnorm=0.777, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25107
2023-02-16 22:47:54 - progress_bar.py[line:274] - INFO: epoch 001:   5437 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.4, ups=0.89, wpb=109.3, bsz=40, num_updates=5430, lr=4.96374e-05, gnorm=0.806, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25118
2023-02-16 22:48:05 - progress_bar.py[line:274] - INFO: epoch 001:   5447 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=5440, lr=4.96329e-05, gnorm=0.775, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25130
2023-02-16 22:48:17 - progress_bar.py[line:274] - INFO: epoch 001:   5457 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.9, wpb=109.9, bsz=40, num_updates=5450, lr=4.96284e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25141
2023-02-16 22:48:28 - progress_bar.py[line:274] - INFO: epoch 001:   5467 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.2, ups=0.89, wpb=109.1, bsz=40, num_updates=5460, lr=4.96239e-05, gnorm=0.792, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25152
2023-02-16 22:48:39 - progress_bar.py[line:274] - INFO: epoch 001:   5477 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=5470, lr=4.96194e-05, gnorm=0.714, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25163
2023-02-16 22:48:50 - progress_bar.py[line:274] - INFO: epoch 001:   5487 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=5480, lr=4.96149e-05, gnorm=0.826, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25174
2023-02-16 22:49:01 - progress_bar.py[line:274] - INFO: epoch 001:   5497 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.5, ups=0.9, wpb=110.9, bsz=40, num_updates=5490, lr=4.96104e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25185
2023-02-16 22:49:13 - progress_bar.py[line:274] - INFO: epoch 001:   5507 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=111.1, bsz=40, num_updates=5500, lr=4.96059e-05, gnorm=0.71, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25197
2023-02-16 22:49:24 - progress_bar.py[line:274] - INFO: epoch 001:   5517 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=5510, lr=4.96014e-05, gnorm=0.824, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25208
2023-02-16 22:49:35 - progress_bar.py[line:274] - INFO: epoch 001:   5527 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.2, ups=0.88, wpb=111.2, bsz=40, num_updates=5520, lr=4.95969e-05, gnorm=0.858, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25219
2023-02-16 22:49:47 - progress_bar.py[line:274] - INFO: epoch 001:   5537 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=5530, lr=4.95924e-05, gnorm=0.617, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25231
2023-02-16 22:49:58 - progress_bar.py[line:274] - INFO: epoch 001:   5547 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=5540, lr=4.95879e-05, gnorm=0.705, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25242
2023-02-16 22:50:09 - progress_bar.py[line:274] - INFO: epoch 001:   5557 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=5550, lr=4.95834e-05, gnorm=0.797, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25253
2023-02-16 22:50:20 - progress_bar.py[line:274] - INFO: epoch 001:   5567 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.4, ups=0.87, wpb=110.6, bsz=40, num_updates=5560, lr=4.95789e-05, gnorm=0.624, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25265
2023-02-16 22:50:32 - progress_bar.py[line:274] - INFO: epoch 001:   5577 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=5570, lr=4.95744e-05, gnorm=0.82, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25276
2023-02-16 22:50:43 - progress_bar.py[line:274] - INFO: epoch 001:   5587 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=5580, lr=4.95699e-05, gnorm=0.904, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25287
2023-02-16 22:50:54 - progress_bar.py[line:274] - INFO: epoch 001:   5597 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=5590, lr=4.95654e-05, gnorm=0.784, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25298
2023-02-16 22:51:05 - progress_bar.py[line:274] - INFO: epoch 001:   5607 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=5600, lr=4.95609e-05, gnorm=0.743, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25309
2023-02-16 22:51:08 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 22:51:17 - progress_bar.py[line:274] - INFO: epoch 001:   5618 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=93.9, ups=0.85, wpb=110.7, bsz=40, num_updates=5610, lr=4.95564e-05, gnorm=0.661, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=25321
2023-02-16 22:51:28 - progress_bar.py[line:274] - INFO: epoch 001:   5628 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=5620, lr=4.95519e-05, gnorm=0.526, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25332
2023-02-16 22:51:39 - progress_bar.py[line:274] - INFO: epoch 001:   5638 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.93, wpb=110.1, bsz=40, num_updates=5630, lr=4.95474e-05, gnorm=0.609, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25343
2023-02-16 22:51:50 - progress_bar.py[line:274] - INFO: epoch 001:   5648 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.87, wpb=111.3, bsz=40, num_updates=5640, lr=4.95429e-05, gnorm=0.831, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25355
2023-02-16 22:52:01 - progress_bar.py[line:274] - INFO: epoch 001:   5658 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=5650, lr=4.95384e-05, gnorm=0.722, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25366
2023-02-16 22:52:12 - progress_bar.py[line:274] - INFO: epoch 001:   5668 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.91, wpb=110, bsz=40, num_updates=5660, lr=4.95338e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25377
2023-02-16 22:52:23 - progress_bar.py[line:274] - INFO: epoch 001:   5678 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.3, ups=0.9, wpb=109.9, bsz=40, num_updates=5670, lr=4.95293e-05, gnorm=0.71, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25388
2023-02-16 22:52:35 - progress_bar.py[line:274] - INFO: epoch 001:   5688 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=5680, lr=4.95248e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25399
2023-02-16 22:52:46 - progress_bar.py[line:274] - INFO: epoch 001:   5698 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=5690, lr=4.95203e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25410
2023-02-16 22:52:57 - progress_bar.py[line:274] - INFO: epoch 001:   5708 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.1, ups=0.9, wpb=109.6, bsz=40, num_updates=5700, lr=4.95158e-05, gnorm=0.797, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25421
2023-02-16 22:53:08 - progress_bar.py[line:274] - INFO: epoch 001:   5718 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=5710, lr=4.95113e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25432
2023-02-16 22:53:19 - progress_bar.py[line:274] - INFO: epoch 001:   5728 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=5720, lr=4.95068e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25443
2023-02-16 22:53:30 - progress_bar.py[line:274] - INFO: epoch 001:   5738 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.1, ups=0.93, wpb=111, bsz=40, num_updates=5730, lr=4.95023e-05, gnorm=0.664, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25454
2023-02-16 22:53:41 - progress_bar.py[line:274] - INFO: epoch 001:   5748 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=5740, lr=4.94978e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25465
2023-02-16 22:53:52 - progress_bar.py[line:274] - INFO: epoch 001:   5758 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.88, wpb=110, bsz=40, num_updates=5750, lr=4.94933e-05, gnorm=0.547, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25477
2023-02-16 22:54:04 - progress_bar.py[line:274] - INFO: epoch 001:   5768 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.88, wpb=110.9, bsz=40, num_updates=5760, lr=4.94888e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25488
2023-02-16 22:54:15 - progress_bar.py[line:274] - INFO: epoch 001:   5778 / 28910 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.9, ups=0.9, wpb=109.4, bsz=40, num_updates=5770, lr=4.94843e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25499
2023-02-16 22:54:26 - progress_bar.py[line:274] - INFO: epoch 001:   5788 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.5, ups=0.88, wpb=109.5, bsz=40, num_updates=5780, lr=4.94798e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25511
2023-02-16 22:54:37 - progress_bar.py[line:274] - INFO: epoch 001:   5798 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=5790, lr=4.94753e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25522
2023-02-16 22:54:48 - progress_bar.py[line:274] - INFO: epoch 001:   5808 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.91, wpb=107.7, bsz=40, num_updates=5800, lr=4.94708e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25533
2023-02-16 22:54:59 - progress_bar.py[line:274] - INFO: epoch 001:   5818 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.91, wpb=111.3, bsz=40, num_updates=5810, lr=4.94663e-05, gnorm=0.634, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25544
2023-02-16 22:55:11 - progress_bar.py[line:274] - INFO: epoch 001:   5828 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=5820, lr=4.94618e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25555
2023-02-16 22:55:22 - progress_bar.py[line:274] - INFO: epoch 001:   5838 / 28910 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=5830, lr=4.94573e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25566
2023-02-16 22:55:33 - progress_bar.py[line:274] - INFO: epoch 001:   5848 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=5840, lr=4.94528e-05, gnorm=0.524, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25577
2023-02-16 22:55:44 - progress_bar.py[line:274] - INFO: epoch 001:   5858 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=5850, lr=4.94483e-05, gnorm=0.727, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25588
2023-02-16 22:55:55 - progress_bar.py[line:274] - INFO: epoch 001:   5868 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=5860, lr=4.94438e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25599
2023-02-16 22:56:06 - progress_bar.py[line:274] - INFO: epoch 001:   5878 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.89, wpb=109.8, bsz=40, num_updates=5870, lr=4.94393e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25611
2023-02-16 22:56:18 - progress_bar.py[line:274] - INFO: epoch 001:   5888 / 28910 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=5880, lr=4.94348e-05, gnorm=0.789, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25622
2023-02-16 22:56:29 - progress_bar.py[line:274] - INFO: epoch 001:   5898 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.93, wpb=111.2, bsz=40, num_updates=5890, lr=4.94303e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25633
2023-02-16 22:56:40 - progress_bar.py[line:274] - INFO: epoch 001:   5908 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.88, wpb=110.1, bsz=40, num_updates=5900, lr=4.94258e-05, gnorm=0.755, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25644
2023-02-16 22:56:51 - progress_bar.py[line:274] - INFO: epoch 001:   5918 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=104.6, ups=0.93, wpb=112.5, bsz=40, num_updates=5910, lr=4.94212e-05, gnorm=0.794, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25655
2023-02-16 22:57:02 - progress_bar.py[line:274] - INFO: epoch 001:   5928 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.91, wpb=109.5, bsz=40, num_updates=5920, lr=4.94167e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25666
2023-02-16 22:57:13 - progress_bar.py[line:274] - INFO: epoch 001:   5938 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=5930, lr=4.94122e-05, gnorm=0.838, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25677
2023-02-16 22:57:24 - progress_bar.py[line:274] - INFO: epoch 001:   5948 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.2, ups=0.88, wpb=109.2, bsz=40, num_updates=5940, lr=4.94077e-05, gnorm=0.771, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25689
2023-02-16 22:57:36 - progress_bar.py[line:274] - INFO: epoch 001:   5958 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.8, ups=0.87, wpb=109.7, bsz=40, num_updates=5950, lr=4.94032e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25700
2023-02-16 22:57:47 - progress_bar.py[line:274] - INFO: epoch 001:   5968 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.9, wpb=109.1, bsz=40, num_updates=5960, lr=4.93987e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25711
2023-02-16 22:57:58 - progress_bar.py[line:274] - INFO: epoch 001:   5978 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.9, wpb=110.4, bsz=40, num_updates=5970, lr=4.93942e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25722
2023-02-16 22:58:09 - progress_bar.py[line:274] - INFO: epoch 001:   5988 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=5980, lr=4.93897e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=25733
2023-02-16 22:58:20 - progress_bar.py[line:274] - INFO: epoch 001:   5998 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.5, ups=0.87, wpb=110.6, bsz=40, num_updates=5990, lr=4.93852e-05, gnorm=0.783, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25745
2023-02-16 22:58:32 - progress_bar.py[line:274] - INFO: epoch 001:   6008 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=6000, lr=4.93807e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25756
2023-02-16 22:58:32 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 22:58:33 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 22:58:33 - train.py[line:551] - INFO: load:0.97 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 23:00:35 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 23:00:35 - train.py[line:551] - INFO: load:1.00 valid_run:122.08 task_valid:118.89 collect_output:2.15
2023-02-16 23:02:35 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 23:02:35 - train.py[line:551] - INFO: load:1.03 valid_run:242.06 task_valid:234.69 collect_output:5.34
2023-02-16 23:04:37 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 23:04:37 - train.py[line:551] - INFO: load:1.05 valid_run:364.01 task_valid:351.18 collect_output:9.80
2023-02-16 23:06:39 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 23:06:39 - train.py[line:551] - INFO: load:1.07 valid_run:486.08 task_valid:465.04 collect_output:17.02
2023-02-16 23:08:40 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 23:08:40 - train.py[line:551] - INFO: load:1.10 valid_run:606.53 task_valid:582.25 collect_output:19.26
2023-02-16 23:10:42 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 23:10:42 - train.py[line:551] - INFO: load:1.12 valid_run:729.29 task_valid:700.87 collect_output:22.38
2023-02-16 23:12:45 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 23:12:45 - train.py[line:551] - INFO: load:1.14 valid_run:852.29 task_valid:818.84 collect_output:26.41
2023-02-16 23:14:47 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 23:14:47 - train.py[line:551] - INFO: load:1.17 valid_run:974.30 task_valid:935.45 collect_output:30.80
2023-02-16 23:16:51 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 23:16:51 - train.py[line:551] - INFO: load:1.19 valid_run:1097.93 task_valid:1052.51 collect_output:36.36
2023-02-16 23:18:53 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 23:18:53 - train.py[line:551] - INFO: load:1.21 valid_run:1219.52 task_valid:1165.19 collect_output:44.25
2023-02-16 23:20:53 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 23:20:53 - train.py[line:551] - INFO: load:1.24 valid_run:1339.65 task_valid:1280.87 collect_output:47.70
2023-02-16 23:22:55 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 23:22:55 - train.py[line:551] - INFO: load:1.26 valid_run:1461.31 task_valid:1397.79 collect_output:51.42
2023-02-16 23:24:54 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 23:24:54 - train.py[line:551] - INFO: load:1.29 valid_run:1580.25 task_valid:1511.63 collect_output:55.52
2023-02-16 23:26:55 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 23:26:55 - train.py[line:551] - INFO: load:1.31 valid_run:1701.29 task_valid:1629.19 collect_output:58.00
2023-02-16 23:28:56 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 23:28:56 - train.py[line:551] - INFO: load:1.33 valid_run:1822.22 task_valid:1745.22 collect_output:61.90
2023-02-16 23:30:57 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 23:30:57 - train.py[line:551] - INFO: load:1.36 valid_run:1943.46 task_valid:1859.30 collect_output:68.04
2023-02-16 23:32:59 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 23:32:59 - train.py[line:551] - INFO: load:1.38 valid_run:2064.85 task_valid:1975.51 collect_output:72.22
2023-02-16 23:34:59 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 23:34:59 - train.py[line:551] - INFO: load:1.41 valid_run:2185.36 task_valid:2093.14 collect_output:74.12
2023-02-16 23:37:00 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 23:37:00 - train.py[line:551] - INFO: load:1.43 valid_run:2306.59 task_valid:2210.03 collect_output:77.45
2023-02-16 23:39:01 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 23:39:01 - train.py[line:551] - INFO: load:1.46 valid_run:2426.95 task_valid:2326.39 collect_output:80.44
2023-02-16 23:41:03 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 23:41:03 - train.py[line:551] - INFO: load:1.48 valid_run:2548.63 task_valid:2442.89 collect_output:84.63
2023-02-16 23:43:05 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 23:43:05 - train.py[line:551] - INFO: load:1.51 valid_run:2670.57 task_valid:2561.58 collect_output:86.85
2023-02-16 23:45:05 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 23:45:05 - train.py[line:551] - INFO: load:1.53 valid_run:2790.88 task_valid:2675.86 collect_output:91.88
2023-02-16 23:47:05 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 23:47:05 - train.py[line:551] - INFO: load:1.55 valid_run:2910.53 task_valid:2791.73 collect_output:94.67
2023-02-16 23:49:06 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 23:49:06 - train.py[line:551] - INFO: load:1.58 valid_run:3032.00 task_valid:2907.66 collect_output:99.19
2023-02-16 23:51:09 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 23:51:09 - train.py[line:551] - INFO: load:1.60 valid_run:3154.76 task_valid:3023.43 collect_output:105.17
2023-02-16 23:53:08 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 23:53:08 - train.py[line:551] - INFO: load:1.63 valid_run:3274.06 task_valid:3137.21 collect_output:109.69
2023-02-16 23:55:10 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 23:55:10 - train.py[line:551] - INFO: load:1.65 valid_run:3395.68 task_valid:3256.33 collect_output:111.20
2023-02-16 23:57:12 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 23:57:12 - train.py[line:551] - INFO: load:1.68 valid_run:3517.34 task_valid:3371.69 collect_output:116.49
2023-02-16 23:59:13 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 23:59:13 - train.py[line:551] - INFO: load:1.70 valid_run:3639.00 task_valid:3489.87 collect_output:118.96
2023-02-17 00:01:14 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 00:01:14 - train.py[line:551] - INFO: load:1.73 valid_run:3759.77 task_valid:3607.87 collect_output:120.71

====================================================================================================
SGG eval:     R @ 50: 0.6045;     R @ 100: 0.6495;     R @ 500: 0.6832;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4204;    mR @ 100: 0.4913;    mR @ 500: 0.5504;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.7273) (growing on:0.5000) (hanging from:0.5484) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:1.0000) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.6905) (standing on:0.2533) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6045;     R @ 100: 0.6495;     R @ 500: 0.6832;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4204;    mR @ 100: 0.4913;    mR @ 500: 0.5504;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.7273) (growing on:0.5000) (hanging from:0.5484) (lying on:0.4000) (mounted on:0.0000) (painted on:0.3333) (parked on:1.0000) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.6905) (standing on:0.2533) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-02-17 00:01:45 - train.py[line:487] - INFO: 0.6494852813852814
2023-02-17 00:01:45 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 00:01:45 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.332 | loss_v1 0 | loss_v2 0 | nll_loss 0.172 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.649485 | ppl 1.13 | vqa_score 0.5394 | wps 118.3 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.649485
2023-02-17 00:01:45 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2023-02-17 00:01:45 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_6000.pt
2023-02-17 00:01:51 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_6000.pt
2023-02-17 00:01:55 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6494852813852814) (writing took 10.350228326395154 seconds)
2023-02-17 00:02:07 - progress_bar.py[line:274] - INFO: epoch 001:   6018 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=0.3, ups=0, wpb=110.7, bsz=40, num_updates=6010, lr=4.93762e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29571
2023-02-17 00:02:17 - progress_bar.py[line:274] - INFO: epoch 001:   6028 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.93, wpb=109.4, bsz=40, num_updates=6020, lr=4.93717e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29582
2023-02-17 00:02:29 - progress_bar.py[line:274] - INFO: epoch 001:   6038 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=6030, lr=4.93672e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29593
2023-02-17 00:02:40 - progress_bar.py[line:274] - INFO: epoch 001:   6048 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.9, wpb=111.6, bsz=40, num_updates=6040, lr=4.93627e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29604
2023-02-17 00:02:51 - progress_bar.py[line:274] - INFO: epoch 001:   6058 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=6050, lr=4.93582e-05, gnorm=0.848, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29615
2023-02-17 00:03:02 - progress_bar.py[line:274] - INFO: epoch 001:   6068 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=6060, lr=4.93537e-05, gnorm=0.566, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29626
2023-02-17 00:03:13 - progress_bar.py[line:274] - INFO: epoch 001:   6078 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.1, ups=0.87, wpb=109.2, bsz=40, num_updates=6070, lr=4.93492e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=29638
2023-02-17 00:03:24 - progress_bar.py[line:274] - INFO: epoch 001:   6088 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.92, wpb=111.6, bsz=40, num_updates=6080, lr=4.93447e-05, gnorm=0.66, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29649
2023-02-17 00:03:35 - progress_bar.py[line:274] - INFO: epoch 001:   6098 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.92, wpb=110.6, bsz=40, num_updates=6090, lr=4.93402e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29659
2023-02-17 00:03:46 - progress_bar.py[line:274] - INFO: epoch 001:   6108 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.8, ups=0.92, wpb=111, bsz=40, num_updates=6100, lr=4.93357e-05, gnorm=0.754, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29670
2023-02-17 00:03:57 - progress_bar.py[line:274] - INFO: epoch 001:   6118 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.91, wpb=109.4, bsz=40, num_updates=6110, lr=4.93312e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29681
2023-02-17 00:04:08 - progress_bar.py[line:274] - INFO: epoch 001:   6128 / 28910 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=6120, lr=4.93267e-05, gnorm=0.805, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29693
2023-02-17 00:04:20 - progress_bar.py[line:274] - INFO: epoch 001:   6138 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.88, wpb=111.2, bsz=40, num_updates=6130, lr=4.93222e-05, gnorm=0.585, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29704
2023-02-17 00:04:31 - progress_bar.py[line:274] - INFO: epoch 001:   6148 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.7, ups=0.88, wpb=111, bsz=40, num_updates=6140, lr=4.93177e-05, gnorm=0.595, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29715
2023-02-17 00:04:42 - progress_bar.py[line:274] - INFO: epoch 001:   6158 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=6150, lr=4.93132e-05, gnorm=0.638, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29727
2023-02-17 00:04:53 - progress_bar.py[line:274] - INFO: epoch 001:   6168 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102, ups=0.92, wpb=110.8, bsz=40, num_updates=6160, lr=4.93087e-05, gnorm=0.52, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29738
2023-02-17 00:05:04 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 00:05:05 - progress_bar.py[line:274] - INFO: epoch 001:   6179 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=89.9, ups=0.82, wpb=110.1, bsz=40, num_updates=6170, lr=4.93041e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29750
2023-02-17 00:05:17 - progress_bar.py[line:274] - INFO: epoch 001:   6189 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=6180, lr=4.92996e-05, gnorm=0.712, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29761
2023-02-17 00:05:28 - progress_bar.py[line:274] - INFO: epoch 001:   6199 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.4, ups=0.88, wpb=109.2, bsz=40, num_updates=6190, lr=4.92951e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29772
2023-02-17 00:05:40 - progress_bar.py[line:274] - INFO: epoch 001:   6209 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=6200, lr=4.92906e-05, gnorm=0.634, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29784
2023-02-17 00:05:51 - progress_bar.py[line:274] - INFO: epoch 001:   6219 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=6210, lr=4.92861e-05, gnorm=0.581, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=29795
2023-02-17 00:06:02 - progress_bar.py[line:274] - INFO: epoch 001:   6229 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=6220, lr=4.92816e-05, gnorm=0.677, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=29806
2023-02-17 00:06:13 - progress_bar.py[line:274] - INFO: epoch 001:   6239 / 28910 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.7, ups=0.88, wpb=109.6, bsz=40, num_updates=6230, lr=4.92771e-05, gnorm=0.683, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29817
2023-02-17 00:06:25 - progress_bar.py[line:274] - INFO: epoch 001:   6249 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=94.1, ups=0.86, wpb=109.3, bsz=40, num_updates=6240, lr=4.92726e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=29829
2023-02-17 00:06:36 - progress_bar.py[line:274] - INFO: epoch 001:   6259 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=6250, lr=4.92681e-05, gnorm=0.776, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29840
2023-02-17 00:06:48 - progress_bar.py[line:274] - INFO: epoch 001:   6269 / 28910 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=95, ups=0.87, wpb=109.2, bsz=40, num_updates=6260, lr=4.92636e-05, gnorm=0.75, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29852
2023-02-17 00:06:59 - progress_bar.py[line:274] - INFO: epoch 001:   6279 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.9, wpb=111.5, bsz=40, num_updates=6270, lr=4.92591e-05, gnorm=0.703, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29863
2023-02-17 00:07:10 - progress_bar.py[line:274] - INFO: epoch 001:   6289 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=6280, lr=4.92546e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29874
2023-02-17 00:07:22 - progress_bar.py[line:274] - INFO: epoch 001:   6299 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.3, ups=0.87, wpb=109.3, bsz=40, num_updates=6290, lr=4.92501e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=29886
2023-02-17 00:07:33 - progress_bar.py[line:274] - INFO: epoch 001:   6309 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=6300, lr=4.92456e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29897
2023-02-17 00:07:44 - progress_bar.py[line:274] - INFO: epoch 001:   6319 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.3, ups=0.92, wpb=109.4, bsz=40, num_updates=6310, lr=4.92411e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29908
2023-02-17 00:07:55 - progress_bar.py[line:274] - INFO: epoch 001:   6329 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=6320, lr=4.92366e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29919
2023-02-17 00:08:06 - progress_bar.py[line:274] - INFO: epoch 001:   6339 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=6330, lr=4.92321e-05, gnorm=0.578, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29930
2023-02-17 00:08:17 - progress_bar.py[line:274] - INFO: epoch 001:   6349 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.2, ups=0.88, wpb=110.5, bsz=40, num_updates=6340, lr=4.92276e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29942
2023-02-17 00:08:28 - progress_bar.py[line:274] - INFO: epoch 001:   6359 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.91, wpb=109.1, bsz=40, num_updates=6350, lr=4.92231e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29953
2023-02-17 00:08:40 - progress_bar.py[line:274] - INFO: epoch 001:   6369 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.4, ups=0.88, wpb=109.7, bsz=40, num_updates=6360, lr=4.92186e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29964
2023-02-17 00:08:51 - progress_bar.py[line:274] - INFO: epoch 001:   6379 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=6370, lr=4.92141e-05, gnorm=0.604, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29975
2023-02-17 00:09:03 - progress_bar.py[line:274] - INFO: epoch 001:   6389 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.2, ups=0.88, wpb=109.3, bsz=40, num_updates=6380, lr=4.92096e-05, gnorm=0.708, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29987
2023-02-17 00:09:14 - progress_bar.py[line:274] - INFO: epoch 001:   6399 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.5, ups=0.92, wpb=110.9, bsz=40, num_updates=6390, lr=4.92051e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29998
2023-02-17 00:09:25 - progress_bar.py[line:274] - INFO: epoch 001:   6409 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=6400, lr=4.92006e-05, gnorm=0.642, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30009
2023-02-17 00:09:36 - progress_bar.py[line:274] - INFO: epoch 001:   6419 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.9, wpb=110, bsz=40, num_updates=6410, lr=4.91961e-05, gnorm=0.52, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30020
2023-02-17 00:09:47 - progress_bar.py[line:274] - INFO: epoch 001:   6429 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=6420, lr=4.91916e-05, gnorm=0.864, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30031
2023-02-17 00:09:58 - progress_bar.py[line:274] - INFO: epoch 001:   6439 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=6430, lr=4.9187e-05, gnorm=0.666, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30042
2023-02-17 00:10:09 - progress_bar.py[line:274] - INFO: epoch 001:   6449 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=6440, lr=4.91825e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30053
2023-02-17 00:10:20 - progress_bar.py[line:274] - INFO: epoch 001:   6459 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.5, ups=0.88, wpb=108.7, bsz=40, num_updates=6450, lr=4.9178e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30065
2023-02-17 00:10:31 - progress_bar.py[line:274] - INFO: epoch 001:   6469 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.9, wpb=111.9, bsz=40, num_updates=6460, lr=4.91735e-05, gnorm=0.547, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30076
2023-02-17 00:10:43 - progress_bar.py[line:274] - INFO: epoch 001:   6479 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=6470, lr=4.9169e-05, gnorm=0.654, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30087
2023-02-17 00:10:54 - progress_bar.py[line:274] - INFO: epoch 001:   6489 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=6480, lr=4.91645e-05, gnorm=0.702, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30098
2023-02-17 00:11:05 - progress_bar.py[line:274] - INFO: epoch 001:   6499 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.9, wpb=109.6, bsz=40, num_updates=6490, lr=4.916e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30109
2023-02-17 00:11:16 - progress_bar.py[line:274] - INFO: epoch 001:   6509 / 28910 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.6, ups=0.91, wpb=110, bsz=40, num_updates=6500, lr=4.91555e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30120
2023-02-17 00:11:27 - progress_bar.py[line:274] - INFO: epoch 001:   6519 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=6510, lr=4.9151e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30131
2023-02-17 00:11:38 - progress_bar.py[line:274] - INFO: epoch 001:   6529 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.9, wpb=110, bsz=40, num_updates=6520, lr=4.91465e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30143
2023-02-17 00:11:49 - progress_bar.py[line:274] - INFO: epoch 001:   6539 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=6530, lr=4.9142e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30154
2023-02-17 00:12:01 - progress_bar.py[line:274] - INFO: epoch 001:   6549 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=6540, lr=4.91375e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30165
2023-02-17 00:12:12 - progress_bar.py[line:274] - INFO: epoch 001:   6559 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=6550, lr=4.9133e-05, gnorm=0.637, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30176
2023-02-17 00:12:23 - progress_bar.py[line:274] - INFO: epoch 001:   6569 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=6560, lr=4.91285e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30188
2023-02-17 00:12:34 - progress_bar.py[line:274] - INFO: epoch 001:   6579 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=6570, lr=4.9124e-05, gnorm=0.697, clip=30, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30199
2023-02-17 00:12:46 - progress_bar.py[line:274] - INFO: epoch 001:   6589 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=6580, lr=4.91195e-05, gnorm=0.862, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30210
2023-02-17 00:12:57 - progress_bar.py[line:274] - INFO: epoch 001:   6599 / 28910 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.4, ups=0.88, wpb=109.2, bsz=40, num_updates=6590, lr=4.9115e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30221
2023-02-17 00:13:08 - progress_bar.py[line:274] - INFO: epoch 001:   6609 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=110.7, bsz=40, num_updates=6600, lr=4.91105e-05, gnorm=0.745, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30232
2023-02-17 00:13:19 - progress_bar.py[line:274] - INFO: epoch 001:   6619 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=6610, lr=4.9106e-05, gnorm=0.618, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30244
2023-02-17 00:13:31 - progress_bar.py[line:274] - INFO: epoch 001:   6629 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.89, wpb=110.6, bsz=40, num_updates=6620, lr=4.91015e-05, gnorm=0.701, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30255
2023-02-17 00:13:42 - progress_bar.py[line:274] - INFO: epoch 001:   6639 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96, ups=0.88, wpb=108.9, bsz=40, num_updates=6630, lr=4.9097e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30266
2023-02-17 00:13:53 - progress_bar.py[line:274] - INFO: epoch 001:   6649 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=6640, lr=4.90925e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30278
2023-02-17 00:14:04 - progress_bar.py[line:274] - INFO: epoch 001:   6659 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=6650, lr=4.9088e-05, gnorm=0.671, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=30289
2023-02-17 00:14:15 - progress_bar.py[line:274] - INFO: epoch 001:   6669 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.92, wpb=109.4, bsz=40, num_updates=6660, lr=4.90835e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30300
2023-02-17 00:14:27 - progress_bar.py[line:274] - INFO: epoch 001:   6679 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=6670, lr=4.9079e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30311
2023-02-17 00:14:37 - progress_bar.py[line:274] - INFO: epoch 001:   6689 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=6680, lr=4.90744e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30322
2023-02-17 00:14:49 - progress_bar.py[line:274] - INFO: epoch 001:   6699 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.5, ups=0.88, wpb=108.7, bsz=40, num_updates=6690, lr=4.90699e-05, gnorm=0.586, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30333
2023-02-17 00:14:53 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 00:15:01 - progress_bar.py[line:274] - INFO: epoch 001:   6710 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=90.5, ups=0.82, wpb=110.9, bsz=40, num_updates=6700, lr=4.90654e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30345
2023-02-17 00:15:12 - progress_bar.py[line:274] - INFO: epoch 001:   6720 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=6710, lr=4.90609e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30357
2023-02-17 00:15:23 - progress_bar.py[line:274] - INFO: epoch 001:   6730 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=6720, lr=4.90564e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30368
2023-02-17 00:15:34 - progress_bar.py[line:274] - INFO: epoch 001:   6740 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=6730, lr=4.90519e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30379
2023-02-17 00:15:46 - progress_bar.py[line:274] - INFO: epoch 001:   6750 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=6740, lr=4.90474e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30390
2023-02-17 00:15:57 - progress_bar.py[line:274] - INFO: epoch 001:   6760 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.91, wpb=110.3, bsz=40, num_updates=6750, lr=4.90429e-05, gnorm=0.714, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30401
2023-02-17 00:16:08 - progress_bar.py[line:274] - INFO: epoch 001:   6770 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=6760, lr=4.90384e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30412
2023-02-17 00:16:19 - progress_bar.py[line:274] - INFO: epoch 001:   6780 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99, ups=0.9, wpb=109.5, bsz=40, num_updates=6770, lr=4.90339e-05, gnorm=0.778, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30423
2023-02-17 00:16:30 - progress_bar.py[line:274] - INFO: epoch 001:   6790 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.9, wpb=108.2, bsz=40, num_updates=6780, lr=4.90294e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30434
2023-02-17 00:16:41 - progress_bar.py[line:274] - INFO: epoch 001:   6800 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=110.8, bsz=40, num_updates=6790, lr=4.90249e-05, gnorm=0.649, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30445
2023-02-17 00:16:52 - progress_bar.py[line:274] - INFO: epoch 001:   6810 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.88, wpb=110.2, bsz=40, num_updates=6800, lr=4.90204e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30457
2023-02-17 00:17:03 - progress_bar.py[line:274] - INFO: epoch 001:   6820 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=6810, lr=4.90159e-05, gnorm=0.627, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30468
2023-02-17 00:17:15 - progress_bar.py[line:274] - INFO: epoch 001:   6830 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96, ups=0.88, wpb=109.2, bsz=40, num_updates=6820, lr=4.90114e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30479
2023-02-17 00:17:26 - progress_bar.py[line:274] - INFO: epoch 001:   6840 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.9, wpb=109.7, bsz=40, num_updates=6830, lr=4.90069e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30490
2023-02-17 00:17:37 - progress_bar.py[line:274] - INFO: epoch 001:   6850 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.91, wpb=110.2, bsz=40, num_updates=6840, lr=4.90024e-05, gnorm=0.538, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30501
2023-02-17 00:17:48 - progress_bar.py[line:274] - INFO: epoch 001:   6860 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.92, wpb=110.4, bsz=40, num_updates=6850, lr=4.89979e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30512
2023-02-17 00:17:59 - progress_bar.py[line:274] - INFO: epoch 001:   6870 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=6860, lr=4.89934e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30523
2023-02-17 00:18:10 - progress_bar.py[line:274] - INFO: epoch 001:   6880 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=6870, lr=4.89889e-05, gnorm=0.71, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30534
2023-02-17 00:18:21 - progress_bar.py[line:274] - INFO: epoch 001:   6890 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=6880, lr=4.89844e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30545
2023-02-17 00:18:32 - progress_bar.py[line:274] - INFO: epoch 001:   6900 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=6890, lr=4.89799e-05, gnorm=0.559, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30557
2023-02-17 00:18:44 - progress_bar.py[line:274] - INFO: epoch 001:   6910 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=6900, lr=4.89754e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30568
2023-02-17 00:18:55 - progress_bar.py[line:274] - INFO: epoch 001:   6920 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=6910, lr=4.89709e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30579
2023-02-17 00:19:06 - progress_bar.py[line:274] - INFO: epoch 001:   6930 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=6920, lr=4.89664e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30590
2023-02-17 00:19:17 - progress_bar.py[line:274] - INFO: epoch 001:   6940 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.9, wpb=110.9, bsz=40, num_updates=6930, lr=4.89619e-05, gnorm=0.622, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30601
2023-02-17 00:19:28 - progress_bar.py[line:274] - INFO: epoch 001:   6950 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=6940, lr=4.89573e-05, gnorm=0.581, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30612
2023-02-17 00:19:40 - progress_bar.py[line:274] - INFO: epoch 001:   6960 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.3, ups=0.87, wpb=110.9, bsz=40, num_updates=6950, lr=4.89528e-05, gnorm=0.561, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30624
2023-02-17 00:19:51 - progress_bar.py[line:274] - INFO: epoch 001:   6970 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.91, wpb=112.2, bsz=40, num_updates=6960, lr=4.89483e-05, gnorm=0.572, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30635
2023-02-17 00:20:02 - progress_bar.py[line:274] - INFO: epoch 001:   6980 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.9, wpb=110.8, bsz=40, num_updates=6970, lr=4.89438e-05, gnorm=0.661, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30646
2023-02-17 00:20:13 - progress_bar.py[line:274] - INFO: epoch 001:   6990 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.9, wpb=110.9, bsz=40, num_updates=6980, lr=4.89393e-05, gnorm=0.712, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30657
2023-02-17 00:20:24 - progress_bar.py[line:274] - INFO: epoch 001:   7000 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.91, wpb=109.9, bsz=40, num_updates=6990, lr=4.89348e-05, gnorm=0.542, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30668
2023-02-17 00:20:35 - progress_bar.py[line:274] - INFO: epoch 001:   7010 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.92, wpb=111.1, bsz=40, num_updates=7000, lr=4.89303e-05, gnorm=0.559, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30679
2023-02-17 00:20:35 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 00:20:36 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 00:20:36 - train.py[line:551] - INFO: load:0.93 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 00:22:38 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 00:22:38 - train.py[line:551] - INFO: load:0.96 valid_run:121.86 task_valid:118.89 collect_output:1.92
2023-02-17 00:24:38 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 00:24:38 - train.py[line:551] - INFO: load:0.98 valid_run:241.87 task_valid:234.68 collect_output:5.13
2023-02-17 00:26:40 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 00:26:40 - train.py[line:551] - INFO: load:1.01 valid_run:363.95 task_valid:351.19 collect_output:9.66
2023-02-17 00:28:42 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 00:28:42 - train.py[line:551] - INFO: load:1.04 valid_run:485.90 task_valid:464.96 collect_output:16.80
2023-02-17 00:30:43 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 00:30:43 - train.py[line:551] - INFO: load:1.06 valid_run:606.33 task_valid:582.14 collect_output:19.02
2023-02-17 00:32:46 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 00:32:46 - train.py[line:551] - INFO: load:1.09 valid_run:729.22 task_valid:700.72 collect_output:22.28
2023-02-17 00:34:48 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 00:34:48 - train.py[line:551] - INFO: load:1.11 valid_run:851.99 task_valid:818.52 collect_output:26.24
2023-02-17 00:36:50 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 00:36:50 - train.py[line:551] - INFO: load:1.14 valid_run:973.69 task_valid:934.87 collect_output:30.57
2023-02-17 00:38:54 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 00:38:54 - train.py[line:551] - INFO: load:1.17 valid_run:1097.27 task_valid:1051.88 collect_output:36.12
2023-02-17 00:40:55 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 00:40:55 - train.py[line:551] - INFO: load:1.19 valid_run:1218.90 task_valid:1164.49 collect_output:44.13
2023-02-17 00:42:56 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 00:42:56 - train.py[line:551] - INFO: load:1.22 valid_run:1339.00 task_valid:1280.05 collect_output:47.65
2023-02-17 00:44:57 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 00:44:57 - train.py[line:551] - INFO: load:1.24 valid_run:1460.59 task_valid:1396.83 collect_output:51.43
2023-02-17 00:46:56 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 00:46:56 - train.py[line:551] - INFO: load:1.27 valid_run:1579.66 task_valid:1510.67 collect_output:55.64
2023-02-17 00:48:57 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 00:48:57 - train.py[line:551] - INFO: load:1.29 valid_run:1700.68 task_valid:1628.34 collect_output:57.98
2023-02-17 00:50:58 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 00:50:58 - train.py[line:551] - INFO: load:1.32 valid_run:1821.61 task_valid:1744.29 collect_output:61.94
2023-02-17 00:53:00 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 00:53:00 - train.py[line:551] - INFO: load:1.35 valid_run:1942.76 task_valid:1858.18 collect_output:68.18
2023-02-17 00:55:01 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 00:55:01 - train.py[line:551] - INFO: load:1.37 valid_run:2064.16 task_valid:1974.20 collect_output:72.50
2023-02-17 00:57:02 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 00:57:02 - train.py[line:551] - INFO: load:1.40 valid_run:2184.68 task_valid:2091.82 collect_output:74.39
2023-02-17 00:59:03 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 00:59:03 - train.py[line:551] - INFO: load:1.42 valid_run:2305.88 task_valid:2208.74 collect_output:77.66
2023-02-17 01:01:03 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 01:01:03 - train.py[line:551] - INFO: load:1.45 valid_run:2426.16 task_valid:2325.07 collect_output:80.59
2023-02-17 01:03:05 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 01:03:05 - train.py[line:551] - INFO: load:1.47 valid_run:2547.72 task_valid:2441.38 collect_output:84.83
2023-02-17 01:05:07 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 01:05:07 - train.py[line:551] - INFO: load:1.50 valid_run:2669.56 task_valid:2559.96 collect_output:87.07
2023-02-17 01:07:07 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 01:07:07 - train.py[line:551] - INFO: load:1.52 valid_run:2789.95 task_valid:2674.24 collect_output:92.19
2023-02-17 01:09:07 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 01:09:07 - train.py[line:551] - INFO: load:1.55 valid_run:2909.76 task_valid:2790.34 collect_output:94.88
2023-02-17 01:11:09 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 01:11:09 - train.py[line:551] - INFO: load:1.57 valid_run:3031.24 task_valid:2906.40 collect_output:99.28
2023-02-17 01:13:11 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 01:13:11 - train.py[line:551] - INFO: load:1.60 valid_run:3153.93 task_valid:3022.15 collect_output:105.20
2023-02-17 01:15:11 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 01:15:11 - train.py[line:551] - INFO: load:1.62 valid_run:3273.39 task_valid:3135.95 collect_output:109.88
2023-02-17 01:17:13 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 01:17:13 - train.py[line:551] - INFO: load:1.65 valid_run:3395.00 task_valid:3255.01 collect_output:111.40
2023-02-17 01:19:14 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 01:19:14 - train.py[line:551] - INFO: load:1.68 valid_run:3516.63 task_valid:3370.47 collect_output:116.56
2023-02-17 01:21:16 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 01:21:16 - train.py[line:551] - INFO: load:1.70 valid_run:3638.21 task_valid:3488.55 collect_output:119.06
2023-02-17 01:23:17 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 01:23:17 - train.py[line:551] - INFO: load:1.73 valid_run:3759.04 task_valid:3606.69 collect_output:120.74

====================================================================================================
SGG eval:     R @ 50: 0.6030;     R @ 100: 0.6483;     R @ 500: 0.6824;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4058;    mR @ 100: 0.4438;    mR @ 500: 0.5155;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.0000) (growing on:0.3750) (hanging from:0.5484) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:1.0000) (playing:0.0000) (riding:0.9722) (says:0.0000) (sitting on:0.6882) (standing on:0.2633) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6030;     R @ 100: 0.6483;     R @ 500: 0.6824;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4058;    mR @ 100: 0.4438;    mR @ 500: 0.5155;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.0000) (growing on:0.3750) (hanging from:0.5484) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:1.0000) (playing:0.0000) (riding:0.9722) (says:0.0000) (sitting on:0.6882) (standing on:0.2633) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-02-17 01:23:48 - train.py[line:487] - INFO: 0.6482761904761905
2023-02-17 01:23:48 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 01:23:48 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.335 | loss_v1 0 | loss_v2 0 | nll_loss 0.173 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.648276 | ppl 1.13 | vqa_score 0.5507 | wps 118.3 | wpb 72 | bsz 24 | num_updates 7000 | best_R@100 0.649485
2023-02-17 01:23:48 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7000 updates
2023-02-17 01:23:48 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_7000.pt
2023-02-17 01:23:53 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_7000.pt
2023-02-17 01:23:56 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 0.6482761904761905) (writing took 7.998726600781083 seconds)
2023-02-17 01:24:07 - progress_bar.py[line:274] - INFO: epoch 001:   7020 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=0.3, ups=0, wpb=111.8, bsz=40, num_updates=7010, lr=4.89258e-05, gnorm=0.582, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34491
2023-02-17 01:24:18 - progress_bar.py[line:274] - INFO: epoch 001:   7030 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=7020, lr=4.89213e-05, gnorm=0.556, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34503
2023-02-17 01:24:30 - progress_bar.py[line:274] - INFO: epoch 001:   7040 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.4, ups=0.88, wpb=108.3, bsz=40, num_updates=7030, lr=4.89168e-05, gnorm=0.772, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34514
2023-02-17 01:24:41 - progress_bar.py[line:274] - INFO: epoch 001:   7050 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=7040, lr=4.89123e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34525
2023-02-17 01:24:52 - progress_bar.py[line:274] - INFO: epoch 001:   7060 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.1, ups=0.93, wpb=111.4, bsz=40, num_updates=7050, lr=4.89078e-05, gnorm=0.639, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34536
2023-02-17 01:25:03 - progress_bar.py[line:274] - INFO: epoch 001:   7070 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=7060, lr=4.89033e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34547
2023-02-17 01:25:14 - progress_bar.py[line:274] - INFO: epoch 001:   7080 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=7070, lr=4.88988e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34559
2023-02-17 01:25:25 - progress_bar.py[line:274] - INFO: epoch 001:   7090 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=7080, lr=4.88943e-05, gnorm=0.641, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34570
2023-02-17 01:25:37 - progress_bar.py[line:274] - INFO: epoch 001:   7100 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=7090, lr=4.88898e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34581
2023-02-17 01:25:48 - progress_bar.py[line:274] - INFO: epoch 001:   7110 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=7100, lr=4.88853e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34592
2023-02-17 01:25:59 - progress_bar.py[line:274] - INFO: epoch 001:   7120 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.9, wpb=109.7, bsz=40, num_updates=7110, lr=4.88808e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34603
2023-02-17 01:26:10 - progress_bar.py[line:274] - INFO: epoch 001:   7130 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=7120, lr=4.88763e-05, gnorm=0.554, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34615
2023-02-17 01:26:22 - progress_bar.py[line:274] - INFO: epoch 001:   7140 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=7130, lr=4.88718e-05, gnorm=0.65, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34626
2023-02-17 01:26:33 - progress_bar.py[line:274] - INFO: epoch 001:   7150 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.91, wpb=108.9, bsz=40, num_updates=7140, lr=4.88673e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34637
2023-02-17 01:26:44 - progress_bar.py[line:274] - INFO: epoch 001:   7160 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=7150, lr=4.88628e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34648
2023-02-17 01:26:55 - progress_bar.py[line:274] - INFO: epoch 001:   7170 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=7160, lr=4.88583e-05, gnorm=0.71, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34659
2023-02-17 01:27:06 - progress_bar.py[line:274] - INFO: epoch 001:   7180 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=7170, lr=4.88538e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34670
2023-02-17 01:27:17 - progress_bar.py[line:274] - INFO: epoch 001:   7190 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=7180, lr=4.88493e-05, gnorm=0.557, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34681
2023-02-17 01:27:29 - progress_bar.py[line:274] - INFO: epoch 001:   7200 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.87, wpb=111.2, bsz=40, num_updates=7190, lr=4.88448e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34693
2023-02-17 01:27:40 - progress_bar.py[line:274] - INFO: epoch 001:   7210 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.91, wpb=109.7, bsz=40, num_updates=7200, lr=4.88402e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34704
2023-02-17 01:27:50 - progress_bar.py[line:274] - INFO: epoch 001:   7220 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.9, ups=0.92, wpb=111.1, bsz=40, num_updates=7210, lr=4.88357e-05, gnorm=0.585, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34715
2023-02-17 01:28:01 - progress_bar.py[line:274] - INFO: epoch 001:   7230 / 28910 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.3, ups=0.91, wpb=110.7, bsz=40, num_updates=7220, lr=4.88312e-05, gnorm=0.611, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34726
2023-02-17 01:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   7240 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=7230, lr=4.88267e-05, gnorm=0.676, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34737
2023-02-17 01:28:23 - progress_bar.py[line:274] - INFO: epoch 001:   7250 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.5, ups=0.92, wpb=111.7, bsz=40, num_updates=7240, lr=4.88222e-05, gnorm=0.684, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34748
2023-02-17 01:28:35 - progress_bar.py[line:274] - INFO: epoch 001:   7260 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=7250, lr=4.88177e-05, gnorm=0.575, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34759
2023-02-17 01:28:46 - progress_bar.py[line:274] - INFO: epoch 001:   7270 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=7260, lr=4.88132e-05, gnorm=0.552, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34770
2023-02-17 01:28:57 - progress_bar.py[line:274] - INFO: epoch 001:   7280 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=7270, lr=4.88087e-05, gnorm=0.58, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34781
2023-02-17 01:29:08 - progress_bar.py[line:274] - INFO: epoch 001:   7290 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.92, wpb=109.2, bsz=40, num_updates=7280, lr=4.88042e-05, gnorm=0.607, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34792
2023-02-17 01:29:20 - progress_bar.py[line:274] - INFO: epoch 001:   7300 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95, ups=0.86, wpb=110.2, bsz=40, num_updates=7290, lr=4.87997e-05, gnorm=0.578, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=34804
2023-02-17 01:29:24 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 01:29:32 - progress_bar.py[line:274] - INFO: epoch 001:   7311 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=89.3, ups=0.81, wpb=110.6, bsz=40, num_updates=7300, lr=4.87952e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=34816
2023-02-17 01:29:43 - progress_bar.py[line:274] - INFO: epoch 001:   7321 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=7310, lr=4.87907e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=34827
2023-02-17 01:29:54 - progress_bar.py[line:274] - INFO: epoch 001:   7331 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.89, wpb=111.3, bsz=40, num_updates=7320, lr=4.87862e-05, gnorm=0.574, clip=0, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=34838
2023-02-17 01:30:05 - progress_bar.py[line:274] - INFO: epoch 001:   7341 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.3, ups=0.92, wpb=111.6, bsz=40, num_updates=7330, lr=4.87817e-05, gnorm=0.775, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34849
2023-02-17 01:30:16 - progress_bar.py[line:274] - INFO: epoch 001:   7351 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.9, wpb=110.5, bsz=40, num_updates=7340, lr=4.87772e-05, gnorm=0.58, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34860
2023-02-17 01:30:28 - progress_bar.py[line:274] - INFO: epoch 001:   7361 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.88, wpb=111.7, bsz=40, num_updates=7350, lr=4.87727e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=34872
2023-02-17 01:30:39 - progress_bar.py[line:274] - INFO: epoch 001:   7371 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.88, wpb=111.9, bsz=40, num_updates=7360, lr=4.87682e-05, gnorm=0.541, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=34883
2023-02-17 01:30:50 - progress_bar.py[line:274] - INFO: epoch 001:   7381 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.9, wpb=111.8, bsz=40, num_updates=7370, lr=4.87637e-05, gnorm=0.72, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=34894
2023-02-17 01:31:01 - progress_bar.py[line:274] - INFO: epoch 001:   7391 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=7380, lr=4.87592e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34905
2023-02-17 01:31:13 - progress_bar.py[line:274] - INFO: epoch 001:   7401 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=7390, lr=4.87547e-05, gnorm=0.757, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34917
2023-02-17 01:31:24 - progress_bar.py[line:274] - INFO: epoch 001:   7411 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.9, wpb=111.2, bsz=40, num_updates=7400, lr=4.87502e-05, gnorm=0.584, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34928
2023-02-17 01:31:35 - progress_bar.py[line:274] - INFO: epoch 001:   7421 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=7410, lr=4.87457e-05, gnorm=0.594, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34939
2023-02-17 01:31:46 - progress_bar.py[line:274] - INFO: epoch 001:   7431 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=7420, lr=4.87412e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34950
2023-02-17 01:31:57 - progress_bar.py[line:274] - INFO: epoch 001:   7441 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.3, ups=0.87, wpb=110.6, bsz=40, num_updates=7430, lr=4.87367e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34962
2023-02-17 01:32:09 - progress_bar.py[line:274] - INFO: epoch 001:   7451 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.4, ups=0.88, wpb=109.2, bsz=40, num_updates=7440, lr=4.87322e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34973
2023-02-17 01:32:20 - progress_bar.py[line:274] - INFO: epoch 001:   7461 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=7450, lr=4.87276e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34984
2023-02-17 01:32:31 - progress_bar.py[line:274] - INFO: epoch 001:   7471 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=7460, lr=4.87231e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34995
2023-02-17 01:32:42 - progress_bar.py[line:274] - INFO: epoch 001:   7481 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111.3, bsz=40, num_updates=7470, lr=4.87186e-05, gnorm=0.673, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35006
2023-02-17 01:32:53 - progress_bar.py[line:274] - INFO: epoch 001:   7491 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=7480, lr=4.87141e-05, gnorm=0.547, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35018
2023-02-17 01:33:04 - progress_bar.py[line:274] - INFO: epoch 001:   7501 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=7490, lr=4.87096e-05, gnorm=0.543, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35028
2023-02-17 01:33:15 - progress_bar.py[line:274] - INFO: epoch 001:   7511 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=7500, lr=4.87051e-05, gnorm=0.617, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35040
2023-02-17 01:33:27 - progress_bar.py[line:274] - INFO: epoch 001:   7521 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.2, ups=0.88, wpb=109.2, bsz=40, num_updates=7510, lr=4.87006e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35051
2023-02-17 01:33:38 - progress_bar.py[line:274] - INFO: epoch 001:   7531 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.9, wpb=110.2, bsz=40, num_updates=7520, lr=4.86961e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=35062
2023-02-17 01:33:49 - progress_bar.py[line:274] - INFO: epoch 001:   7541 / 28910 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=7530, lr=4.86916e-05, gnorm=0.798, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=35073
2023-02-17 01:34:00 - progress_bar.py[line:274] - INFO: epoch 001:   7551 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.92, wpb=109.6, bsz=40, num_updates=7540, lr=4.86871e-05, gnorm=0.583, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35084
2023-02-17 01:34:11 - progress_bar.py[line:274] - INFO: epoch 001:   7561 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.91, wpb=108.4, bsz=40, num_updates=7550, lr=4.86826e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35095
2023-02-17 01:34:22 - progress_bar.py[line:274] - INFO: epoch 001:   7571 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=7560, lr=4.86781e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35106
2023-02-17 01:34:33 - progress_bar.py[line:274] - INFO: epoch 001:   7581 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=109.7, bsz=40, num_updates=7570, lr=4.86736e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35118
2023-02-17 01:34:44 - progress_bar.py[line:274] - INFO: epoch 001:   7591 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.92, wpb=109.7, bsz=40, num_updates=7580, lr=4.86691e-05, gnorm=0.611, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35129
2023-02-17 01:34:56 - progress_bar.py[line:274] - INFO: epoch 001:   7601 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.6, ups=0.87, wpb=109.7, bsz=40, num_updates=7590, lr=4.86646e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35140
2023-02-17 01:35:07 - progress_bar.py[line:274] - INFO: epoch 001:   7611 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=7600, lr=4.86601e-05, gnorm=0.533, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35151
2023-02-17 01:35:18 - progress_bar.py[line:274] - INFO: epoch 001:   7621 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=7610, lr=4.86556e-05, gnorm=0.524, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35163
2023-02-17 01:35:29 - progress_bar.py[line:274] - INFO: epoch 001:   7631 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=7620, lr=4.86511e-05, gnorm=0.533, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35174
2023-02-17 01:35:41 - progress_bar.py[line:274] - INFO: epoch 001:   7641 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.89, wpb=111.5, bsz=40, num_updates=7630, lr=4.86466e-05, gnorm=0.55, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35185
2023-02-17 01:35:52 - progress_bar.py[line:274] - INFO: epoch 001:   7651 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=7640, lr=4.86421e-05, gnorm=0.594, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35196
2023-02-17 01:36:03 - progress_bar.py[line:274] - INFO: epoch 001:   7661 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=94.7, ups=0.87, wpb=109.2, bsz=40, num_updates=7650, lr=4.86376e-05, gnorm=0.59, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=35207
2023-02-17 01:36:14 - progress_bar.py[line:274] - INFO: epoch 001:   7671 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.9, wpb=111.9, bsz=40, num_updates=7660, lr=4.86331e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35218
2023-02-17 01:36:25 - progress_bar.py[line:274] - INFO: epoch 001:   7681 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=110.7, bsz=40, num_updates=7670, lr=4.86286e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35230
2023-02-17 01:36:37 - progress_bar.py[line:274] - INFO: epoch 001:   7691 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=7680, lr=4.86241e-05, gnorm=0.647, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35241
2023-02-17 01:36:48 - progress_bar.py[line:274] - INFO: epoch 001:   7701 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.9, wpb=110.1, bsz=40, num_updates=7690, lr=4.86196e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35252
2023-02-17 01:36:59 - progress_bar.py[line:274] - INFO: epoch 001:   7711 / 28910 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99, ups=0.9, wpb=109.5, bsz=40, num_updates=7700, lr=4.86151e-05, gnorm=0.728, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=35263
2023-02-17 01:37:10 - progress_bar.py[line:274] - INFO: epoch 001:   7721 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=7710, lr=4.86105e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35274
2023-02-17 01:37:22 - progress_bar.py[line:274] - INFO: epoch 001:   7731 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.8, ups=0.87, wpb=111.3, bsz=40, num_updates=7720, lr=4.8606e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35286
2023-02-17 01:37:33 - progress_bar.py[line:274] - INFO: epoch 001:   7741 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.89, wpb=110.7, bsz=40, num_updates=7730, lr=4.86015e-05, gnorm=0.539, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=35297
2023-02-17 01:37:44 - progress_bar.py[line:274] - INFO: epoch 001:   7751 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=7740, lr=4.8597e-05, gnorm=0.537, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35308
2023-02-17 01:37:55 - progress_bar.py[line:274] - INFO: epoch 001:   7761 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.91, wpb=111.9, bsz=40, num_updates=7750, lr=4.85925e-05, gnorm=0.573, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=35319
2023-02-17 01:38:06 - progress_bar.py[line:274] - INFO: epoch 001:   7771 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=7760, lr=4.8588e-05, gnorm=0.55, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35330
2023-02-17 01:38:17 - progress_bar.py[line:274] - INFO: epoch 001:   7781 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.5, ups=0.88, wpb=108.4, bsz=40, num_updates=7770, lr=4.85835e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35342
2023-02-17 01:38:28 - progress_bar.py[line:274] - INFO: epoch 001:   7791 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.93, wpb=109.8, bsz=40, num_updates=7780, lr=4.8579e-05, gnorm=0.666, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35352
2023-02-17 01:38:39 - progress_bar.py[line:274] - INFO: epoch 001:   7801 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=7790, lr=4.85745e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35364
2023-02-17 01:38:51 - progress_bar.py[line:274] - INFO: epoch 001:   7811 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.89, wpb=110.2, bsz=40, num_updates=7800, lr=4.857e-05, gnorm=0.632, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35375
2023-02-17 01:38:57 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 01:39:02 - progress_bar.py[line:274] - INFO: epoch 001:   7822 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=94.1, ups=0.85, wpb=110.8, bsz=40, num_updates=7810, lr=4.85655e-05, gnorm=0.733, clip=20, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=35387
2023-02-17 01:39:14 - progress_bar.py[line:274] - INFO: epoch 001:   7832 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.87, wpb=112.4, bsz=40, num_updates=7820, lr=4.8561e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35398
2023-02-17 01:39:25 - progress_bar.py[line:274] - INFO: epoch 001:   7842 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=7830, lr=4.85565e-05, gnorm=0.606, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=35409
2023-02-17 01:39:36 - progress_bar.py[line:274] - INFO: epoch 001:   7852 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.3, ups=0.87, wpb=109.6, bsz=40, num_updates=7840, lr=4.8552e-05, gnorm=0.561, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35421
2023-02-17 01:39:47 - progress_bar.py[line:274] - INFO: epoch 001:   7862 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=7850, lr=4.85475e-05, gnorm=0.542, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35432
2023-02-17 01:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   7872 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.1, ups=0.87, wpb=108.9, bsz=40, num_updates=7860, lr=4.8543e-05, gnorm=0.583, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35443
2023-02-17 01:40:10 - progress_bar.py[line:274] - INFO: epoch 001:   7882 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=7870, lr=4.85385e-05, gnorm=0.521, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35454
2023-02-17 01:40:21 - progress_bar.py[line:274] - INFO: epoch 001:   7892 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.92, wpb=109.8, bsz=40, num_updates=7880, lr=4.8534e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=35465
2023-02-17 01:40:33 - progress_bar.py[line:274] - INFO: epoch 001:   7902 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.88, wpb=111, bsz=40, num_updates=7890, lr=4.85295e-05, gnorm=0.593, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35477
2023-02-17 01:40:44 - progress_bar.py[line:274] - INFO: epoch 001:   7912 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=7900, lr=4.8525e-05, gnorm=0.563, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35488
2023-02-17 01:40:54 - progress_bar.py[line:274] - INFO: epoch 001:   7922 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.3, ups=0.92, wpb=110.7, bsz=40, num_updates=7910, lr=4.85205e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35499
2023-02-17 01:41:06 - progress_bar.py[line:274] - INFO: epoch 001:   7932 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111, bsz=40, num_updates=7920, lr=4.8516e-05, gnorm=0.529, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35510
2023-02-17 01:41:17 - progress_bar.py[line:274] - INFO: epoch 001:   7942 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=7930, lr=4.85115e-05, gnorm=0.632, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35521
2023-02-17 01:41:28 - progress_bar.py[line:274] - INFO: epoch 001:   7952 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.7, ups=0.87, wpb=109.8, bsz=40, num_updates=7940, lr=4.8507e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35533
2023-02-17 01:41:39 - progress_bar.py[line:274] - INFO: epoch 001:   7962 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.92, wpb=109.6, bsz=40, num_updates=7950, lr=4.85025e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=35544
2023-02-17 01:41:50 - progress_bar.py[line:274] - INFO: epoch 001:   7972 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=7960, lr=4.8498e-05, gnorm=0.71, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35555
2023-02-17 01:42:01 - progress_bar.py[line:274] - INFO: epoch 001:   7982 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.4, ups=0.92, wpb=111.7, bsz=40, num_updates=7970, lr=4.84934e-05, gnorm=0.519, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35566
2023-02-17 01:42:12 - progress_bar.py[line:274] - INFO: epoch 001:   7992 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=110.4, bsz=40, num_updates=7980, lr=4.84889e-05, gnorm=0.432, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35577
2023-02-17 01:42:24 - progress_bar.py[line:274] - INFO: epoch 001:   8002 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.9, wpb=110.9, bsz=40, num_updates=7990, lr=4.84844e-05, gnorm=0.626, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35588
2023-02-17 01:42:35 - progress_bar.py[line:274] - INFO: epoch 001:   8012 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=8000, lr=4.84799e-05, gnorm=0.583, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=35599
2023-02-17 01:42:35 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 01:42:36 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 01:42:36 - train.py[line:551] - INFO: load:1.05 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 01:44:38 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 01:44:38 - train.py[line:551] - INFO: load:1.08 valid_run:121.79 task_valid:118.67 collect_output:2.03
2023-02-17 01:46:38 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 01:46:38 - train.py[line:551] - INFO: load:1.10 valid_run:241.65 task_valid:234.28 collect_output:5.23
2023-02-17 01:48:40 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 01:48:40 - train.py[line:551] - INFO: load:1.13 valid_run:363.47 task_valid:350.58 collect_output:9.70
2023-02-17 01:50:42 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 01:50:42 - train.py[line:551] - INFO: load:1.16 valid_run:485.34 task_valid:464.21 collect_output:16.89
2023-02-17 01:52:42 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 01:52:42 - train.py[line:551] - INFO: load:1.18 valid_run:605.65 task_valid:581.31 collect_output:19.05
2023-02-17 01:54:45 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 01:54:45 - train.py[line:551] - INFO: load:1.21 valid_run:728.38 task_valid:699.74 collect_output:22.31
2023-02-17 01:56:48 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 01:56:48 - train.py[line:551] - INFO: load:1.24 valid_run:851.35 task_valid:817.61 collect_output:26.36
2023-02-17 01:58:50 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 01:58:50 - train.py[line:551] - INFO: load:1.26 valid_run:973.14 task_valid:934.04 collect_output:30.69
2023-02-17 02:00:53 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 02:00:53 - train.py[line:551] - INFO: load:1.29 valid_run:1096.86 task_valid:1051.12 collect_output:36.29
2023-02-17 02:02:55 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 02:02:55 - train.py[line:551] - INFO: load:1.32 valid_run:1218.55 task_valid:1163.85 collect_output:44.20
2023-02-17 02:04:55 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 02:04:55 - train.py[line:551] - INFO: load:1.34 valid_run:1338.46 task_valid:1279.20 collect_output:47.71
2023-02-17 02:06:57 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 02:06:57 - train.py[line:551] - INFO: load:1.37 valid_run:1459.90 task_valid:1395.78 collect_output:51.54
2023-02-17 02:08:56 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 02:08:56 - train.py[line:551] - INFO: load:1.40 valid_run:1578.72 task_valid:1509.40 collect_output:55.69
2023-02-17 02:10:56 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 02:10:56 - train.py[line:551] - INFO: load:1.42 valid_run:1699.42 task_valid:1626.76 collect_output:58.02
2023-02-17 02:12:57 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 02:12:57 - train.py[line:551] - INFO: load:1.45 valid_run:1820.33 task_valid:1742.59 collect_output:62.07
2023-02-17 02:14:58 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 02:14:58 - train.py[line:551] - INFO: load:1.48 valid_run:1941.31 task_valid:1856.40 collect_output:68.20
2023-02-17 02:17:00 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 02:17:00 - train.py[line:551] - INFO: load:1.50 valid_run:2062.50 task_valid:1972.23 collect_output:72.51
2023-02-17 02:19:00 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 02:19:00 - train.py[line:551] - INFO: load:1.53 valid_run:2182.94 task_valid:2089.83 collect_output:74.33
2023-02-17 02:21:01 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 02:21:01 - train.py[line:551] - INFO: load:1.56 valid_run:2303.90 task_valid:2206.41 collect_output:77.68
2023-02-17 02:23:01 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 02:23:01 - train.py[line:551] - INFO: load:1.58 valid_run:2424.03 task_valid:2322.61 collect_output:80.58
2023-02-17 02:25:03 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 02:25:03 - train.py[line:551] - INFO: load:1.61 valid_run:2545.59 task_valid:2438.84 collect_output:84.88
2023-02-17 02:27:05 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 02:27:05 - train.py[line:551] - INFO: load:1.64 valid_run:2667.36 task_valid:2557.43 collect_output:87.05
2023-02-17 02:29:05 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 02:29:05 - train.py[line:551] - INFO: load:1.66 valid_run:2787.60 task_valid:2671.53 collect_output:92.15
2023-02-17 02:31:05 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 02:31:05 - train.py[line:551] - INFO: load:1.69 valid_run:2907.39 task_valid:2787.48 collect_output:94.96
2023-02-17 02:33:07 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 02:33:07 - train.py[line:551] - INFO: load:1.72 valid_run:3028.98 task_valid:2903.43 collect_output:99.56
2023-02-17 02:35:09 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 02:35:09 - train.py[line:551] - INFO: load:1.74 valid_run:3151.86 task_valid:3019.18 collect_output:105.66
2023-02-17 02:37:09 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 02:37:09 - train.py[line:551] - INFO: load:1.77 valid_run:3271.29 task_valid:3133.09 collect_output:110.16
2023-02-17 02:39:11 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 02:39:11 - train.py[line:551] - INFO: load:1.80 valid_run:3392.92 task_valid:3252.16 collect_output:111.69
2023-02-17 02:41:12 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 02:41:12 - train.py[line:551] - INFO: load:1.82 valid_run:3514.48 task_valid:3367.43 collect_output:116.96
2023-02-17 02:43:14 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 02:43:14 - train.py[line:551] - INFO: load:1.85 valid_run:3636.16 task_valid:3485.51 collect_output:119.53
2023-02-17 02:45:15 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 02:45:15 - train.py[line:551] - INFO: load:1.88 valid_run:3756.89 task_valid:3603.49 collect_output:121.24

====================================================================================================
SGG eval:     R @ 50: 0.6009;     R @ 100: 0.6490;     R @ 500: 0.6841;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4015;    mR @ 100: 0.4437;    mR @ 500: 0.5077;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7353) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4903) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.6848) (standing on:0.3133) (using:0.6000) (walking in:0.0000) (walking on:0.7297) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6009;     R @ 100: 0.6490;     R @ 500: 0.6841;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4015;    mR @ 100: 0.4437;    mR @ 500: 0.5077;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7353) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4903) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9614) (says:0.0000) (sitting on:0.6848) (standing on:0.3133) (using:0.6000) (walking in:0.0000) (walking on:0.7297) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-02-17 02:45:45 - train.py[line:487] - INFO: 0.6489761904761905
2023-02-17 02:45:46 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 02:45:46 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.325 | loss_v1 0 | loss_v2 0 | nll_loss 0.168 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.648976 | ppl 1.12 | vqa_score 0.5563 | wps 118.4 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.649485
2023-02-17 02:45:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2023-02-17 02:45:46 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_8000.pt
2023-02-17 02:45:51 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_8000.pt
2023-02-17 02:45:53 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.6489761904761905) (writing took 7.782379910349846 seconds)
2023-02-17 02:46:05 - progress_bar.py[line:274] - INFO: epoch 001:   8022 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.3, ups=0, wpb=111.3, bsz=40, num_updates=8010, lr=4.84754e-05, gnorm=0.565, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39409
2023-02-17 02:46:16 - progress_bar.py[line:274] - INFO: epoch 001:   8032 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.7, ups=0.88, wpb=108.4, bsz=40, num_updates=8020, lr=4.84709e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39420
2023-02-17 02:46:27 - progress_bar.py[line:274] - INFO: epoch 001:   8042 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=8030, lr=4.84664e-05, gnorm=0.564, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39432
2023-02-17 02:46:39 - progress_bar.py[line:274] - INFO: epoch 001:   8052 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.88, wpb=112, bsz=40, num_updates=8040, lr=4.84619e-05, gnorm=0.522, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39443
2023-02-17 02:46:50 - progress_bar.py[line:274] - INFO: epoch 001:   8062 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=8050, lr=4.84574e-05, gnorm=0.585, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39454
2023-02-17 02:47:01 - progress_bar.py[line:274] - INFO: epoch 001:   8072 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=8060, lr=4.84529e-05, gnorm=0.588, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39466
2023-02-17 02:47:12 - progress_bar.py[line:274] - INFO: epoch 001:   8082 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.91, wpb=110.5, bsz=40, num_updates=8070, lr=4.84484e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39477
2023-02-17 02:47:23 - progress_bar.py[line:274] - INFO: epoch 001:   8092 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=8080, lr=4.84439e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39488
2023-02-17 02:47:35 - progress_bar.py[line:274] - INFO: epoch 001:   8102 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.88, wpb=111, bsz=40, num_updates=8090, lr=4.84394e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39499
2023-02-17 02:47:45 - progress_bar.py[line:274] - INFO: epoch 001:   8112 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.3, ups=0.94, wpb=111.4, bsz=40, num_updates=8100, lr=4.84349e-05, gnorm=0.667, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39510
2023-02-17 02:47:56 - progress_bar.py[line:274] - INFO: epoch 001:   8122 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.3, ups=0.94, wpb=108.7, bsz=40, num_updates=8110, lr=4.84304e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39520
2023-02-17 02:48:07 - progress_bar.py[line:274] - INFO: epoch 001:   8132 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=8120, lr=4.84259e-05, gnorm=0.539, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39531
2023-02-17 02:48:18 - progress_bar.py[line:274] - INFO: epoch 001:   8142 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=8130, lr=4.84214e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39543
2023-02-17 02:48:30 - progress_bar.py[line:274] - INFO: epoch 001:   8152 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.4, ups=0.88, wpb=109.6, bsz=40, num_updates=8140, lr=4.84169e-05, gnorm=0.572, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39554
2023-02-17 02:48:41 - progress_bar.py[line:274] - INFO: epoch 001:   8162 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.93, wpb=109.5, bsz=40, num_updates=8150, lr=4.84124e-05, gnorm=0.548, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39565
2023-02-17 02:48:52 - progress_bar.py[line:274] - INFO: epoch 001:   8172 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.89, wpb=108.7, bsz=40, num_updates=8160, lr=4.84079e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39576
2023-02-17 02:49:03 - progress_bar.py[line:274] - INFO: epoch 001:   8182 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=8170, lr=4.84034e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=39587
2023-02-17 02:49:14 - progress_bar.py[line:274] - INFO: epoch 001:   8192 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.92, wpb=110.2, bsz=40, num_updates=8180, lr=4.83989e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39598
2023-02-17 02:49:25 - progress_bar.py[line:274] - INFO: epoch 001:   8202 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=8190, lr=4.83944e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39609
2023-02-17 02:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   8212 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=8200, lr=4.83899e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39621
2023-02-17 02:49:48 - progress_bar.py[line:274] - INFO: epoch 001:   8222 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=8210, lr=4.83854e-05, gnorm=0.583, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39632
2023-02-17 02:49:59 - progress_bar.py[line:274] - INFO: epoch 001:   8232 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=8220, lr=4.83808e-05, gnorm=0.66, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39643
2023-02-17 02:50:10 - progress_bar.py[line:274] - INFO: epoch 001:   8242 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=8230, lr=4.83763e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39655
2023-02-17 02:50:22 - progress_bar.py[line:274] - INFO: epoch 001:   8252 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=8240, lr=4.83718e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39666
2023-02-17 02:50:33 - progress_bar.py[line:274] - INFO: epoch 001:   8262 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=8250, lr=4.83673e-05, gnorm=0.51, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39677
2023-02-17 02:50:44 - progress_bar.py[line:274] - INFO: epoch 001:   8272 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=8260, lr=4.83628e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39688
2023-02-17 02:50:55 - progress_bar.py[line:274] - INFO: epoch 001:   8282 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.1, ups=0.9, wpb=109.5, bsz=40, num_updates=8270, lr=4.83583e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39699
2023-02-17 02:51:06 - progress_bar.py[line:274] - INFO: epoch 001:   8292 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.89, wpb=109, bsz=40, num_updates=8280, lr=4.83538e-05, gnorm=0.593, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39711
2023-02-17 02:51:17 - progress_bar.py[line:274] - INFO: epoch 001:   8302 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=8290, lr=4.83493e-05, gnorm=0.663, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39722
2023-02-17 02:51:28 - progress_bar.py[line:274] - INFO: epoch 001:   8312 / 28910 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.8, ups=0.91, wpb=111.3, bsz=40, num_updates=8300, lr=4.83448e-05, gnorm=0.849, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39733
2023-02-17 02:51:40 - progress_bar.py[line:274] - INFO: epoch 001:   8322 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.9, wpb=109.1, bsz=40, num_updates=8310, lr=4.83403e-05, gnorm=0.607, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39744
2023-02-17 02:51:51 - progress_bar.py[line:274] - INFO: epoch 001:   8332 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.88, wpb=111, bsz=40, num_updates=8320, lr=4.83358e-05, gnorm=0.576, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=39755
2023-02-17 02:52:02 - progress_bar.py[line:274] - INFO: epoch 001:   8342 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=8330, lr=4.83313e-05, gnorm=0.498, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39767
2023-02-17 02:52:13 - progress_bar.py[line:274] - INFO: epoch 001:   8352 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.92, wpb=111.2, bsz=40, num_updates=8340, lr=4.83268e-05, gnorm=0.612, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39778
2023-02-17 02:52:25 - progress_bar.py[line:274] - INFO: epoch 001:   8362 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111.7, bsz=40, num_updates=8350, lr=4.83223e-05, gnorm=0.584, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39789
2023-02-17 02:52:36 - progress_bar.py[line:274] - INFO: epoch 001:   8372 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.89, wpb=109, bsz=40, num_updates=8360, lr=4.83178e-05, gnorm=0.47, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39800
2023-02-17 02:52:47 - progress_bar.py[line:274] - INFO: epoch 001:   8382 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.9, wpb=109.9, bsz=40, num_updates=8370, lr=4.83133e-05, gnorm=0.626, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39811
2023-02-17 02:52:58 - progress_bar.py[line:274] - INFO: epoch 001:   8392 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=8380, lr=4.83088e-05, gnorm=0.595, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39823
2023-02-17 02:53:09 - progress_bar.py[line:274] - INFO: epoch 001:   8402 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.92, wpb=107.8, bsz=40, num_updates=8390, lr=4.83043e-05, gnorm=0.523, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39833
2023-02-17 02:53:20 - progress_bar.py[line:274] - INFO: epoch 001:   8412 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.93, wpb=109.8, bsz=40, num_updates=8400, lr=4.82998e-05, gnorm=0.56, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39844
2023-02-17 02:53:31 - progress_bar.py[line:274] - INFO: epoch 001:   8422 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=8410, lr=4.82953e-05, gnorm=0.645, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39856
2023-02-17 02:53:42 - progress_bar.py[line:274] - INFO: epoch 001:   8432 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.92, wpb=109.9, bsz=40, num_updates=8420, lr=4.82908e-05, gnorm=0.533, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39867
2023-02-17 02:53:53 - progress_bar.py[line:274] - INFO: epoch 001:   8442 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.91, wpb=109.4, bsz=40, num_updates=8430, lr=4.82863e-05, gnorm=0.523, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39878
2023-02-17 02:54:05 - progress_bar.py[line:274] - INFO: epoch 001:   8452 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=8440, lr=4.82818e-05, gnorm=0.597, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39889
2023-02-17 02:54:16 - progress_bar.py[line:274] - INFO: epoch 001:   8462 / 28910 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.6, ups=0.89, wpb=108.1, bsz=40, num_updates=8450, lr=4.82773e-05, gnorm=0.582, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39900
2023-02-17 02:54:27 - progress_bar.py[line:274] - INFO: epoch 001:   8472 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=8460, lr=4.82728e-05, gnorm=0.607, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39912
2023-02-17 02:54:38 - progress_bar.py[line:274] - INFO: epoch 001:   8482 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.5, ups=0.92, wpb=111.9, bsz=40, num_updates=8470, lr=4.82683e-05, gnorm=0.553, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39922
2023-02-17 02:54:49 - progress_bar.py[line:274] - INFO: epoch 001:   8492 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.8, ups=0.89, wpb=109.4, bsz=40, num_updates=8480, lr=4.82637e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39934
2023-02-17 02:55:01 - progress_bar.py[line:274] - INFO: epoch 001:   8502 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.88, wpb=112.2, bsz=40, num_updates=8490, lr=4.82592e-05, gnorm=0.494, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39945
2023-02-17 02:55:12 - progress_bar.py[line:274] - INFO: epoch 001:   8512 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=8500, lr=4.82547e-05, gnorm=0.459, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39957
2023-02-17 02:55:23 - progress_bar.py[line:274] - INFO: epoch 001:   8522 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.6, ups=0.94, wpb=111, bsz=40, num_updates=8510, lr=4.82502e-05, gnorm=0.476, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39967
2023-02-17 02:55:34 - progress_bar.py[line:274] - INFO: epoch 001:   8532 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.92, wpb=110, bsz=40, num_updates=8520, lr=4.82457e-05, gnorm=0.557, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39978
2023-02-17 02:55:45 - progress_bar.py[line:274] - INFO: epoch 001:   8542 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=8530, lr=4.82412e-05, gnorm=0.543, clip=0, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=39989
2023-02-17 02:55:56 - progress_bar.py[line:274] - INFO: epoch 001:   8552 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.87, wpb=111.4, bsz=40, num_updates=8540, lr=4.82367e-05, gnorm=0.505, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40001
2023-02-17 02:56:08 - progress_bar.py[line:274] - INFO: epoch 001:   8562 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=111.4, bsz=40, num_updates=8550, lr=4.82322e-05, gnorm=0.531, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40012
2023-02-17 02:56:19 - progress_bar.py[line:274] - INFO: epoch 001:   8572 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95, ups=0.87, wpb=109.3, bsz=40, num_updates=8560, lr=4.82277e-05, gnorm=0.577, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40023
2023-02-17 02:56:24 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 02:56:31 - progress_bar.py[line:274] - INFO: epoch 001:   8583 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=90.6, ups=0.82, wpb=110.9, bsz=40, num_updates=8570, lr=4.82232e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=40036
2023-02-17 02:56:42 - progress_bar.py[line:274] - INFO: epoch 001:   8593 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=8580, lr=4.82187e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40047
2023-02-17 02:56:54 - progress_bar.py[line:274] - INFO: epoch 001:   8603 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=8590, lr=4.82142e-05, gnorm=0.571, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40058
2023-02-17 02:57:05 - progress_bar.py[line:274] - INFO: epoch 001:   8613 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=8600, lr=4.82097e-05, gnorm=0.572, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40069
2023-02-17 02:57:16 - progress_bar.py[line:274] - INFO: epoch 001:   8623 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=8610, lr=4.82052e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40080
2023-02-17 02:57:27 - progress_bar.py[line:274] - INFO: epoch 001:   8633 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.92, wpb=111.1, bsz=40, num_updates=8620, lr=4.82007e-05, gnorm=0.418, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40091
2023-02-17 02:57:38 - progress_bar.py[line:274] - INFO: epoch 001:   8643 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.8, ups=0.88, wpb=109.8, bsz=40, num_updates=8630, lr=4.81962e-05, gnorm=0.569, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40102
2023-02-17 02:57:49 - progress_bar.py[line:274] - INFO: epoch 001:   8653 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=111.7, bsz=40, num_updates=8640, lr=4.81917e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40114
2023-02-17 02:58:00 - progress_bar.py[line:274] - INFO: epoch 001:   8663 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=8650, lr=4.81872e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40125
2023-02-17 02:58:12 - progress_bar.py[line:274] - INFO: epoch 001:   8673 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96, ups=0.87, wpb=110.3, bsz=40, num_updates=8660, lr=4.81827e-05, gnorm=0.578, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40136
2023-02-17 02:58:23 - progress_bar.py[line:274] - INFO: epoch 001:   8683 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.9, wpb=109.7, bsz=40, num_updates=8670, lr=4.81782e-05, gnorm=0.714, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40147
2023-02-17 02:58:34 - progress_bar.py[line:274] - INFO: epoch 001:   8693 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.9, ups=0.93, wpb=110.8, bsz=40, num_updates=8680, lr=4.81737e-05, gnorm=0.467, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40158
2023-02-17 02:58:45 - progress_bar.py[line:274] - INFO: epoch 001:   8703 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.9, wpb=110, bsz=40, num_updates=8690, lr=4.81692e-05, gnorm=0.588, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40169
2023-02-17 02:58:56 - progress_bar.py[line:274] - INFO: epoch 001:   8713 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=94.6, ups=0.87, wpb=108.5, bsz=40, num_updates=8700, lr=4.81647e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40181
2023-02-17 02:59:08 - progress_bar.py[line:274] - INFO: epoch 001:   8723 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=8710, lr=4.81602e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40192
2023-02-17 02:59:18 - progress_bar.py[line:274] - INFO: epoch 001:   8733 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.5, ups=0.95, wpb=109.6, bsz=40, num_updates=8720, lr=4.81557e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=10, gb_free=10.2, ema_decay=0.9999, wall=40202
2023-02-17 02:59:29 - progress_bar.py[line:274] - INFO: epoch 001:   8743 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=8730, lr=4.81512e-05, gnorm=0.561, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40213
2023-02-17 02:59:40 - progress_bar.py[line:274] - INFO: epoch 001:   8753 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=8740, lr=4.81466e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40224
2023-02-17 02:59:51 - progress_bar.py[line:274] - INFO: epoch 001:   8763 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=8750, lr=4.81421e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40236
2023-02-17 03:00:03 - progress_bar.py[line:274] - INFO: epoch 001:   8773 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.4, ups=0.87, wpb=109.7, bsz=40, num_updates=8760, lr=4.81376e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40247
2023-02-17 03:00:14 - progress_bar.py[line:274] - INFO: epoch 001:   8783 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.9, wpb=109.3, bsz=40, num_updates=8770, lr=4.81331e-05, gnorm=0.575, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=40258
2023-02-17 03:00:25 - progress_bar.py[line:274] - INFO: epoch 001:   8793 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=8780, lr=4.81286e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=40270
2023-02-17 03:00:37 - progress_bar.py[line:274] - INFO: epoch 001:   8803 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.88, wpb=109.2, bsz=40, num_updates=8790, lr=4.81241e-05, gnorm=0.622, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40281
2023-02-17 03:00:48 - progress_bar.py[line:274] - INFO: epoch 001:   8813 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=8800, lr=4.81196e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=40292
2023-02-17 03:00:59 - progress_bar.py[line:274] - INFO: epoch 001:   8823 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.92, wpb=110.3, bsz=40, num_updates=8810, lr=4.81151e-05, gnorm=0.524, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40303
2023-02-17 03:01:10 - progress_bar.py[line:274] - INFO: epoch 001:   8833 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.9, ups=0.93, wpb=110.7, bsz=40, num_updates=8820, lr=4.81106e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40314
2023-02-17 03:01:21 - progress_bar.py[line:274] - INFO: epoch 001:   8843 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=8830, lr=4.81061e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40325
2023-02-17 03:01:32 - progress_bar.py[line:274] - INFO: epoch 001:   8853 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=8840, lr=4.81016e-05, gnorm=0.621, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40337
2023-02-17 03:01:43 - progress_bar.py[line:274] - INFO: epoch 001:   8863 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=8850, lr=4.80971e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40348
2023-02-17 03:01:55 - progress_bar.py[line:274] - INFO: epoch 001:   8873 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.88, wpb=110.2, bsz=40, num_updates=8860, lr=4.80926e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40359
2023-02-17 03:02:06 - progress_bar.py[line:274] - INFO: epoch 001:   8883 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.2, ups=0.88, wpb=109, bsz=40, num_updates=8870, lr=4.80881e-05, gnorm=0.678, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40370
2023-02-17 03:02:17 - progress_bar.py[line:274] - INFO: epoch 001:   8893 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=8880, lr=4.80836e-05, gnorm=0.648, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40382
2023-02-17 03:02:28 - progress_bar.py[line:274] - INFO: epoch 001:   8903 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.9, wpb=112.1, bsz=40, num_updates=8890, lr=4.80791e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40393
2023-02-17 03:02:40 - progress_bar.py[line:274] - INFO: epoch 001:   8913 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.7, ups=0.87, wpb=109.4, bsz=40, num_updates=8900, lr=4.80746e-05, gnorm=0.617, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40404
2023-02-17 03:02:51 - progress_bar.py[line:274] - INFO: epoch 001:   8923 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.9, wpb=109.8, bsz=40, num_updates=8910, lr=4.80701e-05, gnorm=0.553, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40415
2023-02-17 03:03:02 - progress_bar.py[line:274] - INFO: epoch 001:   8933 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=8920, lr=4.80656e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=40426
2023-02-17 03:03:13 - progress_bar.py[line:274] - INFO: epoch 001:   8943 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.92, wpb=111, bsz=40, num_updates=8930, lr=4.80611e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40437
2023-02-17 03:03:24 - progress_bar.py[line:274] - INFO: epoch 001:   8953 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=8940, lr=4.80566e-05, gnorm=0.666, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40448
2023-02-17 03:03:35 - progress_bar.py[line:274] - INFO: epoch 001:   8963 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=8950, lr=4.80521e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40460
2023-02-17 03:03:46 - progress_bar.py[line:274] - INFO: epoch 001:   8973 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=8960, lr=4.80476e-05, gnorm=0.505, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40470
2023-02-17 03:03:57 - progress_bar.py[line:274] - INFO: epoch 001:   8983 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.7, ups=0.93, wpb=110.5, bsz=40, num_updates=8970, lr=4.80431e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40481
2023-02-17 03:04:08 - progress_bar.py[line:274] - INFO: epoch 001:   8993 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.92, wpb=109.2, bsz=40, num_updates=8980, lr=4.80386e-05, gnorm=0.599, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40492
2023-02-17 03:04:19 - progress_bar.py[line:274] - INFO: epoch 001:   9003 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.92, wpb=109.5, bsz=40, num_updates=8990, lr=4.8034e-05, gnorm=0.64, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=40503
2023-02-17 03:04:30 - progress_bar.py[line:274] - INFO: epoch 001:   9013 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.7, bsz=40, num_updates=9000, lr=4.80295e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=40514
2023-02-17 03:04:30 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 03:04:31 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 03:04:31 - train.py[line:551] - INFO: load:0.95 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 03:06:33 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 03:06:33 - train.py[line:551] - INFO: load:0.98 valid_run:122.14 task_valid:118.92 collect_output:2.14
2023-02-17 03:08:33 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 03:08:33 - train.py[line:551] - INFO: load:1.00 valid_run:242.06 task_valid:234.45 collect_output:5.49
2023-02-17 03:10:35 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 03:10:35 - train.py[line:551] - INFO: load:1.03 valid_run:364.01 task_valid:350.75 collect_output:10.10
2023-02-17 03:12:37 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 03:12:37 - train.py[line:551] - INFO: load:1.06 valid_run:485.95 task_valid:464.27 collect_output:17.50
2023-02-17 03:14:38 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 03:14:38 - train.py[line:551] - INFO: load:1.08 valid_run:606.44 task_valid:581.27 collect_output:19.96
2023-02-17 03:16:41 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 03:16:41 - train.py[line:551] - INFO: load:1.11 valid_run:729.19 task_valid:699.65 collect_output:23.30
2023-02-17 03:18:44 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 03:18:44 - train.py[line:551] - INFO: load:1.13 valid_run:852.13 task_valid:817.45 collect_output:27.41
2023-02-17 03:20:46 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 03:20:46 - train.py[line:551] - INFO: load:1.16 valid_run:974.00 task_valid:933.80 collect_output:31.90
2023-02-17 03:22:49 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 03:22:49 - train.py[line:551] - INFO: load:1.18 valid_run:1097.75 task_valid:1050.67 collect_output:37.77
2023-02-17 03:24:51 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 03:24:51 - train.py[line:551] - INFO: load:1.21 valid_run:1219.60 task_valid:1163.26 collect_output:46.00
2023-02-17 03:26:52 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 03:26:52 - train.py[line:551] - INFO: load:1.24 valid_run:1339.88 task_valid:1278.70 collect_output:49.82
2023-02-17 03:28:53 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 03:28:53 - train.py[line:551] - INFO: load:1.26 valid_run:1461.41 task_valid:1395.28 collect_output:53.76
2023-02-17 03:30:52 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 03:30:52 - train.py[line:551] - INFO: load:1.29 valid_run:1580.35 task_valid:1508.88 collect_output:58.08
2023-02-17 03:32:53 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 03:32:53 - train.py[line:551] - INFO: load:1.32 valid_run:1701.33 task_valid:1626.44 collect_output:60.48
2023-02-17 03:34:54 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 03:34:54 - train.py[line:551] - INFO: load:1.34 valid_run:1822.33 task_valid:1742.34 collect_output:64.56
2023-02-17 03:36:56 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 03:36:56 - train.py[line:551] - INFO: load:1.37 valid_run:1943.42 task_valid:1856.26 collect_output:70.73
2023-02-17 03:38:57 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 03:38:57 - train.py[line:551] - INFO: load:1.39 valid_run:2064.72 task_valid:1972.13 collect_output:75.15
2023-02-17 03:40:57 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 03:40:57 - train.py[line:551] - INFO: load:1.42 valid_run:2185.23 task_valid:2089.69 collect_output:77.08
2023-02-17 03:42:59 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 03:42:59 - train.py[line:551] - INFO: load:1.45 valid_run:2306.60 task_valid:2206.41 collect_output:80.72
2023-02-17 03:44:59 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 03:44:59 - train.py[line:551] - INFO: load:1.47 valid_run:2426.94 task_valid:2322.78 collect_output:83.68
2023-02-17 03:47:01 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 03:47:01 - train.py[line:551] - INFO: load:1.50 valid_run:2548.73 task_valid:2439.28 collect_output:87.97
2023-02-17 03:49:03 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 03:49:03 - train.py[line:551] - INFO: load:1.53 valid_run:2670.59 task_valid:2557.92 collect_output:90.18
2023-02-17 03:51:04 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 03:51:04 - train.py[line:551] - INFO: load:1.55 valid_run:2791.01 task_valid:2671.99 collect_output:95.52
2023-02-17 03:53:03 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 03:53:03 - train.py[line:551] - INFO: load:1.58 valid_run:2910.70 task_valid:2787.88 collect_output:98.29
2023-02-17 03:55:05 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 03:55:05 - train.py[line:551] - INFO: load:1.60 valid_run:3032.28 task_valid:2903.94 collect_output:102.79
2023-02-17 03:57:08 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 03:57:08 - train.py[line:551] - INFO: load:1.63 valid_run:3155.07 task_valid:3019.66 collect_output:108.84
2023-02-17 03:59:07 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 03:59:07 - train.py[line:551] - INFO: load:1.66 valid_run:3274.64 task_valid:3133.39 collect_output:113.67
2023-02-17 04:01:09 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 04:01:09 - train.py[line:551] - INFO: load:1.68 valid_run:3396.21 task_valid:3252.23 collect_output:115.40
2023-02-17 04:03:11 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 04:03:11 - train.py[line:551] - INFO: load:1.71 valid_run:3518.15 task_valid:3367.45 collect_output:121.10
2023-02-17 04:05:13 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 04:05:13 - train.py[line:551] - INFO: load:1.74 valid_run:3639.80 task_valid:3485.40 collect_output:123.78
2023-02-17 04:07:14 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 04:07:14 - train.py[line:551] - INFO: load:1.76 valid_run:3760.66 task_valid:3603.35 collect_output:125.67

====================================================================================================
SGG eval:     R @ 50: 0.5877;     R @ 100: 0.6419;     R @ 500: 0.6777;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3847;    mR @ 100: 0.4384;    mR @ 500: 0.4963;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9624) (says:0.0000) (sitting on:0.6644) (standing on:0.3233) (using:0.6000) (walking in:0.0000) (walking on:0.7568) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-02-17 04:07:44 - train.py[line:487] - INFO: 0.641942857142857

====================================================================================================
SGG eval:     R @ 50: 0.5877;     R @ 100: 0.6419;     R @ 500: 0.6777;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3847;    mR @ 100: 0.4384;    mR @ 500: 0.4963;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.8125) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.5000) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9624) (says:0.0000) (sitting on:0.6644) (standing on:0.3233) (using:0.6000) (walking in:0.0000) (walking on:0.7568) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-02-17 04:07:44 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 04:07:44 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.317 | loss_v1 0 | loss_v2 0 | nll_loss 0.157 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.641943 | ppl 1.12 | vqa_score 0.5495 | wps 118.3 | wpb 72 | bsz 24 | num_updates 9000 | best_R@100 0.649485
2023-02-17 04:07:44 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 9000 updates
2023-02-17 04:07:44 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_9000.pt
2023-02-17 04:07:50 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_9000.pt
2023-02-17 04:07:53 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_9000.pt (epoch 1 @ 9000 updates, score 0.641942857142857) (writing took 8.233047915622592 seconds)
2023-02-17 04:08:04 - progress_bar.py[line:274] - INFO: epoch 001:   9023 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=0.3, ups=0, wpb=110.2, bsz=40, num_updates=9010, lr=4.8025e-05, gnorm=0.496, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44328
2023-02-17 04:08:15 - progress_bar.py[line:274] - INFO: epoch 001:   9033 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=9020, lr=4.80205e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44339
2023-02-17 04:08:26 - progress_bar.py[line:274] - INFO: epoch 001:   9043 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.91, wpb=111.1, bsz=40, num_updates=9030, lr=4.8016e-05, gnorm=0.579, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44350
2023-02-17 04:08:37 - progress_bar.py[line:274] - INFO: epoch 001:   9053 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=9040, lr=4.80115e-05, gnorm=0.496, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44362
2023-02-17 04:08:49 - progress_bar.py[line:274] - INFO: epoch 001:   9063 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=9050, lr=4.8007e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44373
2023-02-17 04:09:00 - progress_bar.py[line:274] - INFO: epoch 001:   9073 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.8, ups=0.88, wpb=109, bsz=40, num_updates=9060, lr=4.80025e-05, gnorm=0.66, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44384
2023-02-17 04:09:11 - progress_bar.py[line:274] - INFO: epoch 001:   9083 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=9070, lr=4.7998e-05, gnorm=0.588, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44396
2023-02-17 04:09:23 - progress_bar.py[line:274] - INFO: epoch 001:   9093 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=9080, lr=4.79935e-05, gnorm=0.577, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44407
2023-02-17 04:09:34 - progress_bar.py[line:274] - INFO: epoch 001:   9103 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.89, wpb=108.9, bsz=40, num_updates=9090, lr=4.7989e-05, gnorm=0.565, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44418
2023-02-17 04:09:45 - progress_bar.py[line:274] - INFO: epoch 001:   9113 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=94.7, ups=0.87, wpb=108.8, bsz=40, num_updates=9100, lr=4.79845e-05, gnorm=0.465, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44430
2023-02-17 04:09:57 - progress_bar.py[line:274] - INFO: epoch 001:   9123 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.88, wpb=109.9, bsz=40, num_updates=9110, lr=4.798e-05, gnorm=0.444, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44441
2023-02-17 04:10:08 - progress_bar.py[line:274] - INFO: epoch 001:   9133 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.9, wpb=111.3, bsz=40, num_updates=9120, lr=4.79755e-05, gnorm=0.508, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44452
2023-02-17 04:10:14 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 04:10:20 - progress_bar.py[line:274] - INFO: epoch 001:   9144 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=93, ups=0.83, wpb=112.4, bsz=40, num_updates=9130, lr=4.7971e-05, gnorm=0.576, clip=20, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=44464
2023-02-17 04:10:31 - progress_bar.py[line:274] - INFO: epoch 001:   9154 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.92, wpb=110, bsz=40, num_updates=9140, lr=4.79665e-05, gnorm=0.586, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44475
2023-02-17 04:10:42 - progress_bar.py[line:274] - INFO: epoch 001:   9164 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=110.9, bsz=40, num_updates=9150, lr=4.7962e-05, gnorm=0.478, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=44486
2023-02-17 04:10:53 - progress_bar.py[line:274] - INFO: epoch 001:   9174 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.8, ups=0.87, wpb=110.2, bsz=40, num_updates=9160, lr=4.79575e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44498
2023-02-17 04:11:05 - progress_bar.py[line:274] - INFO: epoch 001:   9184 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.4, ups=0.88, wpb=109.5, bsz=40, num_updates=9170, lr=4.7953e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44509
2023-02-17 04:11:16 - progress_bar.py[line:274] - INFO: epoch 001:   9194 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=9180, lr=4.79485e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44520
2023-02-17 04:11:27 - progress_bar.py[line:274] - INFO: epoch 001:   9204 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=9190, lr=4.7944e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44531
2023-02-17 04:11:38 - progress_bar.py[line:274] - INFO: epoch 001:   9214 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=9200, lr=4.79395e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44542
2023-02-17 04:11:49 - progress_bar.py[line:274] - INFO: epoch 001:   9224 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=9210, lr=4.7935e-05, gnorm=0.579, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44554
2023-02-17 04:12:00 - progress_bar.py[line:274] - INFO: epoch 001:   9234 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.2, ups=0.94, wpb=109.9, bsz=40, num_updates=9220, lr=4.79305e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44564
2023-02-17 04:12:11 - progress_bar.py[line:274] - INFO: epoch 001:   9244 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.6, ups=0.93, wpb=111.5, bsz=40, num_updates=9230, lr=4.7926e-05, gnorm=0.558, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44575
2023-02-17 04:12:22 - progress_bar.py[line:274] - INFO: epoch 001:   9254 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.1, ups=0.86, wpb=110.9, bsz=40, num_updates=9240, lr=4.79215e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44587
2023-02-17 04:12:34 - progress_bar.py[line:274] - INFO: epoch 001:   9264 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=9250, lr=4.79169e-05, gnorm=0.486, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44598
2023-02-17 04:12:44 - progress_bar.py[line:274] - INFO: epoch 001:   9274 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.93, wpb=109.7, bsz=40, num_updates=9260, lr=4.79124e-05, gnorm=0.501, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44609
2023-02-17 04:12:56 - progress_bar.py[line:274] - INFO: epoch 001:   9284 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=9270, lr=4.79079e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44620
2023-02-17 04:13:07 - progress_bar.py[line:274] - INFO: epoch 001:   9294 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=9280, lr=4.79034e-05, gnorm=0.577, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44631
2023-02-17 04:13:18 - progress_bar.py[line:274] - INFO: epoch 001:   9304 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=9290, lr=4.78989e-05, gnorm=0.712, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44642
2023-02-17 04:13:29 - progress_bar.py[line:274] - INFO: epoch 001:   9314 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=9300, lr=4.78944e-05, gnorm=0.417, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44653
2023-02-17 04:13:40 - progress_bar.py[line:274] - INFO: epoch 001:   9324 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=9310, lr=4.78899e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44665
2023-02-17 04:13:52 - progress_bar.py[line:274] - INFO: epoch 001:   9334 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=9320, lr=4.78854e-05, gnorm=0.688, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=44676
2023-02-17 04:14:03 - progress_bar.py[line:274] - INFO: epoch 001:   9344 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.5, ups=0.9, wpb=110.4, bsz=40, num_updates=9330, lr=4.78809e-05, gnorm=0.636, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44687
2023-02-17 04:14:14 - progress_bar.py[line:274] - INFO: epoch 001:   9354 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=9340, lr=4.78764e-05, gnorm=0.796, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44698
2023-02-17 04:14:25 - progress_bar.py[line:274] - INFO: epoch 001:   9364 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=9350, lr=4.78719e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44709
2023-02-17 04:14:36 - progress_bar.py[line:274] - INFO: epoch 001:   9374 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=9360, lr=4.78674e-05, gnorm=0.526, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44721
2023-02-17 04:14:47 - progress_bar.py[line:274] - INFO: epoch 001:   9384 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.92, wpb=110.5, bsz=40, num_updates=9370, lr=4.78629e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44732
2023-02-17 04:14:58 - progress_bar.py[line:274] - INFO: epoch 001:   9394 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=9380, lr=4.78584e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44743
2023-02-17 04:15:10 - progress_bar.py[line:274] - INFO: epoch 001:   9404 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=9390, lr=4.78539e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44754
2023-02-17 04:15:21 - progress_bar.py[line:274] - INFO: epoch 001:   9414 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=9400, lr=4.78494e-05, gnorm=0.491, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44765
2023-02-17 04:15:32 - progress_bar.py[line:274] - INFO: epoch 001:   9424 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.93, wpb=110.3, bsz=40, num_updates=9410, lr=4.78449e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44776
2023-02-17 04:15:43 - progress_bar.py[line:274] - INFO: epoch 001:   9434 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.5, ups=0.88, wpb=109.4, bsz=40, num_updates=9420, lr=4.78404e-05, gnorm=0.572, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44787
2023-02-17 04:15:54 - progress_bar.py[line:274] - INFO: epoch 001:   9444 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=9430, lr=4.78359e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44798
2023-02-17 04:16:06 - progress_bar.py[line:274] - INFO: epoch 001:   9454 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.3, ups=0.87, wpb=109.5, bsz=40, num_updates=9440, lr=4.78314e-05, gnorm=0.529, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44810
2023-02-17 04:16:17 - progress_bar.py[line:274] - INFO: epoch 001:   9464 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.92, wpb=110.6, bsz=40, num_updates=9450, lr=4.78269e-05, gnorm=0.517, clip=0, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=44821
2023-02-17 04:16:28 - progress_bar.py[line:274] - INFO: epoch 001:   9474 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.5, ups=0.89, wpb=111.2, bsz=40, num_updates=9460, lr=4.78224e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44832
2023-02-17 04:16:39 - progress_bar.py[line:274] - INFO: epoch 001:   9484 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=9470, lr=4.78179e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44843
2023-02-17 04:16:50 - progress_bar.py[line:274] - INFO: epoch 001:   9494 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.88, wpb=109.7, bsz=40, num_updates=9480, lr=4.78134e-05, gnorm=0.533, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44855
2023-02-17 04:17:02 - progress_bar.py[line:274] - INFO: epoch 001:   9504 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=9490, lr=4.78089e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44866
2023-02-17 04:17:12 - progress_bar.py[line:274] - INFO: epoch 001:   9514 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.6, ups=0.94, wpb=111.2, bsz=40, num_updates=9500, lr=4.78044e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44876
2023-02-17 04:17:23 - progress_bar.py[line:274] - INFO: epoch 001:   9524 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=9510, lr=4.77998e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44887
2023-02-17 04:17:35 - progress_bar.py[line:274] - INFO: epoch 001:   9534 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.2, ups=0.87, wpb=110.6, bsz=40, num_updates=9520, lr=4.77953e-05, gnorm=0.485, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44899
2023-02-17 04:17:46 - progress_bar.py[line:274] - INFO: epoch 001:   9544 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=9530, lr=4.77908e-05, gnorm=0.4, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44910
2023-02-17 04:17:57 - progress_bar.py[line:274] - INFO: epoch 001:   9554 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98, ups=0.88, wpb=111.2, bsz=40, num_updates=9540, lr=4.77863e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44922
2023-02-17 04:18:08 - progress_bar.py[line:274] - INFO: epoch 001:   9564 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.8, ups=0.94, wpb=110.4, bsz=40, num_updates=9550, lr=4.77818e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44932
2023-02-17 04:18:19 - progress_bar.py[line:274] - INFO: epoch 001:   9574 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=9560, lr=4.77773e-05, gnorm=0.547, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44944
2023-02-17 04:18:31 - progress_bar.py[line:274] - INFO: epoch 001:   9584 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.89, wpb=110, bsz=40, num_updates=9570, lr=4.77728e-05, gnorm=0.552, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44955
2023-02-17 04:18:42 - progress_bar.py[line:274] - INFO: epoch 001:   9594 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=94.9, ups=0.87, wpb=109.2, bsz=40, num_updates=9580, lr=4.77683e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44966
2023-02-17 04:18:54 - progress_bar.py[line:274] - INFO: epoch 001:   9604 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.2, ups=0.86, wpb=110.5, bsz=40, num_updates=9590, lr=4.77638e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44978
2023-02-17 04:19:04 - progress_bar.py[line:274] - INFO: epoch 001:   9614 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.1, ups=0.94, wpb=109.6, bsz=40, num_updates=9600, lr=4.77593e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44989
2023-02-17 04:19:16 - progress_bar.py[line:274] - INFO: epoch 001:   9624 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.89, wpb=111.2, bsz=40, num_updates=9610, lr=4.77548e-05, gnorm=0.764, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45000
2023-02-17 04:19:26 - progress_bar.py[line:274] - INFO: epoch 001:   9634 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.5, ups=0.94, wpb=110, bsz=40, num_updates=9620, lr=4.77503e-05, gnorm=0.559, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45010
2023-02-17 04:19:38 - progress_bar.py[line:274] - INFO: epoch 001:   9644 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=9630, lr=4.77458e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45022
2023-02-17 04:19:49 - progress_bar.py[line:274] - INFO: epoch 001:   9654 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=9640, lr=4.77413e-05, gnorm=0.537, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45033
2023-02-17 04:20:00 - progress_bar.py[line:274] - INFO: epoch 001:   9664 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.6, ups=0.88, wpb=108.6, bsz=40, num_updates=9650, lr=4.77368e-05, gnorm=0.584, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45044
2023-02-17 04:20:11 - progress_bar.py[line:274] - INFO: epoch 001:   9674 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=9660, lr=4.77323e-05, gnorm=0.488, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45056
2023-02-17 04:20:23 - progress_bar.py[line:274] - INFO: epoch 001:   9684 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.88, wpb=110.6, bsz=40, num_updates=9670, lr=4.77278e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45067
2023-02-17 04:20:34 - progress_bar.py[line:274] - INFO: epoch 001:   9694 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.92, wpb=108.2, bsz=40, num_updates=9680, lr=4.77233e-05, gnorm=0.683, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45078
2023-02-17 04:20:44 - progress_bar.py[line:274] - INFO: epoch 001:   9704 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.92, wpb=110.4, bsz=40, num_updates=9690, lr=4.77188e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45089
2023-02-17 04:20:56 - progress_bar.py[line:274] - INFO: epoch 001:   9714 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=9700, lr=4.77143e-05, gnorm=0.464, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45100
2023-02-17 04:21:02 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 04:21:07 - progress_bar.py[line:274] - INFO: epoch 001:   9725 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=93.1, ups=0.84, wpb=111.1, bsz=40, num_updates=9710, lr=4.77098e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=45112
2023-02-17 04:21:18 - progress_bar.py[line:274] - INFO: epoch 001:   9735 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.92, wpb=109.2, bsz=40, num_updates=9720, lr=4.77053e-05, gnorm=0.542, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45123
2023-02-17 04:21:30 - progress_bar.py[line:274] - INFO: epoch 001:   9745 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=94.8, ups=0.87, wpb=108.5, bsz=40, num_updates=9730, lr=4.77008e-05, gnorm=0.544, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45134
2023-02-17 04:21:41 - progress_bar.py[line:274] - INFO: epoch 001:   9755 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.9, ups=0.9, wpb=110.5, bsz=40, num_updates=9740, lr=4.76963e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45145
2023-02-17 04:21:52 - progress_bar.py[line:274] - INFO: epoch 001:   9765 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=9750, lr=4.76918e-05, gnorm=0.462, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45156
2023-02-17 04:22:03 - progress_bar.py[line:274] - INFO: epoch 001:   9775 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.4, ups=0.93, wpb=110.3, bsz=40, num_updates=9760, lr=4.76872e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45167
2023-02-17 04:22:14 - progress_bar.py[line:274] - INFO: epoch 001:   9785 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.3, ups=0.87, wpb=109.6, bsz=40, num_updates=9770, lr=4.76827e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45179
2023-02-17 04:22:25 - progress_bar.py[line:274] - INFO: epoch 001:   9795 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=9780, lr=4.76782e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45190
2023-02-17 04:22:36 - progress_bar.py[line:274] - INFO: epoch 001:   9805 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.6, ups=0.93, wpb=111.8, bsz=40, num_updates=9790, lr=4.76737e-05, gnorm=0.614, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45200
2023-02-17 04:22:48 - progress_bar.py[line:274] - INFO: epoch 001:   9815 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=9800, lr=4.76692e-05, gnorm=0.742, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45212
2023-02-17 04:22:59 - progress_bar.py[line:274] - INFO: epoch 001:   9825 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=9810, lr=4.76647e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45223
2023-02-17 04:23:10 - progress_bar.py[line:274] - INFO: epoch 001:   9835 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98, ups=0.9, wpb=109.4, bsz=40, num_updates=9820, lr=4.76602e-05, gnorm=0.642, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45234
2023-02-17 04:23:21 - progress_bar.py[line:274] - INFO: epoch 001:   9845 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=9830, lr=4.76557e-05, gnorm=0.525, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45245
2023-02-17 04:23:32 - progress_bar.py[line:274] - INFO: epoch 001:   9855 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.91, wpb=109.4, bsz=40, num_updates=9840, lr=4.76512e-05, gnorm=0.538, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45256
2023-02-17 04:23:43 - progress_bar.py[line:274] - INFO: epoch 001:   9865 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=9850, lr=4.76467e-05, gnorm=0.533, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45267
2023-02-17 04:23:54 - progress_bar.py[line:274] - INFO: epoch 001:   9875 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.88, wpb=112.1, bsz=40, num_updates=9860, lr=4.76422e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45279
2023-02-17 04:24:05 - progress_bar.py[line:274] - INFO: epoch 001:   9885 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.91, wpb=108.5, bsz=40, num_updates=9870, lr=4.76377e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45290
2023-02-17 04:24:17 - progress_bar.py[line:274] - INFO: epoch 001:   9895 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.9, wpb=111.6, bsz=40, num_updates=9880, lr=4.76332e-05, gnorm=0.537, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45301
2023-02-17 04:24:28 - progress_bar.py[line:274] - INFO: epoch 001:   9905 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=9890, lr=4.76287e-05, gnorm=0.599, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45312
2023-02-17 04:24:39 - progress_bar.py[line:274] - INFO: epoch 001:   9915 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.2, ups=0.92, wpb=110.3, bsz=40, num_updates=9900, lr=4.76242e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45323
2023-02-17 04:24:50 - progress_bar.py[line:274] - INFO: epoch 001:   9925 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.2, ups=0.88, wpb=109.1, bsz=40, num_updates=9910, lr=4.76197e-05, gnorm=0.536, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45334
2023-02-17 04:25:01 - progress_bar.py[line:274] - INFO: epoch 001:   9935 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.88, wpb=110.2, bsz=40, num_updates=9920, lr=4.76152e-05, gnorm=0.606, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45346
2023-02-17 04:25:13 - progress_bar.py[line:274] - INFO: epoch 001:   9945 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.88, wpb=111.3, bsz=40, num_updates=9930, lr=4.76107e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45357
2023-02-17 04:25:24 - progress_bar.py[line:274] - INFO: epoch 001:   9955 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.7, ups=0.87, wpb=109.5, bsz=40, num_updates=9940, lr=4.76062e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45369
2023-02-17 04:25:36 - progress_bar.py[line:274] - INFO: epoch 001:   9965 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=9950, lr=4.76017e-05, gnorm=0.501, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45380
2023-02-17 04:25:47 - progress_bar.py[line:274] - INFO: epoch 001:   9975 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.89, wpb=110.2, bsz=40, num_updates=9960, lr=4.75972e-05, gnorm=0.592, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45391
2023-02-17 04:25:58 - progress_bar.py[line:274] - INFO: epoch 001:   9985 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.92, wpb=110.5, bsz=40, num_updates=9970, lr=4.75927e-05, gnorm=0.609, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45402
2023-02-17 04:26:09 - progress_bar.py[line:274] - INFO: epoch 001:   9995 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=9980, lr=4.75882e-05, gnorm=0.507, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45413
2023-02-17 04:26:20 - progress_bar.py[line:274] - INFO: epoch 001:  10005 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=9990, lr=4.75837e-05, gnorm=0.556, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45424
2023-02-17 04:26:31 - progress_bar.py[line:274] - INFO: epoch 001:  10015 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.89, wpb=108.6, bsz=40, num_updates=10000, lr=4.75792e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45435
2023-02-17 04:26:31 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 04:26:33 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 04:26:33 - train.py[line:551] - INFO: load:1.27 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 04:28:35 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 04:28:35 - train.py[line:551] - INFO: load:1.29 valid_run:121.96 task_valid:119.03 collect_output:1.83
2023-02-17 04:30:35 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 04:30:35 - train.py[line:551] - INFO: load:1.32 valid_run:241.96 task_valid:234.85 collect_output:4.99
2023-02-17 04:32:37 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 04:32:37 - train.py[line:551] - INFO: load:1.34 valid_run:363.93 task_valid:351.19 collect_output:9.61
2023-02-17 04:34:39 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 04:34:39 - train.py[line:551] - INFO: load:1.37 valid_run:485.85 task_valid:464.89 collect_output:16.80
2023-02-17 04:36:39 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 04:36:39 - train.py[line:551] - INFO: load:1.39 valid_run:606.30 task_valid:582.09 collect_output:19.02
2023-02-17 04:38:42 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 04:38:42 - train.py[line:551] - INFO: load:1.42 valid_run:729.06 task_valid:700.64 collect_output:22.23
2023-02-17 04:40:45 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 04:40:45 - train.py[line:551] - INFO: load:1.44 valid_run:851.94 task_valid:818.56 collect_output:26.19
2023-02-17 04:42:47 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 04:42:47 - train.py[line:551] - INFO: load:1.47 valid_run:973.79 task_valid:934.99 collect_output:30.62
2023-02-17 04:44:51 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 04:44:51 - train.py[line:551] - INFO: load:1.49 valid_run:1097.53 task_valid:1051.97 collect_output:36.37
2023-02-17 04:46:52 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 04:46:52 - train.py[line:551] - INFO: load:1.52 valid_run:1219.24 task_valid:1164.57 collect_output:44.48
2023-02-17 04:48:53 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 04:48:53 - train.py[line:551] - INFO: load:1.54 valid_run:1339.37 task_valid:1280.12 collect_output:48.06
2023-02-17 04:50:54 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 04:50:54 - train.py[line:551] - INFO: load:1.57 valid_run:1460.83 task_valid:1396.73 collect_output:51.91
2023-02-17 04:52:53 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 04:52:53 - train.py[line:551] - INFO: load:1.59 valid_run:1579.83 task_valid:1510.51 collect_output:56.13
2023-02-17 04:54:54 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 04:54:54 - train.py[line:551] - INFO: load:1.62 valid_run:1700.70 task_valid:1628.03 collect_output:58.47
2023-02-17 04:56:55 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 04:56:55 - train.py[line:551] - INFO: load:1.64 valid_run:1821.65 task_valid:1743.92 collect_output:62.54
2023-02-17 04:58:56 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 04:58:56 - train.py[line:551] - INFO: load:1.67 valid_run:1942.72 task_valid:1857.77 collect_output:68.76
2023-02-17 05:00:58 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 05:00:58 - train.py[line:551] - INFO: load:1.69 valid_run:2064.02 task_valid:1973.71 collect_output:73.12
2023-02-17 05:02:58 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 05:02:58 - train.py[line:551] - INFO: load:1.72 valid_run:2184.54 task_valid:2091.39 collect_output:74.96
2023-02-17 05:04:59 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 05:04:59 - train.py[line:551] - INFO: load:1.74 valid_run:2305.67 task_valid:2208.18 collect_output:78.29
2023-02-17 05:07:00 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 05:07:00 - train.py[line:551] - INFO: load:1.77 valid_run:2425.94 task_valid:2324.51 collect_output:81.21
2023-02-17 05:09:02 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 05:09:02 - train.py[line:551] - INFO: load:1.80 valid_run:2547.72 task_valid:2441.30 collect_output:85.19
2023-02-17 05:11:03 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 05:11:03 - train.py[line:551] - INFO: load:1.82 valid_run:2669.54 task_valid:2560.00 collect_output:87.30
2023-02-17 05:13:04 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 05:13:04 - train.py[line:551] - INFO: load:1.85 valid_run:2790.05 task_valid:2674.40 collect_output:92.41
2023-02-17 05:15:04 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 05:15:04 - train.py[line:551] - INFO: load:1.87 valid_run:2909.72 task_valid:2790.36 collect_output:95.10
2023-02-17 05:17:05 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 05:17:05 - train.py[line:551] - INFO: load:1.90 valid_run:3031.19 task_valid:2906.29 collect_output:99.65
2023-02-17 05:19:08 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 05:19:08 - train.py[line:551] - INFO: load:1.92 valid_run:3153.98 task_valid:3021.97 collect_output:105.74
2023-02-17 05:21:08 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 05:21:08 - train.py[line:551] - INFO: load:1.95 valid_run:3273.47 task_valid:3135.71 collect_output:110.49
2023-02-17 05:23:09 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 05:23:09 - train.py[line:551] - INFO: load:1.98 valid_run:3394.97 task_valid:3254.62 collect_output:112.09
2023-02-17 05:25:11 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 05:25:11 - train.py[line:551] - INFO: load:2.00 valid_run:3516.51 task_valid:3369.84 collect_output:117.42
2023-02-17 05:27:12 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 05:27:12 - train.py[line:551] - INFO: load:2.03 valid_run:3638.13 task_valid:3487.95 collect_output:119.94
2023-02-17 05:29:13 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 05:29:13 - train.py[line:551] - INFO: load:2.05 valid_run:3758.96 task_valid:3605.98 collect_output:121.74

====================================================================================================
SGG eval:     R @ 50: 0.5857;     R @ 100: 0.6306;     R @ 500: 0.6779;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3742;    mR @ 100: 0.4208;    mR @ 500: 0.4912;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7707) (covered in:0.8750) (covering:0.2857) (eating:0.6765) (flying in:0.0000) (growing on:0.3750) (hanging from:0.3871) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.6746) (standing on:0.3333) (using:0.6000) (walking in:0.0000) (walking on:0.7297) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5857;     R @ 100: 0.6306;     R @ 500: 0.6779;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3742;    mR @ 100: 0.4208;    mR @ 500: 0.4912;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7707) (covered in:0.8750) (covering:0.2857) (eating:0.6765) (flying in:0.0000) (growing on:0.3750) (hanging from:0.3871) (lying on:0.2000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.6746) (standing on:0.3333) (using:0.6000) (walking in:0.0000) (walking on:0.7297) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-02-17 05:29:44 - train.py[line:487] - INFO: 0.6305714285714284
2023-02-17 05:29:44 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 05:29:44 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.315 | loss_v1 0 | loss_v2 0 | nll_loss 0.156 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.630571 | ppl 1.11 | vqa_score 0.5541 | wps 118.3 | wpb 72 | bsz 24 | num_updates 10000 | best_R@100 0.649485
2023-02-17 05:29:44 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-02-17 05:29:44 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_10000.pt
2023-02-17 05:29:50 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_10000.pt
2023-02-17 05:29:52 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.6305714285714284) (writing took 8.110553726553917 seconds)
2023-02-17 05:30:03 - progress_bar.py[line:274] - INFO: epoch 001:  10025 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=0.3, ups=0, wpb=109.7, bsz=40, num_updates=10010, lr=4.75747e-05, gnorm=0.511, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49248
2023-02-17 05:30:15 - progress_bar.py[line:274] - INFO: epoch 001:  10035 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=10020, lr=4.75701e-05, gnorm=0.548, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49259
2023-02-17 05:30:26 - progress_bar.py[line:274] - INFO: epoch 001:  10045 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=10030, lr=4.75656e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49270
2023-02-17 05:30:37 - progress_bar.py[line:274] - INFO: epoch 001:  10055 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=10040, lr=4.75611e-05, gnorm=0.564, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49281
2023-02-17 05:30:48 - progress_bar.py[line:274] - INFO: epoch 001:  10065 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.9, bsz=40, num_updates=10050, lr=4.75566e-05, gnorm=0.505, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49293
2023-02-17 05:30:59 - progress_bar.py[line:274] - INFO: epoch 001:  10075 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.9, wpb=109.7, bsz=40, num_updates=10060, lr=4.75521e-05, gnorm=0.549, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49304
2023-02-17 05:31:11 - progress_bar.py[line:274] - INFO: epoch 001:  10085 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.88, wpb=110.7, bsz=40, num_updates=10070, lr=4.75476e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49315
2023-02-17 05:31:22 - progress_bar.py[line:274] - INFO: epoch 001:  10095 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=10080, lr=4.75431e-05, gnorm=0.478, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49326
2023-02-17 05:31:33 - progress_bar.py[line:274] - INFO: epoch 001:  10105 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.92, wpb=109.7, bsz=40, num_updates=10090, lr=4.75386e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49337
2023-02-17 05:31:44 - progress_bar.py[line:274] - INFO: epoch 001:  10115 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.92, wpb=108.9, bsz=40, num_updates=10100, lr=4.75341e-05, gnorm=0.538, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49348
2023-02-17 05:31:55 - progress_bar.py[line:274] - INFO: epoch 001:  10125 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=10110, lr=4.75296e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49359
2023-02-17 05:32:06 - progress_bar.py[line:274] - INFO: epoch 001:  10135 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=10120, lr=4.75251e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49371
2023-02-17 05:32:17 - progress_bar.py[line:274] - INFO: epoch 001:  10145 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.9, ups=0.93, wpb=111, bsz=40, num_updates=10130, lr=4.75206e-05, gnorm=0.486, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49381
2023-02-17 05:32:28 - progress_bar.py[line:274] - INFO: epoch 001:  10155 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=10140, lr=4.75161e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49393
2023-02-17 05:32:39 - progress_bar.py[line:274] - INFO: epoch 001:  10165 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.2, ups=0.92, wpb=111.6, bsz=40, num_updates=10150, lr=4.75116e-05, gnorm=0.497, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49404
2023-02-17 05:32:51 - progress_bar.py[line:274] - INFO: epoch 001:  10175 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.88, wpb=110.4, bsz=40, num_updates=10160, lr=4.75071e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49415
2023-02-17 05:33:02 - progress_bar.py[line:274] - INFO: epoch 001:  10185 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=10170, lr=4.75026e-05, gnorm=0.375, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49426
2023-02-17 05:33:13 - progress_bar.py[line:274] - INFO: epoch 001:  10195 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.88, wpb=111.5, bsz=40, num_updates=10180, lr=4.74981e-05, gnorm=0.62, clip=20, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=49437
2023-02-17 05:33:24 - progress_bar.py[line:274] - INFO: epoch 001:  10205 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=10190, lr=4.74936e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49449
2023-02-17 05:33:36 - progress_bar.py[line:274] - INFO: epoch 001:  10215 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=10200, lr=4.74891e-05, gnorm=0.566, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49460
2023-02-17 05:33:47 - progress_bar.py[line:274] - INFO: epoch 001:  10225 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.2, ups=0.87, wpb=110.1, bsz=40, num_updates=10210, lr=4.74846e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49471
2023-02-17 05:33:58 - progress_bar.py[line:274] - INFO: epoch 001:  10235 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.89, wpb=110.3, bsz=40, num_updates=10220, lr=4.74801e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49483
2023-02-17 05:34:10 - progress_bar.py[line:274] - INFO: epoch 001:  10245 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.88, wpb=110.4, bsz=40, num_updates=10230, lr=4.74756e-05, gnorm=0.523, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49494
2023-02-17 05:34:21 - progress_bar.py[line:274] - INFO: epoch 001:  10255 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.7, bsz=40, num_updates=10240, lr=4.74711e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49505
2023-02-17 05:34:32 - progress_bar.py[line:274] - INFO: epoch 001:  10265 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.91, wpb=110.4, bsz=40, num_updates=10250, lr=4.74666e-05, gnorm=0.587, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49516
2023-02-17 05:34:43 - progress_bar.py[line:274] - INFO: epoch 001:  10275 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.93, wpb=109, bsz=40, num_updates=10260, lr=4.74621e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49527
2023-02-17 05:34:54 - progress_bar.py[line:274] - INFO: epoch 001:  10285 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.92, wpb=111.7, bsz=40, num_updates=10270, lr=4.74576e-05, gnorm=0.461, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49538
2023-02-17 05:35:04 - progress_bar.py[line:274] - INFO: epoch 001:  10295 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.5, ups=0.93, wpb=110.6, bsz=40, num_updates=10280, lr=4.7453e-05, gnorm=0.468, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49549
2023-02-17 05:35:16 - progress_bar.py[line:274] - INFO: epoch 001:  10305 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=110.8, bsz=40, num_updates=10290, lr=4.74485e-05, gnorm=0.456, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49560
2023-02-17 05:35:27 - progress_bar.py[line:274] - INFO: epoch 001:  10315 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.9, wpb=110.3, bsz=40, num_updates=10300, lr=4.7444e-05, gnorm=0.717, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49571
2023-02-17 05:35:31 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 05:35:39 - progress_bar.py[line:274] - INFO: epoch 001:  10326 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=91.7, ups=0.83, wpb=110.9, bsz=40, num_updates=10310, lr=4.74395e-05, gnorm=0.576, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=49583
2023-02-17 05:35:50 - progress_bar.py[line:274] - INFO: epoch 001:  10336 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=10320, lr=4.7435e-05, gnorm=0.426, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49594
2023-02-17 05:36:01 - progress_bar.py[line:274] - INFO: epoch 001:  10346 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.89, wpb=110.8, bsz=40, num_updates=10330, lr=4.74305e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49605
2023-02-17 05:36:12 - progress_bar.py[line:274] - INFO: epoch 001:  10356 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.88, wpb=110.2, bsz=40, num_updates=10340, lr=4.7426e-05, gnorm=0.481, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49617
2023-02-17 05:36:23 - progress_bar.py[line:274] - INFO: epoch 001:  10366 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.93, wpb=109.3, bsz=40, num_updates=10350, lr=4.74215e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49627
2023-02-17 05:36:34 - progress_bar.py[line:274] - INFO: epoch 001:  10376 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.91, wpb=108.2, bsz=40, num_updates=10360, lr=4.7417e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49639
2023-02-17 05:36:46 - progress_bar.py[line:274] - INFO: epoch 001:  10386 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=10370, lr=4.74125e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49650
2023-02-17 05:36:56 - progress_bar.py[line:274] - INFO: epoch 001:  10396 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.92, wpb=110.5, bsz=40, num_updates=10380, lr=4.7408e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49661
2023-02-17 05:37:08 - progress_bar.py[line:274] - INFO: epoch 001:  10406 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.89, wpb=108.8, bsz=40, num_updates=10390, lr=4.74035e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49672
2023-02-17 05:37:19 - progress_bar.py[line:274] - INFO: epoch 001:  10416 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.91, wpb=111.5, bsz=40, num_updates=10400, lr=4.7399e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49683
2023-02-17 05:37:30 - progress_bar.py[line:274] - INFO: epoch 001:  10426 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=10410, lr=4.73945e-05, gnorm=0.515, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49694
2023-02-17 05:37:41 - progress_bar.py[line:274] - INFO: epoch 001:  10436 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=110.5, bsz=40, num_updates=10420, lr=4.739e-05, gnorm=0.537, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49705
2023-02-17 05:37:52 - progress_bar.py[line:274] - INFO: epoch 001:  10446 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103, ups=0.93, wpb=110.9, bsz=40, num_updates=10430, lr=4.73855e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49716
2023-02-17 05:38:03 - progress_bar.py[line:274] - INFO: epoch 001:  10456 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=10440, lr=4.7381e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49727
2023-02-17 05:38:14 - progress_bar.py[line:274] - INFO: epoch 001:  10466 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=10450, lr=4.73765e-05, gnorm=0.679, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49738
2023-02-17 05:38:25 - progress_bar.py[line:274] - INFO: epoch 001:  10476 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=10460, lr=4.7372e-05, gnorm=0.55, clip=0, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=49750
2023-02-17 05:38:36 - progress_bar.py[line:274] - INFO: epoch 001:  10486 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.92, wpb=110, bsz=40, num_updates=10470, lr=4.73675e-05, gnorm=0.498, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49761
2023-02-17 05:38:47 - progress_bar.py[line:274] - INFO: epoch 001:  10496 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=10480, lr=4.7363e-05, gnorm=0.445, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49772
2023-02-17 05:38:59 - progress_bar.py[line:274] - INFO: epoch 001:  10506 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=10490, lr=4.73585e-05, gnorm=0.529, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49783
2023-02-17 05:39:09 - progress_bar.py[line:274] - INFO: epoch 001:  10516 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.93, wpb=109.3, bsz=40, num_updates=10500, lr=4.7354e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=49794
2023-02-17 05:39:21 - progress_bar.py[line:274] - INFO: epoch 001:  10526 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=10510, lr=4.73495e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49805
2023-02-17 05:39:32 - progress_bar.py[line:274] - INFO: epoch 001:  10536 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=10520, lr=4.7345e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49816
2023-02-17 05:39:43 - progress_bar.py[line:274] - INFO: epoch 001:  10546 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.9, wpb=108.4, bsz=40, num_updates=10530, lr=4.73404e-05, gnorm=0.631, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49827
2023-02-17 05:39:54 - progress_bar.py[line:274] - INFO: epoch 001:  10556 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=10540, lr=4.73359e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49838
2023-02-17 05:40:05 - progress_bar.py[line:274] - INFO: epoch 001:  10566 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=10550, lr=4.73314e-05, gnorm=0.651, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49850
2023-02-17 05:40:17 - progress_bar.py[line:274] - INFO: epoch 001:  10576 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.88, wpb=110.9, bsz=40, num_updates=10560, lr=4.73269e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49861
2023-02-17 05:40:28 - progress_bar.py[line:274] - INFO: epoch 001:  10586 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.9, wpb=109.7, bsz=40, num_updates=10570, lr=4.73224e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49872
2023-02-17 05:40:39 - progress_bar.py[line:274] - INFO: epoch 001:  10596 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.91, wpb=111.3, bsz=40, num_updates=10580, lr=4.73179e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49883
2023-02-17 05:40:50 - progress_bar.py[line:274] - INFO: epoch 001:  10606 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.1, ups=0.92, wpb=110.7, bsz=40, num_updates=10590, lr=4.73134e-05, gnorm=0.555, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49894
2023-02-17 05:41:00 - progress_bar.py[line:274] - INFO: epoch 001:  10616 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.5, ups=0.95, wpb=110.6, bsz=40, num_updates=10600, lr=4.73089e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=49904
2023-02-17 05:41:11 - progress_bar.py[line:274] - INFO: epoch 001:  10626 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.9, wpb=109.6, bsz=40, num_updates=10610, lr=4.73044e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49915
2023-02-17 05:41:22 - progress_bar.py[line:274] - INFO: epoch 001:  10636 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=10620, lr=4.72999e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49927
2023-02-17 05:41:34 - progress_bar.py[line:274] - INFO: epoch 001:  10646 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=10630, lr=4.72954e-05, gnorm=0.498, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49938
2023-02-17 05:41:45 - progress_bar.py[line:274] - INFO: epoch 001:  10656 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=10640, lr=4.72909e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49949
2023-02-17 05:41:56 - progress_bar.py[line:274] - INFO: epoch 001:  10666 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.3, ups=0.87, wpb=110.4, bsz=40, num_updates=10650, lr=4.72864e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49960
2023-02-17 05:42:07 - progress_bar.py[line:274] - INFO: epoch 001:  10676 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=10660, lr=4.72819e-05, gnorm=0.508, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49972
2023-02-17 05:42:18 - progress_bar.py[line:274] - INFO: epoch 001:  10686 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=10670, lr=4.72774e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49983
2023-02-17 05:42:30 - progress_bar.py[line:274] - INFO: epoch 001:  10696 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.9, wpb=110.6, bsz=40, num_updates=10680, lr=4.72729e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=49994
2023-02-17 05:42:41 - progress_bar.py[line:274] - INFO: epoch 001:  10706 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=10690, lr=4.72684e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50005
2023-02-17 05:42:52 - progress_bar.py[line:274] - INFO: epoch 001:  10716 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.87, wpb=111.8, bsz=40, num_updates=10700, lr=4.72639e-05, gnorm=0.574, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50017
2023-02-17 05:43:04 - progress_bar.py[line:274] - INFO: epoch 001:  10726 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=10710, lr=4.72594e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=50028
2023-02-17 05:43:15 - progress_bar.py[line:274] - INFO: epoch 001:  10736 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=10720, lr=4.72549e-05, gnorm=0.581, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50039
2023-02-17 05:43:26 - progress_bar.py[line:274] - INFO: epoch 001:  10746 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=10730, lr=4.72504e-05, gnorm=0.612, clip=10, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=50050
2023-02-17 05:43:37 - progress_bar.py[line:274] - INFO: epoch 001:  10756 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.2, bsz=40, num_updates=10740, lr=4.72459e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50062
2023-02-17 05:43:48 - progress_bar.py[line:274] - INFO: epoch 001:  10766 / 28910 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.92, wpb=109.8, bsz=40, num_updates=10750, lr=4.72414e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50072
2023-02-17 05:44:00 - progress_bar.py[line:274] - INFO: epoch 001:  10776 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=10760, lr=4.72369e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50084
2023-02-17 05:44:11 - progress_bar.py[line:274] - INFO: epoch 001:  10786 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=10770, lr=4.72324e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50095
2023-02-17 05:44:22 - progress_bar.py[line:274] - INFO: epoch 001:  10796 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.91, wpb=109.5, bsz=40, num_updates=10780, lr=4.72279e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50106
2023-02-17 05:44:33 - progress_bar.py[line:274] - INFO: epoch 001:  10806 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.91, wpb=110.8, bsz=40, num_updates=10790, lr=4.72233e-05, gnorm=0.471, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=50117
2023-02-17 05:44:44 - progress_bar.py[line:274] - INFO: epoch 001:  10816 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=10800, lr=4.72188e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50128
2023-02-17 05:44:55 - progress_bar.py[line:274] - INFO: epoch 001:  10826 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=10810, lr=4.72143e-05, gnorm=0.526, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50139
2023-02-17 05:45:06 - progress_bar.py[line:274] - INFO: epoch 001:  10836 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=10820, lr=4.72098e-05, gnorm=0.541, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50151
2023-02-17 05:45:17 - progress_bar.py[line:274] - INFO: epoch 001:  10846 / 28910 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.92, wpb=110.8, bsz=40, num_updates=10830, lr=4.72053e-05, gnorm=0.529, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50162
2023-02-17 05:45:28 - progress_bar.py[line:274] - INFO: epoch 001:  10856 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=10840, lr=4.72008e-05, gnorm=0.533, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50173
2023-02-17 05:45:39 - progress_bar.py[line:274] - INFO: epoch 001:  10866 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.8, ups=0.9, wpb=111.5, bsz=40, num_updates=10850, lr=4.71963e-05, gnorm=0.606, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50184
2023-02-17 05:45:50 - progress_bar.py[line:274] - INFO: epoch 001:  10876 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.9, wpb=110.5, bsz=40, num_updates=10860, lr=4.71918e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50195
2023-02-17 05:46:02 - progress_bar.py[line:274] - INFO: epoch 001:  10886 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=111.2, bsz=40, num_updates=10870, lr=4.71873e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50206
2023-02-17 05:46:13 - progress_bar.py[line:274] - INFO: epoch 001:  10896 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.9, wpb=109.2, bsz=40, num_updates=10880, lr=4.71828e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50217
2023-02-17 05:46:24 - progress_bar.py[line:274] - INFO: epoch 001:  10906 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96, ups=0.87, wpb=110.5, bsz=40, num_updates=10890, lr=4.71783e-05, gnorm=0.468, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50229
2023-02-17 05:46:36 - progress_bar.py[line:274] - INFO: epoch 001:  10916 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=10900, lr=4.71738e-05, gnorm=0.498, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=50240
2023-02-17 05:46:45 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 05:46:48 - progress_bar.py[line:274] - INFO: epoch 001:  10927 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=88.1, ups=0.8, wpb=110.5, bsz=40, num_updates=10910, lr=4.71693e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=50252
2023-02-17 05:47:00 - progress_bar.py[line:274] - INFO: epoch 001:  10937 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.5, ups=0.87, wpb=110.6, bsz=40, num_updates=10920, lr=4.71648e-05, gnorm=0.529, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50264
2023-02-17 05:47:11 - progress_bar.py[line:274] - INFO: epoch 001:  10947 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=10930, lr=4.71603e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50275
2023-02-17 05:47:22 - progress_bar.py[line:274] - INFO: epoch 001:  10957 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=10940, lr=4.71558e-05, gnorm=0.519, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50286
2023-02-17 05:47:33 - progress_bar.py[line:274] - INFO: epoch 001:  10967 / 28910 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.88, wpb=110.7, bsz=40, num_updates=10950, lr=4.71513e-05, gnorm=0.547, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50297
2023-02-17 05:47:44 - progress_bar.py[line:274] - INFO: epoch 001:  10977 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.5, bsz=40, num_updates=10960, lr=4.71468e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50309
2023-02-17 05:47:56 - progress_bar.py[line:274] - INFO: epoch 001:  10987 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=10970, lr=4.71423e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50320
2023-02-17 05:48:07 - progress_bar.py[line:274] - INFO: epoch 001:  10997 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.91, wpb=109.7, bsz=40, num_updates=10980, lr=4.71378e-05, gnorm=0.413, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=50331
2023-02-17 05:48:18 - progress_bar.py[line:274] - INFO: epoch 001:  11007 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.9, wpb=108.6, bsz=40, num_updates=10990, lr=4.71333e-05, gnorm=0.519, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50342
2023-02-17 05:48:29 - progress_bar.py[line:274] - INFO: epoch 001:  11017 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=11000, lr=4.71288e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=50353
2023-02-17 05:48:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 05:48:30 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 05:48:30 - train.py[line:551] - INFO: load:0.93 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 05:50:32 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 05:50:32 - train.py[line:551] - INFO: load:0.95 valid_run:121.78 task_valid:118.84 collect_output:1.91
2023-02-17 05:52:32 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 05:52:32 - train.py[line:551] - INFO: load:0.98 valid_run:241.67 task_valid:234.57 collect_output:5.04
2023-02-17 05:54:34 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 05:54:34 - train.py[line:551] - INFO: load:1.00 valid_run:363.57 task_valid:350.96 collect_output:9.55
2023-02-17 05:56:36 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 05:56:36 - train.py[line:551] - INFO: load:1.02 valid_run:485.51 task_valid:464.73 collect_output:16.72
2023-02-17 05:58:36 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 05:58:36 - train.py[line:551] - INFO: load:1.05 valid_run:605.94 task_valid:581.90 collect_output:18.96
2023-02-17 06:00:39 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 06:00:39 - train.py[line:551] - INFO: load:1.07 valid_run:728.72 task_valid:700.44 collect_output:22.18
2023-02-17 06:02:42 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 06:02:42 - train.py[line:551] - INFO: load:1.10 valid_run:851.64 task_valid:818.31 collect_output:26.20
2023-02-17 06:04:44 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 06:04:44 - train.py[line:551] - INFO: load:1.12 valid_run:973.44 task_valid:934.73 collect_output:30.56
2023-02-17 06:06:47 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 06:06:47 - train.py[line:551] - INFO: load:1.15 valid_run:1097.01 task_valid:1051.82 collect_output:36.02
2023-02-17 06:08:49 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 06:08:49 - train.py[line:551] - INFO: load:1.18 valid_run:1218.57 task_valid:1164.36 collect_output:44.03
2023-02-17 06:10:49 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 06:10:49 - train.py[line:551] - INFO: load:1.20 valid_run:1338.65 task_valid:1279.82 collect_output:47.65
2023-02-17 06:12:51 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 06:12:51 - train.py[line:551] - INFO: load:1.23 valid_run:1460.25 task_valid:1396.57 collect_output:51.50
2023-02-17 06:14:50 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 06:14:50 - train.py[line:551] - INFO: load:1.25 valid_run:1579.11 task_valid:1510.19 collect_output:55.74
2023-02-17 06:16:51 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 06:16:51 - train.py[line:551] - INFO: load:1.28 valid_run:1699.88 task_valid:1627.61 collect_output:58.07
2023-02-17 06:18:52 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 06:18:52 - train.py[line:551] - INFO: load:1.30 valid_run:1820.80 task_valid:1743.54 collect_output:62.05
2023-02-17 06:20:53 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 06:20:53 - train.py[line:551] - INFO: load:1.33 valid_run:1941.85 task_valid:1857.53 collect_output:68.10
2023-02-17 06:22:54 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 06:22:54 - train.py[line:551] - INFO: load:1.35 valid_run:2063.19 task_valid:1973.52 collect_output:72.44
2023-02-17 06:24:55 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 06:24:55 - train.py[line:551] - INFO: load:1.38 valid_run:2183.67 task_valid:2091.18 collect_output:74.26
2023-02-17 06:26:56 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 06:26:56 - train.py[line:551] - INFO: load:1.40 valid_run:2304.71 task_valid:2207.89 collect_output:77.59
2023-02-17 06:28:56 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 06:28:56 - train.py[line:551] - INFO: load:1.43 valid_run:2424.87 task_valid:2324.08 collect_output:80.54
2023-02-17 06:30:57 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 06:30:57 - train.py[line:551] - INFO: load:1.45 valid_run:2546.43 task_valid:2440.27 collect_output:84.88
2023-02-17 06:32:59 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 06:32:59 - train.py[line:551] - INFO: load:1.48 valid_run:2668.24 task_valid:2558.90 collect_output:87.04
2023-02-17 06:35:00 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 06:35:00 - train.py[line:551] - INFO: load:1.50 valid_run:2788.52 task_valid:2673.12 collect_output:92.08
2023-02-17 06:36:59 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 06:36:59 - train.py[line:551] - INFO: load:1.53 valid_run:2908.12 task_valid:2788.95 collect_output:94.84
2023-02-17 06:39:01 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 06:39:01 - train.py[line:551] - INFO: load:1.55 valid_run:3029.65 task_valid:2904.96 collect_output:99.32
2023-02-17 06:41:04 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 06:41:04 - train.py[line:551] - INFO: load:1.58 valid_run:3152.51 task_valid:3020.72 collect_output:105.42
2023-02-17 06:43:03 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 06:43:03 - train.py[line:551] - INFO: load:1.60 valid_run:3271.85 task_valid:3134.44 collect_output:110.02
2023-02-17 06:45:05 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 06:45:05 - train.py[line:551] - INFO: load:1.63 valid_run:3393.44 task_valid:3253.48 collect_output:111.56
2023-02-17 06:47:07 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 06:47:07 - train.py[line:551] - INFO: load:1.65 valid_run:3515.13 task_valid:3368.86 collect_output:116.87
2023-02-17 06:49:08 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 06:49:08 - train.py[line:551] - INFO: load:1.68 valid_run:3636.75 task_valid:3486.97 collect_output:119.37
2023-02-17 06:51:09 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 06:51:09 - train.py[line:551] - INFO: load:1.71 valid_run:3757.49 task_valid:3604.95 collect_output:121.12

====================================================================================================
SGG eval:     R @ 50: 0.5637;     R @ 100: 0.6212;     R @ 500: 0.6677;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3437;    mR @ 100: 0.4008;    mR @ 500: 0.4698;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7463) (covered in:0.8750) (covering:0.2857) (eating:0.6765) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3871) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.6610) (standing on:0.3483) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5637;     R @ 100: 0.6212;     R @ 500: 0.6677;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3437;    mR @ 100: 0.4008;    mR @ 500: 0.4698;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7463) (covered in:0.8750) (covering:0.2857) (eating:0.6765) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3871) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9526) (says:0.0000) (sitting on:0.6610) (standing on:0.3483) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-02-17 06:51:40 - train.py[line:487] - INFO: 0.6212380952380951
2023-02-17 06:51:40 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 06:51:40 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.33 | loss_v1 0 | loss_v2 0 | nll_loss 0.175 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.621238 | ppl 1.13 | vqa_score 0.5495 | wps 118.4 | wpb 72 | bsz 24 | num_updates 11000 | best_R@100 0.649485
2023-02-17 06:51:40 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 11000 updates
2023-02-17 06:51:40 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_11000.pt
2023-02-17 06:51:46 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_11000.pt
2023-02-17 06:51:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_11000.pt (epoch 1 @ 11000 updates, score 0.6212380952380951) (writing took 7.869095969945192 seconds)
2023-02-17 06:51:59 - progress_bar.py[line:274] - INFO: epoch 001:  11027 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.3, ups=0, wpb=109.8, bsz=40, num_updates=11010, lr=4.71243e-05, gnorm=0.42, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54163
2023-02-17 06:52:10 - progress_bar.py[line:274] - INFO: epoch 001:  11037 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=11020, lr=4.71198e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54175
2023-02-17 06:52:22 - progress_bar.py[line:274] - INFO: epoch 001:  11047 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.3, ups=0.88, wpb=109.5, bsz=40, num_updates=11030, lr=4.71153e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54186
2023-02-17 06:52:33 - progress_bar.py[line:274] - INFO: epoch 001:  11057 / 28910 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.3, ups=0.92, wpb=109.5, bsz=40, num_updates=11040, lr=4.71108e-05, gnorm=0.571, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54197
2023-02-17 06:52:44 - progress_bar.py[line:274] - INFO: epoch 001:  11067 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=11050, lr=4.71062e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54208
2023-02-17 06:52:55 - progress_bar.py[line:274] - INFO: epoch 001:  11077 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96, ups=0.87, wpb=110.4, bsz=40, num_updates=11060, lr=4.71017e-05, gnorm=0.466, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54220
2023-02-17 06:53:07 - progress_bar.py[line:274] - INFO: epoch 001:  11087 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=11070, lr=4.70972e-05, gnorm=0.456, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54231
2023-02-17 06:53:18 - progress_bar.py[line:274] - INFO: epoch 001:  11097 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=11080, lr=4.70927e-05, gnorm=0.537, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54242
2023-02-17 06:53:29 - progress_bar.py[line:274] - INFO: epoch 001:  11107 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=11090, lr=4.70882e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54253
2023-02-17 06:53:40 - progress_bar.py[line:274] - INFO: epoch 001:  11117 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.92, wpb=110.3, bsz=40, num_updates=11100, lr=4.70837e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54264
2023-02-17 06:53:51 - progress_bar.py[line:274] - INFO: epoch 001:  11127 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.88, wpb=109.7, bsz=40, num_updates=11110, lr=4.70792e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54275
2023-02-17 06:54:02 - progress_bar.py[line:274] - INFO: epoch 001:  11137 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=11120, lr=4.70747e-05, gnorm=0.58, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54287
2023-02-17 06:54:14 - progress_bar.py[line:274] - INFO: epoch 001:  11147 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=11130, lr=4.70702e-05, gnorm=0.375, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54298
2023-02-17 06:54:25 - progress_bar.py[line:274] - INFO: epoch 001:  11157 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.92, wpb=110.2, bsz=40, num_updates=11140, lr=4.70657e-05, gnorm=0.574, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=54309
2023-02-17 06:54:36 - progress_bar.py[line:274] - INFO: epoch 001:  11167 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.9, wpb=109.7, bsz=40, num_updates=11150, lr=4.70612e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54320
2023-02-17 06:54:47 - progress_bar.py[line:274] - INFO: epoch 001:  11177 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.93, wpb=110.4, bsz=40, num_updates=11160, lr=4.70567e-05, gnorm=0.41, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54331
2023-02-17 06:54:58 - progress_bar.py[line:274] - INFO: epoch 001:  11187 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.7, ups=0.88, wpb=108.7, bsz=40, num_updates=11170, lr=4.70522e-05, gnorm=0.529, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54342
2023-02-17 06:55:09 - progress_bar.py[line:274] - INFO: epoch 001:  11197 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.9, wpb=109.3, bsz=40, num_updates=11180, lr=4.70477e-05, gnorm=0.357, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54353
2023-02-17 06:55:20 - progress_bar.py[line:274] - INFO: epoch 001:  11207 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=11190, lr=4.70432e-05, gnorm=0.598, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54364
2023-02-17 06:55:31 - progress_bar.py[line:274] - INFO: epoch 001:  11217 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=11200, lr=4.70387e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54376
2023-02-17 06:55:43 - progress_bar.py[line:274] - INFO: epoch 001:  11227 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=11210, lr=4.70342e-05, gnorm=0.452, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54387
2023-02-17 06:55:54 - progress_bar.py[line:274] - INFO: epoch 001:  11237 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=11220, lr=4.70297e-05, gnorm=0.522, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54398
2023-02-17 06:56:05 - progress_bar.py[line:274] - INFO: epoch 001:  11247 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.1, ups=0.88, wpb=109.2, bsz=40, num_updates=11230, lr=4.70252e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54409
2023-02-17 06:56:16 - progress_bar.py[line:274] - INFO: epoch 001:  11257 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=11240, lr=4.70207e-05, gnorm=0.384, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54421
2023-02-17 06:56:27 - progress_bar.py[line:274] - INFO: epoch 001:  11267 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=11250, lr=4.70162e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54432
2023-02-17 06:56:39 - progress_bar.py[line:274] - INFO: epoch 001:  11277 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.87, wpb=110.8, bsz=40, num_updates=11260, lr=4.70117e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54443
2023-02-17 06:56:50 - progress_bar.py[line:274] - INFO: epoch 001:  11287 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.89, wpb=110.6, bsz=40, num_updates=11270, lr=4.70072e-05, gnorm=0.621, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54454
2023-02-17 06:57:01 - progress_bar.py[line:274] - INFO: epoch 001:  11297 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=11280, lr=4.70027e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54466
2023-02-17 06:57:12 - progress_bar.py[line:274] - INFO: epoch 001:  11307 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.91, wpb=111.5, bsz=40, num_updates=11290, lr=4.69982e-05, gnorm=0.548, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54477
2023-02-17 06:57:24 - progress_bar.py[line:274] - INFO: epoch 001:  11317 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=11300, lr=4.69936e-05, gnorm=0.489, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54488
2023-02-17 06:57:35 - progress_bar.py[line:274] - INFO: epoch 001:  11327 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.91, wpb=109.4, bsz=40, num_updates=11310, lr=4.69891e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54499
2023-02-17 06:57:46 - progress_bar.py[line:274] - INFO: epoch 001:  11337 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.91, wpb=109.1, bsz=40, num_updates=11320, lr=4.69846e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54510
2023-02-17 06:57:57 - progress_bar.py[line:274] - INFO: epoch 001:  11347 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=11330, lr=4.69801e-05, gnorm=0.472, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54521
2023-02-17 06:58:08 - progress_bar.py[line:274] - INFO: epoch 001:  11357 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=11340, lr=4.69756e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54532
2023-02-17 06:58:19 - progress_bar.py[line:274] - INFO: epoch 001:  11367 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=11350, lr=4.69711e-05, gnorm=0.514, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54544
2023-02-17 06:58:30 - progress_bar.py[line:274] - INFO: epoch 001:  11377 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=11360, lr=4.69666e-05, gnorm=0.428, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54555
2023-02-17 06:58:42 - progress_bar.py[line:274] - INFO: epoch 001:  11387 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.89, wpb=110.1, bsz=40, num_updates=11370, lr=4.69621e-05, gnorm=0.378, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54566
2023-02-17 06:58:53 - progress_bar.py[line:274] - INFO: epoch 001:  11397 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.91, wpb=111.2, bsz=40, num_updates=11380, lr=4.69576e-05, gnorm=0.394, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54577
2023-02-17 06:59:04 - progress_bar.py[line:274] - INFO: epoch 001:  11407 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=11390, lr=4.69531e-05, gnorm=0.497, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54588
2023-02-17 06:59:15 - progress_bar.py[line:274] - INFO: epoch 001:  11417 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.92, wpb=109, bsz=40, num_updates=11400, lr=4.69486e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54599
2023-02-17 06:59:25 - progress_bar.py[line:274] - INFO: epoch 001:  11427 / 28910 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.92, wpb=109, bsz=40, num_updates=11410, lr=4.69441e-05, gnorm=0.547, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54610
2023-02-17 06:59:37 - progress_bar.py[line:274] - INFO: epoch 001:  11437 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=11420, lr=4.69396e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54621
2023-02-17 06:59:48 - progress_bar.py[line:274] - INFO: epoch 001:  11447 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=11430, lr=4.69351e-05, gnorm=0.551, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54632
2023-02-17 06:59:59 - progress_bar.py[line:274] - INFO: epoch 001:  11457 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=11440, lr=4.69306e-05, gnorm=0.483, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54643
2023-02-17 07:00:10 - progress_bar.py[line:274] - INFO: epoch 001:  11467 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.92, wpb=110.8, bsz=40, num_updates=11450, lr=4.69261e-05, gnorm=0.442, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54654
2023-02-17 07:00:11 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 07:00:22 - progress_bar.py[line:274] - INFO: epoch 001:  11478 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=92.7, ups=0.84, wpb=110.5, bsz=40, num_updates=11460, lr=4.69216e-05, gnorm=0.484, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=54666
2023-02-17 07:00:33 - progress_bar.py[line:274] - INFO: epoch 001:  11488 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=11470, lr=4.69171e-05, gnorm=0.5, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54677
2023-02-17 07:00:44 - progress_bar.py[line:274] - INFO: epoch 001:  11498 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.89, wpb=111.7, bsz=40, num_updates=11480, lr=4.69126e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54688
2023-02-17 07:00:55 - progress_bar.py[line:274] - INFO: epoch 001:  11508 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=111.6, bsz=40, num_updates=11490, lr=4.69081e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54700
2023-02-17 07:01:07 - progress_bar.py[line:274] - INFO: epoch 001:  11518 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=11500, lr=4.69036e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54711
2023-02-17 07:01:18 - progress_bar.py[line:274] - INFO: epoch 001:  11528 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.2, bsz=40, num_updates=11510, lr=4.68991e-05, gnorm=0.398, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54722
2023-02-17 07:01:29 - progress_bar.py[line:274] - INFO: epoch 001:  11538 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.88, wpb=111.4, bsz=40, num_updates=11520, lr=4.68946e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54733
2023-02-17 07:01:40 - progress_bar.py[line:274] - INFO: epoch 001:  11548 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=11530, lr=4.68901e-05, gnorm=0.612, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54745
2023-02-17 07:01:51 - progress_bar.py[line:274] - INFO: epoch 001:  11558 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.92, wpb=110.8, bsz=40, num_updates=11540, lr=4.68856e-05, gnorm=0.432, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54756
2023-02-17 07:02:02 - progress_bar.py[line:274] - INFO: epoch 001:  11568 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=11550, lr=4.68811e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=54767
2023-02-17 07:02:14 - progress_bar.py[line:274] - INFO: epoch 001:  11578 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=11560, lr=4.68765e-05, gnorm=0.398, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54778
2023-02-17 07:02:25 - progress_bar.py[line:274] - INFO: epoch 001:  11588 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.88, wpb=110.9, bsz=40, num_updates=11570, lr=4.6872e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54789
2023-02-17 07:02:36 - progress_bar.py[line:274] - INFO: epoch 001:  11598 / 28910 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.9, wpb=110.1, bsz=40, num_updates=11580, lr=4.68675e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54800
2023-02-17 07:02:47 - progress_bar.py[line:274] - INFO: epoch 001:  11608 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=11590, lr=4.6863e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54812
2023-02-17 07:02:59 - progress_bar.py[line:274] - INFO: epoch 001:  11618 / 28910 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96, ups=0.87, wpb=110, bsz=40, num_updates=11600, lr=4.68585e-05, gnorm=0.608, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54823
2023-02-17 07:03:10 - progress_bar.py[line:274] - INFO: epoch 001:  11628 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=11610, lr=4.6854e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54834
2023-02-17 07:03:21 - progress_bar.py[line:274] - INFO: epoch 001:  11638 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111, bsz=40, num_updates=11620, lr=4.68495e-05, gnorm=0.538, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54845
2023-02-17 07:03:32 - progress_bar.py[line:274] - INFO: epoch 001:  11648 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.91, wpb=110.1, bsz=40, num_updates=11630, lr=4.6845e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54856
2023-02-17 07:03:43 - progress_bar.py[line:274] - INFO: epoch 001:  11658 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=11640, lr=4.68405e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54867
2023-02-17 07:03:54 - progress_bar.py[line:274] - INFO: epoch 001:  11668 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.89, wpb=109, bsz=40, num_updates=11650, lr=4.6836e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54879
2023-02-17 07:04:06 - progress_bar.py[line:274] - INFO: epoch 001:  11678 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.7, ups=0.87, wpb=109.9, bsz=40, num_updates=11660, lr=4.68315e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54890
2023-02-17 07:04:17 - progress_bar.py[line:274] - INFO: epoch 001:  11688 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=11670, lr=4.6827e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54901
2023-02-17 07:04:28 - progress_bar.py[line:274] - INFO: epoch 001:  11698 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=11680, lr=4.68225e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54912
2023-02-17 07:04:39 - progress_bar.py[line:274] - INFO: epoch 001:  11708 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.94, wpb=109, bsz=40, num_updates=11690, lr=4.6818e-05, gnorm=0.48, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54923
2023-02-17 07:04:50 - progress_bar.py[line:274] - INFO: epoch 001:  11718 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.89, wpb=111.4, bsz=40, num_updates=11700, lr=4.68135e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54934
2023-02-17 07:05:01 - progress_bar.py[line:274] - INFO: epoch 001:  11728 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.88, wpb=110.1, bsz=40, num_updates=11710, lr=4.6809e-05, gnorm=0.521, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54945
2023-02-17 07:05:12 - progress_bar.py[line:274] - INFO: epoch 001:  11738 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=11720, lr=4.68045e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54957
2023-02-17 07:05:24 - progress_bar.py[line:274] - INFO: epoch 001:  11748 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=111.1, bsz=40, num_updates=11730, lr=4.68e-05, gnorm=0.444, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54968
2023-02-17 07:05:35 - progress_bar.py[line:274] - INFO: epoch 001:  11758 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.9, wpb=110.2, bsz=40, num_updates=11740, lr=4.67955e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54979
2023-02-17 07:05:46 - progress_bar.py[line:274] - INFO: epoch 001:  11768 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=11750, lr=4.6791e-05, gnorm=0.37, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54990
2023-02-17 07:05:57 - progress_bar.py[line:274] - INFO: epoch 001:  11778 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=110.7, bsz=40, num_updates=11760, lr=4.67865e-05, gnorm=0.468, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=55002
2023-02-17 07:06:08 - progress_bar.py[line:274] - INFO: epoch 001:  11788 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=11770, lr=4.6782e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55013
2023-02-17 07:06:19 - progress_bar.py[line:274] - INFO: epoch 001:  11798 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.92, wpb=109.1, bsz=40, num_updates=11780, lr=4.67775e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55024
2023-02-17 07:06:31 - progress_bar.py[line:274] - INFO: epoch 001:  11808 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=11790, lr=4.6773e-05, gnorm=0.38, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55035
2023-02-17 07:06:42 - progress_bar.py[line:274] - INFO: epoch 001:  11818 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=11800, lr=4.67685e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55046
2023-02-17 07:06:53 - progress_bar.py[line:274] - INFO: epoch 001:  11828 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=11810, lr=4.6764e-05, gnorm=0.54, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55057
2023-02-17 07:07:04 - progress_bar.py[line:274] - INFO: epoch 001:  11838 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=11820, lr=4.67594e-05, gnorm=0.59, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=55068
2023-02-17 07:07:15 - progress_bar.py[line:274] - INFO: epoch 001:  11848 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.4, ups=0.87, wpb=109.7, bsz=40, num_updates=11830, lr=4.67549e-05, gnorm=0.508, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55080
2023-02-17 07:07:27 - progress_bar.py[line:274] - INFO: epoch 001:  11858 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.4, bsz=40, num_updates=11840, lr=4.67504e-05, gnorm=0.379, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55091
2023-02-17 07:07:38 - progress_bar.py[line:274] - INFO: epoch 001:  11868 / 28910 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.9, wpb=110.6, bsz=40, num_updates=11850, lr=4.67459e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55102
2023-02-17 07:07:49 - progress_bar.py[line:274] - INFO: epoch 001:  11878 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.89, wpb=111.2, bsz=40, num_updates=11860, lr=4.67414e-05, gnorm=0.724, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55113
2023-02-17 07:08:01 - progress_bar.py[line:274] - INFO: epoch 001:  11888 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95, ups=0.87, wpb=109.2, bsz=40, num_updates=11870, lr=4.67369e-05, gnorm=0.485, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=55125
2023-02-17 07:08:12 - progress_bar.py[line:274] - INFO: epoch 001:  11898 / 28910 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=11880, lr=4.67324e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55136
2023-02-17 07:08:23 - progress_bar.py[line:274] - INFO: epoch 001:  11908 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.87, wpb=111.3, bsz=40, num_updates=11890, lr=4.67279e-05, gnorm=0.46, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=55147
2023-02-17 07:08:34 - progress_bar.py[line:274] - INFO: epoch 001:  11918 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.92, wpb=109.1, bsz=40, num_updates=11900, lr=4.67234e-05, gnorm=0.558, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55158
2023-02-17 07:08:45 - progress_bar.py[line:274] - INFO: epoch 001:  11928 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.2, ups=0.93, wpb=110.2, bsz=40, num_updates=11910, lr=4.67189e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=55169
2023-02-17 07:08:56 - progress_bar.py[line:274] - INFO: epoch 001:  11938 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.6, ups=0.87, wpb=110.9, bsz=40, num_updates=11920, lr=4.67144e-05, gnorm=0.456, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55181
2023-02-17 07:09:08 - progress_bar.py[line:274] - INFO: epoch 001:  11948 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=11930, lr=4.67099e-05, gnorm=0.578, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55192
2023-02-17 07:09:18 - progress_bar.py[line:274] - INFO: epoch 001:  11958 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.9, ups=0.93, wpb=110.9, bsz=40, num_updates=11940, lr=4.67054e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55203
2023-02-17 07:09:29 - progress_bar.py[line:274] - INFO: epoch 001:  11968 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.91, wpb=110, bsz=40, num_updates=11950, lr=4.67009e-05, gnorm=0.582, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55214
2023-02-17 07:09:41 - progress_bar.py[line:274] - INFO: epoch 001:  11978 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.88, wpb=111.4, bsz=40, num_updates=11960, lr=4.66964e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55225
2023-02-17 07:09:52 - progress_bar.py[line:274] - INFO: epoch 001:  11988 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=11970, lr=4.66919e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55236
2023-02-17 07:10:03 - progress_bar.py[line:274] - INFO: epoch 001:  11998 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=11980, lr=4.66874e-05, gnorm=0.642, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55247
2023-02-17 07:10:14 - progress_bar.py[line:274] - INFO: epoch 001:  12008 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.89, wpb=110.2, bsz=40, num_updates=11990, lr=4.66829e-05, gnorm=0.548, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55258
2023-02-17 07:10:26 - progress_bar.py[line:274] - INFO: epoch 001:  12018 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=12000, lr=4.66784e-05, gnorm=0.37, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55270
2023-02-17 07:10:26 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 07:10:27 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 07:10:27 - train.py[line:551] - INFO: load:1.02 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 07:12:29 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 07:12:29 - train.py[line:551] - INFO: load:1.05 valid_run:122.23 task_valid:119.29 collect_output:1.77
2023-02-17 07:14:29 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 07:14:29 - train.py[line:551] - INFO: load:1.07 valid_run:242.43 task_valid:235.58 collect_output:4.54
2023-02-17 07:16:31 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 07:16:31 - train.py[line:551] - INFO: load:1.10 valid_run:364.40 task_valid:352.19 collect_output:8.82
2023-02-17 07:18:33 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 07:18:33 - train.py[line:551] - INFO: load:1.13 valid_run:486.22 task_valid:465.77 collect_output:16.06
2023-02-17 07:20:34 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 07:20:34 - train.py[line:551] - INFO: load:1.15 valid_run:606.70 task_valid:582.99 collect_output:18.32
2023-02-17 07:22:37 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 07:22:37 - train.py[line:551] - INFO: load:1.18 valid_run:729.58 task_valid:701.68 collect_output:21.46
2023-02-17 07:24:40 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 07:24:40 - train.py[line:551] - INFO: load:1.20 valid_run:852.65 task_valid:819.84 collect_output:25.34
2023-02-17 07:26:42 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 07:26:42 - train.py[line:551] - INFO: load:1.23 valid_run:974.45 task_valid:936.22 collect_output:29.76
2023-02-17 07:28:46 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 07:28:46 - train.py[line:551] - INFO: load:1.25 valid_run:1098.20 task_valid:1053.28 collect_output:35.46
2023-02-17 07:30:47 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 07:30:47 - train.py[line:551] - INFO: load:1.28 valid_run:1219.92 task_valid:1165.76 collect_output:43.72
2023-02-17 07:32:48 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 07:32:48 - train.py[line:551] - INFO: load:1.30 valid_run:1340.01 task_valid:1281.29 collect_output:47.25
2023-02-17 07:34:49 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 07:34:49 - train.py[line:551] - INFO: load:1.33 valid_run:1461.53 task_valid:1397.95 collect_output:51.12
2023-02-17 07:36:48 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 07:36:48 - train.py[line:551] - INFO: load:1.35 valid_run:1580.28 task_valid:1511.43 collect_output:55.37
2023-02-17 07:38:49 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 07:38:49 - train.py[line:551] - INFO: load:1.38 valid_run:1701.04 task_valid:1628.83 collect_output:57.74
2023-02-17 07:40:50 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 07:40:50 - train.py[line:551] - INFO: load:1.40 valid_run:1821.82 task_valid:1744.65 collect_output:61.71
2023-02-17 07:42:51 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 07:42:51 - train.py[line:551] - INFO: load:1.43 valid_run:1942.97 task_valid:1858.52 collect_output:67.98
2023-02-17 07:44:52 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 07:44:52 - train.py[line:551] - INFO: load:1.45 valid_run:2064.17 task_valid:1974.44 collect_output:72.27
2023-02-17 07:46:53 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 07:46:53 - train.py[line:551] - INFO: load:1.48 valid_run:2184.62 task_valid:2091.99 collect_output:74.16
2023-02-17 07:48:54 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 07:48:54 - train.py[line:551] - INFO: load:1.50 valid_run:2305.58 task_valid:2208.50 collect_output:77.61
2023-02-17 07:50:54 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 07:50:54 - train.py[line:551] - INFO: load:1.53 valid_run:2425.72 task_valid:2324.70 collect_output:80.55
2023-02-17 07:52:55 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 07:52:55 - train.py[line:551] - INFO: load:1.55 valid_run:2547.33 task_valid:2440.97 collect_output:84.89
2023-02-17 07:54:57 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 07:54:57 - train.py[line:551] - INFO: load:1.58 valid_run:2669.03 task_valid:2559.45 collect_output:87.12
2023-02-17 07:56:58 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 07:56:58 - train.py[line:551] - INFO: load:1.60 valid_run:2789.39 task_valid:2673.54 collect_output:92.41
2023-02-17 07:58:57 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 07:58:57 - train.py[line:551] - INFO: load:1.63 valid_run:2908.96 task_valid:2789.36 collect_output:95.16
2023-02-17 08:00:59 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 08:00:59 - train.py[line:551] - INFO: load:1.65 valid_run:3030.36 task_valid:2905.11 collect_output:99.80
2023-02-17 08:03:02 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 08:03:02 - train.py[line:551] - INFO: load:1.68 valid_run:3153.29 task_valid:3020.94 collect_output:105.90
2023-02-17 08:05:01 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 08:05:01 - train.py[line:551] - INFO: load:1.70 valid_run:3272.67 task_valid:3134.70 collect_output:110.55
2023-02-17 08:07:03 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 08:07:03 - train.py[line:551] - INFO: load:1.73 valid_run:3394.31 task_valid:3253.83 collect_output:112.05
2023-02-17 08:09:05 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 08:09:05 - train.py[line:551] - INFO: load:1.75 valid_run:3516.01 task_valid:3369.15 collect_output:117.45
2023-02-17 08:11:06 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 08:11:06 - train.py[line:551] - INFO: load:1.78 valid_run:3637.54 task_valid:3487.17 collect_output:119.97
2023-02-17 08:13:07 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 08:13:07 - train.py[line:551] - INFO: load:1.80 valid_run:3758.15 task_valid:3604.99 collect_output:121.74

====================================================================================================
SGG eval:     R @ 50: 0.5555;     R @ 100: 0.6143;     R @ 500: 0.6654;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3381;    mR @ 100: 0.3880;    mR @ 500: 0.4679;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7463) (covered in:0.8125) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3871) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6567) (standing on:0.3383) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5555;     R @ 100: 0.6143;     R @ 500: 0.6654;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3381;    mR @ 100: 0.3880;    mR @ 500: 0.4679;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7463) (covered in:0.8125) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3871) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.6567) (standing on:0.3383) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-02-17 08:13:37 - train.py[line:487] - INFO: 0.6142523809523809
2023-02-17 08:13:37 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 08:13:37 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.339 | loss_v1 0 | loss_v2 0 | nll_loss 0.179 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.614252 | ppl 1.13 | vqa_score 0.545 | wps 118.3 | wpb 72 | bsz 24 | num_updates 12000 | best_R@100 0.649485
2023-02-17 08:13:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 12000 updates
2023-02-17 08:13:37 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_12000.pt
2023-02-17 08:13:43 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_12000.pt
2023-02-17 08:13:45 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 0.6142523809523809) (writing took 7.915331210941076 seconds)
2023-02-17 08:13:57 - progress_bar.py[line:274] - INFO: epoch 001:  12028 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=0.3, ups=0, wpb=111.2, bsz=40, num_updates=12010, lr=4.66739e-05, gnorm=0.443, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59081
2023-02-17 08:14:08 - progress_bar.py[line:274] - INFO: epoch 001:  12038 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=94.1, ups=0.86, wpb=109.5, bsz=40, num_updates=12020, lr=4.66694e-05, gnorm=0.448, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=59093
2023-02-17 08:14:19 - progress_bar.py[line:274] - INFO: epoch 001:  12048 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.92, wpb=109.1, bsz=40, num_updates=12030, lr=4.66649e-05, gnorm=0.448, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59104
2023-02-17 08:14:31 - progress_bar.py[line:274] - INFO: epoch 001:  12058 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.87, wpb=110.7, bsz=40, num_updates=12040, lr=4.66604e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59115
2023-02-17 08:14:42 - progress_bar.py[line:274] - INFO: epoch 001:  12068 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.9, wpb=109.9, bsz=40, num_updates=12050, lr=4.66559e-05, gnorm=0.527, clip=0, loss_scale=1024, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=59126
2023-02-17 08:14:53 - progress_bar.py[line:274] - INFO: epoch 001:  12078 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=12060, lr=4.66514e-05, gnorm=0.513, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59137
2023-02-17 08:15:05 - progress_bar.py[line:274] - INFO: epoch 001:  12088 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.1, ups=0.87, wpb=110.4, bsz=40, num_updates=12070, lr=4.66468e-05, gnorm=0.487, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59149
2023-02-17 08:15:16 - progress_bar.py[line:274] - INFO: epoch 001:  12098 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.91, wpb=109.7, bsz=40, num_updates=12080, lr=4.66423e-05, gnorm=0.348, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59160
2023-02-17 08:15:27 - progress_bar.py[line:274] - INFO: epoch 001:  12108 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.9, wpb=108.8, bsz=40, num_updates=12090, lr=4.66378e-05, gnorm=0.486, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59171
2023-02-17 08:15:38 - progress_bar.py[line:274] - INFO: epoch 001:  12118 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=12100, lr=4.66333e-05, gnorm=0.623, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59182
2023-02-17 08:15:49 - progress_bar.py[line:274] - INFO: epoch 001:  12128 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=12110, lr=4.66288e-05, gnorm=0.703, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59193
2023-02-17 08:16:00 - progress_bar.py[line:274] - INFO: epoch 001:  12138 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.88, wpb=110.6, bsz=40, num_updates=12120, lr=4.66243e-05, gnorm=0.479, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59204
2023-02-17 08:16:11 - progress_bar.py[line:274] - INFO: epoch 001:  12148 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=12130, lr=4.66198e-05, gnorm=0.462, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59216
2023-02-17 08:16:23 - progress_bar.py[line:274] - INFO: epoch 001:  12158 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=12140, lr=4.66153e-05, gnorm=0.433, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59227
2023-02-17 08:16:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 08:16:35 - progress_bar.py[line:274] - INFO: epoch 001:  12169 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=90, ups=0.82, wpb=110.3, bsz=40, num_updates=12150, lr=4.66108e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=59239
2023-02-17 08:16:45 - progress_bar.py[line:274] - INFO: epoch 001:  12179 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=105.5, ups=0.96, wpb=110, bsz=40, num_updates=12160, lr=4.66063e-05, gnorm=0.756, clip=20, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=59249
2023-02-17 08:16:56 - progress_bar.py[line:274] - INFO: epoch 001:  12189 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=12170, lr=4.66018e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59261
2023-02-17 08:17:08 - progress_bar.py[line:274] - INFO: epoch 001:  12199 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=12180, lr=4.65973e-05, gnorm=0.518, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59272
2023-02-17 08:17:19 - progress_bar.py[line:274] - INFO: epoch 001:  12209 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=12190, lr=4.65928e-05, gnorm=0.584, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59283
2023-02-17 08:17:30 - progress_bar.py[line:274] - INFO: epoch 001:  12219 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=12200, lr=4.65883e-05, gnorm=0.305, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59294
2023-02-17 08:17:41 - progress_bar.py[line:274] - INFO: epoch 001:  12229 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=110.8, bsz=40, num_updates=12210, lr=4.65838e-05, gnorm=0.534, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59305
2023-02-17 08:17:52 - progress_bar.py[line:274] - INFO: epoch 001:  12239 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=12220, lr=4.65793e-05, gnorm=0.51, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59317
2023-02-17 08:18:03 - progress_bar.py[line:274] - INFO: epoch 001:  12249 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.92, wpb=110.5, bsz=40, num_updates=12230, lr=4.65748e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59327
2023-02-17 08:18:14 - progress_bar.py[line:274] - INFO: epoch 001:  12259 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.92, wpb=110.3, bsz=40, num_updates=12240, lr=4.65703e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59338
2023-02-17 08:18:25 - progress_bar.py[line:274] - INFO: epoch 001:  12269 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.89, wpb=109.1, bsz=40, num_updates=12250, lr=4.65658e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59350
2023-02-17 08:18:37 - progress_bar.py[line:274] - INFO: epoch 001:  12279 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=12260, lr=4.65613e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59361
2023-02-17 08:18:48 - progress_bar.py[line:274] - INFO: epoch 001:  12289 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.2, ups=0.87, wpb=110.5, bsz=40, num_updates=12270, lr=4.65568e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=59372
2023-02-17 08:18:59 - progress_bar.py[line:274] - INFO: epoch 001:  12299 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.92, wpb=109.7, bsz=40, num_updates=12280, lr=4.65523e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59383
2023-02-17 08:19:10 - progress_bar.py[line:274] - INFO: epoch 001:  12309 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=12290, lr=4.65478e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59395
2023-02-17 08:19:21 - progress_bar.py[line:274] - INFO: epoch 001:  12319 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=111.5, bsz=40, num_updates=12300, lr=4.65433e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59406
2023-02-17 08:19:32 - progress_bar.py[line:274] - INFO: epoch 001:  12329 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=12310, lr=4.65388e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59417
2023-02-17 08:19:44 - progress_bar.py[line:274] - INFO: epoch 001:  12339 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.1, ups=0.88, wpb=109, bsz=40, num_updates=12320, lr=4.65343e-05, gnorm=0.527, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59428
2023-02-17 08:19:55 - progress_bar.py[line:274] - INFO: epoch 001:  12349 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.91, wpb=108.8, bsz=40, num_updates=12330, lr=4.65297e-05, gnorm=0.478, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=59439
2023-02-17 08:20:05 - progress_bar.py[line:274] - INFO: epoch 001:  12359 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.6, ups=0.94, wpb=111.1, bsz=40, num_updates=12340, lr=4.65252e-05, gnorm=0.602, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59450
2023-02-17 08:20:17 - progress_bar.py[line:274] - INFO: epoch 001:  12369 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=12350, lr=4.65207e-05, gnorm=0.524, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59461
2023-02-17 08:20:28 - progress_bar.py[line:274] - INFO: epoch 001:  12379 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.4, ups=0.89, wpb=107.8, bsz=40, num_updates=12360, lr=4.65162e-05, gnorm=0.639, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59472
2023-02-17 08:20:39 - progress_bar.py[line:274] - INFO: epoch 001:  12389 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=12370, lr=4.65117e-05, gnorm=0.598, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59483
2023-02-17 08:20:50 - progress_bar.py[line:274] - INFO: epoch 001:  12399 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.9, wpb=111.9, bsz=40, num_updates=12380, lr=4.65072e-05, gnorm=0.494, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59494
2023-02-17 08:21:01 - progress_bar.py[line:274] - INFO: epoch 001:  12409 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=110.4, bsz=40, num_updates=12390, lr=4.65027e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59506
2023-02-17 08:21:13 - progress_bar.py[line:274] - INFO: epoch 001:  12419 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.91, wpb=111.1, bsz=40, num_updates=12400, lr=4.64982e-05, gnorm=0.387, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59517
2023-02-17 08:21:24 - progress_bar.py[line:274] - INFO: epoch 001:  12429 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=111.6, bsz=40, num_updates=12410, lr=4.64937e-05, gnorm=0.565, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59528
2023-02-17 08:21:35 - progress_bar.py[line:274] - INFO: epoch 001:  12439 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.91, wpb=111.4, bsz=40, num_updates=12420, lr=4.64892e-05, gnorm=0.375, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59539
2023-02-17 08:21:46 - progress_bar.py[line:274] - INFO: epoch 001:  12449 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=12430, lr=4.64847e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59550
2023-02-17 08:21:57 - progress_bar.py[line:274] - INFO: epoch 001:  12459 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.92, wpb=110.8, bsz=40, num_updates=12440, lr=4.64802e-05, gnorm=0.444, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59561
2023-02-17 08:22:08 - progress_bar.py[line:274] - INFO: epoch 001:  12469 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=12450, lr=4.64757e-05, gnorm=0.48, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59572
2023-02-17 08:22:19 - progress_bar.py[line:274] - INFO: epoch 001:  12479 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=94.3, ups=0.87, wpb=108.3, bsz=40, num_updates=12460, lr=4.64712e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59584
2023-02-17 08:22:30 - progress_bar.py[line:274] - INFO: epoch 001:  12489 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=12470, lr=4.64667e-05, gnorm=0.573, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59595
2023-02-17 08:22:42 - progress_bar.py[line:274] - INFO: epoch 001:  12499 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.3, bsz=40, num_updates=12480, lr=4.64622e-05, gnorm=0.37, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59606
2023-02-17 08:22:52 - progress_bar.py[line:274] - INFO: epoch 001:  12509 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.7, ups=0.93, wpb=111.8, bsz=40, num_updates=12490, lr=4.64577e-05, gnorm=0.552, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59617
2023-02-17 08:23:04 - progress_bar.py[line:274] - INFO: epoch 001:  12519 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.9, ups=0.87, wpb=110.4, bsz=40, num_updates=12500, lr=4.64532e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59628
2023-02-17 08:23:15 - progress_bar.py[line:274] - INFO: epoch 001:  12529 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.9, wpb=109.2, bsz=40, num_updates=12510, lr=4.64487e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59639
2023-02-17 08:23:27 - progress_bar.py[line:274] - INFO: epoch 001:  12539 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.5, ups=0.87, wpb=109.7, bsz=40, num_updates=12520, lr=4.64442e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59651
2023-02-17 08:23:38 - progress_bar.py[line:274] - INFO: epoch 001:  12549 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.9, wpb=109.9, bsz=40, num_updates=12530, lr=4.64397e-05, gnorm=0.374, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59662
2023-02-17 08:23:49 - progress_bar.py[line:274] - INFO: epoch 001:  12559 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.91, wpb=109.6, bsz=40, num_updates=12540, lr=4.64352e-05, gnorm=0.347, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59673
2023-02-17 08:24:00 - progress_bar.py[line:274] - INFO: epoch 001:  12569 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.89, wpb=110.6, bsz=40, num_updates=12550, lr=4.64307e-05, gnorm=0.388, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59684
2023-02-17 08:24:11 - progress_bar.py[line:274] - INFO: epoch 001:  12579 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=12560, lr=4.64262e-05, gnorm=0.39, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59695
2023-02-17 08:24:22 - progress_bar.py[line:274] - INFO: epoch 001:  12589 / 28910 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.89, wpb=110.8, bsz=40, num_updates=12570, lr=4.64217e-05, gnorm=0.511, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59707
2023-02-17 08:24:34 - progress_bar.py[line:274] - INFO: epoch 001:  12599 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=12580, lr=4.64172e-05, gnorm=0.523, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59718
2023-02-17 08:24:45 - progress_bar.py[line:274] - INFO: epoch 001:  12609 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=12590, lr=4.64126e-05, gnorm=0.375, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59729
2023-02-17 08:24:56 - progress_bar.py[line:274] - INFO: epoch 001:  12619 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.1, ups=0.88, wpb=108.9, bsz=40, num_updates=12600, lr=4.64081e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59740
2023-02-17 08:25:07 - progress_bar.py[line:274] - INFO: epoch 001:  12629 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.89, wpb=111.4, bsz=40, num_updates=12610, lr=4.64036e-05, gnorm=0.366, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59752
2023-02-17 08:25:18 - progress_bar.py[line:274] - INFO: epoch 001:  12639 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.92, wpb=110.3, bsz=40, num_updates=12620, lr=4.63991e-05, gnorm=0.471, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59763
2023-02-17 08:25:30 - progress_bar.py[line:274] - INFO: epoch 001:  12649 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.2, bsz=40, num_updates=12630, lr=4.63946e-05, gnorm=0.478, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59774
2023-02-17 08:25:41 - progress_bar.py[line:274] - INFO: epoch 001:  12659 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=12640, lr=4.63901e-05, gnorm=0.5, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59785
2023-02-17 08:25:52 - progress_bar.py[line:274] - INFO: epoch 001:  12669 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.93, wpb=108.9, bsz=40, num_updates=12650, lr=4.63856e-05, gnorm=0.477, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59796
2023-02-17 08:26:03 - progress_bar.py[line:274] - INFO: epoch 001:  12679 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=12660, lr=4.63811e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59807
2023-02-17 08:26:14 - progress_bar.py[line:274] - INFO: epoch 001:  12689 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=12670, lr=4.63766e-05, gnorm=0.607, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59819
2023-02-17 08:26:26 - progress_bar.py[line:274] - INFO: epoch 001:  12699 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=12680, lr=4.63721e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59830
2023-02-17 08:26:37 - progress_bar.py[line:274] - INFO: epoch 001:  12709 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.92, wpb=110.8, bsz=40, num_updates=12690, lr=4.63676e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59841
2023-02-17 08:26:48 - progress_bar.py[line:274] - INFO: epoch 001:  12719 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=12700, lr=4.63631e-05, gnorm=0.507, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59852
2023-02-17 08:26:59 - progress_bar.py[line:274] - INFO: epoch 001:  12729 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.4, ups=0.88, wpb=108, bsz=40, num_updates=12710, lr=4.63586e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=59863
2023-02-17 08:27:08 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 08:27:11 - progress_bar.py[line:274] - INFO: epoch 001:  12740 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=91, ups=0.82, wpb=111, bsz=40, num_updates=12720, lr=4.63541e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=59875
2023-02-17 08:27:22 - progress_bar.py[line:274] - INFO: epoch 001:  12750 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.92, wpb=110.8, bsz=40, num_updates=12730, lr=4.63496e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59886
2023-02-17 08:27:33 - progress_bar.py[line:274] - INFO: epoch 001:  12760 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.1, ups=0.87, wpb=110, bsz=40, num_updates=12740, lr=4.63451e-05, gnorm=0.416, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59898
2023-02-17 08:27:45 - progress_bar.py[line:274] - INFO: epoch 001:  12770 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=12750, lr=4.63406e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59909
2023-02-17 08:27:55 - progress_bar.py[line:274] - INFO: epoch 001:  12780 / 28910 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103, ups=0.93, wpb=110.9, bsz=40, num_updates=12760, lr=4.63361e-05, gnorm=0.515, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59920
2023-02-17 08:28:07 - progress_bar.py[line:274] - INFO: epoch 001:  12790 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.8, ups=0.89, wpb=108.9, bsz=40, num_updates=12770, lr=4.63316e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59931
2023-02-17 08:28:18 - progress_bar.py[line:274] - INFO: epoch 001:  12800 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.92, wpb=108.8, bsz=40, num_updates=12780, lr=4.63271e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59942
2023-02-17 08:28:29 - progress_bar.py[line:274] - INFO: epoch 001:  12810 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.9, wpb=110.7, bsz=40, num_updates=12790, lr=4.63226e-05, gnorm=0.43, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59953
2023-02-17 08:28:40 - progress_bar.py[line:274] - INFO: epoch 001:  12820 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.88, wpb=110.3, bsz=40, num_updates=12800, lr=4.63181e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59964
2023-02-17 08:28:51 - progress_bar.py[line:274] - INFO: epoch 001:  12830 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=12810, lr=4.63136e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59975
2023-02-17 08:29:02 - progress_bar.py[line:274] - INFO: epoch 001:  12840 / 28910 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=12820, lr=4.63091e-05, gnorm=0.756, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59987
2023-02-17 08:29:14 - progress_bar.py[line:274] - INFO: epoch 001:  12850 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=94.8, ups=0.86, wpb=109.8, bsz=40, num_updates=12830, lr=4.63046e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=59998
2023-02-17 08:29:25 - progress_bar.py[line:274] - INFO: epoch 001:  12860 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=12840, lr=4.63e-05, gnorm=0.574, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60009
2023-02-17 08:29:36 - progress_bar.py[line:274] - INFO: epoch 001:  12870 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.88, wpb=109.2, bsz=40, num_updates=12850, lr=4.62955e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60021
2023-02-17 08:29:47 - progress_bar.py[line:274] - INFO: epoch 001:  12880 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.93, wpb=110.1, bsz=40, num_updates=12860, lr=4.6291e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=60031
2023-02-17 08:29:58 - progress_bar.py[line:274] - INFO: epoch 001:  12890 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.92, wpb=109.8, bsz=40, num_updates=12870, lr=4.62865e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60042
2023-02-17 08:30:09 - progress_bar.py[line:274] - INFO: epoch 001:  12900 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.91, wpb=111, bsz=40, num_updates=12880, lr=4.6282e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60053
2023-02-17 08:30:20 - progress_bar.py[line:274] - INFO: epoch 001:  12910 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=12890, lr=4.62775e-05, gnorm=0.687, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60065
2023-02-17 08:30:31 - progress_bar.py[line:274] - INFO: epoch 001:  12920 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.92, wpb=111.2, bsz=40, num_updates=12900, lr=4.6273e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60075
2023-02-17 08:30:43 - progress_bar.py[line:274] - INFO: epoch 001:  12930 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=12910, lr=4.62685e-05, gnorm=0.395, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=60087
2023-02-17 08:30:54 - progress_bar.py[line:274] - INFO: epoch 001:  12940 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.91, wpb=112.5, bsz=40, num_updates=12920, lr=4.6264e-05, gnorm=0.568, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60098
2023-02-17 08:31:05 - progress_bar.py[line:274] - INFO: epoch 001:  12950 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=12930, lr=4.62595e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60109
2023-02-17 08:31:16 - progress_bar.py[line:274] - INFO: epoch 001:  12960 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=12940, lr=4.6255e-05, gnorm=0.54, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60120
2023-02-17 08:31:27 - progress_bar.py[line:274] - INFO: epoch 001:  12970 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.91, wpb=112.1, bsz=40, num_updates=12950, lr=4.62505e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=60131
2023-02-17 08:31:38 - progress_bar.py[line:274] - INFO: epoch 001:  12980 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.89, wpb=111.5, bsz=40, num_updates=12960, lr=4.6246e-05, gnorm=0.572, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60142
2023-02-17 08:31:49 - progress_bar.py[line:274] - INFO: epoch 001:  12990 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=12970, lr=4.62415e-05, gnorm=0.363, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=60154
2023-02-17 08:32:00 - progress_bar.py[line:274] - INFO: epoch 001:  13000 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.9, wpb=109.6, bsz=40, num_updates=12980, lr=4.6237e-05, gnorm=0.35, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60165
2023-02-17 08:32:11 - progress_bar.py[line:274] - INFO: epoch 001:  13010 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.91, wpb=110.1, bsz=40, num_updates=12990, lr=4.62325e-05, gnorm=0.42, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=60176
2023-02-17 08:32:23 - progress_bar.py[line:274] - INFO: epoch 001:  13020 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=13000, lr=4.6228e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=60187
2023-02-17 08:32:23 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 08:32:24 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 08:32:24 - train.py[line:551] - INFO: load:1.12 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 08:34:26 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 08:34:26 - train.py[line:551] - INFO: load:1.15 valid_run:121.84 task_valid:118.78 collect_output:1.97
2023-02-17 08:36:26 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 08:36:26 - train.py[line:551] - INFO: load:1.18 valid_run:241.62 task_valid:234.33 collect_output:5.17
2023-02-17 08:38:28 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 08:38:28 - train.py[line:551] - INFO: load:1.20 valid_run:363.51 task_valid:350.72 collect_output:9.63
2023-02-17 08:40:30 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 08:40:30 - train.py[line:551] - INFO: load:1.23 valid_run:485.46 task_valid:464.33 collect_output:16.93
2023-02-17 08:42:30 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 08:42:30 - train.py[line:551] - INFO: load:1.26 valid_run:605.90 task_valid:581.54 collect_output:19.11
2023-02-17 08:44:33 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 08:44:33 - train.py[line:551] - INFO: load:1.29 valid_run:728.71 task_valid:700.05 collect_output:22.36
2023-02-17 08:46:36 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 08:46:36 - train.py[line:551] - INFO: load:1.32 valid_run:851.55 task_valid:817.91 collect_output:26.30
2023-02-17 08:48:38 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 08:48:38 - train.py[line:551] - INFO: load:1.34 valid_run:973.38 task_valid:934.41 collect_output:30.60
2023-02-17 08:50:42 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 08:50:42 - train.py[line:551] - INFO: load:1.37 valid_run:1097.08 task_valid:1051.31 collect_output:36.37
2023-02-17 08:52:43 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 08:52:43 - train.py[line:551] - INFO: load:1.40 valid_run:1218.83 task_valid:1163.83 collect_output:44.54
2023-02-17 08:54:44 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 08:54:44 - train.py[line:551] - INFO: load:1.42 valid_run:1338.91 task_valid:1279.27 collect_output:48.16
2023-02-17 08:56:45 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 08:56:45 - train.py[line:551] - INFO: load:1.45 valid_run:1460.39 task_valid:1395.92 collect_output:51.96
2023-02-17 08:58:44 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 08:58:44 - train.py[line:551] - INFO: load:1.48 valid_run:1579.33 task_valid:1509.51 collect_output:56.26
2023-02-17 09:00:45 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 09:00:45 - train.py[line:551] - INFO: load:1.51 valid_run:1700.33 task_valid:1627.03 collect_output:58.70
2023-02-17 09:02:46 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 09:02:46 - train.py[line:551] - INFO: load:1.53 valid_run:1821.36 task_valid:1742.97 collect_output:62.73
2023-02-17 09:04:48 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 09:04:48 - train.py[line:551] - INFO: load:1.56 valid_run:1942.59 task_valid:1856.84 collect_output:69.04
2023-02-17 09:06:49 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 09:06:49 - train.py[line:551] - INFO: load:1.59 valid_run:2063.94 task_valid:1972.81 collect_output:73.38
2023-02-17 09:08:49 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 09:08:49 - train.py[line:551] - INFO: load:1.62 valid_run:2184.39 task_valid:2090.33 collect_output:75.27
2023-02-17 09:10:51 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 09:10:51 - train.py[line:551] - INFO: load:1.65 valid_run:2305.60 task_valid:2207.04 collect_output:78.74
2023-02-17 09:12:51 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 09:12:51 - train.py[line:551] - INFO: load:1.67 valid_run:2425.93 task_valid:2323.28 collect_output:81.80
2023-02-17 09:14:53 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 09:14:53 - train.py[line:551] - INFO: load:1.70 valid_run:2547.58 task_valid:2439.40 collect_output:86.29
2023-02-17 09:16:55 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 09:16:55 - train.py[line:551] - INFO: load:1.73 valid_run:2669.31 task_valid:2557.87 collect_output:88.51
2023-02-17 09:18:55 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 09:18:55 - train.py[line:551] - INFO: load:1.75 valid_run:2789.51 task_valid:2671.94 collect_output:93.61
2023-02-17 09:20:55 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 09:20:55 - train.py[line:551] - INFO: load:1.78 valid_run:2909.28 task_valid:2787.85 collect_output:96.41
2023-02-17 09:22:56 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 09:22:56 - train.py[line:551] - INFO: load:1.81 valid_run:3030.87 task_valid:2903.92 collect_output:100.87
2023-02-17 09:24:59 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 09:24:59 - train.py[line:551] - INFO: load:1.84 valid_run:3153.53 task_valid:3019.76 collect_output:106.66
2023-02-17 09:26:59 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 09:26:59 - train.py[line:551] - INFO: load:1.87 valid_run:3272.97 task_valid:3133.42 collect_output:111.40
2023-02-17 09:29:00 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 09:29:00 - train.py[line:551] - INFO: load:1.89 valid_run:3394.46 task_valid:3252.32 collect_output:112.95
2023-02-17 09:31:02 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 09:31:02 - train.py[line:551] - INFO: load:1.92 valid_run:3516.04 task_valid:3367.46 collect_output:118.36
2023-02-17 09:33:03 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 09:33:03 - train.py[line:551] - INFO: load:1.95 valid_run:3637.58 task_valid:3485.41 collect_output:120.92
2023-02-17 09:35:04 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 09:35:04 - train.py[line:551] - INFO: load:1.98 valid_run:3758.14 task_valid:3603.18 collect_output:122.68

====================================================================================================
SGG eval:     R @ 50: 0.5344;     R @ 100: 0.5962;     R @ 500: 0.6447;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3301;    mR @ 100: 0.3758;    mR @ 500: 0.4547;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7220) (covered in:0.6875) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3226) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9222) (says:0.0000) (sitting on:0.6423) (standing on:0.3283) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5344;     R @ 100: 0.5962;     R @ 500: 0.6447;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3301;    mR @ 100: 0.3758;    mR @ 500: 0.4547;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7220) (covered in:0.6875) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3226) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9222) (says:0.0000) (sitting on:0.6423) (standing on:0.3283) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-02-17 09:35:35 - train.py[line:487] - INFO: 0.5961857142857143
2023-02-17 09:35:35 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 09:35:35 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.308 | loss_v1 0 | loss_v2 0 | nll_loss 0.151 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.596186 | ppl 1.11 | vqa_score 0.5417 | wps 118.3 | wpb 72 | bsz 24 | num_updates 13000 | best_R@100 0.649485
2023-02-17 09:35:35 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 13000 updates
2023-02-17 09:35:35 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_13000.pt
2023-02-17 09:35:40 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_13000.pt
2023-02-17 09:35:43 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_13000.pt (epoch 1 @ 13000 updates, score 0.5961857142857143) (writing took 8.02105069719255 seconds)
2023-02-17 09:35:54 - progress_bar.py[line:274] - INFO: epoch 001:  13030 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=0.3, ups=0, wpb=110.5, bsz=40, num_updates=13010, lr=4.62235e-05, gnorm=0.477, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=63998
2023-02-17 09:36:05 - progress_bar.py[line:274] - INFO: epoch 001:  13040 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=13020, lr=4.6219e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64009
2023-02-17 09:36:16 - progress_bar.py[line:274] - INFO: epoch 001:  13050 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.9, wpb=109.6, bsz=40, num_updates=13030, lr=4.62145e-05, gnorm=0.482, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64020
2023-02-17 09:36:28 - progress_bar.py[line:274] - INFO: epoch 001:  13060 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.2, ups=0.88, wpb=110.7, bsz=40, num_updates=13040, lr=4.621e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=64032
2023-02-17 09:36:39 - progress_bar.py[line:274] - INFO: epoch 001:  13070 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=13050, lr=4.62055e-05, gnorm=0.421, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64043
2023-02-17 09:36:50 - progress_bar.py[line:274] - INFO: epoch 001:  13080 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.5, ups=0.94, wpb=111.3, bsz=40, num_updates=13060, lr=4.6201e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64054
2023-02-17 09:37:01 - progress_bar.py[line:274] - INFO: epoch 001:  13090 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=13070, lr=4.61965e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64065
2023-02-17 09:37:12 - progress_bar.py[line:274] - INFO: epoch 001:  13100 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.6, ups=0.87, wpb=110.1, bsz=40, num_updates=13080, lr=4.6192e-05, gnorm=0.53, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64077
2023-02-17 09:37:24 - progress_bar.py[line:274] - INFO: epoch 001:  13110 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=13090, lr=4.61875e-05, gnorm=0.387, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64088
2023-02-17 09:37:35 - progress_bar.py[line:274] - INFO: epoch 001:  13120 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.3, ups=0.88, wpb=109.3, bsz=40, num_updates=13100, lr=4.61829e-05, gnorm=0.384, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64099
2023-02-17 09:37:46 - progress_bar.py[line:274] - INFO: epoch 001:  13130 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.88, wpb=110.7, bsz=40, num_updates=13110, lr=4.61784e-05, gnorm=0.38, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64111
2023-02-17 09:37:58 - progress_bar.py[line:274] - INFO: epoch 001:  13140 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.88, wpb=110.7, bsz=40, num_updates=13120, lr=4.61739e-05, gnorm=0.418, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64122
2023-02-17 09:38:09 - progress_bar.py[line:274] - INFO: epoch 001:  13150 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=13130, lr=4.61694e-05, gnorm=0.42, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=64133
2023-02-17 09:38:20 - progress_bar.py[line:274] - INFO: epoch 001:  13160 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=13140, lr=4.61649e-05, gnorm=0.489, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64145
2023-02-17 09:38:32 - progress_bar.py[line:274] - INFO: epoch 001:  13170 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.2, ups=0.87, wpb=109.5, bsz=40, num_updates=13150, lr=4.61604e-05, gnorm=0.494, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64156
2023-02-17 09:38:43 - progress_bar.py[line:274] - INFO: epoch 001:  13180 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=13160, lr=4.61559e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64167
2023-02-17 09:38:54 - progress_bar.py[line:274] - INFO: epoch 001:  13190 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=13170, lr=4.61514e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64179
2023-02-17 09:39:06 - progress_bar.py[line:274] - INFO: epoch 001:  13200 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=13180, lr=4.61469e-05, gnorm=0.485, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=64190
2023-02-17 09:39:17 - progress_bar.py[line:274] - INFO: epoch 001:  13210 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=13190, lr=4.61424e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64201
2023-02-17 09:39:28 - progress_bar.py[line:274] - INFO: epoch 001:  13220 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=13200, lr=4.61379e-05, gnorm=0.48, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64212
2023-02-17 09:39:39 - progress_bar.py[line:274] - INFO: epoch 001:  13230 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=13210, lr=4.61334e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64223
2023-02-17 09:39:50 - progress_bar.py[line:274] - INFO: epoch 001:  13240 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=111.4, bsz=40, num_updates=13220, lr=4.61289e-05, gnorm=0.51, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64235
2023-02-17 09:40:01 - progress_bar.py[line:274] - INFO: epoch 001:  13250 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=13230, lr=4.61244e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64246
2023-02-17 09:40:13 - progress_bar.py[line:274] - INFO: epoch 001:  13260 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=13240, lr=4.61199e-05, gnorm=0.732, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64257
2023-02-17 09:40:24 - progress_bar.py[line:274] - INFO: epoch 001:  13270 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.5, bsz=40, num_updates=13250, lr=4.61154e-05, gnorm=0.565, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64268
2023-02-17 09:40:35 - progress_bar.py[line:274] - INFO: epoch 001:  13280 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.4, bsz=40, num_updates=13260, lr=4.61109e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64279
2023-02-17 09:40:47 - progress_bar.py[line:274] - INFO: epoch 001:  13290 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.2, ups=0.88, wpb=110.3, bsz=40, num_updates=13270, lr=4.61064e-05, gnorm=0.367, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64291
2023-02-17 09:40:58 - progress_bar.py[line:274] - INFO: epoch 001:  13300 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=13280, lr=4.61019e-05, gnorm=0.476, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64302
2023-02-17 09:41:09 - progress_bar.py[line:274] - INFO: epoch 001:  13310 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.9, ups=0.87, wpb=110.2, bsz=40, num_updates=13290, lr=4.60974e-05, gnorm=0.46, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64314
2023-02-17 09:41:20 - progress_bar.py[line:274] - INFO: epoch 001:  13320 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=13300, lr=4.60929e-05, gnorm=0.473, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64325
2023-02-17 09:41:32 - progress_bar.py[line:274] - INFO: epoch 001:  13330 / 28910 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.87, wpb=110.6, bsz=40, num_updates=13310, lr=4.60884e-05, gnorm=0.547, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64336
2023-02-17 09:41:43 - progress_bar.py[line:274] - INFO: epoch 001:  13340 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.9, wpb=109.9, bsz=40, num_updates=13320, lr=4.60839e-05, gnorm=0.389, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64347
2023-02-17 09:41:54 - progress_bar.py[line:274] - INFO: epoch 001:  13350 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=13330, lr=4.60794e-05, gnorm=0.484, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64358
2023-02-17 09:42:05 - progress_bar.py[line:274] - INFO: epoch 001:  13360 / 28910 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.9, wpb=108.9, bsz=40, num_updates=13340, lr=4.60749e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64369
2023-02-17 09:42:16 - progress_bar.py[line:274] - INFO: epoch 001:  13370 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.92, wpb=111.4, bsz=40, num_updates=13350, lr=4.60704e-05, gnorm=0.432, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64380
2023-02-17 09:42:27 - progress_bar.py[line:274] - INFO: epoch 001:  13380 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=13360, lr=4.60658e-05, gnorm=0.424, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64392
2023-02-17 09:42:39 - progress_bar.py[line:274] - INFO: epoch 001:  13390 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=13370, lr=4.60613e-05, gnorm=0.41, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=64403
2023-02-17 09:42:49 - progress_bar.py[line:274] - INFO: epoch 001:  13400 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.92, wpb=111.7, bsz=40, num_updates=13380, lr=4.60568e-05, gnorm=0.544, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64414
2023-02-17 09:43:01 - progress_bar.py[line:274] - INFO: epoch 001:  13410 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=111.3, bsz=40, num_updates=13390, lr=4.60523e-05, gnorm=0.528, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64425
2023-02-17 09:43:12 - progress_bar.py[line:274] - INFO: epoch 001:  13420 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.5, ups=0.88, wpb=110.5, bsz=40, num_updates=13400, lr=4.60478e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64436
2023-02-17 09:43:23 - progress_bar.py[line:274] - INFO: epoch 001:  13430 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.88, wpb=109.1, bsz=40, num_updates=13410, lr=4.60433e-05, gnorm=0.542, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64447
2023-02-17 09:43:34 - progress_bar.py[line:274] - INFO: epoch 001:  13440 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.9, wpb=110.2, bsz=40, num_updates=13420, lr=4.60388e-05, gnorm=0.444, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64459
2023-02-17 09:43:45 - progress_bar.py[line:274] - INFO: epoch 001:  13450 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.91, wpb=109.3, bsz=40, num_updates=13430, lr=4.60343e-05, gnorm=0.463, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64470
2023-02-17 09:43:57 - progress_bar.py[line:274] - INFO: epoch 001:  13460 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=13440, lr=4.60298e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64481
2023-02-17 09:44:08 - progress_bar.py[line:274] - INFO: epoch 001:  13470 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.88, wpb=109.4, bsz=40, num_updates=13450, lr=4.60253e-05, gnorm=0.53, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=64492
2023-02-17 09:44:19 - progress_bar.py[line:274] - INFO: epoch 001:  13480 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.93, wpb=110.4, bsz=40, num_updates=13460, lr=4.60208e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64503
2023-02-17 09:44:30 - progress_bar.py[line:274] - INFO: epoch 001:  13490 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.89, wpb=111.1, bsz=40, num_updates=13470, lr=4.60163e-05, gnorm=0.409, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64514
2023-02-17 09:44:41 - progress_bar.py[line:274] - INFO: epoch 001:  13500 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.89, wpb=109.2, bsz=40, num_updates=13480, lr=4.60118e-05, gnorm=0.448, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=64525
2023-02-17 09:44:52 - progress_bar.py[line:274] - INFO: epoch 001:  13510 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.92, wpb=110.7, bsz=40, num_updates=13490, lr=4.60073e-05, gnorm=0.546, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64536
2023-02-17 09:45:03 - progress_bar.py[line:274] - INFO: epoch 001:  13520 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=13500, lr=4.60028e-05, gnorm=0.407, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64547
2023-02-17 09:45:12 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 09:45:15 - progress_bar.py[line:274] - INFO: epoch 001:  13531 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=91.9, ups=0.84, wpb=109.5, bsz=40, num_updates=13510, lr=4.59983e-05, gnorm=0.51, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=64559
2023-02-17 09:45:26 - progress_bar.py[line:274] - INFO: epoch 001:  13541 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.91, wpb=111.5, bsz=40, num_updates=13520, lr=4.59938e-05, gnorm=0.39, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64570
2023-02-17 09:45:37 - progress_bar.py[line:274] - INFO: epoch 001:  13551 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.91, wpb=111.1, bsz=40, num_updates=13530, lr=4.59893e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64581
2023-02-17 09:45:48 - progress_bar.py[line:274] - INFO: epoch 001:  13561 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.91, wpb=109.1, bsz=40, num_updates=13540, lr=4.59848e-05, gnorm=0.382, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=64592
2023-02-17 09:45:59 - progress_bar.py[line:274] - INFO: epoch 001:  13571 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.91, wpb=110.8, bsz=40, num_updates=13550, lr=4.59803e-05, gnorm=0.466, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64603
2023-02-17 09:46:10 - progress_bar.py[line:274] - INFO: epoch 001:  13581 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.92, wpb=109.2, bsz=40, num_updates=13560, lr=4.59758e-05, gnorm=0.46, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64614
2023-02-17 09:46:21 - progress_bar.py[line:274] - INFO: epoch 001:  13591 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.9, wpb=109.8, bsz=40, num_updates=13570, lr=4.59713e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64625
2023-02-17 09:46:32 - progress_bar.py[line:274] - INFO: epoch 001:  13601 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.6, ups=0.88, wpb=109.8, bsz=40, num_updates=13580, lr=4.59668e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64637
2023-02-17 09:46:44 - progress_bar.py[line:274] - INFO: epoch 001:  13611 / 28910 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.9, wpb=109.3, bsz=40, num_updates=13590, lr=4.59623e-05, gnorm=0.586, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64648
2023-02-17 09:46:55 - progress_bar.py[line:274] - INFO: epoch 001:  13621 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=13600, lr=4.59578e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64659
2023-02-17 09:47:06 - progress_bar.py[line:274] - INFO: epoch 001:  13631 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=13610, lr=4.59532e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=64670
2023-02-17 09:47:17 - progress_bar.py[line:274] - INFO: epoch 001:  13641 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.88, wpb=109.9, bsz=40, num_updates=13620, lr=4.59487e-05, gnorm=0.588, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64682
2023-02-17 09:47:29 - progress_bar.py[line:274] - INFO: epoch 001:  13651 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=13630, lr=4.59442e-05, gnorm=0.358, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=64693
2023-02-17 09:47:39 - progress_bar.py[line:274] - INFO: epoch 001:  13661 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.1, ups=0.93, wpb=111.1, bsz=40, num_updates=13640, lr=4.59397e-05, gnorm=0.487, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64704
2023-02-17 09:47:51 - progress_bar.py[line:274] - INFO: epoch 001:  13671 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95, ups=0.87, wpb=108.8, bsz=40, num_updates=13650, lr=4.59352e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64715
2023-02-17 09:48:02 - progress_bar.py[line:274] - INFO: epoch 001:  13681 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103, ups=0.93, wpb=111.2, bsz=40, num_updates=13660, lr=4.59307e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64726
2023-02-17 09:48:13 - progress_bar.py[line:274] - INFO: epoch 001:  13691 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.92, wpb=110.8, bsz=40, num_updates=13670, lr=4.59262e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64737
2023-02-17 09:48:24 - progress_bar.py[line:274] - INFO: epoch 001:  13701 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.5, ups=0.89, wpb=110.8, bsz=40, num_updates=13680, lr=4.59217e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=64748
2023-02-17 09:48:35 - progress_bar.py[line:274] - INFO: epoch 001:  13711 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=13690, lr=4.59172e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64759
2023-02-17 09:48:46 - progress_bar.py[line:274] - INFO: epoch 001:  13721 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.88, wpb=109.6, bsz=40, num_updates=13700, lr=4.59127e-05, gnorm=0.434, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64771
2023-02-17 09:48:58 - progress_bar.py[line:274] - INFO: epoch 001:  13731 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=110.5, bsz=40, num_updates=13710, lr=4.59082e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=64782
2023-02-17 09:49:09 - progress_bar.py[line:274] - INFO: epoch 001:  13741 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=13720, lr=4.59037e-05, gnorm=0.524, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64793
2023-02-17 09:49:20 - progress_bar.py[line:274] - INFO: epoch 001:  13751 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.91, wpb=109.9, bsz=40, num_updates=13730, lr=4.58992e-05, gnorm=0.548, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64804
2023-02-17 09:49:31 - progress_bar.py[line:274] - INFO: epoch 001:  13761 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.92, wpb=111.2, bsz=40, num_updates=13740, lr=4.58947e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64815
2023-02-17 09:49:42 - progress_bar.py[line:274] - INFO: epoch 001:  13771 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=13750, lr=4.58902e-05, gnorm=0.533, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64826
2023-02-17 09:49:53 - progress_bar.py[line:274] - INFO: epoch 001:  13781 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=13760, lr=4.58857e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64837
2023-02-17 09:50:05 - progress_bar.py[line:274] - INFO: epoch 001:  13791 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.9, ups=0.87, wpb=109.8, bsz=40, num_updates=13770, lr=4.58812e-05, gnorm=0.378, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64849
2023-02-17 09:50:16 - progress_bar.py[line:274] - INFO: epoch 001:  13801 / 28910 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=13780, lr=4.58767e-05, gnorm=0.591, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64860
2023-02-17 09:50:27 - progress_bar.py[line:274] - INFO: epoch 001:  13811 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.89, wpb=108.8, bsz=40, num_updates=13790, lr=4.58722e-05, gnorm=0.538, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64871
2023-02-17 09:50:38 - progress_bar.py[line:274] - INFO: epoch 001:  13821 / 28910 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.88, wpb=110.6, bsz=40, num_updates=13800, lr=4.58677e-05, gnorm=0.486, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64883
2023-02-17 09:50:50 - progress_bar.py[line:274] - INFO: epoch 001:  13831 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=13810, lr=4.58632e-05, gnorm=0.491, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64894
2023-02-17 09:51:01 - progress_bar.py[line:274] - INFO: epoch 001:  13841 / 28910 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.9, ups=0.87, wpb=109.9, bsz=40, num_updates=13820, lr=4.58587e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=64905
2023-02-17 09:51:12 - progress_bar.py[line:274] - INFO: epoch 001:  13851 / 28910 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=13830, lr=4.58542e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64917
2023-02-17 09:51:24 - progress_bar.py[line:274] - INFO: epoch 001:  13861 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.87, wpb=112.2, bsz=40, num_updates=13840, lr=4.58497e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64928
2023-02-17 09:51:35 - progress_bar.py[line:274] - INFO: epoch 001:  13871 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=13850, lr=4.58452e-05, gnorm=0.472, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64939
2023-02-17 09:51:46 - progress_bar.py[line:274] - INFO: epoch 001:  13881 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.92, wpb=110.6, bsz=40, num_updates=13860, lr=4.58407e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=64950
2023-02-17 09:51:57 - progress_bar.py[line:274] - INFO: epoch 001:  13891 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=13870, lr=4.58361e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64962
2023-02-17 09:52:09 - progress_bar.py[line:274] - INFO: epoch 001:  13901 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.88, wpb=111.4, bsz=40, num_updates=13880, lr=4.58316e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=64973
2023-02-17 09:52:20 - progress_bar.py[line:274] - INFO: epoch 001:  13911 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.4, ups=0.89, wpb=108.9, bsz=40, num_updates=13890, lr=4.58271e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64984
2023-02-17 09:52:31 - progress_bar.py[line:274] - INFO: epoch 001:  13921 / 28910 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.9, wpb=110, bsz=40, num_updates=13900, lr=4.58226e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64995
2023-02-17 09:52:42 - progress_bar.py[line:274] - INFO: epoch 001:  13931 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=13910, lr=4.58181e-05, gnorm=0.506, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=65007
2023-02-17 09:52:54 - progress_bar.py[line:274] - INFO: epoch 001:  13941 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.88, wpb=110.9, bsz=40, num_updates=13920, lr=4.58136e-05, gnorm=0.544, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=65018
2023-02-17 09:53:05 - progress_bar.py[line:274] - INFO: epoch 001:  13951 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.89, wpb=109, bsz=40, num_updates=13930, lr=4.58091e-05, gnorm=0.443, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=65029
2023-02-17 09:53:16 - progress_bar.py[line:274] - INFO: epoch 001:  13961 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.89, wpb=110.1, bsz=40, num_updates=13940, lr=4.58046e-05, gnorm=0.422, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=65040
2023-02-17 09:53:27 - progress_bar.py[line:274] - INFO: epoch 001:  13971 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=13950, lr=4.58001e-05, gnorm=0.605, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=65051
2023-02-17 09:53:38 - progress_bar.py[line:274] - INFO: epoch 001:  13981 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=13960, lr=4.57956e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=65063
2023-02-17 09:53:49 - progress_bar.py[line:274] - INFO: epoch 001:  13991 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=13970, lr=4.57911e-05, gnorm=0.398, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=65074
2023-02-17 09:54:01 - progress_bar.py[line:274] - INFO: epoch 001:  14001 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.91, wpb=111.5, bsz=40, num_updates=13980, lr=4.57866e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=65085
2023-02-17 09:54:11 - progress_bar.py[line:274] - INFO: epoch 001:  14011 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.92, wpb=110.7, bsz=40, num_updates=13990, lr=4.57821e-05, gnorm=0.436, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=65096
2023-02-17 09:54:23 - progress_bar.py[line:274] - INFO: epoch 001:  14021 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.88, wpb=111.1, bsz=40, num_updates=14000, lr=4.57776e-05, gnorm=0.462, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=65107
2023-02-17 09:54:23 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 09:54:24 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 09:54:24 - train.py[line:551] - INFO: load:0.87 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 09:56:26 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 09:56:26 - train.py[line:551] - INFO: load:0.90 valid_run:122.25 task_valid:119.13 collect_output:2.01
2023-02-17 09:58:27 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 09:58:27 - train.py[line:551] - INFO: load:0.93 valid_run:242.52 task_valid:234.84 collect_output:5.52
2023-02-17 10:00:29 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 10:00:29 - train.py[line:551] - INFO: load:0.96 valid_run:364.80 task_valid:351.19 collect_output:10.38
2023-02-17 10:02:31 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 10:02:31 - train.py[line:551] - INFO: load:0.99 valid_run:487.18 task_valid:464.85 collect_output:18.04
2023-02-17 10:04:32 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 10:04:32 - train.py[line:551] - INFO: load:1.01 valid_run:607.68 task_valid:581.96 collect_output:20.36
2023-02-17 10:06:35 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 10:06:35 - train.py[line:551] - INFO: load:1.04 valid_run:730.44 task_valid:700.47 collect_output:23.55
2023-02-17 10:08:38 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 10:08:38 - train.py[line:551] - INFO: load:1.07 valid_run:853.43 task_valid:818.32 collect_output:27.65
2023-02-17 10:10:40 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 10:10:40 - train.py[line:551] - INFO: load:1.10 valid_run:975.25 task_valid:934.55 collect_output:32.19
2023-02-17 10:12:43 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 10:12:43 - train.py[line:551] - INFO: load:1.12 valid_run:1099.03 task_valid:1051.46 collect_output:38.01
2023-02-17 10:14:45 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 10:14:45 - train.py[line:551] - INFO: load:1.15 valid_run:1220.73 task_valid:1164.05 collect_output:46.07
2023-02-17 10:16:46 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 10:16:46 - train.py[line:551] - INFO: load:1.18 valid_run:1341.05 task_valid:1279.60 collect_output:49.82
2023-02-17 10:18:47 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 10:18:47 - train.py[line:551] - INFO: load:1.21 valid_run:1462.54 task_valid:1396.27 collect_output:53.60
2023-02-17 10:20:46 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 10:20:46 - train.py[line:551] - INFO: load:1.24 valid_run:1581.56 task_valid:1509.98 collect_output:57.87
2023-02-17 10:22:47 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 10:22:47 - train.py[line:551] - INFO: load:1.26 valid_run:1702.64 task_valid:1627.60 collect_output:60.29
2023-02-17 10:24:49 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 10:24:49 - train.py[line:551] - INFO: load:1.29 valid_run:1823.79 task_valid:1743.65 collect_output:64.35
2023-02-17 10:26:50 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 10:26:50 - train.py[line:551] - INFO: load:1.32 valid_run:1944.97 task_valid:1857.56 collect_output:70.59
2023-02-17 10:28:51 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 10:28:51 - train.py[line:551] - INFO: load:1.35 valid_run:2066.28 task_valid:1973.49 collect_output:74.93
2023-02-17 10:30:52 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 10:30:52 - train.py[line:551] - INFO: load:1.38 valid_run:2186.83 task_valid:2091.14 collect_output:76.79
2023-02-17 10:32:53 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 10:32:53 - train.py[line:551] - INFO: load:1.40 valid_run:2308.00 task_valid:2207.81 collect_output:80.28
2023-02-17 10:34:53 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 10:34:53 - train.py[line:551] - INFO: load:1.43 valid_run:2428.20 task_valid:2324.11 collect_output:83.16
2023-02-17 10:36:55 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 10:36:55 - train.py[line:551] - INFO: load:1.46 valid_run:2549.97 task_valid:2440.47 collect_output:87.54
2023-02-17 10:38:57 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 10:38:57 - train.py[line:551] - INFO: load:1.49 valid_run:2671.86 task_valid:2558.95 collect_output:89.92
2023-02-17 10:40:58 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 10:40:58 - train.py[line:551] - INFO: load:1.51 valid_run:2792.42 task_valid:2673.14 collect_output:95.26
2023-02-17 10:42:58 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 10:42:58 - train.py[line:551] - INFO: load:1.54 valid_run:2912.35 task_valid:2789.23 collect_output:98.08
2023-02-17 10:44:59 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 10:44:59 - train.py[line:551] - INFO: load:1.57 valid_run:3034.00 task_valid:2905.15 collect_output:102.78
2023-02-17 10:47:02 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 10:47:02 - train.py[line:551] - INFO: load:1.59 valid_run:3156.93 task_valid:3020.86 collect_output:108.98
2023-02-17 10:49:02 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 10:49:02 - train.py[line:551] - INFO: load:1.62 valid_run:3276.35 task_valid:3134.56 collect_output:113.69
2023-02-17 10:51:04 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 10:51:04 - train.py[line:551] - INFO: load:1.65 valid_run:3397.98 task_valid:3253.55 collect_output:115.29
2023-02-17 10:53:06 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 10:53:06 - train.py[line:551] - INFO: load:1.67 valid_run:3519.95 task_valid:3368.98 collect_output:120.83
2023-02-17 10:55:07 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 10:55:07 - train.py[line:551] - INFO: load:1.70 valid_run:3641.68 task_valid:3487.19 collect_output:123.30
2023-02-17 10:57:08 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 10:57:08 - train.py[line:551] - INFO: load:1.73 valid_run:3762.43 task_valid:3605.11 collect_output:125.11

====================================================================================================
SGG eval:     R @ 50: 0.5312;     R @ 100: 0.5974;     R @ 500: 0.6482;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3274;    mR @ 100: 0.3779;    mR @ 500: 0.4559;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7293) (covered in:0.6875) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9212) (says:0.0000) (sitting on:0.6372) (standing on:0.3383) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-02-17 10:57:39 - train.py[line:487] - INFO: 0.5973523809523809

====================================================================================================
SGG eval:     R @ 50: 0.5312;     R @ 100: 0.5974;     R @ 500: 0.6482;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3274;    mR @ 100: 0.3779;    mR @ 500: 0.4559;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7293) (covered in:0.6875) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9212) (says:0.0000) (sitting on:0.6372) (standing on:0.3383) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-02-17 10:57:39 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 10:57:39 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.311 | loss_v1 0 | loss_v2 0 | nll_loss 0.148 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.597352 | ppl 1.11 | vqa_score 0.5495 | wps 118.2 | wpb 72 | bsz 24 | num_updates 14000 | best_R@100 0.649485
2023-02-17 10:57:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 14000 updates
2023-02-17 10:57:39 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_14000.pt
2023-02-17 10:57:44 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_14000.pt
2023-02-17 10:57:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 0.5973523809523809) (writing took 7.864734373986721 seconds)
2023-02-17 10:57:58 - progress_bar.py[line:274] - INFO: epoch 001:  14031 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=0.3, ups=0, wpb=110.9, bsz=40, num_updates=14010, lr=4.57731e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=68922
2023-02-17 10:58:09 - progress_bar.py[line:274] - INFO: epoch 001:  14041 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.89, wpb=108.6, bsz=40, num_updates=14020, lr=4.57686e-05, gnorm=0.481, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=68934
2023-02-17 10:58:21 - progress_bar.py[line:274] - INFO: epoch 001:  14051 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=14030, lr=4.57641e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=68945
2023-02-17 10:58:31 - progress_bar.py[line:274] - INFO: epoch 001:  14061 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=106.5, ups=0.95, wpb=112.4, bsz=40, num_updates=14040, lr=4.57596e-05, gnorm=0.428, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=68956
2023-02-17 10:58:43 - progress_bar.py[line:274] - INFO: epoch 001:  14071 / 28910 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.7, ups=0.87, wpb=110.1, bsz=40, num_updates=14050, lr=4.57551e-05, gnorm=0.56, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=68967
2023-02-17 10:58:54 - progress_bar.py[line:274] - INFO: epoch 001:  14081 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=110.4, bsz=40, num_updates=14060, lr=4.57506e-05, gnorm=0.43, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=68978
2023-02-17 10:59:05 - progress_bar.py[line:274] - INFO: epoch 001:  14091 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=14070, lr=4.57461e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=68989
2023-02-17 10:59:17 - progress_bar.py[line:274] - INFO: epoch 001:  14101 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=14080, lr=4.57416e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=69001
2023-02-17 10:59:28 - progress_bar.py[line:274] - INFO: epoch 001:  14111 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95, ups=0.87, wpb=109.4, bsz=40, num_updates=14090, lr=4.57371e-05, gnorm=0.488, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=69012
2023-02-17 10:59:39 - progress_bar.py[line:274] - INFO: epoch 001:  14121 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=14100, lr=4.57326e-05, gnorm=0.385, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69024
2023-02-17 10:59:50 - progress_bar.py[line:274] - INFO: epoch 001:  14131 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=14110, lr=4.57281e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69035
2023-02-17 11:00:02 - progress_bar.py[line:274] - INFO: epoch 001:  14141 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=14120, lr=4.57236e-05, gnorm=0.417, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69046
2023-02-17 11:00:13 - progress_bar.py[line:274] - INFO: epoch 001:  14151 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.5, bsz=40, num_updates=14130, lr=4.5719e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69057
2023-02-17 11:00:24 - progress_bar.py[line:274] - INFO: epoch 001:  14161 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=111.3, bsz=40, num_updates=14140, lr=4.57145e-05, gnorm=0.438, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=69068
2023-02-17 11:00:35 - progress_bar.py[line:274] - INFO: epoch 001:  14171 / 28910 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.9, wpb=110.4, bsz=40, num_updates=14150, lr=4.571e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=69079
2023-02-17 11:00:46 - progress_bar.py[line:274] - INFO: epoch 001:  14181 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=14160, lr=4.57055e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69090
2023-02-17 11:00:57 - progress_bar.py[line:274] - INFO: epoch 001:  14191 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.89, wpb=110, bsz=40, num_updates=14170, lr=4.5701e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69102
2023-02-17 11:01:09 - progress_bar.py[line:274] - INFO: epoch 001:  14201 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=111.4, bsz=40, num_updates=14180, lr=4.56965e-05, gnorm=0.499, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69113
2023-02-17 11:01:19 - progress_bar.py[line:274] - INFO: epoch 001:  14211 / 28910 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.93, wpb=110.2, bsz=40, num_updates=14190, lr=4.5692e-05, gnorm=0.488, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=69124
2023-02-17 11:01:30 - progress_bar.py[line:274] - INFO: epoch 001:  14221 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.2, ups=0.93, wpb=110.1, bsz=40, num_updates=14200, lr=4.56875e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=69134
2023-02-17 11:01:41 - progress_bar.py[line:274] - INFO: epoch 001:  14231 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=14210, lr=4.5683e-05, gnorm=0.423, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69146
2023-02-17 11:01:53 - progress_bar.py[line:274] - INFO: epoch 001:  14241 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.88, wpb=109.7, bsz=40, num_updates=14220, lr=4.56785e-05, gnorm=0.356, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69157
2023-02-17 11:02:04 - progress_bar.py[line:274] - INFO: epoch 001:  14251 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=14230, lr=4.5674e-05, gnorm=0.433, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69168
2023-02-17 11:02:15 - progress_bar.py[line:274] - INFO: epoch 001:  14261 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=110.6, bsz=40, num_updates=14240, lr=4.56695e-05, gnorm=0.545, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69179
2023-02-17 11:02:26 - progress_bar.py[line:274] - INFO: epoch 001:  14271 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=14250, lr=4.5665e-05, gnorm=0.37, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69191
2023-02-17 11:02:37 - progress_bar.py[line:274] - INFO: epoch 001:  14281 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.9, wpb=109.6, bsz=40, num_updates=14260, lr=4.56605e-05, gnorm=0.437, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69202
2023-02-17 11:02:48 - progress_bar.py[line:274] - INFO: epoch 001:  14291 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.9, wpb=109.4, bsz=40, num_updates=14270, lr=4.5656e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69213
2023-02-17 11:03:00 - progress_bar.py[line:274] - INFO: epoch 001:  14301 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.3, ups=0.87, wpb=110.7, bsz=40, num_updates=14280, lr=4.56515e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=69224
2023-02-17 11:03:11 - progress_bar.py[line:274] - INFO: epoch 001:  14311 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=14290, lr=4.5647e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69235
2023-02-17 11:03:23 - progress_bar.py[line:274] - INFO: epoch 001:  14321 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.3, ups=0.87, wpb=110.7, bsz=40, num_updates=14300, lr=4.56425e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69247
2023-02-17 11:03:33 - progress_bar.py[line:274] - INFO: epoch 001:  14331 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.2, ups=0.94, wpb=111.6, bsz=40, num_updates=14310, lr=4.5638e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69258
2023-02-17 11:03:45 - progress_bar.py[line:274] - INFO: epoch 001:  14341 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=14320, lr=4.56335e-05, gnorm=0.533, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69269
2023-02-17 11:03:56 - progress_bar.py[line:274] - INFO: epoch 001:  14351 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.92, wpb=110.4, bsz=40, num_updates=14330, lr=4.5629e-05, gnorm=0.548, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69280
2023-02-17 11:04:07 - progress_bar.py[line:274] - INFO: epoch 001:  14361 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.8, ups=0.87, wpb=109.9, bsz=40, num_updates=14340, lr=4.56245e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69291
2023-02-17 11:04:18 - progress_bar.py[line:274] - INFO: epoch 001:  14371 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.93, wpb=111, bsz=40, num_updates=14350, lr=4.562e-05, gnorm=0.473, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69302
2023-02-17 11:04:29 - progress_bar.py[line:274] - INFO: epoch 001:  14381 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=14360, lr=4.56155e-05, gnorm=0.47, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69313
2023-02-17 11:04:40 - progress_bar.py[line:274] - INFO: epoch 001:  14391 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.9, wpb=110.6, bsz=40, num_updates=14370, lr=4.5611e-05, gnorm=0.476, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69325
2023-02-17 11:04:51 - progress_bar.py[line:274] - INFO: epoch 001:  14401 / 28910 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=14380, lr=4.56064e-05, gnorm=0.377, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69336
2023-02-17 11:05:02 - progress_bar.py[line:274] - INFO: epoch 001:  14411 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=110.6, bsz=40, num_updates=14390, lr=4.56019e-05, gnorm=0.559, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=69347
2023-02-17 11:05:14 - progress_bar.py[line:274] - INFO: epoch 001:  14421 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=14400, lr=4.55974e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69358
2023-02-17 11:05:25 - progress_bar.py[line:274] - INFO: epoch 001:  14431 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.5, ups=0.87, wpb=110.8, bsz=40, num_updates=14410, lr=4.55929e-05, gnorm=0.554, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69369
2023-02-17 11:05:37 - progress_bar.py[line:274] - INFO: epoch 001:  14441 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=14420, lr=4.55884e-05, gnorm=0.515, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69381
2023-02-17 11:05:40 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 11:05:49 - progress_bar.py[line:274] - INFO: epoch 001:  14452 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91, ups=0.83, wpb=110, bsz=40, num_updates=14430, lr=4.55839e-05, gnorm=0.531, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=69393
2023-02-17 11:05:59 - progress_bar.py[line:274] - INFO: epoch 001:  14462 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.1, ups=0.94, wpb=110.2, bsz=40, num_updates=14440, lr=4.55794e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69403
2023-02-17 11:06:11 - progress_bar.py[line:274] - INFO: epoch 001:  14472 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.5, ups=0.88, wpb=109.7, bsz=40, num_updates=14450, lr=4.55749e-05, gnorm=0.501, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69415
2023-02-17 11:06:22 - progress_bar.py[line:274] - INFO: epoch 001:  14482 / 28910 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.89, wpb=109.7, bsz=40, num_updates=14460, lr=4.55704e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69426
2023-02-17 11:06:33 - progress_bar.py[line:274] - INFO: epoch 001:  14492 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=14470, lr=4.55659e-05, gnorm=0.39, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69437
2023-02-17 11:06:44 - progress_bar.py[line:274] - INFO: epoch 001:  14502 / 28910 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.89, wpb=109.4, bsz=40, num_updates=14480, lr=4.55614e-05, gnorm=0.433, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69448
2023-02-17 11:06:55 - progress_bar.py[line:274] - INFO: epoch 001:  14512 / 28910 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=14490, lr=4.55569e-05, gnorm=0.519, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=69459
2023-02-17 11:07:06 - progress_bar.py[line:274] - INFO: epoch 001:  14522 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.88, wpb=110.5, bsz=40, num_updates=14500, lr=4.55524e-05, gnorm=0.43, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69471
2023-02-17 11:07:17 - progress_bar.py[line:274] - INFO: epoch 001:  14532 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.91, wpb=109.8, bsz=40, num_updates=14510, lr=4.55479e-05, gnorm=0.361, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69482
2023-02-17 11:07:29 - progress_bar.py[line:274] - INFO: epoch 001:  14542 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.2, ups=0.88, wpb=110.2, bsz=40, num_updates=14520, lr=4.55434e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69493
2023-02-17 11:07:40 - progress_bar.py[line:274] - INFO: epoch 001:  14552 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.92, wpb=110.2, bsz=40, num_updates=14530, lr=4.55389e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69504
2023-02-17 11:07:51 - progress_bar.py[line:274] - INFO: epoch 001:  14562 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.7, ups=0.87, wpb=109.9, bsz=40, num_updates=14540, lr=4.55344e-05, gnorm=0.522, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69515
2023-02-17 11:08:03 - progress_bar.py[line:274] - INFO: epoch 001:  14572 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.7, ups=0.88, wpb=108.6, bsz=40, num_updates=14550, lr=4.55299e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69527
2023-02-17 11:08:14 - progress_bar.py[line:274] - INFO: epoch 001:  14582 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.88, wpb=110, bsz=40, num_updates=14560, lr=4.55254e-05, gnorm=0.472, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69538
2023-02-17 11:08:25 - progress_bar.py[line:274] - INFO: epoch 001:  14592 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.9, wpb=109, bsz=40, num_updates=14570, lr=4.55209e-05, gnorm=0.534, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69549
2023-02-17 11:08:36 - progress_bar.py[line:274] - INFO: epoch 001:  14602 / 28910 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=14580, lr=4.55164e-05, gnorm=0.6, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69560
2023-02-17 11:08:47 - progress_bar.py[line:274] - INFO: epoch 001:  14612 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.92, wpb=110.6, bsz=40, num_updates=14590, lr=4.55119e-05, gnorm=0.51, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69571
2023-02-17 11:08:58 - progress_bar.py[line:274] - INFO: epoch 001:  14622 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.89, wpb=109.7, bsz=40, num_updates=14600, lr=4.55074e-05, gnorm=0.489, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69582
2023-02-17 11:09:09 - progress_bar.py[line:274] - INFO: epoch 001:  14632 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=14610, lr=4.55029e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69594
2023-02-17 11:09:21 - progress_bar.py[line:274] - INFO: epoch 001:  14642 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.3, ups=0.87, wpb=110.3, bsz=40, num_updates=14620, lr=4.54984e-05, gnorm=0.444, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69605
2023-02-17 11:09:32 - progress_bar.py[line:274] - INFO: epoch 001:  14652 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.93, wpb=109, bsz=40, num_updates=14630, lr=4.54939e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69616
2023-02-17 11:09:43 - progress_bar.py[line:274] - INFO: epoch 001:  14662 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=14640, lr=4.54893e-05, gnorm=0.489, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69627
2023-02-17 11:09:54 - progress_bar.py[line:274] - INFO: epoch 001:  14672 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=14650, lr=4.54848e-05, gnorm=0.428, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69638
2023-02-17 11:10:05 - progress_bar.py[line:274] - INFO: epoch 001:  14682 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.1, bsz=40, num_updates=14660, lr=4.54803e-05, gnorm=0.52, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69649
2023-02-17 11:10:16 - progress_bar.py[line:274] - INFO: epoch 001:  14692 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.92, wpb=109.1, bsz=40, num_updates=14670, lr=4.54758e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69660
2023-02-17 11:10:27 - progress_bar.py[line:274] - INFO: epoch 001:  14702 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.1, ups=0.87, wpb=109.4, bsz=40, num_updates=14680, lr=4.54713e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69672
2023-02-17 11:10:38 - progress_bar.py[line:274] - INFO: epoch 001:  14712 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111.6, bsz=40, num_updates=14690, lr=4.54668e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69683
2023-02-17 11:10:49 - progress_bar.py[line:274] - INFO: epoch 001:  14722 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.9, wpb=110.6, bsz=40, num_updates=14700, lr=4.54623e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69694
2023-02-17 11:11:01 - progress_bar.py[line:274] - INFO: epoch 001:  14732 / 28910 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=14710, lr=4.54578e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69705
2023-02-17 11:11:12 - progress_bar.py[line:274] - INFO: epoch 001:  14742 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.89, wpb=109.6, bsz=40, num_updates=14720, lr=4.54533e-05, gnorm=0.418, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69716
2023-02-17 11:11:23 - progress_bar.py[line:274] - INFO: epoch 001:  14752 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=94.5, ups=0.87, wpb=108.3, bsz=40, num_updates=14730, lr=4.54488e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69728
2023-02-17 11:11:34 - progress_bar.py[line:274] - INFO: epoch 001:  14762 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.9, wpb=108.9, bsz=40, num_updates=14740, lr=4.54443e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=69739
2023-02-17 11:11:46 - progress_bar.py[line:274] - INFO: epoch 001:  14772 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.9, wpb=110.5, bsz=40, num_updates=14750, lr=4.54398e-05, gnorm=0.429, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69750
2023-02-17 11:11:56 - progress_bar.py[line:274] - INFO: epoch 001:  14782 / 28910 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.93, wpb=110, bsz=40, num_updates=14760, lr=4.54353e-05, gnorm=0.35, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69761
2023-02-17 11:12:07 - progress_bar.py[line:274] - INFO: epoch 001:  14792 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.93, wpb=110, bsz=40, num_updates=14770, lr=4.54308e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69771
2023-02-17 11:12:18 - progress_bar.py[line:274] - INFO: epoch 001:  14802 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.88, wpb=109, bsz=40, num_updates=14780, lr=4.54263e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69783
2023-02-17 11:12:29 - progress_bar.py[line:274] - INFO: epoch 001:  14812 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=110.3, bsz=40, num_updates=14790, lr=4.54218e-05, gnorm=0.544, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69794
2023-02-17 11:12:41 - progress_bar.py[line:274] - INFO: epoch 001:  14822 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=14800, lr=4.54173e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69805
2023-02-17 11:12:52 - progress_bar.py[line:274] - INFO: epoch 001:  14832 / 28910 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.88, wpb=110.2, bsz=40, num_updates=14810, lr=4.54128e-05, gnorm=0.444, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69816
2023-02-17 11:13:03 - progress_bar.py[line:274] - INFO: epoch 001:  14842 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=14820, lr=4.54083e-05, gnorm=0.426, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69827
2023-02-17 11:13:14 - progress_bar.py[line:274] - INFO: epoch 001:  14852 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.91, wpb=110.5, bsz=40, num_updates=14830, lr=4.54038e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69838
2023-02-17 11:13:25 - progress_bar.py[line:274] - INFO: epoch 001:  14862 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.5, bsz=40, num_updates=14840, lr=4.53993e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69849
2023-02-17 11:13:36 - progress_bar.py[line:274] - INFO: epoch 001:  14872 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=14850, lr=4.53948e-05, gnorm=0.383, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69861
2023-02-17 11:13:48 - progress_bar.py[line:274] - INFO: epoch 001:  14882 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=14860, lr=4.53903e-05, gnorm=0.377, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69872
2023-02-17 11:13:58 - progress_bar.py[line:274] - INFO: epoch 001:  14892 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.93, wpb=109.8, bsz=40, num_updates=14870, lr=4.53858e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69883
2023-02-17 11:14:10 - progress_bar.py[line:274] - INFO: epoch 001:  14902 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=111.1, bsz=40, num_updates=14880, lr=4.53813e-05, gnorm=0.505, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=69894
2023-02-17 11:14:21 - progress_bar.py[line:274] - INFO: epoch 001:  14912 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.9, ups=0.87, wpb=110.3, bsz=40, num_updates=14890, lr=4.53768e-05, gnorm=0.536, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69906
2023-02-17 11:14:33 - progress_bar.py[line:274] - INFO: epoch 001:  14922 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=14900, lr=4.53722e-05, gnorm=0.556, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69917
2023-02-17 11:14:44 - progress_bar.py[line:274] - INFO: epoch 001:  14932 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=14910, lr=4.53677e-05, gnorm=0.5, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69928
2023-02-17 11:14:55 - progress_bar.py[line:274] - INFO: epoch 001:  14942 / 28910 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.91, wpb=109.7, bsz=40, num_updates=14920, lr=4.53632e-05, gnorm=0.462, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69939
2023-02-17 11:15:06 - progress_bar.py[line:274] - INFO: epoch 001:  14952 / 28910 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=14930, lr=4.53587e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69950
2023-02-17 11:15:18 - progress_bar.py[line:274] - INFO: epoch 001:  14962 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=14940, lr=4.53542e-05, gnorm=0.555, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69962
2023-02-17 11:15:29 - progress_bar.py[line:274] - INFO: epoch 001:  14972 / 28910 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.6, ups=0.89, wpb=109.1, bsz=40, num_updates=14950, lr=4.53497e-05, gnorm=0.557, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69973
2023-02-17 11:15:40 - progress_bar.py[line:274] - INFO: epoch 001:  14982 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95, ups=0.88, wpb=107.9, bsz=40, num_updates=14960, lr=4.53452e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69984
2023-02-17 11:15:52 - progress_bar.py[line:274] - INFO: epoch 001:  14992 / 28910 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=14970, lr=4.53407e-05, gnorm=0.44, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69996
2023-02-17 11:16:03 - progress_bar.py[line:274] - INFO: epoch 001:  15002 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.91, wpb=109.1, bsz=40, num_updates=14980, lr=4.53362e-05, gnorm=0.516, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=70007
2023-02-17 11:16:14 - progress_bar.py[line:274] - INFO: epoch 001:  15012 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=14990, lr=4.53317e-05, gnorm=0.429, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=70018
2023-02-17 11:16:25 - progress_bar.py[line:274] - INFO: epoch 001:  15022 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.91, wpb=109, bsz=40, num_updates=15000, lr=4.53272e-05, gnorm=0.689, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=70029
2023-02-17 11:16:25 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 11:16:26 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 11:16:26 - train.py[line:551] - INFO: load:0.91 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 11:18:28 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 11:18:28 - train.py[line:551] - INFO: load:0.94 valid_run:121.86 task_valid:119.06 collect_output:1.74
2023-02-17 11:20:28 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 11:20:28 - train.py[line:551] - INFO: load:0.97 valid_run:241.79 task_valid:234.92 collect_output:4.76
2023-02-17 11:22:30 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 11:22:30 - train.py[line:551] - INFO: load:0.99 valid_run:363.69 task_valid:351.34 collect_output:9.21
2023-02-17 11:24:32 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 11:24:32 - train.py[line:551] - INFO: load:1.02 valid_run:485.64 task_valid:465.14 collect_output:16.34
2023-02-17 11:26:32 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 11:26:32 - train.py[line:551] - INFO: load:1.04 valid_run:605.96 task_valid:582.23 collect_output:18.54
2023-02-17 11:28:35 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 11:28:35 - train.py[line:551] - INFO: load:1.07 valid_run:728.75 task_valid:700.64 collect_output:21.91
2023-02-17 11:30:38 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 11:30:38 - train.py[line:551] - INFO: load:1.10 valid_run:851.63 task_valid:818.51 collect_output:25.89
2023-02-17 11:32:40 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 11:32:40 - train.py[line:551] - INFO: load:1.12 valid_run:973.31 task_valid:934.84 collect_output:30.20
2023-02-17 11:34:44 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 11:34:44 - train.py[line:551] - INFO: load:1.15 valid_run:1096.98 task_valid:1051.93 collect_output:35.74
2023-02-17 11:36:45 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 11:36:45 - train.py[line:551] - INFO: load:1.18 valid_run:1218.71 task_valid:1164.61 collect_output:43.74
2023-02-17 11:38:46 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 11:38:46 - train.py[line:551] - INFO: load:1.20 valid_run:1338.88 task_valid:1280.22 collect_output:47.27
2023-02-17 11:40:47 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 11:40:47 - train.py[line:551] - INFO: load:1.23 valid_run:1460.39 task_valid:1397.03 collect_output:50.94
2023-02-17 11:42:46 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 11:42:46 - train.py[line:551] - INFO: load:1.26 valid_run:1579.25 task_valid:1510.67 collect_output:55.13
2023-02-17 11:44:47 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 11:44:47 - train.py[line:551] - INFO: load:1.28 valid_run:1700.13 task_valid:1628.25 collect_output:57.40
2023-02-17 11:46:48 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 11:46:48 - train.py[line:551] - INFO: load:1.31 valid_run:1821.01 task_valid:1744.32 collect_output:61.18
2023-02-17 11:48:49 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 11:48:49 - train.py[line:551] - INFO: load:1.34 valid_run:1942.14 task_valid:1858.05 collect_output:67.52
2023-02-17 11:50:51 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 11:50:51 - train.py[line:551] - INFO: load:1.36 valid_run:2063.44 task_valid:1973.92 collect_output:71.90
2023-02-17 11:52:51 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 11:52:51 - train.py[line:551] - INFO: load:1.39 valid_run:2183.84 task_valid:2091.44 collect_output:73.74
2023-02-17 11:54:52 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 11:54:52 - train.py[line:551] - INFO: load:1.42 valid_run:2304.99 task_valid:2208.31 collect_output:76.97
2023-02-17 11:56:53 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 11:56:53 - train.py[line:551] - INFO: load:1.45 valid_run:2425.29 task_valid:2324.67 collect_output:79.86
2023-02-17 11:58:54 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 11:58:54 - train.py[line:551] - INFO: load:1.47 valid_run:2547.02 task_valid:2441.14 collect_output:84.10
2023-02-17 12:00:56 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 12:00:56 - train.py[line:551] - INFO: load:1.50 valid_run:2668.90 task_valid:2559.78 collect_output:86.30
2023-02-17 12:02:57 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 12:02:57 - train.py[line:551] - INFO: load:1.53 valid_run:2789.22 task_valid:2674.04 collect_output:91.33
2023-02-17 12:04:57 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 12:04:57 - train.py[line:551] - INFO: load:1.55 valid_run:2908.99 task_valid:2790.07 collect_output:94.04
2023-02-17 12:06:58 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 12:06:58 - train.py[line:551] - INFO: load:1.58 valid_run:3030.56 task_valid:2906.16 collect_output:98.50
2023-02-17 12:09:01 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 12:09:01 - train.py[line:551] - INFO: load:1.61 valid_run:3153.36 task_valid:3021.97 collect_output:104.45
2023-02-17 12:11:01 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 12:11:01 - train.py[line:551] - INFO: load:1.64 valid_run:3272.75 task_valid:3135.82 collect_output:108.96
2023-02-17 12:13:02 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 12:13:02 - train.py[line:551] - INFO: load:1.66 valid_run:3394.49 task_valid:3254.95 collect_output:110.54
2023-02-17 12:15:04 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 12:15:04 - train.py[line:551] - INFO: load:1.69 valid_run:3516.09 task_valid:3370.37 collect_output:115.70
2023-02-17 12:17:06 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 12:17:06 - train.py[line:551] - INFO: load:1.72 valid_run:3637.75 task_valid:3488.53 collect_output:118.17
2023-02-17 12:19:07 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 12:19:07 - train.py[line:551] - INFO: load:1.74 valid_run:3758.53 task_valid:3606.62 collect_output:119.83

====================================================================================================
SGG eval:     R @ 50: 0.5277;     R @ 100: 0.5912;     R @ 500: 0.6442;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3255;    mR @ 100: 0.3708;    mR @ 500: 0.4523;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7049) (covered in:0.6875) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.8961) (says:0.0000) (sitting on:0.6361) (standing on:0.3383) (using:0.6000) (walking in:0.0000) (walking on:0.6396) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5277;     R @ 100: 0.5912;     R @ 500: 0.6442;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3255;    mR @ 100: 0.3708;    mR @ 500: 0.4523;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7049) (covered in:0.6875) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.1000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9167) (playing:0.0000) (riding:0.8961) (says:0.0000) (sitting on:0.6361) (standing on:0.3383) (using:0.6000) (walking in:0.0000) (walking on:0.6396) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-02-17 12:19:37 - train.py[line:487] - INFO: 0.5912190476190476
2023-02-17 12:19:37 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 12:19:37 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.321 | loss_v1 0 | loss_v2 0 | nll_loss 0.166 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.591219 | ppl 1.12 | vqa_score 0.5372 | wps 118.3 | wpb 72 | bsz 24 | num_updates 15000 | best_R@100 0.649485
2023-02-17 12:19:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 15000 updates
2023-02-17 12:19:37 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_15000.pt
2023-02-17 12:19:43 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_15000.pt
2023-02-17 12:19:45 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_15000.pt (epoch 1 @ 15000 updates, score 0.5912190476190476) (writing took 7.940002270042896 seconds)
2023-02-17 12:19:56 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 12:19:58 - progress_bar.py[line:274] - INFO: epoch 001:  15033 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=109.3, bsz=40, num_updates=15010, lr=4.53227e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=73842
2023-02-17 12:20:09 - progress_bar.py[line:274] - INFO: epoch 001:  15043 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.92, wpb=110.1, bsz=40, num_updates=15020, lr=4.53182e-05, gnorm=0.484, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73853
2023-02-17 12:20:20 - progress_bar.py[line:274] - INFO: epoch 001:  15053 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.88, wpb=112, bsz=40, num_updates=15030, lr=4.53137e-05, gnorm=0.502, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=73864
2023-02-17 12:20:31 - progress_bar.py[line:274] - INFO: epoch 001:  15063 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=112, bsz=40, num_updates=15040, lr=4.53092e-05, gnorm=0.602, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73876
2023-02-17 12:20:42 - progress_bar.py[line:274] - INFO: epoch 001:  15073 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.4, bsz=40, num_updates=15050, lr=4.53047e-05, gnorm=0.526, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=73887
2023-02-17 12:20:54 - progress_bar.py[line:274] - INFO: epoch 001:  15083 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=15060, lr=4.53002e-05, gnorm=0.527, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73898
2023-02-17 12:21:05 - progress_bar.py[line:274] - INFO: epoch 001:  15093 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=15070, lr=4.52957e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73909
2023-02-17 12:21:16 - progress_bar.py[line:274] - INFO: epoch 001:  15103 / 28910 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=15080, lr=4.52912e-05, gnorm=0.521, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=73920
2023-02-17 12:21:27 - progress_bar.py[line:274] - INFO: epoch 001:  15113 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.1, ups=0.93, wpb=112.1, bsz=40, num_updates=15090, lr=4.52867e-05, gnorm=0.48, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73931
2023-02-17 12:21:38 - progress_bar.py[line:274] - INFO: epoch 001:  15123 / 28910 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=94.3, ups=0.87, wpb=108.1, bsz=40, num_updates=15100, lr=4.52822e-05, gnorm=0.439, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=73943
2023-02-17 12:21:50 - progress_bar.py[line:274] - INFO: epoch 001:  15133 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=111, bsz=40, num_updates=15110, lr=4.52777e-05, gnorm=0.428, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73954
2023-02-17 12:22:00 - progress_bar.py[line:274] - INFO: epoch 001:  15143 / 28910 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.93, wpb=111, bsz=40, num_updates=15120, lr=4.52732e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=73965
2023-02-17 12:22:12 - progress_bar.py[line:274] - INFO: epoch 001:  15153 / 28910 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=15130, lr=4.52687e-05, gnorm=0.699, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73976
2023-02-17 12:22:23 - progress_bar.py[line:274] - INFO: epoch 001:  15163 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=15140, lr=4.52642e-05, gnorm=0.48, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73987
2023-02-17 12:22:34 - progress_bar.py[line:274] - INFO: epoch 001:  15173 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=15150, lr=4.52596e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73999
2023-02-17 12:22:46 - progress_bar.py[line:274] - INFO: epoch 001:  15183 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=110.7, bsz=40, num_updates=15160, lr=4.52551e-05, gnorm=0.456, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=74010
2023-02-17 12:22:57 - progress_bar.py[line:274] - INFO: epoch 001:  15193 / 28910 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.92, wpb=111.1, bsz=40, num_updates=15170, lr=4.52506e-05, gnorm=0.571, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74021
2023-02-17 12:23:08 - progress_bar.py[line:274] - INFO: epoch 001:  15203 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=111, bsz=40, num_updates=15180, lr=4.52461e-05, gnorm=0.415, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74032
2023-02-17 12:23:19 - progress_bar.py[line:274] - INFO: epoch 001:  15213 / 28910 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.88, wpb=110.7, bsz=40, num_updates=15190, lr=4.52416e-05, gnorm=0.491, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74043
2023-02-17 12:23:31 - progress_bar.py[line:274] - INFO: epoch 001:  15223 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.5, bsz=40, num_updates=15200, lr=4.52371e-05, gnorm=0.522, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=74055
2023-02-17 12:23:42 - progress_bar.py[line:274] - INFO: epoch 001:  15233 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=109.9, bsz=40, num_updates=15210, lr=4.52326e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74066
2023-02-17 12:23:53 - progress_bar.py[line:274] - INFO: epoch 001:  15243 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.8, bsz=40, num_updates=15220, lr=4.52281e-05, gnorm=0.483, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74077
2023-02-17 12:24:04 - progress_bar.py[line:274] - INFO: epoch 001:  15253 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.88, wpb=110.5, bsz=40, num_updates=15230, lr=4.52236e-05, gnorm=0.439, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74088
2023-02-17 12:24:15 - progress_bar.py[line:274] - INFO: epoch 001:  15263 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=15240, lr=4.52191e-05, gnorm=0.544, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74100
2023-02-17 12:24:26 - progress_bar.py[line:274] - INFO: epoch 001:  15273 / 28910 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.92, wpb=111.1, bsz=40, num_updates=15250, lr=4.52146e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74111
2023-02-17 12:24:38 - progress_bar.py[line:274] - INFO: epoch 001:  15283 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=15260, lr=4.52101e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=74122
2023-02-17 12:24:49 - progress_bar.py[line:274] - INFO: epoch 001:  15293 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=15270, lr=4.52056e-05, gnorm=0.364, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74133
2023-02-17 12:25:00 - progress_bar.py[line:274] - INFO: epoch 001:  15303 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.89, wpb=109.3, bsz=40, num_updates=15280, lr=4.52011e-05, gnorm=0.364, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74144
2023-02-17 12:25:11 - progress_bar.py[line:274] - INFO: epoch 001:  15313 / 28910 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.93, wpb=109.5, bsz=40, num_updates=15290, lr=4.51966e-05, gnorm=0.56, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=74155
2023-02-17 12:25:22 - progress_bar.py[line:274] - INFO: epoch 001:  15323 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=110.8, bsz=40, num_updates=15300, lr=4.51921e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74166
2023-02-17 12:25:33 - progress_bar.py[line:274] - INFO: epoch 001:  15333 / 28910 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=15310, lr=4.51876e-05, gnorm=0.527, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74177
2023-02-17 12:25:44 - progress_bar.py[line:274] - INFO: epoch 001:  15343 / 28910 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=15320, lr=4.51831e-05, gnorm=0.418, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74188
2023-02-17 12:25:55 - progress_bar.py[line:274] - INFO: epoch 001:  15353 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=111.2, bsz=40, num_updates=15330, lr=4.51786e-05, gnorm=0.543, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=74200
2023-02-17 12:26:06 - progress_bar.py[line:274] - INFO: epoch 001:  15363 / 28910 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=111.4, bsz=40, num_updates=15340, lr=4.51741e-05, gnorm=0.549, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74211
2023-02-17 12:26:18 - progress_bar.py[line:274] - INFO: epoch 001:  15373 / 28910 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.88, wpb=110.8, bsz=40, num_updates=15350, lr=4.51696e-05, gnorm=0.384, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74222
2023-02-17 12:26:29 - progress_bar.py[line:274] - INFO: epoch 001:  15383 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.1, ups=0.87, wpb=110.1, bsz=40, num_updates=15360, lr=4.51651e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74233
2023-02-17 12:26:40 - progress_bar.py[line:274] - INFO: epoch 001:  15393 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=15370, lr=4.51606e-05, gnorm=0.599, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=74245
2023-02-17 12:26:52 - progress_bar.py[line:274] - INFO: epoch 001:  15403 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.88, wpb=110.6, bsz=40, num_updates=15380, lr=4.51561e-05, gnorm=0.439, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=74256
2023-02-17 12:27:03 - progress_bar.py[line:274] - INFO: epoch 001:  15413 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.9, ups=0.88, wpb=108.9, bsz=40, num_updates=15390, lr=4.51516e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=74267
2023-02-17 12:27:14 - progress_bar.py[line:274] - INFO: epoch 001:  15423 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.89, wpb=108.8, bsz=40, num_updates=15400, lr=4.51471e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74279
2023-02-17 12:27:25 - progress_bar.py[line:274] - INFO: epoch 001:  15433 / 28910 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.1, ups=0.94, wpb=110.6, bsz=40, num_updates=15410, lr=4.51425e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74289
2023-02-17 12:27:36 - progress_bar.py[line:274] - INFO: epoch 001:  15443 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.88, wpb=110.6, bsz=40, num_updates=15420, lr=4.5138e-05, gnorm=0.484, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=74301
2023-02-17 12:27:48 - progress_bar.py[line:274] - INFO: epoch 001:  15453 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=15430, lr=4.51335e-05, gnorm=0.655, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74312
2023-02-17 12:27:58 - progress_bar.py[line:274] - INFO: epoch 001:  15463 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.92, wpb=109.9, bsz=40, num_updates=15440, lr=4.5129e-05, gnorm=0.383, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=74323
2023-02-17 12:28:10 - progress_bar.py[line:274] - INFO: epoch 001:  15473 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97, ups=0.88, wpb=109.9, bsz=40, num_updates=15450, lr=4.51245e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74334
2023-02-17 12:28:21 - progress_bar.py[line:274] - INFO: epoch 001:  15483 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=15460, lr=4.512e-05, gnorm=0.436, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74345
2023-02-17 12:28:32 - progress_bar.py[line:274] - INFO: epoch 001:  15493 / 28910 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=15470, lr=4.51155e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74356
2023-02-17 12:28:43 - progress_bar.py[line:274] - INFO: epoch 001:  15503 / 28910 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.91, wpb=110.4, bsz=40, num_updates=15480, lr=4.5111e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74367
2023-02-17 12:28:54 - progress_bar.py[line:274] - INFO: epoch 001:  15513 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.9, ups=0.87, wpb=110.2, bsz=40, num_updates=15490, lr=4.51065e-05, gnorm=0.408, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74379
2023-02-17 12:29:06 - progress_bar.py[line:274] - INFO: epoch 001:  15523 / 28910 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.89, wpb=109.9, bsz=40, num_updates=15500, lr=4.5102e-05, gnorm=0.507, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74390
2023-02-17 12:29:17 - progress_bar.py[line:274] - INFO: epoch 001:  15533 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.91, wpb=110.6, bsz=40, num_updates=15510, lr=4.50975e-05, gnorm=0.508, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74401
2023-02-17 12:29:28 - progress_bar.py[line:274] - INFO: epoch 001:  15543 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=15520, lr=4.5093e-05, gnorm=0.477, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74412
2023-02-17 12:29:38 - progress_bar.py[line:274] - INFO: epoch 001:  15553 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.92, wpb=111.2, bsz=40, num_updates=15530, lr=4.50885e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74423
2023-02-17 12:29:49 - progress_bar.py[line:274] - INFO: epoch 001:  15563 / 28910 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.93, wpb=110.3, bsz=40, num_updates=15540, lr=4.5084e-05, gnorm=0.514, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74433
2023-02-17 12:30:01 - progress_bar.py[line:274] - INFO: epoch 001:  15573 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.88, wpb=109.8, bsz=40, num_updates=15550, lr=4.50795e-05, gnorm=0.461, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74445
2023-02-17 12:30:11 - progress_bar.py[line:274] - INFO: epoch 001:  15583 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.92, wpb=110.7, bsz=40, num_updates=15560, lr=4.5075e-05, gnorm=0.377, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74456
2023-02-17 12:30:18 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 12:30:23 - progress_bar.py[line:274] - INFO: epoch 001:  15594 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=91.2, ups=0.83, wpb=110.1, bsz=40, num_updates=15570, lr=4.50705e-05, gnorm=0.542, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=74468
2023-02-17 12:30:35 - progress_bar.py[line:274] - INFO: epoch 001:  15604 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.88, wpb=109.8, bsz=40, num_updates=15580, lr=4.5066e-05, gnorm=0.428, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74479
2023-02-17 12:30:46 - progress_bar.py[line:274] - INFO: epoch 001:  15614 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=111.9, bsz=40, num_updates=15590, lr=4.50615e-05, gnorm=0.573, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74490
2023-02-17 12:30:57 - progress_bar.py[line:274] - INFO: epoch 001:  15624 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=110.7, bsz=40, num_updates=15600, lr=4.5057e-05, gnorm=0.65, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74501
2023-02-17 12:31:08 - progress_bar.py[line:274] - INFO: epoch 001:  15634 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.8, ups=0.89, wpb=109.7, bsz=40, num_updates=15610, lr=4.50525e-05, gnorm=0.46, clip=0, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=74512
2023-02-17 12:31:20 - progress_bar.py[line:274] - INFO: epoch 001:  15644 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=111.1, bsz=40, num_updates=15620, lr=4.5048e-05, gnorm=0.469, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74524
2023-02-17 12:31:31 - progress_bar.py[line:274] - INFO: epoch 001:  15654 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=15630, lr=4.50435e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74535
2023-02-17 12:31:42 - progress_bar.py[line:274] - INFO: epoch 001:  15664 / 28910 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=15640, lr=4.5039e-05, gnorm=0.378, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74546
2023-02-17 12:31:53 - progress_bar.py[line:274] - INFO: epoch 001:  15674 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.92, wpb=111.2, bsz=40, num_updates=15650, lr=4.50345e-05, gnorm=0.443, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74557
2023-02-17 12:32:04 - progress_bar.py[line:274] - INFO: epoch 001:  15684 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.93, wpb=110.4, bsz=40, num_updates=15660, lr=4.503e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74568
2023-02-17 12:32:15 - progress_bar.py[line:274] - INFO: epoch 001:  15694 / 28910 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.91, wpb=109.8, bsz=40, num_updates=15670, lr=4.50254e-05, gnorm=0.64, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74579
2023-02-17 12:32:26 - progress_bar.py[line:274] - INFO: epoch 001:  15704 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=15680, lr=4.50209e-05, gnorm=0.577, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74590
2023-02-17 12:32:37 - progress_bar.py[line:274] - INFO: epoch 001:  15714 / 28910 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98, ups=0.88, wpb=111.3, bsz=40, num_updates=15690, lr=4.50164e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74602
2023-02-17 12:32:49 - progress_bar.py[line:274] - INFO: epoch 001:  15724 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.87, wpb=110.4, bsz=40, num_updates=15700, lr=4.50119e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74613
2023-02-17 12:33:00 - progress_bar.py[line:274] - INFO: epoch 001:  15734 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.89, wpb=109.8, bsz=40, num_updates=15710, lr=4.50074e-05, gnorm=0.358, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=74624
2023-02-17 12:33:12 - progress_bar.py[line:274] - INFO: epoch 001:  15744 / 28910 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.3, ups=0.88, wpb=109.3, bsz=40, num_updates=15720, lr=4.50029e-05, gnorm=0.383, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=74636
2023-02-17 12:33:23 - progress_bar.py[line:274] - INFO: epoch 001:  15754 / 28910 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.89, wpb=110.5, bsz=40, num_updates=15730, lr=4.49984e-05, gnorm=0.376, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=74647
2023-02-17 12:33:34 - progress_bar.py[line:274] - INFO: epoch 001:  15764 / 28910 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=15740, lr=4.49939e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74658
2023-02-17 12:33:45 - progress_bar.py[line:274] - INFO: epoch 001:  15774 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=15750, lr=4.49894e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74669
2023-02-17 12:33:57 - progress_bar.py[line:274] - INFO: epoch 001:  15784 / 28910 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=15760, lr=4.49849e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=74681
2023-02-17 12:34:08 - progress_bar.py[line:274] - INFO: epoch 001:  15794 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=111.7, bsz=40, num_updates=15770, lr=4.49804e-05, gnorm=0.444, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74692
2023-02-17 12:34:19 - progress_bar.py[line:274] - INFO: epoch 001:  15804 / 28910 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.92, wpb=111, bsz=40, num_updates=15780, lr=4.49759e-05, gnorm=0.29, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74703
2023-02-17 12:34:30 - progress_bar.py[line:274] - INFO: epoch 001:  15814 / 28910 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.92, wpb=109.3, bsz=40, num_updates=15790, lr=4.49714e-05, gnorm=0.298, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74714
2023-02-17 12:34:41 - progress_bar.py[line:274] - INFO: epoch 001:  15824 / 28910 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.89, wpb=109.9, bsz=40, num_updates=15800, lr=4.49669e-05, gnorm=0.515, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74725
2023-02-17 12:34:52 - progress_bar.py[line:274] - INFO: epoch 001:  15834 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=15810, lr=4.49624e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74736
2023-02-17 12:35:03 - progress_bar.py[line:274] - INFO: epoch 001:  15844 / 28910 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.91, wpb=109.9, bsz=40, num_updates=15820, lr=4.49579e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74747
2023-02-17 12:35:14 - progress_bar.py[line:274] - INFO: epoch 001:  15854 / 28910 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=15830, lr=4.49534e-05, gnorm=0.433, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74758
2023-02-17 12:35:25 - progress_bar.py[line:274] - INFO: epoch 001:  15864 / 28910 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=110.5, bsz=40, num_updates=15840, lr=4.49489e-05, gnorm=0.484, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=74769
2023-02-17 12:35:36 - progress_bar.py[line:274] - INFO: epoch 001:  15874 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=110.3, bsz=40, num_updates=15850, lr=4.49444e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74780
2023-02-17 12:35:47 - progress_bar.py[line:274] - INFO: epoch 001:  15884 / 28910 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=15860, lr=4.49399e-05, gnorm=0.361, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74792
2023-02-17 12:35:59 - progress_bar.py[line:274] - INFO: epoch 001:  15894 / 28910 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.3, ups=0.87, wpb=110.8, bsz=40, num_updates=15870, lr=4.49354e-05, gnorm=0.511, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74803
2023-02-17 12:36:10 - progress_bar.py[line:274] - INFO: epoch 001:  15904 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=15880, lr=4.49309e-05, gnorm=0.503, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=74814
2023-02-17 12:36:21 - progress_bar.py[line:274] - INFO: epoch 001:  15914 / 28910 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.9, ups=0.88, wpb=110.1, bsz=40, num_updates=15890, lr=4.49264e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74826
2023-02-17 12:36:33 - progress_bar.py[line:274] - INFO: epoch 001:  15924 / 28910 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=15900, lr=4.49219e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74837
2023-02-17 12:36:44 - progress_bar.py[line:274] - INFO: epoch 001:  15934 / 28910 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.3, ups=0.88, wpb=109.1, bsz=40, num_updates=15910, lr=4.49174e-05, gnorm=0.347, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74848
2023-02-17 12:36:55 - progress_bar.py[line:274] - INFO: epoch 001:  15944 / 28910 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96, ups=0.87, wpb=110.3, bsz=40, num_updates=15920, lr=4.49128e-05, gnorm=0.397, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74860
2023-02-17 12:37:06 - progress_bar.py[line:274] - INFO: epoch 001:  15954 / 28910 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.92, wpb=109.2, bsz=40, num_updates=15930, lr=4.49083e-05, gnorm=0.508, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74871
2023-02-17 12:37:17 - progress_bar.py[line:274] - INFO: epoch 001:  15964 / 28910 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=15940, lr=4.49038e-05, gnorm=0.377, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74882
2023-02-17 12:37:28 - progress_bar.py[line:274] - INFO: epoch 001:  15974 / 28910 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=15950, lr=4.48993e-05, gnorm=0.331, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74893
2023-02-17 12:37:40 - progress_bar.py[line:274] - INFO: epoch 001:  15984 / 28910 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=15960, lr=4.48948e-05, gnorm=0.504, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=74904
2023-02-17 12:37:51 - progress_bar.py[line:274] - INFO: epoch 001:  15994 / 28910 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=15970, lr=4.48903e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74915
2023-02-17 12:38:02 - progress_bar.py[line:274] - INFO: epoch 001:  16004 / 28910 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.89, wpb=112.2, bsz=40, num_updates=15980, lr=4.48858e-05, gnorm=0.528, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74927
2023-02-17 12:38:14 - progress_bar.py[line:274] - INFO: epoch 001:  16014 / 28910 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.87, wpb=111, bsz=40, num_updates=15990, lr=4.48813e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74938
2023-02-17 12:38:25 - progress_bar.py[line:274] - INFO: epoch 001:  16024 / 28910 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.9, wpb=110.4, bsz=40, num_updates=16000, lr=4.48768e-05, gnorm=0.498, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74949
2023-02-17 12:38:25 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 12:38:26 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 12:38:26 - train.py[line:551] - INFO: load:1.03 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 12:40:28 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 12:40:28 - train.py[line:551] - INFO: load:1.06 valid_run:122.23 task_valid:119.04 collect_output:2.09
2023-02-17 12:42:29 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 12:42:29 - train.py[line:551] - INFO: load:1.09 valid_run:242.19 task_valid:234.80 collect_output:5.26
2023-02-17 12:44:31 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 12:44:31 - train.py[line:551] - INFO: load:1.11 valid_run:364.28 task_valid:351.42 collect_output:9.70
2023-02-17 12:46:33 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 12:46:33 - train.py[line:551] - INFO: load:1.14 valid_run:486.22 task_valid:465.05 collect_output:16.96
2023-02-17 12:48:33 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 12:48:33 - train.py[line:551] - INFO: load:1.16 valid_run:606.65 task_valid:582.20 collect_output:19.21
2023-02-17 12:50:36 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 12:50:36 - train.py[line:551] - INFO: load:1.19 valid_run:729.46 task_valid:700.67 collect_output:22.47
2023-02-17 12:52:39 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 12:52:39 - train.py[line:551] - INFO: load:1.22 valid_run:852.48 task_valid:818.61 collect_output:26.50
2023-02-17 12:54:41 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 12:54:41 - train.py[line:551] - INFO: load:1.25 valid_run:974.28 task_valid:934.94 collect_output:30.91
2023-02-17 12:56:45 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 12:56:45 - train.py[line:551] - INFO: load:1.27 valid_run:1097.98 task_valid:1052.11 collect_output:36.40
2023-02-17 12:58:47 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 12:58:47 - train.py[line:551] - INFO: load:1.30 valid_run:1219.76 task_valid:1164.77 collect_output:44.47
2023-02-17 13:00:47 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 13:00:47 - train.py[line:551] - INFO: load:1.33 valid_run:1340.00 task_valid:1280.38 collect_output:48.06
2023-02-17 13:02:48 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 13:02:48 - train.py[line:551] - INFO: load:1.35 valid_run:1461.44 task_valid:1396.93 collect_output:51.92
2023-02-17 13:04:47 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 13:04:47 - train.py[line:551] - INFO: load:1.38 valid_run:1580.48 task_valid:1510.74 collect_output:56.12
2023-02-17 13:06:49 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 13:06:49 - train.py[line:551] - INFO: load:1.41 valid_run:1701.68 task_valid:1628.51 collect_output:58.50
2023-02-17 13:08:50 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 13:08:50 - train.py[line:551] - INFO: load:1.43 valid_run:1822.84 task_valid:1744.67 collect_output:62.45
2023-02-17 13:10:51 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 13:10:51 - train.py[line:551] - INFO: load:1.46 valid_run:1943.96 task_valid:1858.47 collect_output:68.74
2023-02-17 13:12:53 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 13:12:53 - train.py[line:551] - INFO: load:1.49 valid_run:2065.45 task_valid:1974.49 collect_output:73.17
2023-02-17 13:14:53 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 13:14:53 - train.py[line:551] - INFO: load:1.51 valid_run:2186.06 task_valid:2092.19 collect_output:75.04
2023-02-17 13:16:55 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 13:16:55 - train.py[line:551] - INFO: load:1.54 valid_run:2307.33 task_valid:2209.04 collect_output:78.43
2023-02-17 13:18:55 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 13:18:55 - train.py[line:551] - INFO: load:1.57 valid_run:2427.85 task_valid:2325.59 collect_output:81.38
2023-02-17 13:20:57 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 13:20:57 - train.py[line:551] - INFO: load:1.59 valid_run:2549.75 task_valid:2442.16 collect_output:85.68
2023-02-17 13:22:59 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 13:22:59 - train.py[line:551] - INFO: load:1.62 valid_run:2671.68 task_valid:2560.84 collect_output:87.90
2023-02-17 13:25:00 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 13:25:00 - train.py[line:551] - INFO: load:1.65 valid_run:2792.09 task_valid:2675.08 collect_output:93.04
2023-02-17 13:26:59 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 13:26:59 - train.py[line:551] - INFO: load:1.67 valid_run:2911.79 task_valid:2791.05 collect_output:95.75
2023-02-17 13:29:01 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 13:29:01 - train.py[line:551] - INFO: load:1.70 valid_run:3033.50 task_valid:2907.30 collect_output:100.18
2023-02-17 13:31:04 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 13:31:04 - train.py[line:551] - INFO: load:1.73 valid_run:3156.41 task_valid:3023.20 collect_output:106.16
2023-02-17 13:33:04 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 13:33:04 - train.py[line:551] - INFO: load:1.75 valid_run:3275.85 task_valid:3137.06 collect_output:110.71
2023-02-17 13:35:05 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 13:35:05 - train.py[line:551] - INFO: load:1.78 valid_run:3397.56 task_valid:3256.18 collect_output:112.28
2023-02-17 13:37:07 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 13:37:07 - train.py[line:551] - INFO: load:1.81 valid_run:3519.29 task_valid:3371.56 collect_output:117.61
2023-02-17 13:39:09 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 13:39:09 - train.py[line:551] - INFO: load:1.83 valid_run:3641.03 task_valid:3489.71 collect_output:120.17
2023-02-17 13:41:10 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 13:41:10 - train.py[line:551] - INFO: load:1.86 valid_run:3761.86 task_valid:3607.80 collect_output:121.87

====================================================================================================
SGG eval:     R @ 50: 0.5253;     R @ 100: 0.5901;     R @ 500: 0.6439;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3247;    mR @ 100: 0.3667;    mR @ 500: 0.4466;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6805) (covered in:0.6458) (covering:0.1429) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.1000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9167) (playing:0.0000) (riding:0.8830) (says:0.0000) (sitting on:0.6267) (standing on:0.3583) (using:0.6000) (walking in:0.0000) (walking on:0.6396) (watching:0.1806) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5253;     R @ 100: 0.5901;     R @ 500: 0.6439;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3247;    mR @ 100: 0.3667;    mR @ 500: 0.4466;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6805) (covered in:0.6458) (covering:0.1429) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.1000) (mounted on:0.0000) (painted on:0.4167) (parked on:0.9167) (playing:0.0000) (riding:0.8830) (says:0.0000) (sitting on:0.6267) (standing on:0.3583) (using:0.6000) (walking in:0.0000) (walking on:0.6396) (watching:0.1806) 
--------------------------------------------------------
====================================================================================================

2023-02-17 13:41:41 - train.py[line:487] - INFO: 0.5900857142857143
2023-02-17 13:41:41 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 13:41:41 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.334 | loss_v1 0 | loss_v2 0 | nll_loss 0.177 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.590086 | ppl 1.13 | vqa_score 0.5293 | wps 118.2 | wpb 72 | bsz 24 | num_updates 16000 | best_R@100 0.649485
2023-02-17 13:41:41 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 16000 updates
2023-02-17 13:41:41 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k25alpha1.0_/1_B20_A1_E4_0.04_5e-5_480/checkpoint_1_16000.pt
