2023-02-19 17:19:51 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-19 17:19:51 - utils.py[line:261] - INFO: Start init
2023-02-19 17:19:51 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-19 17:19:51 - utils.py[line:261] - INFO: Start init
2023-02-19 17:19:51 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-19 17:19:51 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-19 17:19:51 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-19 17:19:51 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-19 17:20:05 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-19 17:20:05 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-19 17:20:05 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-19 17:20:09 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-19 17:20:09 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-19 17:20:09 - train.py[line:119] - INFO: model: OFAModel
2023-02-19 17:20:09 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-19 17:20:09 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-19 17:20:09 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-19 17:20:10 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-19 17:20:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-19 17:20:11 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-19 17:20:11 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 17:20:11 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 17:20:11 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-19 17:20:12 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-19 17:20:12 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-19 17:20:12 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-02-19 17:20:21 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-19 17:20:22 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 17:20:22 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 17:20:22 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-19 17:20:22 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-19 17:20:23 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-19 17:20:23 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv slice_id 1 row count 336808 total row count 673616
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv slice_id 0 row count 336808 total row count 673616
2023-02-19 17:20:29 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
    cli_main()
  File "../../train.py", line 625, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 721, in get_train_iterator
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 233, in __getitem__
    item = self.dataset[index]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in __getitem__
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in <listcomp>
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
IndexError: list index out of range
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
    cli_main()
  File "../../train.py", line 625, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 721, in get_train_iterator
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 233, in __getitem__
    item = self.dataset[index]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in __getitem__
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in <listcomp>
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
IndexError: list index out of range
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=10', '--warmup-ratio=0.027', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=1000', '--validate-interval-updates=1000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3146721
Killing subprocess 3146722
2023-02-19 17:30:20 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-19 17:30:20 - utils.py[line:261] - INFO: Start init
2023-02-19 17:30:20 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-19 17:30:20 - utils.py[line:261] - INFO: Start init
2023-02-19 17:30:21 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-19 17:30:22 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-19 17:30:22 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-19 17:30:22 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-19 17:30:32 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-19 17:30:32 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-19 17:30:32 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-19 17:30:37 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-19 17:30:37 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-19 17:30:37 - train.py[line:119] - INFO: model: OFAModel
2023-02-19 17:30:37 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-19 17:30:37 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-19 17:30:37 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-19 17:30:37 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-19 17:30:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-19 17:30:38 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-19 17:30:38 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 17:30:38 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 17:30:38 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-19 17:30:39 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-19 17:30:39 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-19 17:30:39 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-02-19 17:30:47 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 17:30:47 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-19 17:30:47 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 17:30:48 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-19 17:30:48 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-19 17:30:48 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-19 17:30:48 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv slice_id 0 row count 336808 total row count 673616
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv slice_id 1 row count 336808 total row count 673616
2023-02-19 17:30:49 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
    cli_main()
  File "../../train.py", line 625, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 721, in get_train_iterator
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 233, in __getitem__
    item = self.dataset[index]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in __getitem__
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in <listcomp>
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
IndexError: list index out of range
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
    cli_main()
  File "../../train.py", line 625, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 721, in get_train_iterator
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 233, in __getitem__
    item = self.dataset[index]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in __getitem__
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
  File "/data/private/yutianyu/OFA/data/file_dataset.py", line 115, in <listcomp>
    column_l = [dtype(column_l[col_id]) for col_id, dtype in zip(self.selected_col_ids, self.dtypes)]
IndexError: list index out of range
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=10', '--warmup-ratio=0.027', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=2000', '--validate-interval-updates=2000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3152012
Killing subprocess 3152013
2023-02-19 21:11:07 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-19 21:11:07 - utils.py[line:261] - INFO: Start init
2023-02-19 21:11:07 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-19 21:11:07 - utils.py[line:261] - INFO: Start init
2023-02-19 21:11:07 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-19 21:11:07 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-19 21:11:07 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-19 21:11:07 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-19 21:11:19 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-19 21:11:19 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-19 21:11:19 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-19 21:11:23 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-19 21:11:23 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-19 21:11:23 - train.py[line:119] - INFO: model: OFAModel
2023-02-19 21:11:23 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-19 21:11:23 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-19 21:11:23 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-19 21:11:24 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-19 21:11:24 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-19 21:11:25 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-19 21:11:25 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 21:11:25 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 21:11:25 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-19 21:11:27 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-19 21:11:27 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-19 21:11:27 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-02-19 21:11:35 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-19 21:11:35 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 21:11:35 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 21:11:35 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-19 21:11:36 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-19 21:11:37 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-19 21:11:37 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv slice_id 0 row count 284044 total row count 568088
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E0.tsv slice_id 1 row count 284044 total row count 568088
2023-02-19 21:11:38 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 142030, warmup steps 3834, warmup_factor 0.0002608242044861763
Total steps 142030, warmup steps 3834, warmup_factor 0.0002608242044861763
2023-02-19 21:11:39 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-19 21:11:39 - train.py[line:312] - INFO: Start iterating over samples
2023-02-19 21:11:59 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 14203 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=77.7, ups=0.7, wpb=111.7, bsz=40, num_updates=10, lr=1.30412e-07, gnorm=6.958, clip=100, loss_scale=128, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=33
2023-02-19 21:12:10 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 14203 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.5, ups=0.87, wpb=113, bsz=40, num_updates=20, lr=2.60824e-07, gnorm=6.795, clip=100, loss_scale=128, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=45
2023-02-19 21:12:22 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 14203 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.6, ups=0.84, wpb=113.9, bsz=40, num_updates=30, lr=3.91236e-07, gnorm=7.174, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=57
2023-02-19 21:12:34 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 14203 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=94.9, ups=0.84, wpb=113.5, bsz=40, num_updates=40, lr=5.21648e-07, gnorm=7.015, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=69
2023-02-19 21:12:46 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 14203 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.86, wpb=112.7, bsz=40, num_updates=50, lr=6.52061e-07, gnorm=5.459, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=80
2023-02-19 21:12:58 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 14203 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=94.5, ups=0.84, wpb=113.1, bsz=40, num_updates=60, lr=7.82473e-07, gnorm=5.361, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=92
2023-02-19 21:13:09 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 14203 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101, ups=0.88, wpb=115, bsz=40, num_updates=70, lr=9.12885e-07, gnorm=4.333, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=104
2023-02-19 21:13:21 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 14203 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.1, ups=0.86, wpb=113.7, bsz=40, num_updates=80, lr=1.0433e-06, gnorm=4.239, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=115
2023-02-19 21:13:32 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 14203 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=103.1, ups=0.9, wpb=114.6, bsz=40, num_updates=90, lr=1.17371e-06, gnorm=3.9, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=127
2023-02-19 21:13:43 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 14203 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.6, ups=0.86, wpb=113.4, bsz=40, num_updates=100, lr=1.30412e-06, gnorm=3.121, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=138
2023-02-19 21:13:55 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 14203 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=94.7, ups=0.84, wpb=112.9, bsz=40, num_updates=110, lr=1.43453e-06, gnorm=3.012, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=150
2023-02-19 21:14:07 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 14203 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.7, ups=0.86, wpb=113.5, bsz=40, num_updates=120, lr=1.56495e-06, gnorm=2.429, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=162
2023-02-19 21:14:19 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 14203 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=95.1, ups=0.85, wpb=112.1, bsz=40, num_updates=130, lr=1.69536e-06, gnorm=2.391, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=174
2023-02-19 21:14:30 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 14203 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.6, ups=0.86, wpb=114, bsz=40, num_updates=140, lr=1.82577e-06, gnorm=2.093, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=185
2023-02-19 21:14:41 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 14203 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=104.5, ups=0.91, wpb=114.7, bsz=40, num_updates=150, lr=1.95618e-06, gnorm=2.342, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=196
2023-02-19 21:14:53 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 14203 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.8, ups=0.88, wpb=113.3, bsz=40, num_updates=160, lr=2.08659e-06, gnorm=2.053, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=208
2023-02-19 21:15:04 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 14203 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.3, ups=0.86, wpb=113.6, bsz=40, num_updates=170, lr=2.21701e-06, gnorm=2.004, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=219
2023-02-19 21:15:16 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 14203 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.7, ups=0.87, wpb=112.3, bsz=40, num_updates=180, lr=2.34742e-06, gnorm=1.985, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=231
2023-02-19 21:15:27 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 14203 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.1, ups=0.87, wpb=113.5, bsz=40, num_updates=190, lr=2.47783e-06, gnorm=1.682, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=242
2023-02-19 21:15:39 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 14203 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.1, ups=0.88, wpb=114.8, bsz=40, num_updates=200, lr=2.60824e-06, gnorm=1.633, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=253
2023-02-19 21:15:49 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 14203 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=104.2, ups=0.92, wpb=113.3, bsz=40, num_updates=210, lr=2.73865e-06, gnorm=1.735, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=264
2023-02-19 21:16:01 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 14203 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98, ups=0.87, wpb=113, bsz=40, num_updates=220, lr=2.86907e-06, gnorm=1.481, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=276
2023-02-19 21:16:12 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 14203 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.9, ups=0.88, wpb=113, bsz=40, num_updates=230, lr=2.99948e-06, gnorm=1.387, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=287
2023-02-19 21:16:23 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 14203 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.3, ups=0.91, wpb=114, bsz=40, num_updates=240, lr=3.12989e-06, gnorm=1.463, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=298
2023-02-19 21:16:35 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 14203 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.88, wpb=113.2, bsz=40, num_updates=250, lr=3.2603e-06, gnorm=1.183, clip=70, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=310
2023-02-19 21:16:46 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 14203 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=103.1, ups=0.91, wpb=113, bsz=40, num_updates=260, lr=3.39071e-06, gnorm=1.46, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=321
2023-02-19 21:16:57 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 14203 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.86, wpb=113.1, bsz=40, num_updates=270, lr=3.52113e-06, gnorm=1.397, clip=90, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=332
2023-02-19 21:17:09 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 14203 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102, ups=0.89, wpb=114, bsz=40, num_updates=280, lr=3.65154e-06, gnorm=1.371, clip=70, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=343
2023-02-19 21:17:20 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 14203 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.4, ups=0.91, wpb=113.3, bsz=40, num_updates=290, lr=3.78195e-06, gnorm=1.447, clip=90, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=354
2023-02-19 21:17:31 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 14203 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=300, lr=3.91236e-06, gnorm=1.374, clip=90, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=366
2023-02-19 21:17:42 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 14203 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.5, ups=0.85, wpb=114.2, bsz=40, num_updates=310, lr=4.04278e-06, gnorm=1.477, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=377
2023-02-19 21:17:54 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 14203 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.4, ups=0.89, wpb=114.1, bsz=40, num_updates=320, lr=4.17319e-06, gnorm=1.383, clip=90, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=389
2023-02-19 21:18:05 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 14203 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98, ups=0.87, wpb=112.8, bsz=40, num_updates=330, lr=4.3036e-06, gnorm=1.358, clip=90, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=400
2023-02-19 21:18:17 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 14203 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.7, ups=0.88, wpb=114.7, bsz=40, num_updates=340, lr=4.43401e-06, gnorm=1.33, clip=80, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=411
2023-02-19 21:18:28 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 14203 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.1, ups=0.88, wpb=112, bsz=40, num_updates=350, lr=4.56442e-06, gnorm=1.221, clip=90, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=423
2023-02-19 21:18:39 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 14203 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.5, ups=0.89, wpb=112.4, bsz=40, num_updates=360, lr=4.69484e-06, gnorm=1.228, clip=80, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=434
2023-02-19 21:18:51 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 14203 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.87, wpb=112.2, bsz=40, num_updates=370, lr=4.82525e-06, gnorm=1.296, clip=70, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=446
2023-02-19 21:19:02 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 14203 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.87, wpb=114.1, bsz=40, num_updates=380, lr=4.95566e-06, gnorm=1.164, clip=80, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=457
2023-02-19 21:19:14 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 14203 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.87, wpb=113.3, bsz=40, num_updates=390, lr=5.08607e-06, gnorm=1.151, clip=90, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=469
2023-02-19 21:19:25 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 14203 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.88, wpb=113, bsz=40, num_updates=400, lr=5.21648e-06, gnorm=1.358, clip=80, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=480
2023-02-19 21:19:37 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 14203 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.88, wpb=113.4, bsz=40, num_updates=410, lr=5.3469e-06, gnorm=1.331, clip=90, loss_scale=128, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=492
2023-02-19 21:19:48 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 14203 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.87, wpb=114.1, bsz=40, num_updates=420, lr=5.47731e-06, gnorm=1.23, clip=80, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=503
2023-02-19 21:20:00 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 14203 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.86, wpb=114.3, bsz=40, num_updates=430, lr=5.60772e-06, gnorm=1.174, clip=80, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=515
2023-02-19 21:20:11 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 14203 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.89, wpb=113.5, bsz=40, num_updates=440, lr=5.73813e-06, gnorm=1.244, clip=60, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=526
2023-02-19 21:20:22 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 14203 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.88, wpb=115, bsz=40, num_updates=450, lr=5.86854e-06, gnorm=1.214, clip=70, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=537
2023-02-19 21:20:34 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 14203 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.87, wpb=113.9, bsz=40, num_updates=460, lr=5.99896e-06, gnorm=1.146, clip=70, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=549
2023-02-19 21:20:45 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 14203 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.87, wpb=113.2, bsz=40, num_updates=470, lr=6.12937e-06, gnorm=1.009, clip=50, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=560
2023-02-19 21:20:57 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 14203 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.87, wpb=113, bsz=40, num_updates=480, lr=6.25978e-06, gnorm=1.14, clip=60, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=572
2023-02-19 21:21:08 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 14203 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.5, bsz=40, num_updates=490, lr=6.39019e-06, gnorm=1.264, clip=80, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=583
2023-02-19 21:21:19 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 14203 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.1, ups=0.91, wpb=114.4, bsz=40, num_updates=500, lr=6.52061e-06, gnorm=1.028, clip=50, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=594
2023-02-19 21:21:30 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 14203 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.87, wpb=113.5, bsz=40, num_updates=510, lr=6.65102e-06, gnorm=0.948, clip=40, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=605
2023-02-19 21:21:42 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 14203 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.87, wpb=114.1, bsz=40, num_updates=520, lr=6.78143e-06, gnorm=1.165, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=617
2023-02-19 21:21:54 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 14203 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.86, wpb=113.6, bsz=40, num_updates=530, lr=6.91184e-06, gnorm=1.032, clip=50, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=628
2023-02-19 21:22:05 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 14203 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=113, bsz=40, num_updates=540, lr=7.04225e-06, gnorm=0.863, clip=30, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=640
2023-02-19 21:22:16 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=550, lr=7.17267e-06, gnorm=0.888, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=651
2023-02-19 21:22:27 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 14203 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.87, wpb=114, bsz=40, num_updates=560, lr=7.30308e-06, gnorm=0.905, clip=20, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=662
2023-02-19 21:22:39 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.89, wpb=114.4, bsz=40, num_updates=570, lr=7.43349e-06, gnorm=0.908, clip=30, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=674
2023-02-19 21:22:50 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.87, wpb=113.3, bsz=40, num_updates=580, lr=7.5639e-06, gnorm=0.85, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=685
2023-02-19 21:23:02 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 14203 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.86, wpb=113.8, bsz=40, num_updates=590, lr=7.69431e-06, gnorm=1.036, clip=50, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=697
2023-02-19 21:23:13 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 14203 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.89, wpb=114.5, bsz=40, num_updates=600, lr=7.82473e-06, gnorm=1.054, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=708
2023-02-19 21:23:24 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 14203 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.89, wpb=114.4, bsz=40, num_updates=610, lr=7.95514e-06, gnorm=1.121, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=719
2023-02-19 21:23:35 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 14203 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.7, ups=0.92, wpb=114.4, bsz=40, num_updates=620, lr=8.08555e-06, gnorm=1.159, clip=60, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=730
2023-02-19 21:23:46 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 14203 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.89, wpb=113.7, bsz=40, num_updates=630, lr=8.21596e-06, gnorm=1.143, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=741
2023-02-19 21:23:58 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 14203 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=113.4, bsz=40, num_updates=640, lr=8.34637e-06, gnorm=0.987, clip=50, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=752
2023-02-19 21:24:09 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.9, wpb=113.2, bsz=40, num_updates=650, lr=8.47679e-06, gnorm=1.111, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=764
2023-02-19 21:24:20 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.88, wpb=113.7, bsz=40, num_updates=660, lr=8.6072e-06, gnorm=0.956, clip=30, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=775
2023-02-19 21:24:32 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 14203 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.87, wpb=112.4, bsz=40, num_updates=670, lr=8.73761e-06, gnorm=1.147, clip=50, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=787
2023-02-19 21:24:43 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 14203 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.88, wpb=113.5, bsz=40, num_updates=680, lr=8.86802e-06, gnorm=0.837, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=798
2023-02-19 21:24:54 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 14203 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.87, wpb=113.7, bsz=40, num_updates=690, lr=8.99844e-06, gnorm=1.126, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=809
2023-02-19 21:25:06 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 14203 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=700, lr=9.12885e-06, gnorm=1.155, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=821
2023-02-19 21:25:17 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.9, wpb=114.8, bsz=40, num_updates=710, lr=9.25926e-06, gnorm=0.918, clip=40, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=832
2023-02-19 21:25:28 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 14203 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.92, wpb=113.3, bsz=40, num_updates=720, lr=9.38967e-06, gnorm=0.797, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=843
2023-02-19 21:25:39 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 14203 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.86, wpb=114, bsz=40, num_updates=730, lr=9.52008e-06, gnorm=0.876, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=854
2023-02-19 21:26:12 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=740, lr=9.6505e-06, gnorm=0.84, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=887
2023-02-19 21:26:23 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.91, wpb=112.9, bsz=40, num_updates=750, lr=9.78091e-06, gnorm=0.87, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=898
2023-02-19 21:26:35 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.7, bsz=40, num_updates=760, lr=9.91132e-06, gnorm=0.979, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=910
2023-02-19 21:26:46 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.87, wpb=113.4, bsz=40, num_updates=770, lr=1.00417e-05, gnorm=1.03, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=921
2023-02-19 21:26:58 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.87, wpb=113.9, bsz=40, num_updates=780, lr=1.01721e-05, gnorm=0.73, clip=20, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=933
2023-02-19 21:27:09 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.1, ups=0.86, wpb=111.2, bsz=40, num_updates=790, lr=1.03026e-05, gnorm=0.928, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=944
2023-02-19 21:27:21 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.87, wpb=115, bsz=40, num_updates=800, lr=1.0433e-05, gnorm=0.842, clip=30, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=956
2023-02-19 21:27:32 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.87, wpb=113.1, bsz=40, num_updates=810, lr=1.05634e-05, gnorm=1.051, clip=70, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=967
2023-02-19 21:27:44 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=113.9, bsz=40, num_updates=820, lr=1.06938e-05, gnorm=0.717, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=979
2023-02-19 21:27:55 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=113.4, bsz=40, num_updates=830, lr=1.08242e-05, gnorm=0.941, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=990
2023-02-19 21:28:06 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 14203 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=113, bsz=40, num_updates=840, lr=1.09546e-05, gnorm=0.857, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1001
2023-02-19 21:28:18 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 14203 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.87, wpb=113.6, bsz=40, num_updates=850, lr=1.1085e-05, gnorm=1.149, clip=70, loss_scale=256, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=1013
2023-02-19 21:28:29 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.5, ups=0.92, wpb=113.8, bsz=40, num_updates=860, lr=1.12154e-05, gnorm=0.853, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1023
2023-02-19 21:28:40 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.3, bsz=40, num_updates=870, lr=1.13459e-05, gnorm=0.813, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1035
2023-02-19 21:28:51 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.88, wpb=113.8, bsz=40, num_updates=880, lr=1.14763e-05, gnorm=1.014, clip=40, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1046
2023-02-19 21:29:03 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=113.2, bsz=40, num_updates=890, lr=1.16067e-05, gnorm=0.901, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1058
2023-02-19 21:29:14 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.88, wpb=113.9, bsz=40, num_updates=900, lr=1.17371e-05, gnorm=0.806, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1069
2023-02-19 21:29:25 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.8, bsz=40, num_updates=910, lr=1.18675e-05, gnorm=0.78, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1080
2023-02-19 21:29:37 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.87, wpb=113.6, bsz=40, num_updates=920, lr=1.19979e-05, gnorm=0.831, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1092
2023-02-19 21:29:48 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.9, bsz=40, num_updates=930, lr=1.21283e-05, gnorm=0.863, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1103
2023-02-19 21:29:59 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.87, wpb=112.4, bsz=40, num_updates=940, lr=1.22587e-05, gnorm=0.971, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1114
2023-02-19 21:30:11 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 14203 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=113.6, bsz=40, num_updates=950, lr=1.23891e-05, gnorm=0.812, clip=30, loss_scale=256, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=1126
2023-02-19 21:30:22 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.87, wpb=113.5, bsz=40, num_updates=960, lr=1.25196e-05, gnorm=0.86, clip=30, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1137
2023-02-19 21:30:35 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113.4, bsz=40, num_updates=970, lr=1.265e-05, gnorm=0.724, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1148
2023-02-19 21:30:46 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.1, ups=0.93, wpb=112.8, bsz=40, num_updates=980, lr=1.27804e-05, gnorm=0.819, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1161
2023-02-19 21:30:57 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 14203 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=990, lr=1.29108e-05, gnorm=0.79, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1172
2023-02-19 21:31:09 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.86, wpb=113.3, bsz=40, num_updates=1000, lr=1.30412e-05, gnorm=0.667, clip=20, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1183
2023-02-19 21:31:20 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.91, wpb=114.3, bsz=40, num_updates=1010, lr=1.31716e-05, gnorm=0.642, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1194
2023-02-19 21:31:31 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.87, wpb=113.3, bsz=40, num_updates=1020, lr=1.3302e-05, gnorm=0.714, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1206
2023-02-19 21:31:43 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.89, wpb=114, bsz=40, num_updates=1030, lr=1.34324e-05, gnorm=0.828, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1217
2023-02-19 21:31:54 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 14203 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.7, bsz=40, num_updates=1040, lr=1.35629e-05, gnorm=0.792, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1229
2023-02-19 21:32:05 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.89, wpb=115.2, bsz=40, num_updates=1050, lr=1.36933e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1240
2023-02-19 21:32:16 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=107.5, ups=0.94, wpb=114.4, bsz=40, num_updates=1060, lr=1.38237e-05, gnorm=0.743, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1250
2023-02-19 21:32:27 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.86, wpb=113.2, bsz=40, num_updates=1070, lr=1.39541e-05, gnorm=0.751, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1262
2023-02-19 21:32:38 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.9, wpb=114, bsz=40, num_updates=1080, lr=1.40845e-05, gnorm=0.602, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1273
2023-02-19 21:32:50 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=115.1, nsentences=40, sample_size=115.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.88, wpb=115.1, bsz=40, num_updates=1090, lr=1.42149e-05, gnorm=0.581, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1285
2023-02-19 21:33:01 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=113.6, bsz=40, num_updates=1100, lr=1.43453e-05, gnorm=0.744, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1296
2023-02-19 21:33:12 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.87, wpb=114, bsz=40, num_updates=1110, lr=1.44757e-05, gnorm=0.629, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1307
2023-02-19 21:33:24 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.87, wpb=113.6, bsz=40, num_updates=1120, lr=1.46062e-05, gnorm=0.714, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=1319
2023-02-19 21:33:35 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.87, wpb=114.8, bsz=40, num_updates=1130, lr=1.47366e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1330
2023-02-19 21:33:47 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.4, bsz=40, num_updates=1140, lr=1.4867e-05, gnorm=0.909, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1341
2023-02-19 21:33:58 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.88, wpb=113.8, bsz=40, num_updates=1150, lr=1.49974e-05, gnorm=0.965, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1353
2023-02-19 21:34:09 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.89, wpb=114.5, bsz=40, num_updates=1160, lr=1.51278e-05, gnorm=0.675, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1364
2023-02-19 21:34:21 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=113.5, bsz=40, num_updates=1170, lr=1.52582e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1375
2023-02-19 21:34:32 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=113.1, bsz=40, num_updates=1180, lr=1.53886e-05, gnorm=0.736, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1387
2023-02-19 21:34:43 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113.6, bsz=40, num_updates=1190, lr=1.5519e-05, gnorm=0.973, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1398
2023-02-19 21:34:55 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 14203 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.88, wpb=113.4, bsz=40, num_updates=1200, lr=1.56495e-05, gnorm=0.999, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1410
2023-02-19 21:35:06 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 14203 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.88, wpb=114.4, bsz=40, num_updates=1210, lr=1.57799e-05, gnorm=0.747, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1421
2023-02-19 21:35:18 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.87, wpb=114.2, bsz=40, num_updates=1220, lr=1.59103e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1432
2023-02-19 21:35:29 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.86, wpb=113.8, bsz=40, num_updates=1230, lr=1.60407e-05, gnorm=0.732, clip=30, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=1444
2023-02-19 21:35:41 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 14203 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.88, wpb=113.8, bsz=40, num_updates=1240, lr=1.61711e-05, gnorm=0.789, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1455
2023-02-19 21:35:53 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=94.8, ups=0.83, wpb=113.9, bsz=40, num_updates=1250, lr=1.63015e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1467
2023-02-19 21:36:04 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=115.4, nsentences=40, sample_size=115.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.5, ups=0.91, wpb=115.4, bsz=40, num_updates=1260, lr=1.64319e-05, gnorm=0.539, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1478
2023-02-19 21:36:15 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.88, wpb=114.9, bsz=40, num_updates=1270, lr=1.65623e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1490
2023-02-19 21:36:26 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.88, wpb=113, bsz=40, num_updates=1280, lr=1.66927e-05, gnorm=0.57, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1501
2023-02-19 21:36:38 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 14203 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.9, bsz=40, num_updates=1290, lr=1.68232e-05, gnorm=0.736, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1513
2023-02-19 21:36:49 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=113.4, bsz=40, num_updates=1300, lr=1.69536e-05, gnorm=0.728, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1524
2023-02-19 21:37:00 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.4, bsz=40, num_updates=1310, lr=1.7084e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1535
2023-02-19 21:37:12 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=113, bsz=40, num_updates=1320, lr=1.72144e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=1546
2023-02-19 21:37:23 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 14203 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.86, wpb=114.5, bsz=40, num_updates=1330, lr=1.73448e-05, gnorm=0.84, clip=30, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=1558
2023-02-19 21:37:35 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.87, wpb=113.5, bsz=40, num_updates=1340, lr=1.74752e-05, gnorm=0.827, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1570
2023-02-19 21:37:46 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.87, wpb=113.4, bsz=40, num_updates=1350, lr=1.76056e-05, gnorm=0.739, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1581
2023-02-19 21:37:58 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=1360, lr=1.7736e-05, gnorm=0.709, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1592
2023-02-19 21:38:10 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=113.2, bsz=40, num_updates=1370, lr=1.78665e-05, gnorm=0.716, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1604
2023-02-19 21:38:21 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 14203 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.92, wpb=113.4, bsz=40, num_updates=1380, lr=1.79969e-05, gnorm=0.934, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1616
2023-02-19 21:38:32 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.88, wpb=114, bsz=40, num_updates=1390, lr=1.81273e-05, gnorm=0.797, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1627
2023-02-19 21:38:43 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.9, wpb=115, bsz=40, num_updates=1400, lr=1.82577e-05, gnorm=0.642, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1638
2023-02-19 21:38:55 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 14203 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.86, wpb=113.4, bsz=40, num_updates=1410, lr=1.83881e-05, gnorm=0.778, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1650
2023-02-19 21:39:06 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=1420, lr=1.85185e-05, gnorm=0.728, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1661
2023-02-19 21:39:17 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 14203 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.89, wpb=114.6, bsz=40, num_updates=1430, lr=1.86489e-05, gnorm=0.734, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1672
2023-02-19 21:39:28 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.9, wpb=114, bsz=40, num_updates=1440, lr=1.87793e-05, gnorm=0.641, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1683
2023-02-19 21:39:40 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 14203 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.88, wpb=112.9, bsz=40, num_updates=1450, lr=1.89098e-05, gnorm=0.781, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1695
2023-02-19 21:39:51 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.8, bsz=40, num_updates=1460, lr=1.90402e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1706
2023-02-19 21:40:03 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.3, ups=0.85, wpb=113.6, bsz=40, num_updates=1470, lr=1.91706e-05, gnorm=0.633, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=1718
2023-02-19 21:40:15 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 14203 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=113.7, bsz=40, num_updates=1480, lr=1.9301e-05, gnorm=0.692, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1729
2023-02-19 21:40:26 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.9, wpb=113.6, bsz=40, num_updates=1490, lr=1.94314e-05, gnorm=0.73, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1741
2023-02-19 21:40:37 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.87, wpb=114.3, bsz=40, num_updates=1500, lr=1.95618e-05, gnorm=0.463, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1752
2023-02-19 21:40:48 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.4, bsz=40, num_updates=1510, lr=1.96922e-05, gnorm=0.696, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1763
2023-02-19 21:41:00 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=114.8, bsz=40, num_updates=1520, lr=1.98226e-05, gnorm=0.627, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1774
2023-02-19 21:41:11 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.88, wpb=114.1, bsz=40, num_updates=1530, lr=1.99531e-05, gnorm=0.752, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1786
2023-02-19 21:41:22 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=1540, lr=2.00835e-05, gnorm=0.787, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1797
2023-02-19 21:41:24 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-19 21:41:34 - progress_bar.py[line:274] - INFO: epoch 001:   1551 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.4, ups=0.83, wpb=114.4, bsz=40, num_updates=1550, lr=2.02139e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=1809
2023-02-19 21:41:46 - progress_bar.py[line:274] - INFO: epoch 001:   1561 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.4, bsz=40, num_updates=1560, lr=2.03443e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1820
2023-02-19 21:41:57 - progress_bar.py[line:274] - INFO: epoch 001:   1571 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=111.8, bsz=40, num_updates=1570, lr=2.04747e-05, gnorm=0.748, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1831
2023-02-19 21:42:08 - progress_bar.py[line:274] - INFO: epoch 001:   1581 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.9, wpb=113.2, bsz=40, num_updates=1580, lr=2.06051e-05, gnorm=0.633, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1843
2023-02-19 21:42:19 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.8, bsz=40, num_updates=1590, lr=2.07355e-05, gnorm=0.807, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1854
2023-02-19 21:42:31 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.86, wpb=114.3, bsz=40, num_updates=1600, lr=2.08659e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1865
2023-02-19 21:42:42 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=113.3, bsz=40, num_updates=1610, lr=2.09963e-05, gnorm=0.606, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1876
2023-02-19 21:42:53 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.5, bsz=40, num_updates=1620, lr=2.11268e-05, gnorm=0.51, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1888
2023-02-19 21:43:04 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=1630, lr=2.12572e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1899
2023-02-19 21:43:15 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.91, wpb=113.7, bsz=40, num_updates=1640, lr=2.13876e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1910
2023-02-19 21:43:26 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.9, wpb=113.5, bsz=40, num_updates=1650, lr=2.1518e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1921
2023-02-19 21:43:37 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=112.8, bsz=40, num_updates=1660, lr=2.16484e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1932
2023-02-19 21:43:49 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=113.2, bsz=40, num_updates=1670, lr=2.17788e-05, gnorm=0.428, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1943
2023-02-19 21:44:01 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.8, ups=0.86, wpb=112.2, bsz=40, num_updates=1680, lr=2.19092e-05, gnorm=0.526, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1955
2023-02-19 21:44:12 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.87, wpb=113.8, bsz=40, num_updates=1690, lr=2.20396e-05, gnorm=0.676, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=1967
2023-02-19 21:44:24 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.88, wpb=114, bsz=40, num_updates=1700, lr=2.21701e-05, gnorm=0.624, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1978
2023-02-19 21:44:36 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=94.7, ups=0.83, wpb=113.5, bsz=40, num_updates=1710, lr=2.23005e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1990
2023-02-19 21:44:47 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.87, wpb=114.2, bsz=40, num_updates=1720, lr=2.24309e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2002
2023-02-19 21:44:59 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.1, ups=0.86, wpb=112.9, bsz=40, num_updates=1730, lr=2.25613e-05, gnorm=0.466, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2014
2023-02-19 21:45:10 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.87, wpb=114.1, bsz=40, num_updates=1740, lr=2.26917e-05, gnorm=0.692, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2025
2023-02-19 21:45:22 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.5, bsz=40, num_updates=1750, lr=2.28221e-05, gnorm=0.57, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2037
2023-02-19 21:45:33 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.9, wpb=114.6, bsz=40, num_updates=1760, lr=2.29525e-05, gnorm=0.689, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2048
2023-02-19 21:45:44 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.89, wpb=114.7, bsz=40, num_updates=1770, lr=2.30829e-05, gnorm=0.583, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2059
2023-02-19 21:45:56 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.2, bsz=40, num_updates=1780, lr=2.32134e-05, gnorm=0.439, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2070
2023-02-19 21:46:07 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.3, bsz=40, num_updates=1790, lr=2.33438e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2082
2023-02-19 21:46:18 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=113.6, bsz=40, num_updates=1800, lr=2.34742e-05, gnorm=0.566, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2093
2023-02-19 21:46:29 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=1810, lr=2.36046e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2104
2023-02-19 21:46:41 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.87, wpb=114.5, bsz=40, num_updates=1820, lr=2.3735e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2116
2023-02-19 21:46:53 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.86, wpb=114.2, bsz=40, num_updates=1830, lr=2.38654e-05, gnorm=0.549, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2128
2023-02-19 21:47:04 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=112.7, bsz=40, num_updates=1840, lr=2.39958e-05, gnorm=0.544, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2139
2023-02-19 21:47:15 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.91, wpb=114.3, bsz=40, num_updates=1850, lr=2.41262e-05, gnorm=0.581, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2150
2023-02-19 21:47:27 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=113.1, bsz=40, num_updates=1860, lr=2.42567e-05, gnorm=0.603, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=2161
2023-02-19 21:47:38 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=113.2, bsz=40, num_updates=1870, lr=2.43871e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2173
2023-02-19 21:47:49 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.87, wpb=113.7, bsz=40, num_updates=1880, lr=2.45175e-05, gnorm=0.477, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2184
2023-02-19 21:48:00 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.9, wpb=112.9, bsz=40, num_updates=1890, lr=2.46479e-05, gnorm=0.578, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2195
2023-02-19 21:48:12 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.87, wpb=114.4, bsz=40, num_updates=1900, lr=2.47783e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2207
2023-02-19 21:48:23 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.7, bsz=40, num_updates=1910, lr=2.49087e-05, gnorm=0.575, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2218
2023-02-19 21:48:34 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.88, wpb=113.5, bsz=40, num_updates=1920, lr=2.50391e-05, gnorm=0.593, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2229
2023-02-19 21:48:46 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=113.4, bsz=40, num_updates=1930, lr=2.51695e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2241
2023-02-19 21:48:57 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.91, wpb=114.3, bsz=40, num_updates=1940, lr=2.52999e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2252
2023-02-19 21:49:08 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=114.1, bsz=40, num_updates=1950, lr=2.54304e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2263
2023-02-19 21:49:19 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=1960, lr=2.55608e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2274
2023-02-19 21:49:31 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=114.1, bsz=40, num_updates=1970, lr=2.56912e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=2285
2023-02-19 21:49:42 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113.1, bsz=40, num_updates=1980, lr=2.58216e-05, gnorm=0.682, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2297
2023-02-19 21:49:53 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.87, wpb=113.5, bsz=40, num_updates=1990, lr=2.5952e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2308
2023-02-19 21:50:04 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.91, wpb=114.7, bsz=40, num_updates=2000, lr=2.60824e-05, gnorm=0.575, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2319
2023-02-19 21:50:04 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-19 21:50:04 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-19 21:50:06 - train.py[line:549] - INFO: 0 / 6234
2023-02-19 21:50:06 - train.py[line:551] - INFO: load:1.23 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-19 21:52:13 - train.py[line:549] - INFO: 200 / 6234
2023-02-19 21:52:13 - train.py[line:551] - INFO: load:1.26 valid_run:126.71 task_valid:123.27 collect_output:2.14
2023-02-19 21:54:15 - train.py[line:549] - INFO: 400 / 6234
2023-02-19 21:54:15 - train.py[line:551] - INFO: load:1.29 valid_run:248.99 task_valid:240.93 collect_output:5.46
2023-02-19 21:56:19 - train.py[line:549] - INFO: 600 / 6234
2023-02-19 21:56:19 - train.py[line:551] - INFO: load:1.32 valid_run:372.67 task_valid:357.81 collect_output:11.15
2023-02-19 21:58:21 - train.py[line:549] - INFO: 800 / 6234
2023-02-19 21:58:21 - train.py[line:551] - INFO: load:1.34 valid_run:494.66 task_valid:471.65 collect_output:18.22
2023-02-19 22:00:22 - train.py[line:549] - INFO: 1000 / 6234
2023-02-19 22:00:22 - train.py[line:551] - INFO: load:1.37 valid_run:615.18 task_valid:588.95 collect_output:20.34
2023-02-19 22:02:25 - train.py[line:549] - INFO: 1200 / 6234
2023-02-19 22:02:25 - train.py[line:551] - INFO: load:1.40 valid_run:738.21 task_valid:707.74 collect_output:23.48
2023-02-19 22:04:28 - train.py[line:549] - INFO: 1400 / 6234
2023-02-19 22:04:28 - train.py[line:551] - INFO: load:1.43 valid_run:861.34 task_valid:825.87 collect_output:27.38
2023-02-19 22:06:30 - train.py[line:549] - INFO: 1600 / 6234
2023-02-19 22:06:30 - train.py[line:551] - INFO: load:1.46 valid_run:983.41 task_valid:942.66 collect_output:31.54
2023-02-19 22:08:34 - train.py[line:549] - INFO: 1800 / 6234
2023-02-19 22:08:34 - train.py[line:551] - INFO: load:1.48 valid_run:1107.39 task_valid:1059.95 collect_output:37.16
2023-02-19 22:10:36 - train.py[line:549] - INFO: 2000 / 6234
2023-02-19 22:10:36 - train.py[line:551] - INFO: load:1.51 valid_run:1229.23 task_valid:1172.83 collect_output:45.02
2023-02-19 22:12:36 - train.py[line:549] - INFO: 2200 / 6234
2023-02-19 22:12:36 - train.py[line:551] - INFO: load:1.54 valid_run:1349.62 task_valid:1288.70 collect_output:48.46
2023-02-19 22:14:38 - train.py[line:549] - INFO: 2400 / 6234
2023-02-19 22:14:38 - train.py[line:551] - INFO: load:1.57 valid_run:1471.48 task_valid:1405.74 collect_output:52.21
2023-02-19 22:16:38 - train.py[line:549] - INFO: 2600 / 6234
2023-02-19 22:16:38 - train.py[line:551] - INFO: load:1.59 valid_run:1590.79 task_valid:1519.74 collect_output:56.43
2023-02-19 22:18:39 - train.py[line:549] - INFO: 2800 / 6234
2023-02-19 22:18:39 - train.py[line:551] - INFO: load:1.62 valid_run:1712.29 task_valid:1638.06 collect_output:58.51
2023-02-19 22:20:41 - train.py[line:549] - INFO: 3000 / 6234
2023-02-19 22:20:41 - train.py[line:551] - INFO: load:1.65 valid_run:1833.72 task_valid:1754.74 collect_output:62.12
2023-02-19 22:22:43 - train.py[line:549] - INFO: 3200 / 6234
2023-02-19 22:22:43 - train.py[line:551] - INFO: load:1.68 valid_run:1955.63 task_valid:1870.19 collect_output:67.48
2023-02-19 22:24:44 - train.py[line:549] - INFO: 3400 / 6234
2023-02-19 22:24:44 - train.py[line:551] - INFO: load:1.70 valid_run:2077.11 task_valid:1986.59 collect_output:71.48
2023-02-19 22:26:45 - train.py[line:549] - INFO: 3600 / 6234
2023-02-19 22:26:45 - train.py[line:551] - INFO: load:1.73 valid_run:2198.07 task_valid:2104.71 collect_output:73.26
2023-02-19 22:28:47 - train.py[line:549] - INFO: 3800 / 6234
2023-02-19 22:28:47 - train.py[line:551] - INFO: load:1.76 valid_run:2319.84 task_valid:2222.23 collect_output:76.44
2023-02-19 22:30:48 - train.py[line:549] - INFO: 4000 / 6234
2023-02-19 22:30:48 - train.py[line:551] - INFO: load:1.79 valid_run:2440.66 task_valid:2339.12 collect_output:79.30
2023-02-19 22:32:50 - train.py[line:549] - INFO: 4200 / 6234
2023-02-19 22:32:50 - train.py[line:551] - INFO: load:1.81 valid_run:2562.39 task_valid:2455.83 collect_output:83.27
2023-02-19 22:34:52 - train.py[line:549] - INFO: 4400 / 6234
2023-02-19 22:34:52 - train.py[line:551] - INFO: load:1.84 valid_run:2684.49 task_valid:2574.94 collect_output:85.20
2023-02-19 22:36:53 - train.py[line:549] - INFO: 4600 / 6234
2023-02-19 22:36:53 - train.py[line:551] - INFO: load:1.86 valid_run:2805.11 task_valid:2689.58 collect_output:90.12
2023-02-19 22:38:53 - train.py[line:549] - INFO: 4800 / 6234
2023-02-19 22:38:53 - train.py[line:551] - INFO: load:1.89 valid_run:2925.05 task_valid:2805.81 collect_output:92.78
2023-02-19 22:40:54 - train.py[line:549] - INFO: 5000 / 6234
2023-02-19 22:40:54 - train.py[line:551] - INFO: load:1.92 valid_run:3046.83 task_valid:2922.30 collect_output:97.01
2023-02-19 22:42:57 - train.py[line:549] - INFO: 5200 / 6234
2023-02-19 22:42:57 - train.py[line:551] - INFO: load:1.94 valid_run:3169.85 task_valid:3038.48 collect_output:102.76
2023-02-19 22:44:57 - train.py[line:549] - INFO: 5400 / 6234
2023-02-19 22:44:57 - train.py[line:551] - INFO: load:1.97 valid_run:3289.68 task_valid:3152.73 collect_output:107.29
2023-02-19 22:47:00 - train.py[line:549] - INFO: 5600 / 6234
2023-02-19 22:47:00 - train.py[line:551] - INFO: load:2.00 valid_run:3411.84 task_valid:3272.39 collect_output:108.72
2023-02-19 22:49:02 - train.py[line:549] - INFO: 5800 / 6234
2023-02-19 22:49:02 - train.py[line:551] - INFO: load:2.02 valid_run:3533.84 task_valid:3388.23 collect_output:113.80
2023-02-19 22:51:04 - train.py[line:549] - INFO: 6000 / 6234
2023-02-19 22:51:04 - train.py[line:551] - INFO: load:2.05 valid_run:3656.13 task_valid:3507.04 collect_output:116.20
2023-02-19 22:53:06 - train.py[line:549] - INFO: 6200 / 6234
2023-02-19 22:53:06 - train.py[line:551] - INFO: load:2.07 valid_run:3777.83 task_valid:3625.98 collect_output:117.88

====================================================================================================
SGG eval:     R @ 50: 0.3393;     R @ 100: 0.3897;     R @ 500: 0.4598;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1421;    mR @ 100: 0.2044;    mR @ 500: 0.2461;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1463) (covered in:0.0000) (covering:0.1429) (eating:0.5000) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5355) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.2917) (playing:0.0000) (riding:0.5059) (says:0.0000) (sitting on:0.4683) (standing on:0.5058) (using:0.1500) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.3393;     R @ 100: 0.3897;     R @ 500: 0.4598;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1421;    mR @ 100: 0.2044;    mR @ 500: 0.2461;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1463) (covered in:0.0000) (covering:0.1429) (eating:0.5000) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5355) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.2917) (playing:0.0000) (riding:0.5059) (says:0.0000) (sitting on:0.4683) (standing on:0.5058) (using:0.1500) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-19 22:53:36 - train.py[line:487] - INFO: 0.3896666666666666
2023-02-19 22:53:36 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-19 22:53:36 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.275 | loss_v1 0 | loss_v2 0 | nll_loss 0.106 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.389667 | ppl 1.08 | vqa_score 0.1678 | wps 117.8 | wpb 72 | bsz 24 | num_updates 2000
2023-02-19 22:53:36 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-02-19 22:53:36 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_2000.pt
2023-02-19 22:53:42 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_2000.pt
2023-02-19 22:53:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.3896666666666666) (writing took 11.001051433384418 seconds)
2023-02-19 22:53:59 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=113.8, bsz=40, num_updates=2010, lr=2.62128e-05, gnorm=0.722, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6154
2023-02-19 22:54:10 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 14203 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.91, wpb=113.6, bsz=40, num_updates=2020, lr=2.63432e-05, gnorm=0.67, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6165
2023-02-19 22:54:21 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.87, wpb=114, bsz=40, num_updates=2030, lr=2.64737e-05, gnorm=0.552, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6176
2023-02-19 22:54:33 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.5, bsz=40, num_updates=2040, lr=2.66041e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6187
2023-02-19 22:54:44 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114.1, bsz=40, num_updates=2050, lr=2.67345e-05, gnorm=0.537, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6199
2023-02-19 22:54:55 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=113.1, bsz=40, num_updates=2060, lr=2.68649e-05, gnorm=0.519, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6210
2023-02-19 22:55:06 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 14203 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.89, wpb=113.4, bsz=40, num_updates=2070, lr=2.69953e-05, gnorm=0.646, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6221
2023-02-19 22:55:18 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.6, bsz=40, num_updates=2080, lr=2.71257e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6233
2023-02-19 22:55:28 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.8, ups=0.94, wpb=112.5, bsz=40, num_updates=2090, lr=2.72561e-05, gnorm=0.679, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6243
2023-02-19 22:55:40 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=112.4, bsz=40, num_updates=2100, lr=2.73865e-05, gnorm=0.474, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6255
2023-02-19 22:55:51 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.89, wpb=114.6, bsz=40, num_updates=2110, lr=2.7517e-05, gnorm=0.507, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6266
2023-02-19 22:56:02 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.89, wpb=113.4, bsz=40, num_updates=2120, lr=2.76474e-05, gnorm=0.55, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6277
2023-02-19 22:56:13 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.8, bsz=40, num_updates=2130, lr=2.77778e-05, gnorm=0.462, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6288
2023-02-19 22:56:25 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=2140, lr=2.79082e-05, gnorm=0.507, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6300
2023-02-19 22:56:36 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.91, wpb=113.3, bsz=40, num_updates=2150, lr=2.80386e-05, gnorm=0.494, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6311
2023-02-19 22:56:47 - progress_bar.py[line:274] - INFO: epoch 001:   2161 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=112.9, bsz=40, num_updates=2160, lr=2.8169e-05, gnorm=0.507, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6322
2023-02-19 22:56:58 - progress_bar.py[line:274] - INFO: epoch 001:   2171 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=113.3, bsz=40, num_updates=2170, lr=2.82994e-05, gnorm=0.569, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6333
2023-02-19 22:57:09 - progress_bar.py[line:274] - INFO: epoch 001:   2181 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.9, ups=0.92, wpb=114.5, bsz=40, num_updates=2180, lr=2.84298e-05, gnorm=0.513, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6343
2023-02-19 22:57:19 - progress_bar.py[line:274] - INFO: epoch 001:   2191 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.92, wpb=112.7, bsz=40, num_updates=2190, lr=2.85603e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6354
2023-02-19 22:57:30 - progress_bar.py[line:274] - INFO: epoch 001:   2201 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.91, wpb=113.8, bsz=40, num_updates=2200, lr=2.86907e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6365
2023-02-19 22:57:42 - progress_bar.py[line:274] - INFO: epoch 001:   2211 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.9, bsz=40, num_updates=2210, lr=2.88211e-05, gnorm=0.587, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6377
2023-02-19 22:57:53 - progress_bar.py[line:274] - INFO: epoch 001:   2221 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.87, wpb=112.7, bsz=40, num_updates=2220, lr=2.89515e-05, gnorm=0.46, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6388
2023-02-19 22:58:05 - progress_bar.py[line:274] - INFO: epoch 001:   2231 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=112.8, bsz=40, num_updates=2230, lr=2.90819e-05, gnorm=0.657, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6400
2023-02-19 22:58:16 - progress_bar.py[line:274] - INFO: epoch 001:   2241 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.7, bsz=40, num_updates=2240, lr=2.92123e-05, gnorm=0.506, clip=10, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=6411
2023-02-19 22:58:27 - progress_bar.py[line:274] - INFO: epoch 001:   2251 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=114.2, bsz=40, num_updates=2250, lr=2.93427e-05, gnorm=0.554, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6422
2023-02-19 22:58:38 - progress_bar.py[line:274] - INFO: epoch 001:   2261 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=113.5, bsz=40, num_updates=2260, lr=2.94731e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6433
2023-02-19 22:58:50 - progress_bar.py[line:274] - INFO: epoch 001:   2271 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=112.3, bsz=40, num_updates=2270, lr=2.96035e-05, gnorm=0.534, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6445
2023-02-19 22:59:01 - progress_bar.py[line:274] - INFO: epoch 001:   2281 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=2280, lr=2.9734e-05, gnorm=0.707, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6455
2023-02-19 22:59:12 - progress_bar.py[line:274] - INFO: epoch 001:   2291 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.88, wpb=113.5, bsz=40, num_updates=2290, lr=2.98644e-05, gnorm=0.552, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6467
2023-02-19 22:59:23 - progress_bar.py[line:274] - INFO: epoch 001:   2301 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.9, ups=0.93, wpb=113.2, bsz=40, num_updates=2300, lr=2.99948e-05, gnorm=0.555, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6478
2023-02-19 22:59:34 - progress_bar.py[line:274] - INFO: epoch 001:   2311 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=113, bsz=40, num_updates=2310, lr=3.01252e-05, gnorm=0.423, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6489
2023-02-19 22:59:45 - progress_bar.py[line:274] - INFO: epoch 001:   2321 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.91, wpb=113.5, bsz=40, num_updates=2320, lr=3.02556e-05, gnorm=0.566, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6500
2023-02-19 22:59:56 - progress_bar.py[line:274] - INFO: epoch 001:   2331 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=114.5, bsz=40, num_updates=2330, lr=3.0386e-05, gnorm=0.599, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6511
2023-02-19 23:00:08 - progress_bar.py[line:274] - INFO: epoch 001:   2341 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.87, wpb=114.9, bsz=40, num_updates=2340, lr=3.05164e-05, gnorm=0.625, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6523
2023-02-19 23:00:19 - progress_bar.py[line:274] - INFO: epoch 001:   2351 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.91, wpb=113.7, bsz=40, num_updates=2350, lr=3.06468e-05, gnorm=0.579, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6534
2023-02-19 23:00:30 - progress_bar.py[line:274] - INFO: epoch 001:   2361 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=112.4, bsz=40, num_updates=2360, lr=3.07773e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6545
2023-02-19 23:00:41 - progress_bar.py[line:274] - INFO: epoch 001:   2371 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=114.3, bsz=40, num_updates=2370, lr=3.09077e-05, gnorm=0.41, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6556
2023-02-19 23:00:52 - progress_bar.py[line:274] - INFO: epoch 001:   2381 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.6, bsz=40, num_updates=2380, lr=3.10381e-05, gnorm=0.586, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6567
2023-02-19 23:01:04 - progress_bar.py[line:274] - INFO: epoch 001:   2391 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.88, wpb=114.6, bsz=40, num_updates=2390, lr=3.11685e-05, gnorm=0.452, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6579
2023-02-19 23:01:15 - progress_bar.py[line:274] - INFO: epoch 001:   2401 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=112.3, bsz=40, num_updates=2400, lr=3.12989e-05, gnorm=0.566, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6590
2023-02-19 23:01:26 - progress_bar.py[line:274] - INFO: epoch 001:   2411 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.89, wpb=113.8, bsz=40, num_updates=2410, lr=3.14293e-05, gnorm=0.656, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6601
2023-02-19 23:01:38 - progress_bar.py[line:274] - INFO: epoch 001:   2421 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.88, wpb=113.1, bsz=40, num_updates=2420, lr=3.15597e-05, gnorm=0.53, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6612
2023-02-19 23:01:49 - progress_bar.py[line:274] - INFO: epoch 001:   2431 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=2430, lr=3.16901e-05, gnorm=0.482, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6624
2023-02-19 23:02:00 - progress_bar.py[line:274] - INFO: epoch 001:   2441 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.9, wpb=113.6, bsz=40, num_updates=2440, lr=3.18206e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6635
2023-02-19 23:02:11 - progress_bar.py[line:274] - INFO: epoch 001:   2451 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.3, bsz=40, num_updates=2450, lr=3.1951e-05, gnorm=0.515, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6646
2023-02-19 23:02:23 - progress_bar.py[line:274] - INFO: epoch 001:   2461 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=113.3, bsz=40, num_updates=2460, lr=3.20814e-05, gnorm=0.582, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6657
2023-02-19 23:02:34 - progress_bar.py[line:274] - INFO: epoch 001:   2471 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.9, wpb=114.7, bsz=40, num_updates=2470, lr=3.22118e-05, gnorm=0.506, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6669
2023-02-19 23:02:45 - progress_bar.py[line:274] - INFO: epoch 001:   2481 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.88, wpb=112.7, bsz=40, num_updates=2480, lr=3.23422e-05, gnorm=0.57, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=6680
2023-02-19 23:02:56 - progress_bar.py[line:274] - INFO: epoch 001:   2491 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.89, wpb=115, bsz=40, num_updates=2490, lr=3.24726e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6691
2023-02-19 23:03:07 - progress_bar.py[line:274] - INFO: epoch 001:   2501 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=113.1, bsz=40, num_updates=2500, lr=3.2603e-05, gnorm=0.666, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6702
2023-02-19 23:03:18 - progress_bar.py[line:274] - INFO: epoch 001:   2511 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.6, ups=0.91, wpb=114.3, bsz=40, num_updates=2510, lr=3.27334e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6713
2023-02-19 23:03:29 - progress_bar.py[line:274] - INFO: epoch 001:   2521 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.91, wpb=113.9, bsz=40, num_updates=2520, lr=3.28638e-05, gnorm=0.555, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6724
2023-02-19 23:03:41 - progress_bar.py[line:274] - INFO: epoch 001:   2531 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.87, wpb=113.1, bsz=40, num_updates=2530, lr=3.29943e-05, gnorm=0.485, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6736
2023-02-19 23:03:52 - progress_bar.py[line:274] - INFO: epoch 001:   2541 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=113.1, bsz=40, num_updates=2540, lr=3.31247e-05, gnorm=0.505, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6747
2023-02-19 23:04:03 - progress_bar.py[line:274] - INFO: epoch 001:   2551 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=114, bsz=40, num_updates=2550, lr=3.32551e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6758
2023-02-19 23:04:14 - progress_bar.py[line:274] - INFO: epoch 001:   2561 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.88, wpb=112.8, bsz=40, num_updates=2560, lr=3.33855e-05, gnorm=0.724, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6769
2023-02-19 23:04:26 - progress_bar.py[line:274] - INFO: epoch 001:   2571 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.8, bsz=40, num_updates=2570, lr=3.35159e-05, gnorm=0.382, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6781
2023-02-19 23:04:37 - progress_bar.py[line:274] - INFO: epoch 001:   2581 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.92, wpb=112.8, bsz=40, num_updates=2580, lr=3.36463e-05, gnorm=0.57, clip=10, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6792
2023-02-19 23:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   2591 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.92, wpb=112.1, bsz=40, num_updates=2590, lr=3.37767e-05, gnorm=0.526, clip=10, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6802
2023-02-19 23:04:59 - progress_bar.py[line:274] - INFO: epoch 001:   2601 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.92, wpb=113.3, bsz=40, num_updates=2600, lr=3.39071e-05, gnorm=0.355, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6813
2023-02-19 23:05:09 - progress_bar.py[line:274] - INFO: epoch 001:   2611 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.91, wpb=114, bsz=40, num_updates=2610, lr=3.40376e-05, gnorm=0.503, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6824
2023-02-19 23:05:20 - progress_bar.py[line:274] - INFO: epoch 001:   2621 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.6, bsz=40, num_updates=2620, lr=3.4168e-05, gnorm=0.475, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6835
2023-02-19 23:05:32 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-19 23:05:33 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=90.6, ups=0.8, wpb=113.6, bsz=40, num_updates=2630, lr=3.42984e-05, gnorm=0.432, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6848
2023-02-19 23:05:44 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.87, wpb=114.1, bsz=40, num_updates=2640, lr=3.44288e-05, gnorm=0.48, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=6859
2023-02-19 23:05:56 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.9, wpb=113.6, bsz=40, num_updates=2650, lr=3.45592e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6870
2023-02-19 23:06:06 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.6, bsz=40, num_updates=2660, lr=3.46896e-05, gnorm=0.542, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6881
2023-02-19 23:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.9, wpb=114.1, bsz=40, num_updates=2670, lr=3.482e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6893
2023-02-19 23:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.8, bsz=40, num_updates=2680, lr=3.49504e-05, gnorm=0.507, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6904
2023-02-19 23:06:40 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=2690, lr=3.50809e-05, gnorm=0.472, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6915
2023-02-19 23:06:52 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=2700, lr=3.52113e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6927
2023-02-19 23:07:03 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.89, wpb=113.6, bsz=40, num_updates=2710, lr=3.53417e-05, gnorm=0.543, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6938
2023-02-19 23:07:14 - progress_bar.py[line:274] - INFO: epoch 001:   2722 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=2720, lr=3.54721e-05, gnorm=0.512, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6949
2023-02-19 23:07:25 - progress_bar.py[line:274] - INFO: epoch 001:   2732 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=113.9, bsz=40, num_updates=2730, lr=3.56025e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6960
2023-02-19 23:07:36 - progress_bar.py[line:274] - INFO: epoch 001:   2742 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.92, wpb=113.3, bsz=40, num_updates=2740, lr=3.57329e-05, gnorm=0.491, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6971
2023-02-19 23:07:48 - progress_bar.py[line:274] - INFO: epoch 001:   2752 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=115.1, nsentences=40, sample_size=115.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.86, wpb=115.1, bsz=40, num_updates=2750, lr=3.58633e-05, gnorm=0.55, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=6983
2023-02-19 23:07:59 - progress_bar.py[line:274] - INFO: epoch 001:   2762 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=2760, lr=3.59937e-05, gnorm=0.522, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6994
2023-02-19 23:08:10 - progress_bar.py[line:274] - INFO: epoch 001:   2772 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.9, wpb=113.3, bsz=40, num_updates=2770, lr=3.61242e-05, gnorm=0.52, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7005
2023-02-19 23:08:22 - progress_bar.py[line:274] - INFO: epoch 001:   2782 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=2780, lr=3.62546e-05, gnorm=0.515, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7016
2023-02-19 23:08:33 - progress_bar.py[line:274] - INFO: epoch 001:   2792 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.3, bsz=40, num_updates=2790, lr=3.6385e-05, gnorm=0.496, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7028
2023-02-19 23:08:44 - progress_bar.py[line:274] - INFO: epoch 001:   2802 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.9, wpb=113.9, bsz=40, num_updates=2800, lr=3.65154e-05, gnorm=0.353, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7039
2023-02-19 23:08:55 - progress_bar.py[line:274] - INFO: epoch 001:   2812 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=113.3, bsz=40, num_updates=2810, lr=3.66458e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7050
2023-02-19 23:09:07 - progress_bar.py[line:274] - INFO: epoch 001:   2822 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=114.2, bsz=40, num_updates=2820, lr=3.67762e-05, gnorm=0.366, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7061
2023-02-19 23:09:18 - progress_bar.py[line:274] - INFO: epoch 001:   2832 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=2830, lr=3.69066e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7073
2023-02-19 23:09:29 - progress_bar.py[line:274] - INFO: epoch 001:   2842 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=112.8, bsz=40, num_updates=2840, lr=3.7037e-05, gnorm=0.449, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7084
2023-02-19 23:09:40 - progress_bar.py[line:274] - INFO: epoch 001:   2852 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.9, ups=0.93, wpb=113, bsz=40, num_updates=2850, lr=3.71674e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7094
2023-02-19 23:09:51 - progress_bar.py[line:274] - INFO: epoch 001:   2862 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.3, bsz=40, num_updates=2860, lr=3.72979e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7106
2023-02-19 23:10:02 - progress_bar.py[line:274] - INFO: epoch 001:   2872 / 14203 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=115.4, nsentences=40, sample_size=115.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.4, ups=0.9, wpb=115.4, bsz=40, num_updates=2870, lr=3.74283e-05, gnorm=0.612, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7117
2023-02-19 23:10:13 - progress_bar.py[line:274] - INFO: epoch 001:   2882 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.89, wpb=112.1, bsz=40, num_updates=2880, lr=3.75587e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7128
2023-02-19 23:10:25 - progress_bar.py[line:274] - INFO: epoch 001:   2892 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.87, wpb=112.4, bsz=40, num_updates=2890, lr=3.76891e-05, gnorm=0.539, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7140
2023-02-19 23:10:35 - progress_bar.py[line:274] - INFO: epoch 001:   2902 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.93, wpb=112.7, bsz=40, num_updates=2900, lr=3.78195e-05, gnorm=0.616, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7150
2023-02-19 23:10:47 - progress_bar.py[line:274] - INFO: epoch 001:   2912 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.88, wpb=114.3, bsz=40, num_updates=2910, lr=3.79499e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7162
2023-02-19 23:10:58 - progress_bar.py[line:274] - INFO: epoch 001:   2922 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=114, bsz=40, num_updates=2920, lr=3.80803e-05, gnorm=0.376, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7173
2023-02-19 23:11:09 - progress_bar.py[line:274] - INFO: epoch 001:   2932 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.89, wpb=113.4, bsz=40, num_updates=2930, lr=3.82107e-05, gnorm=0.664, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7184
2023-02-19 23:11:20 - progress_bar.py[line:274] - INFO: epoch 001:   2942 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=114.1, bsz=40, num_updates=2940, lr=3.83412e-05, gnorm=0.473, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7195
2023-02-19 23:11:31 - progress_bar.py[line:274] - INFO: epoch 001:   2952 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104, ups=0.92, wpb=113.7, bsz=40, num_updates=2950, lr=3.84716e-05, gnorm=0.457, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7206
2023-02-19 23:11:42 - progress_bar.py[line:274] - INFO: epoch 001:   2962 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.6, bsz=40, num_updates=2960, lr=3.8602e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7217
2023-02-19 23:11:54 - progress_bar.py[line:274] - INFO: epoch 001:   2972 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.9, bsz=40, num_updates=2970, lr=3.87324e-05, gnorm=0.335, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7229
2023-02-19 23:12:05 - progress_bar.py[line:274] - INFO: epoch 001:   2982 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=114.2, bsz=40, num_updates=2980, lr=3.88628e-05, gnorm=0.609, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7240
2023-02-19 23:12:16 - progress_bar.py[line:274] - INFO: epoch 001:   2992 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.93, wpb=113.5, bsz=40, num_updates=2990, lr=3.89932e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7251
2023-02-19 23:12:27 - progress_bar.py[line:274] - INFO: epoch 001:   3002 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.91, wpb=113.4, bsz=40, num_updates=3000, lr=3.91236e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7261
2023-02-19 23:12:38 - progress_bar.py[line:274] - INFO: epoch 001:   3012 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.87, wpb=112.6, bsz=40, num_updates=3010, lr=3.9254e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7273
2023-02-19 23:12:49 - progress_bar.py[line:274] - INFO: epoch 001:   3022 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.93, wpb=113.6, bsz=40, num_updates=3020, lr=3.93845e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7284
2023-02-19 23:13:00 - progress_bar.py[line:274] - INFO: epoch 001:   3032 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.5, bsz=40, num_updates=3030, lr=3.95149e-05, gnorm=0.55, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7295
2023-02-19 23:13:11 - progress_bar.py[line:274] - INFO: epoch 001:   3042 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.88, wpb=113.5, bsz=40, num_updates=3040, lr=3.96453e-05, gnorm=0.436, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7306
2023-02-19 23:13:23 - progress_bar.py[line:274] - INFO: epoch 001:   3052 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.8, bsz=40, num_updates=3050, lr=3.97757e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7318
2023-02-19 23:13:34 - progress_bar.py[line:274] - INFO: epoch 001:   3062 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=114.1, bsz=40, num_updates=3060, lr=3.99061e-05, gnorm=0.394, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7329
2023-02-19 23:13:45 - progress_bar.py[line:274] - INFO: epoch 001:   3072 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113.1, bsz=40, num_updates=3070, lr=4.00365e-05, gnorm=0.426, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7340
2023-02-19 23:13:56 - progress_bar.py[line:274] - INFO: epoch 001:   3082 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=114.8, bsz=40, num_updates=3080, lr=4.01669e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7351
2023-02-19 23:14:07 - progress_bar.py[line:274] - INFO: epoch 001:   3092 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.88, wpb=114.2, bsz=40, num_updates=3090, lr=4.02973e-05, gnorm=0.364, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7362
2023-02-19 23:14:11 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-19 23:14:20 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=93.5, ups=0.83, wpb=113, bsz=40, num_updates=3100, lr=4.04278e-05, gnorm=0.418, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7374
2023-02-19 23:14:31 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.9, wpb=113.6, bsz=40, num_updates=3110, lr=4.05582e-05, gnorm=0.402, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7386
2023-02-19 23:14:42 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.9, wpb=113.6, bsz=40, num_updates=3120, lr=4.06886e-05, gnorm=0.472, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7397
2023-02-19 23:14:53 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=113, bsz=40, num_updates=3130, lr=4.0819e-05, gnorm=0.473, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7408
2023-02-19 23:15:04 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=112.7, bsz=40, num_updates=3140, lr=4.09494e-05, gnorm=0.425, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7419
2023-02-19 23:15:16 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.88, wpb=114.3, bsz=40, num_updates=3150, lr=4.10798e-05, gnorm=0.374, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7431
2023-02-19 23:15:27 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=112.5, bsz=40, num_updates=3160, lr=4.12102e-05, gnorm=0.479, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7442
2023-02-19 23:15:38 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.91, wpb=113.9, bsz=40, num_updates=3170, lr=4.13406e-05, gnorm=0.405, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7453
2023-02-19 23:15:49 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.92, wpb=112.8, bsz=40, num_updates=3180, lr=4.1471e-05, gnorm=0.376, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7464
2023-02-19 23:16:00 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.91, wpb=112.9, bsz=40, num_updates=3190, lr=4.16015e-05, gnorm=0.449, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7475
2023-02-19 23:16:11 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.89, wpb=113.9, bsz=40, num_updates=3200, lr=4.17319e-05, gnorm=0.542, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7486
2023-02-19 23:16:22 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.9, wpb=114.8, bsz=40, num_updates=3210, lr=4.18623e-05, gnorm=0.446, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7497
2023-02-19 23:16:33 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=3220, lr=4.19927e-05, gnorm=0.54, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7508
2023-02-19 23:16:45 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.87, wpb=114.4, bsz=40, num_updates=3230, lr=4.21231e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7519
2023-02-19 23:16:55 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=112.2, bsz=40, num_updates=3240, lr=4.22535e-05, gnorm=0.363, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7530
2023-02-19 23:17:06 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.8, bsz=40, num_updates=3250, lr=4.23839e-05, gnorm=0.462, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7541
2023-02-19 23:17:18 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.5, bsz=40, num_updates=3260, lr=4.25143e-05, gnorm=0.374, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7553
2023-02-19 23:17:29 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.91, wpb=114.8, bsz=40, num_updates=3270, lr=4.26448e-05, gnorm=0.34, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7564
2023-02-19 23:17:40 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113.1, bsz=40, num_updates=3280, lr=4.27752e-05, gnorm=0.521, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7575
2023-02-19 23:17:51 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.9, wpb=113.8, bsz=40, num_updates=3290, lr=4.29056e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7586
2023-02-19 23:18:02 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=113.8, bsz=40, num_updates=3300, lr=4.3036e-05, gnorm=0.338, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7597
2023-02-19 23:18:13 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.9, bsz=40, num_updates=3310, lr=4.31664e-05, gnorm=0.624, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7608
2023-02-19 23:18:25 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.4, bsz=40, num_updates=3320, lr=4.32968e-05, gnorm=0.484, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7619
2023-02-19 23:18:36 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=112.7, bsz=40, num_updates=3330, lr=4.34272e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7631
2023-02-19 23:18:47 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.2, ups=0.93, wpb=113.6, bsz=40, num_updates=3340, lr=4.35576e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7642
2023-02-19 23:18:58 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=115.8, nsentences=40, sample_size=115.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.88, wpb=115.8, bsz=40, num_updates=3350, lr=4.36881e-05, gnorm=0.329, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7653
2023-02-19 23:19:09 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=3360, lr=4.38185e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7664
2023-02-19 23:19:20 - progress_bar.py[line:274] - INFO: epoch 001:   3373 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=112.4, bsz=40, num_updates=3370, lr=4.39489e-05, gnorm=0.443, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7675
2023-02-19 23:19:32 - progress_bar.py[line:274] - INFO: epoch 001:   3383 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.6, ups=0.92, wpb=114.3, bsz=40, num_updates=3380, lr=4.40793e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7686
2023-02-19 23:19:43 - progress_bar.py[line:274] - INFO: epoch 001:   3393 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=3390, lr=4.42097e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7698
2023-02-19 23:19:54 - progress_bar.py[line:274] - INFO: epoch 001:   3403 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=112.7, bsz=40, num_updates=3400, lr=4.43401e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7709
2023-02-19 23:20:05 - progress_bar.py[line:274] - INFO: epoch 001:   3413 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=113.4, bsz=40, num_updates=3410, lr=4.44705e-05, gnorm=0.437, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7720
2023-02-19 23:20:16 - progress_bar.py[line:274] - INFO: epoch 001:   3423 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.93, wpb=112.3, bsz=40, num_updates=3420, lr=4.46009e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7731
2023-02-19 23:20:27 - progress_bar.py[line:274] - INFO: epoch 001:   3433 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=114, bsz=40, num_updates=3430, lr=4.47314e-05, gnorm=0.48, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7742
2023-02-19 23:20:38 - progress_bar.py[line:274] - INFO: epoch 001:   3443 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.2, bsz=40, num_updates=3440, lr=4.48618e-05, gnorm=0.57, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7753
2023-02-19 23:20:49 - progress_bar.py[line:274] - INFO: epoch 001:   3453 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.7, ups=0.93, wpb=114.1, bsz=40, num_updates=3450, lr=4.49922e-05, gnorm=0.478, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7764
2023-02-19 23:21:00 - progress_bar.py[line:274] - INFO: epoch 001:   3463 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.8, bsz=40, num_updates=3460, lr=4.51226e-05, gnorm=0.334, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7775
2023-02-19 23:21:12 - progress_bar.py[line:274] - INFO: epoch 001:   3473 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.9, wpb=115, bsz=40, num_updates=3470, lr=4.5253e-05, gnorm=0.456, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7786
2023-02-19 23:21:23 - progress_bar.py[line:274] - INFO: epoch 001:   3483 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=113.9, bsz=40, num_updates=3480, lr=4.53834e-05, gnorm=0.387, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7798
2023-02-19 23:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   3493 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=113, bsz=40, num_updates=3490, lr=4.55138e-05, gnorm=0.451, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7809
2023-02-19 23:21:45 - progress_bar.py[line:274] - INFO: epoch 001:   3503 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.5, bsz=40, num_updates=3500, lr=4.56442e-05, gnorm=0.41, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7820
2023-02-19 23:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   3513 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.88, wpb=113.3, bsz=40, num_updates=3510, lr=4.57746e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7832
2023-02-19 23:22:08 - progress_bar.py[line:274] - INFO: epoch 001:   3523 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.87, wpb=114.1, bsz=40, num_updates=3520, lr=4.59051e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7843
2023-02-19 23:22:19 - progress_bar.py[line:274] - INFO: epoch 001:   3533 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=112.5, bsz=40, num_updates=3530, lr=4.60355e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7854
2023-02-19 23:22:30 - progress_bar.py[line:274] - INFO: epoch 001:   3543 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.9, wpb=113.5, bsz=40, num_updates=3540, lr=4.61659e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7865
2023-02-19 23:22:42 - progress_bar.py[line:274] - INFO: epoch 001:   3553 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.6, bsz=40, num_updates=3550, lr=4.62963e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7877
2023-02-19 23:22:53 - progress_bar.py[line:274] - INFO: epoch 001:   3563 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=113, bsz=40, num_updates=3560, lr=4.64267e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7888
2023-02-19 23:23:04 - progress_bar.py[line:274] - INFO: epoch 001:   3573 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.91, wpb=113.9, bsz=40, num_updates=3570, lr=4.65571e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7899
2023-02-19 23:23:15 - progress_bar.py[line:274] - INFO: epoch 001:   3583 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.89, wpb=113, bsz=40, num_updates=3580, lr=4.66875e-05, gnorm=0.293, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7910
2023-02-19 23:23:26 - progress_bar.py[line:274] - INFO: epoch 001:   3593 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.4, bsz=40, num_updates=3590, lr=4.68179e-05, gnorm=0.388, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7921
2023-02-19 23:23:38 - progress_bar.py[line:274] - INFO: epoch 001:   3603 / 14203 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.87, wpb=111.9, bsz=40, num_updates=3600, lr=4.69484e-05, gnorm=0.535, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7932
2023-02-19 23:23:49 - progress_bar.py[line:274] - INFO: epoch 001:   3613 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113.2, bsz=40, num_updates=3610, lr=4.70788e-05, gnorm=0.436, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7944
2023-02-19 23:24:00 - progress_bar.py[line:274] - INFO: epoch 001:   3623 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.88, wpb=113.7, bsz=40, num_updates=3620, lr=4.72092e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7955
2023-02-19 23:24:11 - progress_bar.py[line:274] - INFO: epoch 001:   3633 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.89, wpb=113, bsz=40, num_updates=3630, lr=4.73396e-05, gnorm=0.308, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7966
2023-02-19 23:24:23 - progress_bar.py[line:274] - INFO: epoch 001:   3643 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.9, wpb=115.2, bsz=40, num_updates=3640, lr=4.747e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7977
2023-02-19 23:24:34 - progress_bar.py[line:274] - INFO: epoch 001:   3653 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=3650, lr=4.76004e-05, gnorm=0.483, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7989
2023-02-19 23:24:45 - progress_bar.py[line:274] - INFO: epoch 001:   3663 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.87, wpb=113.9, bsz=40, num_updates=3660, lr=4.77308e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8000
2023-02-19 23:24:57 - progress_bar.py[line:274] - INFO: epoch 001:   3673 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.9, wpb=114.5, bsz=40, num_updates=3670, lr=4.78612e-05, gnorm=0.431, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8011
2023-02-19 23:25:08 - progress_bar.py[line:274] - INFO: epoch 001:   3683 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.9, wpb=113.5, bsz=40, num_updates=3680, lr=4.79917e-05, gnorm=0.373, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8023
2023-02-19 23:25:18 - progress_bar.py[line:274] - INFO: epoch 001:   3693 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106, ups=0.93, wpb=113.5, bsz=40, num_updates=3690, lr=4.81221e-05, gnorm=0.448, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8033
2023-02-19 23:25:29 - progress_bar.py[line:274] - INFO: epoch 001:   3703 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.9, wpb=113.7, bsz=40, num_updates=3700, lr=4.82525e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8044
2023-02-19 23:25:41 - progress_bar.py[line:274] - INFO: epoch 001:   3713 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112, bsz=40, num_updates=3710, lr=4.83829e-05, gnorm=0.247, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8056
2023-02-19 23:25:52 - progress_bar.py[line:274] - INFO: epoch 001:   3723 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.7, bsz=40, num_updates=3720, lr=4.85133e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=8067
2023-02-19 23:26:03 - progress_bar.py[line:274] - INFO: epoch 001:   3733 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113.1, bsz=40, num_updates=3730, lr=4.86437e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8078
2023-02-19 23:26:14 - progress_bar.py[line:274] - INFO: epoch 001:   3743 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=113, bsz=40, num_updates=3740, lr=4.87741e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8089
2023-02-19 23:26:26 - progress_bar.py[line:274] - INFO: epoch 001:   3753 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.9, wpb=114.3, bsz=40, num_updates=3750, lr=4.89045e-05, gnorm=0.432, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8100
2023-02-19 23:26:37 - progress_bar.py[line:274] - INFO: epoch 001:   3763 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.9, wpb=114.3, bsz=40, num_updates=3760, lr=4.9035e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8111
2023-02-19 23:26:48 - progress_bar.py[line:274] - INFO: epoch 001:   3773 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=113, bsz=40, num_updates=3770, lr=4.91654e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8123
2023-02-19 23:26:59 - progress_bar.py[line:274] - INFO: epoch 001:   3783 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.3, bsz=40, num_updates=3780, lr=4.92958e-05, gnorm=0.459, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8134
2023-02-19 23:27:10 - progress_bar.py[line:274] - INFO: epoch 001:   3793 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.87, wpb=114.1, bsz=40, num_updates=3790, lr=4.94262e-05, gnorm=0.452, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8145
2023-02-19 23:27:22 - progress_bar.py[line:274] - INFO: epoch 001:   3803 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.9, wpb=114.5, bsz=40, num_updates=3800, lr=4.95566e-05, gnorm=0.406, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8156
2023-02-19 23:27:33 - progress_bar.py[line:274] - INFO: epoch 001:   3813 / 14203 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=115.4, nsentences=40, sample_size=115.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.87, wpb=115.4, bsz=40, num_updates=3810, lr=4.9687e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8168
2023-02-19 23:27:44 - progress_bar.py[line:274] - INFO: epoch 001:   3823 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.4, bsz=40, num_updates=3820, lr=4.98174e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8179
2023-02-19 23:27:55 - progress_bar.py[line:274] - INFO: epoch 001:   3833 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.91, wpb=113.9, bsz=40, num_updates=3830, lr=4.99478e-05, gnorm=0.425, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8190
2023-02-19 23:28:06 - progress_bar.py[line:274] - INFO: epoch 001:   3843 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.89, wpb=113.8, bsz=40, num_updates=3840, lr=4.99978e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8201
2023-02-19 23:28:17 - progress_bar.py[line:274] - INFO: epoch 001:   3853 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.1, ups=0.93, wpb=113.5, bsz=40, num_updates=3850, lr=4.99942e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8212
2023-02-19 23:28:28 - progress_bar.py[line:274] - INFO: epoch 001:   3863 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.9, wpb=113.1, bsz=40, num_updates=3860, lr=4.99906e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8223
2023-02-19 23:28:40 - progress_bar.py[line:274] - INFO: epoch 001:   3873 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.87, wpb=113.7, bsz=40, num_updates=3870, lr=4.9987e-05, gnorm=0.475, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8235
2023-02-19 23:28:51 - progress_bar.py[line:274] - INFO: epoch 001:   3883 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.88, wpb=113.4, bsz=40, num_updates=3880, lr=4.99834e-05, gnorm=0.385, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8246
2023-02-19 23:29:02 - progress_bar.py[line:274] - INFO: epoch 001:   3893 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=3890, lr=4.99797e-05, gnorm=0.337, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=8257
2023-02-19 23:29:13 - progress_bar.py[line:274] - INFO: epoch 001:   3903 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.9, wpb=114.5, bsz=40, num_updates=3900, lr=4.99761e-05, gnorm=0.399, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8268
2023-02-19 23:29:24 - progress_bar.py[line:274] - INFO: epoch 001:   3913 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.9, wpb=113.6, bsz=40, num_updates=3910, lr=4.99725e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8279
2023-02-19 23:29:36 - progress_bar.py[line:274] - INFO: epoch 001:   3923 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=113.3, bsz=40, num_updates=3920, lr=4.99689e-05, gnorm=0.267, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8291
2023-02-19 23:29:47 - progress_bar.py[line:274] - INFO: epoch 001:   3933 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.6, bsz=40, num_updates=3930, lr=4.99653e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8302
2023-02-19 23:29:58 - progress_bar.py[line:274] - INFO: epoch 001:   3943 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.91, wpb=112.4, bsz=40, num_updates=3940, lr=4.99616e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8313
2023-02-19 23:30:09 - progress_bar.py[line:274] - INFO: epoch 001:   3953 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=113.3, bsz=40, num_updates=3950, lr=4.9958e-05, gnorm=0.295, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8324
2023-02-19 23:30:21 - progress_bar.py[line:274] - INFO: epoch 001:   3963 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.3, bsz=40, num_updates=3960, lr=4.99544e-05, gnorm=0.398, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8335
2023-02-19 23:30:32 - progress_bar.py[line:274] - INFO: epoch 001:   3973 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=113.2, bsz=40, num_updates=3970, lr=4.99508e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8347
2023-02-19 23:30:43 - progress_bar.py[line:274] - INFO: epoch 001:   3983 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.91, wpb=112.7, bsz=40, num_updates=3980, lr=4.99472e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8357
2023-02-19 23:30:54 - progress_bar.py[line:274] - INFO: epoch 001:   3993 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.92, wpb=113.6, bsz=40, num_updates=3990, lr=4.99436e-05, gnorm=0.37, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8368
2023-02-19 23:31:05 - progress_bar.py[line:274] - INFO: epoch 001:   4003 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.87, wpb=113.9, bsz=40, num_updates=4000, lr=4.99399e-05, gnorm=0.35, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8380
2023-02-19 23:31:05 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-19 23:31:06 - train.py[line:549] - INFO: 0 / 6234
2023-02-19 23:31:06 - train.py[line:551] - INFO: load:1.22 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-19 23:33:10 - train.py[line:549] - INFO: 200 / 6234
2023-02-19 23:33:10 - train.py[line:551] - INFO: load:1.25 valid_run:123.32 task_valid:120.26 collect_output:1.97
2023-02-19 23:35:10 - train.py[line:549] - INFO: 400 / 6234
2023-02-19 23:35:10 - train.py[line:551] - INFO: load:1.27 valid_run:243.26 task_valid:236.21 collect_output:4.90
2023-02-19 23:37:13 - train.py[line:549] - INFO: 600 / 6234
2023-02-19 23:37:13 - train.py[line:551] - INFO: load:1.30 valid_run:365.90 task_valid:352.72 collect_output:9.98
2023-02-19 23:39:14 - train.py[line:549] - INFO: 800 / 6234
2023-02-19 23:39:14 - train.py[line:551] - INFO: load:1.32 valid_run:487.75 task_valid:466.61 collect_output:16.90
2023-02-19 23:41:15 - train.py[line:549] - INFO: 1000 / 6234
2023-02-19 23:41:15 - train.py[line:551] - INFO: load:1.35 valid_run:608.25 task_valid:584.06 collect_output:18.90
2023-02-19 23:43:18 - train.py[line:549] - INFO: 1200 / 6234
2023-02-19 23:43:18 - train.py[line:551] - INFO: load:1.37 valid_run:731.17 task_valid:702.75 collect_output:22.08
2023-02-19 23:45:21 - train.py[line:549] - INFO: 1400 / 6234
2023-02-19 23:45:21 - train.py[line:551] - INFO: load:1.40 valid_run:854.15 task_valid:820.91 collect_output:25.88
2023-02-19 23:47:23 - train.py[line:549] - INFO: 1600 / 6234
2023-02-19 23:47:23 - train.py[line:551] - INFO: load:1.43 valid_run:976.23 task_valid:937.58 collect_output:30.23
2023-02-19 23:49:27 - train.py[line:549] - INFO: 1800 / 6234
2023-02-19 23:49:27 - train.py[line:551] - INFO: load:1.45 valid_run:1099.99 task_valid:1054.86 collect_output:35.69
2023-02-19 23:51:29 - train.py[line:549] - INFO: 2000 / 6234
2023-02-19 23:51:29 - train.py[line:551] - INFO: load:1.48 valid_run:1221.77 task_valid:1167.49 collect_output:43.81
2023-02-19 23:53:29 - train.py[line:549] - INFO: 2200 / 6234
2023-02-19 23:53:29 - train.py[line:551] - INFO: load:1.51 valid_run:1342.09 task_valid:1283.34 collect_output:47.22
2023-02-19 23:55:31 - train.py[line:549] - INFO: 2400 / 6234
2023-02-19 23:55:31 - train.py[line:551] - INFO: load:1.53 valid_run:1463.82 task_valid:1400.49 collect_output:50.76
2023-02-19 23:57:30 - train.py[line:549] - INFO: 2600 / 6234
2023-02-19 23:57:30 - train.py[line:551] - INFO: load:1.56 valid_run:1582.85 task_valid:1514.47 collect_output:54.78
2023-02-19 23:59:31 - train.py[line:549] - INFO: 2800 / 6234
2023-02-19 23:59:31 - train.py[line:551] - INFO: load:1.59 valid_run:1703.91 task_valid:1632.36 collect_output:56.91
2023-02-20 00:01:32 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 00:01:32 - train.py[line:551] - INFO: load:1.61 valid_run:1825.05 task_valid:1748.74 collect_output:60.61
2023-02-20 00:03:34 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 00:03:34 - train.py[line:551] - INFO: load:1.64 valid_run:1946.33 task_valid:1862.73 collect_output:66.85
2023-02-20 00:05:35 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 00:05:35 - train.py[line:551] - INFO: load:1.67 valid_run:2067.78 task_valid:1979.00 collect_output:70.97
2023-02-20 00:07:36 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 00:07:36 - train.py[line:551] - INFO: load:1.69 valid_run:2188.56 task_valid:2097.06 collect_output:72.62
2023-02-20 00:09:37 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 00:09:37 - train.py[line:551] - INFO: load:1.72 valid_run:2309.91 task_valid:2214.17 collect_output:75.82
2023-02-20 00:11:38 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 00:11:38 - train.py[line:551] - INFO: load:1.75 valid_run:2430.40 task_valid:2330.98 collect_output:78.46
2023-02-20 00:13:40 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 00:13:40 - train.py[line:551] - INFO: load:1.77 valid_run:2552.28 task_valid:2447.73 collect_output:82.54
2023-02-20 00:15:42 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 00:15:42 - train.py[line:551] - INFO: load:1.80 valid_run:2674.47 task_valid:2566.89 collect_output:84.49
2023-02-20 00:17:43 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 00:17:43 - train.py[line:551] - INFO: load:1.83 valid_run:2794.94 task_valid:2681.38 collect_output:89.42
2023-02-20 00:19:43 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 00:19:43 - train.py[line:551] - INFO: load:1.85 valid_run:2914.81 task_valid:2797.70 collect_output:91.92
2023-02-20 00:21:44 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 00:21:44 - train.py[line:551] - INFO: load:1.88 valid_run:3036.57 task_valid:2914.26 collect_output:96.05
2023-02-20 00:23:48 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 00:23:48 - train.py[line:551] - INFO: load:1.91 valid_run:3159.69 task_valid:3030.44 collect_output:101.95
2023-02-20 00:25:47 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 00:25:47 - train.py[line:551] - INFO: load:1.93 valid_run:3279.43 task_valid:3144.92 collect_output:106.14
2023-02-20 00:27:50 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 00:27:50 - train.py[line:551] - INFO: load:1.96 valid_run:3401.51 task_valid:3264.61 collect_output:107.48
2023-02-20 00:29:52 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 00:29:52 - train.py[line:551] - INFO: load:1.99 valid_run:3523.57 task_valid:3380.38 collect_output:112.72
2023-02-20 00:31:54 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 00:31:54 - train.py[line:551] - INFO: load:2.02 valid_run:3645.71 task_valid:3499.05 collect_output:115.14
2023-02-20 00:33:55 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 00:33:55 - train.py[line:551] - INFO: load:2.04 valid_run:3767.06 task_valid:3617.83 collect_output:116.66

====================================================================================================
SGG eval:     R @ 50: 0.5644;     R @ 100: 0.5973;     R @ 500: 0.6503;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3480;    mR @ 100: 0.3822;    mR @ 500: 0.4496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.2500) (covering:0.3714) (eating:0.7059) (flying in:0.7273) (growing on:0.1250) (hanging from:0.4677) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8333) (playing:0.0000) (riding:0.9418) (says:0.0000) (sitting on:0.6137) (standing on:0.4862) (using:0.3000) (walking in:0.0000) (walking on:0.5000) (watching:0.3333) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5644;     R @ 100: 0.5973;     R @ 500: 0.6503;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3480;    mR @ 100: 0.3822;    mR @ 500: 0.4496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.2500) (covering:0.3714) (eating:0.7059) (flying in:0.7273) (growing on:0.1250) (hanging from:0.4677) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8333) (playing:0.0000) (riding:0.9418) (says:0.0000) (sitting on:0.6137) (standing on:0.4862) (using:0.3000) (walking in:0.0000) (walking on:0.5000) (watching:0.3333) 
--------------------------------------------------------
====================================================================================================

2023-02-20 00:34:26 - train.py[line:487] - INFO: 0.5973191749427043
2023-02-20 00:34:26 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 00:34:26 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.251 | loss_v1 0 | loss_v2 0 | nll_loss 0.085 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.597319 | ppl 1.06 | vqa_score 0.2703 | wps 118.1 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.597319
2023-02-20 00:34:26 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-02-20 00:34:26 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_4000.pt
2023-02-20 00:34:33 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_4000.pt
2023-02-20 00:34:39 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.5973191749427043) (writing took 12.301299387589097 seconds)
2023-02-20 00:34:50 - progress_bar.py[line:274] - INFO: epoch 001:   4013 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=113.6, bsz=40, num_updates=4010, lr=4.99363e-05, gnorm=0.385, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12205
2023-02-20 00:35:01 - progress_bar.py[line:274] - INFO: epoch 001:   4023 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.5, bsz=40, num_updates=4020, lr=4.99327e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12216
2023-02-20 00:35:12 - progress_bar.py[line:274] - INFO: epoch 001:   4033 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.7, bsz=40, num_updates=4030, lr=4.99291e-05, gnorm=0.334, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12227
2023-02-20 00:35:24 - progress_bar.py[line:274] - INFO: epoch 001:   4043 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.89, wpb=114, bsz=40, num_updates=4040, lr=4.99255e-05, gnorm=0.379, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12238
2023-02-20 00:35:35 - progress_bar.py[line:274] - INFO: epoch 001:   4053 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.87, wpb=113.3, bsz=40, num_updates=4050, lr=4.99219e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=12250
2023-02-20 00:35:46 - progress_bar.py[line:274] - INFO: epoch 001:   4063 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.6, bsz=40, num_updates=4060, lr=4.99182e-05, gnorm=0.432, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12261
2023-02-20 00:35:57 - progress_bar.py[line:274] - INFO: epoch 001:   4073 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=113.6, bsz=40, num_updates=4070, lr=4.99146e-05, gnorm=0.389, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12272
2023-02-20 00:36:08 - progress_bar.py[line:274] - INFO: epoch 001:   4083 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.3, ups=0.92, wpb=113.6, bsz=40, num_updates=4080, lr=4.9911e-05, gnorm=0.4, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12283
2023-02-20 00:36:20 - progress_bar.py[line:274] - INFO: epoch 001:   4093 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.1, bsz=40, num_updates=4090, lr=4.99074e-05, gnorm=0.376, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12295
2023-02-20 00:36:30 - progress_bar.py[line:274] - INFO: epoch 001:   4103 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.7, ups=0.93, wpb=112.8, bsz=40, num_updates=4100, lr=4.99038e-05, gnorm=0.416, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12305
2023-02-20 00:36:42 - progress_bar.py[line:274] - INFO: epoch 001:   4113 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.87, wpb=113.8, bsz=40, num_updates=4110, lr=4.99001e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12317
2023-02-20 00:36:53 - progress_bar.py[line:274] - INFO: epoch 001:   4123 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=113.8, bsz=40, num_updates=4120, lr=4.98965e-05, gnorm=0.263, clip=0, loss_scale=2048, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=12328
2023-02-20 00:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   4133 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.6, bsz=40, num_updates=4130, lr=4.98929e-05, gnorm=0.33, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12339
2023-02-20 00:37:15 - progress_bar.py[line:274] - INFO: epoch 001:   4143 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.6, bsz=40, num_updates=4140, lr=4.98893e-05, gnorm=0.314, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=12350
2023-02-20 00:37:26 - progress_bar.py[line:274] - INFO: epoch 001:   4153 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=4150, lr=4.98857e-05, gnorm=0.361, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12361
2023-02-20 00:37:37 - progress_bar.py[line:274] - INFO: epoch 001:   4163 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.9, wpb=112.9, bsz=40, num_updates=4160, lr=4.98821e-05, gnorm=0.356, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12372
2023-02-20 00:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   4173 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=112.4, bsz=40, num_updates=4170, lr=4.98784e-05, gnorm=0.322, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12383
2023-02-20 00:37:52 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 00:38:00 - progress_bar.py[line:274] - INFO: epoch 001:   4184 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.2, ups=0.83, wpb=114.1, bsz=40, num_updates=4180, lr=4.98748e-05, gnorm=0.355, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=12395
2023-02-20 00:38:12 - progress_bar.py[line:274] - INFO: epoch 001:   4194 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.9, wpb=113.4, bsz=40, num_updates=4190, lr=4.98712e-05, gnorm=0.455, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12406
2023-02-20 00:38:23 - progress_bar.py[line:274] - INFO: epoch 001:   4204 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.2, bsz=40, num_updates=4200, lr=4.98676e-05, gnorm=0.406, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12418
2023-02-20 00:38:34 - progress_bar.py[line:274] - INFO: epoch 001:   4214 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.89, wpb=113.5, bsz=40, num_updates=4210, lr=4.9864e-05, gnorm=0.329, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12429
2023-02-20 00:38:45 - progress_bar.py[line:274] - INFO: epoch 001:   4224 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.91, wpb=114.2, bsz=40, num_updates=4220, lr=4.98603e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12440
2023-02-20 00:38:56 - progress_bar.py[line:274] - INFO: epoch 001:   4234 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.87, wpb=112.6, bsz=40, num_updates=4230, lr=4.98567e-05, gnorm=0.436, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12451
2023-02-20 00:39:07 - progress_bar.py[line:274] - INFO: epoch 001:   4244 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=112.3, bsz=40, num_updates=4240, lr=4.98531e-05, gnorm=0.404, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=12462
2023-02-20 00:39:19 - progress_bar.py[line:274] - INFO: epoch 001:   4254 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=113, bsz=40, num_updates=4250, lr=4.98495e-05, gnorm=0.295, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12473
2023-02-20 00:39:30 - progress_bar.py[line:274] - INFO: epoch 001:   4264 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=112.8, bsz=40, num_updates=4260, lr=4.98459e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12484
2023-02-20 00:39:41 - progress_bar.py[line:274] - INFO: epoch 001:   4274 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=4270, lr=4.98423e-05, gnorm=0.3, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12496
2023-02-20 00:39:52 - progress_bar.py[line:274] - INFO: epoch 001:   4284 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.9, wpb=113.4, bsz=40, num_updates=4280, lr=4.98386e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12507
2023-02-20 00:40:03 - progress_bar.py[line:274] - INFO: epoch 001:   4294 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.92, wpb=113.1, bsz=40, num_updates=4290, lr=4.9835e-05, gnorm=0.573, clip=30, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=12518
2023-02-20 00:40:14 - progress_bar.py[line:274] - INFO: epoch 001:   4304 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.6, bsz=40, num_updates=4300, lr=4.98314e-05, gnorm=0.383, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12529
2023-02-20 00:40:25 - progress_bar.py[line:274] - INFO: epoch 001:   4314 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.6, ups=0.93, wpb=114.8, bsz=40, num_updates=4310, lr=4.98278e-05, gnorm=0.276, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12540
2023-02-20 00:40:36 - progress_bar.py[line:274] - INFO: epoch 001:   4324 / 14203 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=112.7, bsz=40, num_updates=4320, lr=4.98242e-05, gnorm=0.431, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12551
2023-02-20 00:40:48 - progress_bar.py[line:274] - INFO: epoch 001:   4334 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.87, wpb=114, bsz=40, num_updates=4330, lr=4.98205e-05, gnorm=0.341, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12563
2023-02-20 00:40:59 - progress_bar.py[line:274] - INFO: epoch 001:   4344 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.89, wpb=114.3, bsz=40, num_updates=4340, lr=4.98169e-05, gnorm=0.33, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12574
2023-02-20 00:41:10 - progress_bar.py[line:274] - INFO: epoch 001:   4354 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.3, bsz=40, num_updates=4350, lr=4.98133e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12585
2023-02-20 00:41:22 - progress_bar.py[line:274] - INFO: epoch 001:   4364 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=4360, lr=4.98097e-05, gnorm=0.3, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=12596
2023-02-20 00:41:33 - progress_bar.py[line:274] - INFO: epoch 001:   4374 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.4, bsz=40, num_updates=4370, lr=4.98061e-05, gnorm=0.258, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12608
2023-02-20 00:41:45 - progress_bar.py[line:274] - INFO: epoch 001:   4384 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=4380, lr=4.98025e-05, gnorm=0.425, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12620
2023-02-20 00:41:56 - progress_bar.py[line:274] - INFO: epoch 001:   4394 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=4390, lr=4.97988e-05, gnorm=0.343, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12631
2023-02-20 00:42:07 - progress_bar.py[line:274] - INFO: epoch 001:   4404 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.92, wpb=113.7, bsz=40, num_updates=4400, lr=4.97952e-05, gnorm=0.309, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12642
2023-02-20 00:42:18 - progress_bar.py[line:274] - INFO: epoch 001:   4414 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.4, bsz=40, num_updates=4410, lr=4.97916e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12653
2023-02-20 00:42:29 - progress_bar.py[line:274] - INFO: epoch 001:   4424 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=4420, lr=4.9788e-05, gnorm=0.404, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12664
2023-02-20 00:42:40 - progress_bar.py[line:274] - INFO: epoch 001:   4434 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=113.3, bsz=40, num_updates=4430, lr=4.97844e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12675
2023-02-20 00:42:52 - progress_bar.py[line:274] - INFO: epoch 001:   4444 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.87, wpb=114, bsz=40, num_updates=4440, lr=4.97807e-05, gnorm=0.394, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12687
2023-02-20 00:43:03 - progress_bar.py[line:274] - INFO: epoch 001:   4454 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.87, wpb=113.1, bsz=40, num_updates=4450, lr=4.97771e-05, gnorm=0.344, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=12698
2023-02-20 00:43:15 - progress_bar.py[line:274] - INFO: epoch 001:   4464 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.87, wpb=114.7, bsz=40, num_updates=4460, lr=4.97735e-05, gnorm=0.31, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12710
2023-02-20 00:43:26 - progress_bar.py[line:274] - INFO: epoch 001:   4474 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.87, wpb=111.5, bsz=40, num_updates=4470, lr=4.97699e-05, gnorm=0.355, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12721
2023-02-20 00:43:37 - progress_bar.py[line:274] - INFO: epoch 001:   4484 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106, ups=0.93, wpb=114.1, bsz=40, num_updates=4480, lr=4.97663e-05, gnorm=0.52, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12732
2023-02-20 00:43:48 - progress_bar.py[line:274] - INFO: epoch 001:   4494 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.4, ups=0.93, wpb=113.3, bsz=40, num_updates=4490, lr=4.97627e-05, gnorm=0.397, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12743
2023-02-20 00:43:59 - progress_bar.py[line:274] - INFO: epoch 001:   4504 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.6, bsz=40, num_updates=4500, lr=4.9759e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12754
2023-02-20 00:44:11 - progress_bar.py[line:274] - INFO: epoch 001:   4514 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.87, wpb=112.5, bsz=40, num_updates=4510, lr=4.97554e-05, gnorm=0.333, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12766
2023-02-20 00:44:22 - progress_bar.py[line:274] - INFO: epoch 001:   4524 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.9, wpb=113.4, bsz=40, num_updates=4520, lr=4.97518e-05, gnorm=0.34, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12777
2023-02-20 00:44:33 - progress_bar.py[line:274] - INFO: epoch 001:   4534 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.89, wpb=114.6, bsz=40, num_updates=4530, lr=4.97482e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12788
2023-02-20 00:44:44 - progress_bar.py[line:274] - INFO: epoch 001:   4544 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.87, wpb=112.5, bsz=40, num_updates=4540, lr=4.97446e-05, gnorm=0.37, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12799
2023-02-20 00:44:56 - progress_bar.py[line:274] - INFO: epoch 001:   4554 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112.9, bsz=40, num_updates=4550, lr=4.97409e-05, gnorm=0.541, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12810
2023-02-20 00:45:07 - progress_bar.py[line:274] - INFO: epoch 001:   4564 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.5, bsz=40, num_updates=4560, lr=4.97373e-05, gnorm=0.525, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12822
2023-02-20 00:45:18 - progress_bar.py[line:274] - INFO: epoch 001:   4574 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.4, ups=0.92, wpb=114, bsz=40, num_updates=4570, lr=4.97337e-05, gnorm=0.429, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12832
2023-02-20 00:45:29 - progress_bar.py[line:274] - INFO: epoch 001:   4584 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.88, wpb=114.1, bsz=40, num_updates=4580, lr=4.97301e-05, gnorm=0.362, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12844
2023-02-20 00:45:40 - progress_bar.py[line:274] - INFO: epoch 001:   4594 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=113, bsz=40, num_updates=4590, lr=4.97265e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12855
2023-02-20 00:45:51 - progress_bar.py[line:274] - INFO: epoch 001:   4604 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.93, wpb=113.4, bsz=40, num_updates=4600, lr=4.97229e-05, gnorm=0.337, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12866
2023-02-20 00:46:02 - progress_bar.py[line:274] - INFO: epoch 001:   4614 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.91, wpb=114.1, bsz=40, num_updates=4610, lr=4.97192e-05, gnorm=0.257, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12877
2023-02-20 00:46:13 - progress_bar.py[line:274] - INFO: epoch 001:   4624 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.88, wpb=113.9, bsz=40, num_updates=4620, lr=4.97156e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12888
2023-02-20 00:46:25 - progress_bar.py[line:274] - INFO: epoch 001:   4634 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.89, wpb=114.3, bsz=40, num_updates=4630, lr=4.9712e-05, gnorm=0.384, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12900
2023-02-20 00:46:36 - progress_bar.py[line:274] - INFO: epoch 001:   4644 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=113.2, bsz=40, num_updates=4640, lr=4.97084e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12911
2023-02-20 00:46:47 - progress_bar.py[line:274] - INFO: epoch 001:   4654 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.89, wpb=113.1, bsz=40, num_updates=4650, lr=4.97048e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12922
2023-02-20 00:46:59 - progress_bar.py[line:274] - INFO: epoch 001:   4664 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.86, wpb=113.1, bsz=40, num_updates=4660, lr=4.97011e-05, gnorm=0.406, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=12933
2023-02-20 00:47:10 - progress_bar.py[line:274] - INFO: epoch 001:   4674 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.7, bsz=40, num_updates=4670, lr=4.96975e-05, gnorm=0.287, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12945
2023-02-20 00:47:21 - progress_bar.py[line:274] - INFO: epoch 001:   4684 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=4680, lr=4.96939e-05, gnorm=0.47, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12956
2023-02-20 00:47:32 - progress_bar.py[line:274] - INFO: epoch 001:   4694 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.89, wpb=112.7, bsz=40, num_updates=4690, lr=4.96903e-05, gnorm=0.235, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12967
2023-02-20 00:47:43 - progress_bar.py[line:274] - INFO: epoch 001:   4704 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=114, bsz=40, num_updates=4700, lr=4.96867e-05, gnorm=0.271, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12978
2023-02-20 00:47:55 - progress_bar.py[line:274] - INFO: epoch 001:   4714 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.3, bsz=40, num_updates=4710, lr=4.96831e-05, gnorm=0.328, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12989
2023-02-20 00:47:58 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 00:48:07 - progress_bar.py[line:274] - INFO: epoch 001:   4725 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.3, ups=0.84, wpb=114.9, bsz=40, num_updates=4720, lr=4.96794e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13001
2023-02-20 00:48:18 - progress_bar.py[line:274] - INFO: epoch 001:   4735 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=112.8, bsz=40, num_updates=4730, lr=4.96758e-05, gnorm=0.413, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13012
2023-02-20 00:48:29 - progress_bar.py[line:274] - INFO: epoch 001:   4745 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=114.3, bsz=40, num_updates=4740, lr=4.96722e-05, gnorm=0.38, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13024
2023-02-20 00:48:40 - progress_bar.py[line:274] - INFO: epoch 001:   4755 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.1, bsz=40, num_updates=4750, lr=4.96686e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13035
2023-02-20 00:48:51 - progress_bar.py[line:274] - INFO: epoch 001:   4765 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.92, wpb=113.4, bsz=40, num_updates=4760, lr=4.9665e-05, gnorm=0.254, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13046
2023-02-20 00:49:02 - progress_bar.py[line:274] - INFO: epoch 001:   4775 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.88, wpb=113, bsz=40, num_updates=4770, lr=4.96614e-05, gnorm=0.428, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=13057
2023-02-20 00:49:14 - progress_bar.py[line:274] - INFO: epoch 001:   4785 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.9, wpb=113.2, bsz=40, num_updates=4780, lr=4.96577e-05, gnorm=0.357, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13068
2023-02-20 00:49:25 - progress_bar.py[line:274] - INFO: epoch 001:   4795 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.4, bsz=40, num_updates=4790, lr=4.96541e-05, gnorm=0.33, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13080
2023-02-20 00:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   4805 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.4, bsz=40, num_updates=4800, lr=4.96505e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13091
2023-02-20 00:49:47 - progress_bar.py[line:274] - INFO: epoch 001:   4815 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.89, wpb=114.8, bsz=40, num_updates=4810, lr=4.96469e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13102
2023-02-20 00:49:59 - progress_bar.py[line:274] - INFO: epoch 001:   4825 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.4, bsz=40, num_updates=4820, lr=4.96433e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13113
2023-02-20 00:50:10 - progress_bar.py[line:274] - INFO: epoch 001:   4835 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=114.1, bsz=40, num_updates=4830, lr=4.96396e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13125
2023-02-20 00:50:21 - progress_bar.py[line:274] - INFO: epoch 001:   4845 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.3, bsz=40, num_updates=4840, lr=4.9636e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13136
2023-02-20 00:50:32 - progress_bar.py[line:274] - INFO: epoch 001:   4855 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.89, wpb=114, bsz=40, num_updates=4850, lr=4.96324e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13147
2023-02-20 00:50:43 - progress_bar.py[line:274] - INFO: epoch 001:   4865 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.9, wpb=114.9, bsz=40, num_updates=4860, lr=4.96288e-05, gnorm=0.333, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13158
2023-02-20 00:50:54 - progress_bar.py[line:274] - INFO: epoch 001:   4875 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.5, bsz=40, num_updates=4870, lr=4.96252e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13169
2023-02-20 00:51:06 - progress_bar.py[line:274] - INFO: epoch 001:   4885 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.87, wpb=114.2, bsz=40, num_updates=4880, lr=4.96216e-05, gnorm=0.313, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=13181
2023-02-20 00:51:17 - progress_bar.py[line:274] - INFO: epoch 001:   4895 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.4, bsz=40, num_updates=4890, lr=4.96179e-05, gnorm=0.353, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13192
2023-02-20 00:51:28 - progress_bar.py[line:274] - INFO: epoch 001:   4905 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=4900, lr=4.96143e-05, gnorm=0.265, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13203
2023-02-20 00:51:39 - progress_bar.py[line:274] - INFO: epoch 001:   4915 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.7, bsz=40, num_updates=4910, lr=4.96107e-05, gnorm=0.267, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13214
2023-02-20 00:51:51 - progress_bar.py[line:274] - INFO: epoch 001:   4925 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=4920, lr=4.96071e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13226
2023-02-20 00:52:02 - progress_bar.py[line:274] - INFO: epoch 001:   4935 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.3, ups=0.92, wpb=114.7, bsz=40, num_updates=4930, lr=4.96035e-05, gnorm=0.254, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13237
2023-02-20 00:52:12 - progress_bar.py[line:274] - INFO: epoch 001:   4945 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.9, ups=0.94, wpb=112.3, bsz=40, num_updates=4940, lr=4.95998e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13247
2023-02-20 00:52:24 - progress_bar.py[line:274] - INFO: epoch 001:   4955 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.8, bsz=40, num_updates=4950, lr=4.95962e-05, gnorm=0.372, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13258
2023-02-20 00:52:35 - progress_bar.py[line:274] - INFO: epoch 001:   4965 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.6, bsz=40, num_updates=4960, lr=4.95926e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13270
2023-02-20 00:52:46 - progress_bar.py[line:274] - INFO: epoch 001:   4975 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=4970, lr=4.9589e-05, gnorm=0.328, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=13281
2023-02-20 00:52:57 - progress_bar.py[line:274] - INFO: epoch 001:   4985 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=4980, lr=4.95854e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13292
2023-02-20 00:53:08 - progress_bar.py[line:274] - INFO: epoch 001:   4995 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=114, bsz=40, num_updates=4990, lr=4.95818e-05, gnorm=0.274, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13303
2023-02-20 00:53:19 - progress_bar.py[line:274] - INFO: epoch 001:   5005 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.3, bsz=40, num_updates=5000, lr=4.95781e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=13314
2023-02-20 00:53:31 - progress_bar.py[line:274] - INFO: epoch 001:   5015 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.89, wpb=114.5, bsz=40, num_updates=5010, lr=4.95745e-05, gnorm=0.424, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13326
2023-02-20 00:53:42 - progress_bar.py[line:274] - INFO: epoch 001:   5025 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.91, wpb=112.9, bsz=40, num_updates=5020, lr=4.95709e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13337
2023-02-20 00:53:53 - progress_bar.py[line:274] - INFO: epoch 001:   5035 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.89, wpb=114.7, bsz=40, num_updates=5030, lr=4.95673e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13348
2023-02-20 00:54:04 - progress_bar.py[line:274] - INFO: epoch 001:   5045 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.7, bsz=40, num_updates=5040, lr=4.95637e-05, gnorm=0.351, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13359
2023-02-20 00:54:16 - progress_bar.py[line:274] - INFO: epoch 001:   5055 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.87, wpb=114.2, bsz=40, num_updates=5050, lr=4.956e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13370
2023-02-20 00:54:27 - progress_bar.py[line:274] - INFO: epoch 001:   5065 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.89, wpb=115, bsz=40, num_updates=5060, lr=4.95564e-05, gnorm=0.289, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13382
2023-02-20 00:54:38 - progress_bar.py[line:274] - INFO: epoch 001:   5075 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.4, bsz=40, num_updates=5070, lr=4.95528e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13393
2023-02-20 00:54:49 - progress_bar.py[line:274] - INFO: epoch 001:   5085 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.6, bsz=40, num_updates=5080, lr=4.95492e-05, gnorm=0.318, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13404
2023-02-20 00:55:00 - progress_bar.py[line:274] - INFO: epoch 001:   5095 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=114, bsz=40, num_updates=5090, lr=4.95456e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13415
2023-02-20 00:55:12 - progress_bar.py[line:274] - INFO: epoch 001:   5105 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=112.9, bsz=40, num_updates=5100, lr=4.9542e-05, gnorm=0.343, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13427
2023-02-20 00:55:23 - progress_bar.py[line:274] - INFO: epoch 001:   5115 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.6, bsz=40, num_updates=5110, lr=4.95383e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13438
2023-02-20 00:55:34 - progress_bar.py[line:274] - INFO: epoch 001:   5125 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113.5, bsz=40, num_updates=5120, lr=4.95347e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13449
2023-02-20 00:55:45 - progress_bar.py[line:274] - INFO: epoch 001:   5135 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=109.5, ups=0.97, wpb=113.3, bsz=40, num_updates=5130, lr=4.95311e-05, gnorm=0.256, clip=0, loss_scale=1024, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=13460
2023-02-20 00:55:56 - progress_bar.py[line:274] - INFO: epoch 001:   5145 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.6, bsz=40, num_updates=5140, lr=4.95275e-05, gnorm=0.288, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13471
2023-02-20 00:56:06 - progress_bar.py[line:274] - INFO: epoch 001:   5155 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.1, ups=0.93, wpb=113.5, bsz=40, num_updates=5150, lr=4.95239e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13481
2023-02-20 00:56:17 - progress_bar.py[line:274] - INFO: epoch 001:   5165 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.92, wpb=113.8, bsz=40, num_updates=5160, lr=4.95202e-05, gnorm=0.287, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13492
2023-02-20 00:56:29 - progress_bar.py[line:274] - INFO: epoch 001:   5175 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.86, wpb=113.3, bsz=40, num_updates=5170, lr=4.95166e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13504
2023-02-20 00:56:40 - progress_bar.py[line:274] - INFO: epoch 001:   5185 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=5180, lr=4.9513e-05, gnorm=0.307, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13515
2023-02-20 00:56:52 - progress_bar.py[line:274] - INFO: epoch 001:   5195 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.87, wpb=112.2, bsz=40, num_updates=5190, lr=4.95094e-05, gnorm=0.48, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13527
2023-02-20 00:57:03 - progress_bar.py[line:274] - INFO: epoch 001:   5205 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=114, bsz=40, num_updates=5200, lr=4.95058e-05, gnorm=0.274, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13538
2023-02-20 00:57:15 - progress_bar.py[line:274] - INFO: epoch 001:   5215 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.87, wpb=114.4, bsz=40, num_updates=5210, lr=4.95022e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13550
2023-02-20 00:57:26 - progress_bar.py[line:274] - INFO: epoch 001:   5225 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.88, wpb=114.4, bsz=40, num_updates=5220, lr=4.94985e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13561
2023-02-20 00:57:38 - progress_bar.py[line:274] - INFO: epoch 001:   5235 / 14203 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.8, bsz=40, num_updates=5230, lr=4.94949e-05, gnorm=0.381, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13572
2023-02-20 00:57:49 - progress_bar.py[line:274] - INFO: epoch 001:   5245 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.9, wpb=113.3, bsz=40, num_updates=5240, lr=4.94913e-05, gnorm=0.342, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13583
2023-02-20 00:58:00 - progress_bar.py[line:274] - INFO: epoch 001:   5255 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.8, bsz=40, num_updates=5250, lr=4.94877e-05, gnorm=0.359, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13595
2023-02-20 00:58:11 - progress_bar.py[line:274] - INFO: epoch 001:   5265 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.8, bsz=40, num_updates=5260, lr=4.94841e-05, gnorm=0.316, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=13606
2023-02-20 00:58:23 - progress_bar.py[line:274] - INFO: epoch 001:   5275 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.4, bsz=40, num_updates=5270, lr=4.94804e-05, gnorm=0.31, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13617
2023-02-20 00:58:34 - progress_bar.py[line:274] - INFO: epoch 001:   5285 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.3, bsz=40, num_updates=5280, lr=4.94768e-05, gnorm=0.282, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13629
2023-02-20 00:58:45 - progress_bar.py[line:274] - INFO: epoch 001:   5295 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.89, wpb=114.8, bsz=40, num_updates=5290, lr=4.94732e-05, gnorm=0.375, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13640
2023-02-20 00:58:56 - progress_bar.py[line:274] - INFO: epoch 001:   5305 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.7, bsz=40, num_updates=5300, lr=4.94696e-05, gnorm=0.352, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13651
2023-02-20 00:59:08 - progress_bar.py[line:274] - INFO: epoch 001:   5315 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=112.8, bsz=40, num_updates=5310, lr=4.9466e-05, gnorm=0.281, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13663
2023-02-20 00:59:19 - progress_bar.py[line:274] - INFO: epoch 001:   5325 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.89, wpb=113.7, bsz=40, num_updates=5320, lr=4.94624e-05, gnorm=0.297, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13674
2023-02-20 00:59:30 - progress_bar.py[line:274] - INFO: epoch 001:   5335 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.3, bsz=40, num_updates=5330, lr=4.94587e-05, gnorm=0.248, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13685
2023-02-20 00:59:40 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 00:59:43 - progress_bar.py[line:274] - INFO: epoch 001:   5346 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=90.8, ups=0.81, wpb=112.4, bsz=40, num_updates=5340, lr=4.94551e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=13698
2023-02-20 00:59:54 - progress_bar.py[line:274] - INFO: epoch 001:   5356 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=113, bsz=40, num_updates=5350, lr=4.94515e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13709
2023-02-20 01:00:05 - progress_bar.py[line:274] - INFO: epoch 001:   5366 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.87, wpb=112.8, bsz=40, num_updates=5360, lr=4.94479e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=13720
2023-02-20 01:00:16 - progress_bar.py[line:274] - INFO: epoch 001:   5376 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.8, bsz=40, num_updates=5370, lr=4.94443e-05, gnorm=0.319, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13731
2023-02-20 01:00:27 - progress_bar.py[line:274] - INFO: epoch 001:   5386 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=114, bsz=40, num_updates=5380, lr=4.94406e-05, gnorm=0.283, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13742
2023-02-20 01:00:38 - progress_bar.py[line:274] - INFO: epoch 001:   5396 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106, ups=0.93, wpb=113.6, bsz=40, num_updates=5390, lr=4.9437e-05, gnorm=0.394, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13753
2023-02-20 01:00:49 - progress_bar.py[line:274] - INFO: epoch 001:   5406 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=5400, lr=4.94334e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13764
2023-02-20 01:01:00 - progress_bar.py[line:274] - INFO: epoch 001:   5416 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.9, wpb=114.1, bsz=40, num_updates=5410, lr=4.94298e-05, gnorm=0.313, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13775
2023-02-20 01:01:11 - progress_bar.py[line:274] - INFO: epoch 001:   5426 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.8, ups=0.91, wpb=114.7, bsz=40, num_updates=5420, lr=4.94262e-05, gnorm=0.334, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13786
2023-02-20 01:01:23 - progress_bar.py[line:274] - INFO: epoch 001:   5436 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.88, wpb=113.9, bsz=40, num_updates=5430, lr=4.94226e-05, gnorm=0.274, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13798
2023-02-20 01:01:34 - progress_bar.py[line:274] - INFO: epoch 001:   5446 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.4, bsz=40, num_updates=5440, lr=4.94189e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13809
2023-02-20 01:01:45 - progress_bar.py[line:274] - INFO: epoch 001:   5456 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.4, bsz=40, num_updates=5450, lr=4.94153e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13820
2023-02-20 01:01:56 - progress_bar.py[line:274] - INFO: epoch 001:   5466 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.2, bsz=40, num_updates=5460, lr=4.94117e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13831
2023-02-20 01:02:07 - progress_bar.py[line:274] - INFO: epoch 001:   5476 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.5, bsz=40, num_updates=5470, lr=4.94081e-05, gnorm=0.303, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13842
2023-02-20 01:02:19 - progress_bar.py[line:274] - INFO: epoch 001:   5486 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.87, wpb=114, bsz=40, num_updates=5480, lr=4.94045e-05, gnorm=0.343, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13854
2023-02-20 01:02:29 - progress_bar.py[line:274] - INFO: epoch 001:   5496 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.6, ups=0.93, wpb=112.7, bsz=40, num_updates=5490, lr=4.94009e-05, gnorm=0.338, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13864
2023-02-20 01:02:41 - progress_bar.py[line:274] - INFO: epoch 001:   5506 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=113.7, bsz=40, num_updates=5500, lr=4.93972e-05, gnorm=0.344, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13876
2023-02-20 01:02:52 - progress_bar.py[line:274] - INFO: epoch 001:   5516 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=5510, lr=4.93936e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13887
2023-02-20 01:03:03 - progress_bar.py[line:274] - INFO: epoch 001:   5526 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.1, ups=0.93, wpb=113.2, bsz=40, num_updates=5520, lr=4.939e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13898
2023-02-20 01:03:14 - progress_bar.py[line:274] - INFO: epoch 001:   5536 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106, ups=0.93, wpb=114.2, bsz=40, num_updates=5530, lr=4.93864e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13908
2023-02-20 01:03:25 - progress_bar.py[line:274] - INFO: epoch 001:   5546 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=114.6, bsz=40, num_updates=5540, lr=4.93828e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13920
2023-02-20 01:03:36 - progress_bar.py[line:274] - INFO: epoch 001:   5556 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=112.8, bsz=40, num_updates=5550, lr=4.93791e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13931
2023-02-20 01:03:47 - progress_bar.py[line:274] - INFO: epoch 001:   5566 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.91, wpb=113.3, bsz=40, num_updates=5560, lr=4.93755e-05, gnorm=0.404, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13942
2023-02-20 01:03:58 - progress_bar.py[line:274] - INFO: epoch 001:   5576 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.89, wpb=114.6, bsz=40, num_updates=5570, lr=4.93719e-05, gnorm=0.314, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13953
2023-02-20 01:04:10 - progress_bar.py[line:274] - INFO: epoch 001:   5586 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.87, wpb=112.9, bsz=40, num_updates=5580, lr=4.93683e-05, gnorm=0.325, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13965
2023-02-20 01:04:21 - progress_bar.py[line:274] - INFO: epoch 001:   5596 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=112.9, bsz=40, num_updates=5590, lr=4.93647e-05, gnorm=0.258, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13976
2023-02-20 01:04:32 - progress_bar.py[line:274] - INFO: epoch 001:   5606 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=5600, lr=4.93611e-05, gnorm=0.201, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13987
2023-02-20 01:04:43 - progress_bar.py[line:274] - INFO: epoch 001:   5616 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=113.9, bsz=40, num_updates=5610, lr=4.93574e-05, gnorm=0.326, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13998
2023-02-20 01:04:54 - progress_bar.py[line:274] - INFO: epoch 001:   5626 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.92, wpb=112.9, bsz=40, num_updates=5620, lr=4.93538e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14009
2023-02-20 01:05:05 - progress_bar.py[line:274] - INFO: epoch 001:   5636 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.7, bsz=40, num_updates=5630, lr=4.93502e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14020
2023-02-20 01:05:16 - progress_bar.py[line:274] - INFO: epoch 001:   5646 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=114, bsz=40, num_updates=5640, lr=4.93466e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14031
2023-02-20 01:05:28 - progress_bar.py[line:274] - INFO: epoch 001:   5656 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.9, wpb=114.1, bsz=40, num_updates=5650, lr=4.9343e-05, gnorm=0.288, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14042
2023-02-20 01:05:39 - progress_bar.py[line:274] - INFO: epoch 001:   5666 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=113.4, bsz=40, num_updates=5660, lr=4.93393e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14054
2023-02-20 01:05:50 - progress_bar.py[line:274] - INFO: epoch 001:   5676 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.8, bsz=40, num_updates=5670, lr=4.93357e-05, gnorm=0.299, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14065
2023-02-20 01:06:01 - progress_bar.py[line:274] - INFO: epoch 001:   5686 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.9, wpb=113.7, bsz=40, num_updates=5680, lr=4.93321e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=14076
2023-02-20 01:06:12 - progress_bar.py[line:274] - INFO: epoch 001:   5696 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.8, bsz=40, num_updates=5690, lr=4.93285e-05, gnorm=0.303, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14087
2023-02-20 01:06:24 - progress_bar.py[line:274] - INFO: epoch 001:   5706 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.9, wpb=113.2, bsz=40, num_updates=5700, lr=4.93249e-05, gnorm=0.417, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14098
2023-02-20 01:06:35 - progress_bar.py[line:274] - INFO: epoch 001:   5716 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.4, bsz=40, num_updates=5710, lr=4.93213e-05, gnorm=0.276, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14109
2023-02-20 01:06:46 - progress_bar.py[line:274] - INFO: epoch 001:   5726 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.87, wpb=114.3, bsz=40, num_updates=5720, lr=4.93176e-05, gnorm=0.268, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14121
2023-02-20 01:06:57 - progress_bar.py[line:274] - INFO: epoch 001:   5736 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=5730, lr=4.9314e-05, gnorm=0.299, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14132
2023-02-20 01:07:09 - progress_bar.py[line:274] - INFO: epoch 001:   5746 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.8, bsz=40, num_updates=5740, lr=4.93104e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14144
2023-02-20 01:07:20 - progress_bar.py[line:274] - INFO: epoch 001:   5756 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.89, wpb=113.9, bsz=40, num_updates=5750, lr=4.93068e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14155
2023-02-20 01:07:31 - progress_bar.py[line:274] - INFO: epoch 001:   5766 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.92, wpb=112.8, bsz=40, num_updates=5760, lr=4.93032e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14166
2023-02-20 01:07:42 - progress_bar.py[line:274] - INFO: epoch 001:   5776 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=5770, lr=4.92995e-05, gnorm=0.35, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14177
2023-02-20 01:07:53 - progress_bar.py[line:274] - INFO: epoch 001:   5786 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.1, ups=0.93, wpb=114.3, bsz=40, num_updates=5780, lr=4.92959e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14188
2023-02-20 01:08:04 - progress_bar.py[line:274] - INFO: epoch 001:   5796 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.2, bsz=40, num_updates=5790, lr=4.92923e-05, gnorm=0.235, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14199
2023-02-20 01:08:15 - progress_bar.py[line:274] - INFO: epoch 001:   5806 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.9, wpb=114, bsz=40, num_updates=5800, lr=4.92887e-05, gnorm=0.31, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14210
2023-02-20 01:08:26 - progress_bar.py[line:274] - INFO: epoch 001:   5816 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.7, bsz=40, num_updates=5810, lr=4.92851e-05, gnorm=0.199, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14221
2023-02-20 01:08:37 - progress_bar.py[line:274] - INFO: epoch 001:   5826 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.9, bsz=40, num_updates=5820, lr=4.92815e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14232
2023-02-20 01:08:49 - progress_bar.py[line:274] - INFO: epoch 001:   5836 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.87, wpb=111.8, bsz=40, num_updates=5830, lr=4.92778e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14244
2023-02-20 01:09:00 - progress_bar.py[line:274] - INFO: epoch 001:   5846 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=112.5, bsz=40, num_updates=5840, lr=4.92742e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14255
2023-02-20 01:09:11 - progress_bar.py[line:274] - INFO: epoch 001:   5856 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.6, bsz=40, num_updates=5850, lr=4.92706e-05, gnorm=0.382, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14266
2023-02-20 01:09:22 - progress_bar.py[line:274] - INFO: epoch 001:   5866 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.3, ups=0.92, wpb=113.9, bsz=40, num_updates=5860, lr=4.9267e-05, gnorm=0.198, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14277
2023-02-20 01:09:34 - progress_bar.py[line:274] - INFO: epoch 001:   5876 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=113.1, bsz=40, num_updates=5870, lr=4.92634e-05, gnorm=0.38, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=14288
2023-02-20 01:09:45 - progress_bar.py[line:274] - INFO: epoch 001:   5886 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.3, bsz=40, num_updates=5880, lr=4.92597e-05, gnorm=0.259, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14300
2023-02-20 01:09:56 - progress_bar.py[line:274] - INFO: epoch 001:   5896 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=113.1, bsz=40, num_updates=5890, lr=4.92561e-05, gnorm=0.301, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14311
2023-02-20 01:10:07 - progress_bar.py[line:274] - INFO: epoch 001:   5906 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=115.5, nsentences=40, sample_size=115.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.9, wpb=115.5, bsz=40, num_updates=5900, lr=4.92525e-05, gnorm=0.267, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14322
2023-02-20 01:10:18 - progress_bar.py[line:274] - INFO: epoch 001:   5916 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.7, bsz=40, num_updates=5910, lr=4.92489e-05, gnorm=0.257, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14333
2023-02-20 01:10:30 - progress_bar.py[line:274] - INFO: epoch 001:   5926 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=114.1, bsz=40, num_updates=5920, lr=4.92453e-05, gnorm=0.306, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14345
2023-02-20 01:10:41 - progress_bar.py[line:274] - INFO: epoch 001:   5936 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.9, wpb=114.5, bsz=40, num_updates=5930, lr=4.92417e-05, gnorm=0.234, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14356
2023-02-20 01:10:52 - progress_bar.py[line:274] - INFO: epoch 001:   5946 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=114.1, bsz=40, num_updates=5940, lr=4.9238e-05, gnorm=0.282, clip=0, loss_scale=2048, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14367
2023-02-20 01:11:03 - progress_bar.py[line:274] - INFO: epoch 001:   5956 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.9, wpb=113.8, bsz=40, num_updates=5950, lr=4.92344e-05, gnorm=0.319, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14378
2023-02-20 01:11:15 - progress_bar.py[line:274] - INFO: epoch 001:   5966 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.88, wpb=114.9, bsz=40, num_updates=5960, lr=4.92308e-05, gnorm=0.325, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14390
2023-02-20 01:11:26 - progress_bar.py[line:274] - INFO: epoch 001:   5976 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.2, bsz=40, num_updates=5970, lr=4.92272e-05, gnorm=0.242, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14401
2023-02-20 01:11:28 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 01:11:38 - progress_bar.py[line:274] - INFO: epoch 001:   5987 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.5, ups=0.84, wpb=113.2, bsz=40, num_updates=5980, lr=4.92236e-05, gnorm=0.372, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=14413
2023-02-20 01:11:49 - progress_bar.py[line:274] - INFO: epoch 001:   5997 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=5990, lr=4.92199e-05, gnorm=0.313, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14424
2023-02-20 01:12:00 - progress_bar.py[line:274] - INFO: epoch 001:   6007 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.9, wpb=113.7, bsz=40, num_updates=6000, lr=4.92163e-05, gnorm=0.256, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14435
2023-02-20 01:12:00 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 01:12:01 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 01:12:01 - train.py[line:551] - INFO: load:0.84 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 01:14:04 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 01:14:04 - train.py[line:551] - INFO: load:0.87 valid_run:123.06 task_valid:120.17 collect_output:1.82
2023-02-20 01:16:04 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 01:16:04 - train.py[line:551] - INFO: load:0.89 valid_run:242.98 task_valid:236.09 collect_output:4.78
2023-02-20 01:18:06 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 01:18:06 - train.py[line:551] - INFO: load:0.92 valid_run:364.98 task_valid:352.68 collect_output:9.12
2023-02-20 01:20:08 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 01:20:08 - train.py[line:551] - INFO: load:0.94 valid_run:486.95 task_valid:466.58 collect_output:16.13
2023-02-20 01:22:09 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 01:22:09 - train.py[line:551] - INFO: load:0.97 valid_run:607.28 task_valid:583.72 collect_output:18.26
2023-02-20 01:24:12 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 01:24:12 - train.py[line:551] - INFO: load:1.00 valid_run:730.16 task_valid:702.32 collect_output:21.47
2023-02-20 01:26:15 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 01:26:15 - train.py[line:551] - INFO: load:1.02 valid_run:853.16 task_valid:820.35 collect_output:25.41
2023-02-20 01:28:17 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 01:28:17 - train.py[line:551] - INFO: load:1.05 valid_run:975.03 task_valid:936.85 collect_output:29.73
2023-02-20 01:30:20 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 01:30:20 - train.py[line:551] - INFO: load:1.08 valid_run:1098.55 task_valid:1054.01 collect_output:35.02
2023-02-20 01:32:22 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 01:32:22 - train.py[line:551] - INFO: load:1.10 valid_run:1220.12 task_valid:1166.47 collect_output:43.07
2023-02-20 01:34:22 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 01:34:22 - train.py[line:551] - INFO: load:1.13 valid_run:1340.12 task_valid:1281.80 collect_output:46.69
2023-02-20 01:36:24 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 01:36:24 - train.py[line:551] - INFO: load:1.16 valid_run:1461.69 task_valid:1398.62 collect_output:50.38
2023-02-20 01:38:23 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 01:38:23 - train.py[line:551] - INFO: load:1.18 valid_run:1580.58 task_valid:1512.30 collect_output:54.55
2023-02-20 01:40:23 - train.py[line:549] - INFO: 2800 / 6234
2023-02-20 01:40:23 - train.py[line:551] - INFO: load:1.21 valid_run:1701.48 task_valid:1629.89 collect_output:56.81
2023-02-20 01:42:24 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 01:42:24 - train.py[line:551] - INFO: load:1.24 valid_run:1822.34 task_valid:1745.80 collect_output:60.71
2023-02-20 01:44:25 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 01:44:25 - train.py[line:551] - INFO: load:1.26 valid_run:1943.22 task_valid:1859.56 collect_output:66.77
2023-02-20 01:46:27 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 01:46:27 - train.py[line:551] - INFO: load:1.29 valid_run:2064.45 task_valid:1975.48 collect_output:71.03
2023-02-20 01:48:27 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 01:48:27 - train.py[line:551] - INFO: load:1.32 valid_run:2185.18 task_valid:2093.37 collect_output:72.83
2023-02-20 01:50:29 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 01:50:29 - train.py[line:551] - INFO: load:1.34 valid_run:2306.32 task_valid:2210.17 collect_output:76.14
2023-02-20 01:52:29 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 01:52:29 - train.py[line:551] - INFO: load:1.37 valid_run:2426.56 task_valid:2326.54 collect_output:78.97
2023-02-20 01:54:31 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 01:54:31 - train.py[line:551] - INFO: load:1.40 valid_run:2548.12 task_valid:2442.91 collect_output:83.12
2023-02-20 01:56:32 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 01:56:32 - train.py[line:551] - INFO: load:1.43 valid_run:2669.94 task_valid:2561.63 collect_output:85.16
2023-02-20 01:58:33 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 01:58:33 - train.py[line:551] - INFO: load:1.45 valid_run:2790.22 task_valid:2675.80 collect_output:90.22
2023-02-20 02:00:33 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 02:00:33 - train.py[line:551] - INFO: load:1.48 valid_run:2909.99 task_valid:2791.78 collect_output:92.96
2023-02-20 02:02:34 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 02:02:34 - train.py[line:551] - INFO: load:1.51 valid_run:3031.53 task_valid:2907.70 collect_output:97.55
2023-02-20 02:04:37 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 02:04:37 - train.py[line:551] - INFO: load:1.53 valid_run:3154.44 task_valid:3023.53 collect_output:103.58
2023-02-20 02:06:37 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 02:06:37 - train.py[line:551] - INFO: load:1.56 valid_run:3273.93 task_valid:3137.44 collect_output:108.11
2023-02-20 02:08:38 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 02:08:38 - train.py[line:551] - INFO: load:1.59 valid_run:3395.63 task_valid:3256.60 collect_output:109.60
2023-02-20 02:10:40 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 02:10:40 - train.py[line:551] - INFO: load:1.61 valid_run:3517.22 task_valid:3371.89 collect_output:114.87
2023-02-20 02:12:42 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 02:12:42 - train.py[line:551] - INFO: load:1.64 valid_run:3639.04 task_valid:3490.13 collect_output:117.39
2023-02-20 02:14:43 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 02:14:43 - train.py[line:551] - INFO: load:1.67 valid_run:3760.08 task_valid:3608.49 collect_output:119.03

====================================================================================================
SGG eval:     R @ 50: 0.6311;     R @ 100: 0.6697;     R @ 500: 0.6963;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4279;    mR @ 100: 0.4673;    mR @ 500: 0.5087;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.4375) (covering:0.3714) (eating:0.8235) (flying in:0.9091) (growing on:0.3750) (hanging from:0.5097) (lying on:0.4000) (mounted on:0.0000) (painted on:0.0833) (parked on:1.0000) (playing:0.0000) (riding:0.9712) (says:0.0000) (sitting on:0.7029) (standing on:0.3893) (using:0.5000) (walking in:0.0000) (walking on:0.7162) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-02-20 02:15:14 - train.py[line:487] - INFO: 0.6696654952890246

====================================================================================================
SGG eval:     R @ 50: 0.6311;     R @ 100: 0.6697;     R @ 500: 0.6963;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4279;    mR @ 100: 0.4673;    mR @ 500: 0.5087;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.4375) (covering:0.3714) (eating:0.8235) (flying in:0.9091) (growing on:0.3750) (hanging from:0.5097) (lying on:0.4000) (mounted on:0.0000) (painted on:0.0833) (parked on:1.0000) (playing:0.0000) (riding:0.9712) (says:0.0000) (sitting on:0.7029) (standing on:0.3893) (using:0.5000) (walking in:0.0000) (walking on:0.7162) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-02-20 02:15:14 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 02:15:14 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.252 | loss_v1 0 | loss_v2 0 | nll_loss 0.089 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.669665 | ppl 1.06 | vqa_score 0.4707 | wps 118.3 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.669665
2023-02-20 02:15:14 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2023-02-20 02:15:14 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_6000.pt
2023-02-20 02:15:20 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_6000.pt
2023-02-20 02:15:25 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6696654952890246) (writing took 11.416097864508629 seconds)
2023-02-20 02:15:36 - progress_bar.py[line:274] - INFO: epoch 001:   6017 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=114.1, bsz=40, num_updates=6010, lr=4.92127e-05, gnorm=0.24, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18251
2023-02-20 02:15:48 - progress_bar.py[line:274] - INFO: epoch 001:   6027 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.3, bsz=40, num_updates=6020, lr=4.92091e-05, gnorm=0.276, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18262
2023-02-20 02:15:59 - progress_bar.py[line:274] - INFO: epoch 001:   6037 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.91, wpb=113.7, bsz=40, num_updates=6030, lr=4.92055e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18273
2023-02-20 02:16:10 - progress_bar.py[line:274] - INFO: epoch 001:   6047 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.9, wpb=114.2, bsz=40, num_updates=6040, lr=4.92019e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18285
2023-02-20 02:16:21 - progress_bar.py[line:274] - INFO: epoch 001:   6057 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=112.4, bsz=40, num_updates=6050, lr=4.91982e-05, gnorm=0.258, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18296
2023-02-20 02:16:32 - progress_bar.py[line:274] - INFO: epoch 001:   6067 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=6060, lr=4.91946e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18307
2023-02-20 02:16:43 - progress_bar.py[line:274] - INFO: epoch 001:   6077 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.9, wpb=113.5, bsz=40, num_updates=6070, lr=4.9191e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18318
2023-02-20 02:16:54 - progress_bar.py[line:274] - INFO: epoch 001:   6087 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.9, wpb=113.1, bsz=40, num_updates=6080, lr=4.91874e-05, gnorm=0.365, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18329
2023-02-20 02:17:05 - progress_bar.py[line:274] - INFO: epoch 001:   6097 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.3, bsz=40, num_updates=6090, lr=4.91838e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18340
2023-02-20 02:17:16 - progress_bar.py[line:274] - INFO: epoch 001:   6107 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.89, wpb=114.6, bsz=40, num_updates=6100, lr=4.91801e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18351
2023-02-20 02:17:27 - progress_bar.py[line:274] - INFO: epoch 001:   6117 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.91, wpb=113, bsz=40, num_updates=6110, lr=4.91765e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18362
2023-02-20 02:17:38 - progress_bar.py[line:274] - INFO: epoch 001:   6127 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.8, bsz=40, num_updates=6120, lr=4.91729e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18373
2023-02-20 02:17:50 - progress_bar.py[line:274] - INFO: epoch 001:   6137 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=114, bsz=40, num_updates=6130, lr=4.91693e-05, gnorm=0.283, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18385
2023-02-20 02:18:01 - progress_bar.py[line:274] - INFO: epoch 001:   6147 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.7, bsz=40, num_updates=6140, lr=4.91657e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18396
2023-02-20 02:18:13 - progress_bar.py[line:274] - INFO: epoch 001:   6157 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=114, bsz=40, num_updates=6150, lr=4.91621e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=18407
2023-02-20 02:18:24 - progress_bar.py[line:274] - INFO: epoch 001:   6167 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.2, bsz=40, num_updates=6160, lr=4.91584e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18418
2023-02-20 02:18:35 - progress_bar.py[line:274] - INFO: epoch 001:   6177 / 14203 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.91, wpb=114.4, bsz=40, num_updates=6170, lr=4.91548e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18429
2023-02-20 02:18:46 - progress_bar.py[line:274] - INFO: epoch 001:   6187 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.91, wpb=115, bsz=40, num_updates=6180, lr=4.91512e-05, gnorm=0.26, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=18440
2023-02-20 02:18:57 - progress_bar.py[line:274] - INFO: epoch 001:   6197 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.91, wpb=114.2, bsz=40, num_updates=6190, lr=4.91476e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18451
2023-02-20 02:19:08 - progress_bar.py[line:274] - INFO: epoch 001:   6207 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.3, bsz=40, num_updates=6200, lr=4.9144e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=18463
2023-02-20 02:19:19 - progress_bar.py[line:274] - INFO: epoch 001:   6217 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=6210, lr=4.91404e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18474
2023-02-20 02:19:30 - progress_bar.py[line:274] - INFO: epoch 001:   6227 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=112.3, bsz=40, num_updates=6220, lr=4.91367e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18485
2023-02-20 02:19:42 - progress_bar.py[line:274] - INFO: epoch 001:   6237 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.9, bsz=40, num_updates=6230, lr=4.91331e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18496
2023-02-20 02:19:53 - progress_bar.py[line:274] - INFO: epoch 001:   6247 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.2, bsz=40, num_updates=6240, lr=4.91295e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18508
2023-02-20 02:20:04 - progress_bar.py[line:274] - INFO: epoch 001:   6257 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107.1, ups=0.94, wpb=113.9, bsz=40, num_updates=6250, lr=4.91259e-05, gnorm=0.165, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18518
2023-02-20 02:20:15 - progress_bar.py[line:274] - INFO: epoch 001:   6267 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=113, bsz=40, num_updates=6260, lr=4.91223e-05, gnorm=0.316, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18529
2023-02-20 02:20:25 - progress_bar.py[line:274] - INFO: epoch 001:   6277 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106, ups=0.93, wpb=113.9, bsz=40, num_updates=6270, lr=4.91186e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18540
2023-02-20 02:20:37 - progress_bar.py[line:274] - INFO: epoch 001:   6287 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.88, wpb=114.1, bsz=40, num_updates=6280, lr=4.9115e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18552
2023-02-20 02:20:48 - progress_bar.py[line:274] - INFO: epoch 001:   6297 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.89, wpb=114.6, bsz=40, num_updates=6290, lr=4.91114e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18563
2023-02-20 02:20:59 - progress_bar.py[line:274] - INFO: epoch 001:   6307 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.91, wpb=113.8, bsz=40, num_updates=6300, lr=4.91078e-05, gnorm=0.337, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18574
2023-02-20 02:21:10 - progress_bar.py[line:274] - INFO: epoch 001:   6317 / 14203 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.9, wpb=113.7, bsz=40, num_updates=6310, lr=4.91042e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18585
2023-02-20 02:21:21 - progress_bar.py[line:274] - INFO: epoch 001:   6327 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=113.9, bsz=40, num_updates=6320, lr=4.91006e-05, gnorm=0.215, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18596
2023-02-20 02:21:32 - progress_bar.py[line:274] - INFO: epoch 001:   6337 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.1, bsz=40, num_updates=6330, lr=4.90969e-05, gnorm=0.339, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18607
2023-02-20 02:21:43 - progress_bar.py[line:274] - INFO: epoch 001:   6347 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.91, wpb=114.8, bsz=40, num_updates=6340, lr=4.90933e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18618
2023-02-20 02:21:54 - progress_bar.py[line:274] - INFO: epoch 001:   6357 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.9, wpb=113.7, bsz=40, num_updates=6350, lr=4.90897e-05, gnorm=0.328, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=18629
2023-02-20 02:22:05 - progress_bar.py[line:274] - INFO: epoch 001:   6367 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113.1, bsz=40, num_updates=6360, lr=4.90861e-05, gnorm=0.226, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18640
2023-02-20 02:22:17 - progress_bar.py[line:274] - INFO: epoch 001:   6377 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=6370, lr=4.90825e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=18652
2023-02-20 02:22:28 - progress_bar.py[line:274] - INFO: epoch 001:   6387 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.4, bsz=40, num_updates=6380, lr=4.90788e-05, gnorm=0.416, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18663
2023-02-20 02:22:39 - progress_bar.py[line:274] - INFO: epoch 001:   6397 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=113.9, bsz=40, num_updates=6390, lr=4.90752e-05, gnorm=0.267, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18674
2023-02-20 02:22:50 - progress_bar.py[line:274] - INFO: epoch 001:   6407 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=6400, lr=4.90716e-05, gnorm=0.323, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18685
2023-02-20 02:23:01 - progress_bar.py[line:274] - INFO: epoch 001:   6417 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.8, bsz=40, num_updates=6410, lr=4.9068e-05, gnorm=0.305, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18696
2023-02-20 02:23:12 - progress_bar.py[line:274] - INFO: epoch 001:   6427 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.5, bsz=40, num_updates=6420, lr=4.90644e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18707
2023-02-20 02:23:23 - progress_bar.py[line:274] - INFO: epoch 001:   6437 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.93, wpb=113.3, bsz=40, num_updates=6430, lr=4.90608e-05, gnorm=0.232, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18718
2023-02-20 02:23:35 - progress_bar.py[line:274] - INFO: epoch 001:   6447 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.87, wpb=112.1, bsz=40, num_updates=6440, lr=4.90571e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18729
2023-02-20 02:23:46 - progress_bar.py[line:274] - INFO: epoch 001:   6457 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.4, bsz=40, num_updates=6450, lr=4.90535e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18741
2023-02-20 02:23:57 - progress_bar.py[line:274] - INFO: epoch 001:   6467 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=113.3, bsz=40, num_updates=6460, lr=4.90499e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18752
2023-02-20 02:24:08 - progress_bar.py[line:274] - INFO: epoch 001:   6477 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.88, wpb=112.4, bsz=40, num_updates=6470, lr=4.90463e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18763
2023-02-20 02:24:19 - progress_bar.py[line:274] - INFO: epoch 001:   6487 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.6, ups=0.92, wpb=113.9, bsz=40, num_updates=6480, lr=4.90427e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18774
2023-02-20 02:24:31 - progress_bar.py[line:274] - INFO: epoch 001:   6497 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114.2, bsz=40, num_updates=6490, lr=4.9039e-05, gnorm=0.317, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18786
2023-02-20 02:24:42 - progress_bar.py[line:274] - INFO: epoch 001:   6507 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.2, bsz=40, num_updates=6500, lr=4.90354e-05, gnorm=0.307, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18797
2023-02-20 02:24:53 - progress_bar.py[line:274] - INFO: epoch 001:   6517 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.89, wpb=114, bsz=40, num_updates=6510, lr=4.90318e-05, gnorm=0.332, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18808
2023-02-20 02:25:04 - progress_bar.py[line:274] - INFO: epoch 001:   6527 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=113, bsz=40, num_updates=6520, lr=4.90282e-05, gnorm=0.27, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18819
2023-02-20 02:25:15 - progress_bar.py[line:274] - INFO: epoch 001:   6537 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.89, wpb=114.6, bsz=40, num_updates=6530, lr=4.90246e-05, gnorm=0.316, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18830
2023-02-20 02:25:27 - progress_bar.py[line:274] - INFO: epoch 001:   6547 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=6540, lr=4.9021e-05, gnorm=0.252, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=18842
2023-02-20 02:25:38 - progress_bar.py[line:274] - INFO: epoch 001:   6557 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.8, bsz=40, num_updates=6550, lr=4.90173e-05, gnorm=0.289, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18853
2023-02-20 02:25:49 - progress_bar.py[line:274] - INFO: epoch 001:   6567 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.86, wpb=113.9, bsz=40, num_updates=6560, lr=4.90137e-05, gnorm=0.244, clip=0, loss_scale=2048, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=18864
2023-02-20 02:26:01 - progress_bar.py[line:274] - INFO: epoch 001:   6577 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.87, wpb=113.9, bsz=40, num_updates=6570, lr=4.90101e-05, gnorm=0.305, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18876
2023-02-20 02:26:12 - progress_bar.py[line:274] - INFO: epoch 001:   6587 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=113.1, bsz=40, num_updates=6580, lr=4.90065e-05, gnorm=0.36, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18887
2023-02-20 02:26:23 - progress_bar.py[line:274] - INFO: epoch 001:   6597 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107.2, ups=0.94, wpb=113.6, bsz=40, num_updates=6590, lr=4.90029e-05, gnorm=0.367, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18897
2023-02-20 02:26:34 - progress_bar.py[line:274] - INFO: epoch 001:   6607 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=112, bsz=40, num_updates=6600, lr=4.89992e-05, gnorm=0.341, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18908
2023-02-20 02:26:45 - progress_bar.py[line:274] - INFO: epoch 001:   6617 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.91, wpb=114, bsz=40, num_updates=6610, lr=4.89956e-05, gnorm=0.346, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18920
2023-02-20 02:26:56 - progress_bar.py[line:274] - INFO: epoch 001:   6627 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.89, wpb=114.1, bsz=40, num_updates=6620, lr=4.8992e-05, gnorm=0.293, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18931
2023-02-20 02:27:07 - progress_bar.py[line:274] - INFO: epoch 001:   6637 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.88, wpb=114.6, bsz=40, num_updates=6630, lr=4.89884e-05, gnorm=0.24, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18942
2023-02-20 02:27:18 - progress_bar.py[line:274] - INFO: epoch 001:   6647 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=113.8, bsz=40, num_updates=6640, lr=4.89848e-05, gnorm=0.228, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18953
2023-02-20 02:27:29 - progress_bar.py[line:274] - INFO: epoch 001:   6657 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.88, wpb=111, bsz=40, num_updates=6650, lr=4.89812e-05, gnorm=0.24, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=18964
2023-02-20 02:27:41 - progress_bar.py[line:274] - INFO: epoch 001:   6667 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.2, bsz=40, num_updates=6660, lr=4.89775e-05, gnorm=0.247, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=18976
2023-02-20 02:27:52 - progress_bar.py[line:274] - INFO: epoch 001:   6677 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.86, wpb=114.7, bsz=40, num_updates=6670, lr=4.89739e-05, gnorm=0.23, clip=0, loss_scale=2048, train_wall=12, gb_free=11, ema_decay=0.9999, wall=18987
2023-02-20 02:28:04 - progress_bar.py[line:274] - INFO: epoch 001:   6687 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.87, wpb=113.6, bsz=40, num_updates=6680, lr=4.89703e-05, gnorm=0.258, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18999
2023-02-20 02:28:15 - progress_bar.py[line:274] - INFO: epoch 001:   6697 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.91, wpb=111.6, bsz=40, num_updates=6690, lr=4.89667e-05, gnorm=0.239, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19010
2023-02-20 02:28:26 - progress_bar.py[line:274] - INFO: epoch 001:   6707 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.4, bsz=40, num_updates=6700, lr=4.89631e-05, gnorm=0.24, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19021
2023-02-20 02:28:37 - progress_bar.py[line:274] - INFO: epoch 001:   6717 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.92, wpb=114.8, bsz=40, num_updates=6710, lr=4.89594e-05, gnorm=0.305, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19032
2023-02-20 02:28:48 - progress_bar.py[line:274] - INFO: epoch 001:   6727 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.1, ups=0.87, wpb=112.6, bsz=40, num_updates=6720, lr=4.89558e-05, gnorm=0.306, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19043
2023-02-20 02:29:00 - progress_bar.py[line:274] - INFO: epoch 001:   6737 / 14203 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.87, wpb=113.2, bsz=40, num_updates=6730, lr=4.89522e-05, gnorm=0.378, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19055
2023-02-20 02:29:11 - progress_bar.py[line:274] - INFO: epoch 001:   6747 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=114, bsz=40, num_updates=6740, lr=4.89486e-05, gnorm=0.274, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19066
2023-02-20 02:29:22 - progress_bar.py[line:274] - INFO: epoch 001:   6757 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.92, wpb=113.6, bsz=40, num_updates=6750, lr=4.8945e-05, gnorm=0.293, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19076
2023-02-20 02:29:33 - progress_bar.py[line:274] - INFO: epoch 001:   6767 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=112.9, bsz=40, num_updates=6760, lr=4.89414e-05, gnorm=0.228, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19088
2023-02-20 02:29:44 - progress_bar.py[line:274] - INFO: epoch 001:   6777 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.89, wpb=113, bsz=40, num_updates=6770, lr=4.89377e-05, gnorm=0.282, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19099
2023-02-20 02:29:55 - progress_bar.py[line:274] - INFO: epoch 001:   6787 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.9, wpb=111.5, bsz=40, num_updates=6780, lr=4.89341e-05, gnorm=0.264, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19110
2023-02-20 02:30:07 - progress_bar.py[line:274] - INFO: epoch 001:   6797 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.87, wpb=113.1, bsz=40, num_updates=6790, lr=4.89305e-05, gnorm=0.313, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19122
2023-02-20 02:30:18 - progress_bar.py[line:274] - INFO: epoch 001:   6807 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.91, wpb=113.9, bsz=40, num_updates=6800, lr=4.89269e-05, gnorm=0.332, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19133
2023-02-20 02:30:29 - progress_bar.py[line:274] - INFO: epoch 001:   6817 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.6, bsz=40, num_updates=6810, lr=4.89233e-05, gnorm=0.3, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19144
2023-02-20 02:30:40 - progress_bar.py[line:274] - INFO: epoch 001:   6827 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=6820, lr=4.89197e-05, gnorm=0.29, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19155
2023-02-20 02:30:52 - progress_bar.py[line:274] - INFO: epoch 001:   6837 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=113.1, bsz=40, num_updates=6830, lr=4.8916e-05, gnorm=0.237, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19167
2023-02-20 02:31:03 - progress_bar.py[line:274] - INFO: epoch 001:   6847 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.92, wpb=114.6, bsz=40, num_updates=6840, lr=4.89124e-05, gnorm=0.275, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19177
2023-02-20 02:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   6857 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.7, bsz=40, num_updates=6850, lr=4.89088e-05, gnorm=0.305, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19189
2023-02-20 02:31:25 - progress_bar.py[line:274] - INFO: epoch 001:   6867 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.8, ups=0.92, wpb=113.6, bsz=40, num_updates=6860, lr=4.89052e-05, gnorm=0.219, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19200
2023-02-20 02:31:36 - progress_bar.py[line:274] - INFO: epoch 001:   6877 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.3, bsz=40, num_updates=6870, lr=4.89016e-05, gnorm=0.286, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19211
2023-02-20 02:31:47 - progress_bar.py[line:274] - INFO: epoch 001:   6887 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.92, wpb=114.1, bsz=40, num_updates=6880, lr=4.88979e-05, gnorm=0.267, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19222
2023-02-20 02:31:59 - progress_bar.py[line:274] - INFO: epoch 001:   6897 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.87, wpb=113, bsz=40, num_updates=6890, lr=4.88943e-05, gnorm=0.328, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19233
2023-02-20 02:32:10 - progress_bar.py[line:274] - INFO: epoch 001:   6907 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=6900, lr=4.88907e-05, gnorm=0.335, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19245
2023-02-20 02:32:21 - progress_bar.py[line:274] - INFO: epoch 001:   6917 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=113, bsz=40, num_updates=6910, lr=4.88871e-05, gnorm=0.29, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19256
2023-02-20 02:32:32 - progress_bar.py[line:274] - INFO: epoch 001:   6927 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.87, wpb=113.9, bsz=40, num_updates=6920, lr=4.88835e-05, gnorm=0.328, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19267
2023-02-20 02:32:44 - progress_bar.py[line:274] - INFO: epoch 001:   6937 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.88, wpb=114.1, bsz=40, num_updates=6930, lr=4.88799e-05, gnorm=0.269, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19279
2023-02-20 02:32:55 - progress_bar.py[line:274] - INFO: epoch 001:   6947 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.6, bsz=40, num_updates=6940, lr=4.88762e-05, gnorm=0.209, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19290
2023-02-20 02:33:06 - progress_bar.py[line:274] - INFO: epoch 001:   6957 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.7, bsz=40, num_updates=6950, lr=4.88726e-05, gnorm=0.371, clip=10, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19301
2023-02-20 02:33:18 - progress_bar.py[line:274] - INFO: epoch 001:   6967 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=6960, lr=4.8869e-05, gnorm=0.319, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19312
2023-02-20 02:33:29 - progress_bar.py[line:274] - INFO: epoch 001:   6977 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113.1, bsz=40, num_updates=6970, lr=4.88654e-05, gnorm=0.3, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19324
2023-02-20 02:33:40 - progress_bar.py[line:274] - INFO: epoch 001:   6987 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=113.1, bsz=40, num_updates=6980, lr=4.88618e-05, gnorm=0.293, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19335
2023-02-20 02:33:51 - progress_bar.py[line:274] - INFO: epoch 001:   6997 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.91, wpb=113, bsz=40, num_updates=6990, lr=4.88581e-05, gnorm=0.277, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19346
2023-02-20 02:34:02 - progress_bar.py[line:274] - INFO: epoch 001:   7007 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=7000, lr=4.88545e-05, gnorm=0.355, clip=0, loss_scale=4096, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19357
2023-02-20 02:34:09 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0
2023-02-20 02:34:14 - progress_bar.py[line:274] - INFO: epoch 001:   7018 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.1, ups=0.85, wpb=113.3, bsz=40, num_updates=7010, lr=4.88509e-05, gnorm=0.25, clip=0, loss_scale=2048, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=19369
2023-02-20 02:34:25 - progress_bar.py[line:274] - INFO: epoch 001:   7028 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.7, bsz=40, num_updates=7020, lr=4.88473e-05, gnorm=0.284, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19380
2023-02-20 02:34:35 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 02:34:37 - progress_bar.py[line:274] - INFO: epoch 001:   7039 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=94.4, ups=0.83, wpb=114.1, bsz=40, num_updates=7030, lr=4.88437e-05, gnorm=0.235, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19392
2023-02-20 02:34:48 - progress_bar.py[line:274] - INFO: epoch 001:   7049 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.5, bsz=40, num_updates=7040, lr=4.88401e-05, gnorm=0.313, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19403
2023-02-20 02:35:00 - progress_bar.py[line:274] - INFO: epoch 001:   7059 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.9, bsz=40, num_updates=7050, lr=4.88364e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19414
2023-02-20 02:35:11 - progress_bar.py[line:274] - INFO: epoch 001:   7069 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.2, bsz=40, num_updates=7060, lr=4.88328e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19426
2023-02-20 02:35:22 - progress_bar.py[line:274] - INFO: epoch 001:   7079 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=113.7, bsz=40, num_updates=7070, lr=4.88292e-05, gnorm=0.271, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19437
2023-02-20 02:35:33 - progress_bar.py[line:274] - INFO: epoch 001:   7089 / 14203 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=7080, lr=4.88256e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19448
2023-02-20 02:35:44 - progress_bar.py[line:274] - INFO: epoch 001:   7099 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=114, bsz=40, num_updates=7090, lr=4.8822e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19459
2023-02-20 02:35:55 - progress_bar.py[line:274] - INFO: epoch 001:   7109 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.92, wpb=113.1, bsz=40, num_updates=7100, lr=4.88183e-05, gnorm=0.319, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19470
2023-02-20 02:36:06 - progress_bar.py[line:274] - INFO: epoch 001:   7119 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=112.8, bsz=40, num_updates=7110, lr=4.88147e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19481
2023-02-20 02:36:17 - progress_bar.py[line:274] - INFO: epoch 001:   7129 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.041, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.89, wpb=114.1, bsz=40, num_updates=7120, lr=4.88111e-05, gnorm=0.164, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19492
2023-02-20 02:36:28 - progress_bar.py[line:274] - INFO: epoch 001:   7139 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=113.3, bsz=40, num_updates=7130, lr=4.88075e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19503
2023-02-20 02:36:39 - progress_bar.py[line:274] - INFO: epoch 001:   7149 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=113.8, bsz=40, num_updates=7140, lr=4.88039e-05, gnorm=0.247, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19514
2023-02-20 02:36:51 - progress_bar.py[line:274] - INFO: epoch 001:   7159 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=113.9, bsz=40, num_updates=7150, lr=4.88003e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19526
2023-02-20 02:37:02 - progress_bar.py[line:274] - INFO: epoch 001:   7169 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.9, ups=0.93, wpb=113.9, bsz=40, num_updates=7160, lr=4.87966e-05, gnorm=0.235, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19536
2023-02-20 02:37:12 - progress_bar.py[line:274] - INFO: epoch 001:   7179 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.5, ups=0.92, wpb=114.8, bsz=40, num_updates=7170, lr=4.8793e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19547
2023-02-20 02:37:23 - progress_bar.py[line:274] - INFO: epoch 001:   7189 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.92, wpb=113.1, bsz=40, num_updates=7180, lr=4.87894e-05, gnorm=0.323, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19558
2023-02-20 02:37:34 - progress_bar.py[line:274] - INFO: epoch 001:   7199 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.4, bsz=40, num_updates=7190, lr=4.87858e-05, gnorm=0.276, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19569
2023-02-20 02:37:45 - progress_bar.py[line:274] - INFO: epoch 001:   7209 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.2, ups=0.93, wpb=113, bsz=40, num_updates=7200, lr=4.87822e-05, gnorm=0.266, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19580
2023-02-20 02:37:56 - progress_bar.py[line:274] - INFO: epoch 001:   7219 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.9, wpb=113.2, bsz=40, num_updates=7210, lr=4.87785e-05, gnorm=0.182, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19591
2023-02-20 02:38:08 - progress_bar.py[line:274] - INFO: epoch 001:   7229 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.87, wpb=114, bsz=40, num_updates=7220, lr=4.87749e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19603
2023-02-20 02:38:19 - progress_bar.py[line:274] - INFO: epoch 001:   7239 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105, ups=0.92, wpb=113.8, bsz=40, num_updates=7230, lr=4.87713e-05, gnorm=0.325, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19613
2023-02-20 02:38:29 - progress_bar.py[line:274] - INFO: epoch 001:   7249 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=113.8, bsz=40, num_updates=7240, lr=4.87677e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19624
2023-02-20 02:38:40 - progress_bar.py[line:274] - INFO: epoch 001:   7259 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.91, wpb=114.2, bsz=40, num_updates=7250, lr=4.87641e-05, gnorm=0.242, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19635
2023-02-20 02:38:52 - progress_bar.py[line:274] - INFO: epoch 001:   7269 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.87, wpb=112.6, bsz=40, num_updates=7260, lr=4.87605e-05, gnorm=0.328, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19647
2023-02-20 02:39:03 - progress_bar.py[line:274] - INFO: epoch 001:   7279 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112.4, bsz=40, num_updates=7270, lr=4.87568e-05, gnorm=0.317, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19658
2023-02-20 02:39:14 - progress_bar.py[line:274] - INFO: epoch 001:   7289 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=7280, lr=4.87532e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19669
2023-02-20 02:39:25 - progress_bar.py[line:274] - INFO: epoch 001:   7299 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=113.8, bsz=40, num_updates=7290, lr=4.87496e-05, gnorm=0.348, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19680
2023-02-20 02:39:36 - progress_bar.py[line:274] - INFO: epoch 001:   7309 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=113.8, bsz=40, num_updates=7300, lr=4.8746e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19691
2023-02-20 02:39:48 - progress_bar.py[line:274] - INFO: epoch 001:   7319 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=7310, lr=4.87424e-05, gnorm=0.272, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19702
2023-02-20 02:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   7329 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=7320, lr=4.87387e-05, gnorm=0.268, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=19714
2023-02-20 02:40:10 - progress_bar.py[line:274] - INFO: epoch 001:   7339 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.87, wpb=113.6, bsz=40, num_updates=7330, lr=4.87351e-05, gnorm=0.256, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19725
2023-02-20 02:40:21 - progress_bar.py[line:274] - INFO: epoch 001:   7349 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.93, wpb=113, bsz=40, num_updates=7340, lr=4.87315e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19736
2023-02-20 02:40:32 - progress_bar.py[line:274] - INFO: epoch 001:   7359 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.92, wpb=113.5, bsz=40, num_updates=7350, lr=4.87279e-05, gnorm=0.214, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19747
2023-02-20 02:40:43 - progress_bar.py[line:274] - INFO: epoch 001:   7369 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.92, wpb=113.2, bsz=40, num_updates=7360, lr=4.87243e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19758
2023-02-20 02:40:54 - progress_bar.py[line:274] - INFO: epoch 001:   7379 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.3, bsz=40, num_updates=7370, lr=4.87207e-05, gnorm=0.254, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19769
2023-02-20 02:41:05 - progress_bar.py[line:274] - INFO: epoch 001:   7389 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=113.4, bsz=40, num_updates=7380, lr=4.8717e-05, gnorm=0.232, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19780
2023-02-20 02:41:16 - progress_bar.py[line:274] - INFO: epoch 001:   7399 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.4, bsz=40, num_updates=7390, lr=4.87134e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19791
2023-02-20 02:41:28 - progress_bar.py[line:274] - INFO: epoch 001:   7409 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=113.4, bsz=40, num_updates=7400, lr=4.87098e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19802
2023-02-20 02:41:39 - progress_bar.py[line:274] - INFO: epoch 001:   7419 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.4, bsz=40, num_updates=7410, lr=4.87062e-05, gnorm=0.347, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19814
2023-02-20 02:41:50 - progress_bar.py[line:274] - INFO: epoch 001:   7429 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.8, bsz=40, num_updates=7420, lr=4.87026e-05, gnorm=0.231, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19825
2023-02-20 02:42:01 - progress_bar.py[line:274] - INFO: epoch 001:   7439 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.8, bsz=40, num_updates=7430, lr=4.86989e-05, gnorm=0.308, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19836
2023-02-20 02:42:12 - progress_bar.py[line:274] - INFO: epoch 001:   7449 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.3, ups=0.9, wpb=113.6, bsz=40, num_updates=7440, lr=4.86953e-05, gnorm=0.188, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19847
2023-02-20 02:42:23 - progress_bar.py[line:274] - INFO: epoch 001:   7459 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.4, bsz=40, num_updates=7450, lr=4.86917e-05, gnorm=0.228, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19858
2023-02-20 02:42:34 - progress_bar.py[line:274] - INFO: epoch 001:   7469 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.3, bsz=40, num_updates=7460, lr=4.86881e-05, gnorm=0.29, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19869
2023-02-20 02:42:46 - progress_bar.py[line:274] - INFO: epoch 001:   7479 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114, bsz=40, num_updates=7470, lr=4.86845e-05, gnorm=0.347, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19881
2023-02-20 02:42:57 - progress_bar.py[line:274] - INFO: epoch 001:   7489 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.88, wpb=114.4, bsz=40, num_updates=7480, lr=4.86809e-05, gnorm=0.383, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=19892
2023-02-20 02:43:09 - progress_bar.py[line:274] - INFO: epoch 001:   7499 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.88, wpb=113.1, bsz=40, num_updates=7490, lr=4.86772e-05, gnorm=0.35, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19903
2023-02-20 02:43:20 - progress_bar.py[line:274] - INFO: epoch 001:   7509 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.88, wpb=114.2, bsz=40, num_updates=7500, lr=4.86736e-05, gnorm=0.3, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19915
2023-02-20 02:43:31 - progress_bar.py[line:274] - INFO: epoch 001:   7519 / 14203 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.3, bsz=40, num_updates=7510, lr=4.867e-05, gnorm=0.303, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19926
2023-02-20 02:43:42 - progress_bar.py[line:274] - INFO: epoch 001:   7529 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.9, wpb=115, bsz=40, num_updates=7520, lr=4.86664e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19937
2023-02-20 02:43:53 - progress_bar.py[line:274] - INFO: epoch 001:   7539 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=112.8, bsz=40, num_updates=7530, lr=4.86628e-05, gnorm=0.287, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19948
2023-02-20 02:44:04 - progress_bar.py[line:274] - INFO: epoch 001:   7549 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.92, wpb=113, bsz=40, num_updates=7540, lr=4.86592e-05, gnorm=0.301, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19959
2023-02-20 02:44:13 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 02:44:17 - progress_bar.py[line:274] - INFO: epoch 001:   7560 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=89.5, ups=0.79, wpb=113, bsz=40, num_updates=7550, lr=4.86555e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=19972
2023-02-20 02:44:28 - progress_bar.py[line:274] - INFO: epoch 001:   7570 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=115.6, nsentences=40, sample_size=115.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.6, ups=0.91, wpb=115.6, bsz=40, num_updates=7560, lr=4.86519e-05, gnorm=0.254, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19983
2023-02-20 02:44:39 - progress_bar.py[line:274] - INFO: epoch 001:   7580 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.89, wpb=114.7, bsz=40, num_updates=7570, lr=4.86483e-05, gnorm=0.216, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19994
2023-02-20 02:44:50 - progress_bar.py[line:274] - INFO: epoch 001:   7590 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.8, bsz=40, num_updates=7580, lr=4.86447e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20005
2023-02-20 02:45:01 - progress_bar.py[line:274] - INFO: epoch 001:   7600 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104, ups=0.92, wpb=113.6, bsz=40, num_updates=7590, lr=4.86411e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20016
2023-02-20 02:45:12 - progress_bar.py[line:274] - INFO: epoch 001:   7610 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=112.7, bsz=40, num_updates=7600, lr=4.86374e-05, gnorm=0.213, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20027
2023-02-20 02:45:24 - progress_bar.py[line:274] - INFO: epoch 001:   7620 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.87, wpb=112.4, bsz=40, num_updates=7610, lr=4.86338e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20038
2023-02-20 02:45:34 - progress_bar.py[line:274] - INFO: epoch 001:   7630 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=7620, lr=4.86302e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20049
2023-02-20 02:45:45 - progress_bar.py[line:274] - INFO: epoch 001:   7640 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=7630, lr=4.86266e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20060
2023-02-20 02:45:56 - progress_bar.py[line:274] - INFO: epoch 001:   7650 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=7640, lr=4.8623e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20071
2023-02-20 02:46:08 - progress_bar.py[line:274] - INFO: epoch 001:   7660 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.4, bsz=40, num_updates=7650, lr=4.86194e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20082
2023-02-20 02:46:19 - progress_bar.py[line:274] - INFO: epoch 001:   7670 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.1, bsz=40, num_updates=7660, lr=4.86157e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20094
2023-02-20 02:46:30 - progress_bar.py[line:274] - INFO: epoch 001:   7680 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.7, bsz=40, num_updates=7670, lr=4.86121e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20105
2023-02-20 02:46:41 - progress_bar.py[line:274] - INFO: epoch 001:   7690 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.9, wpb=112.7, bsz=40, num_updates=7680, lr=4.86085e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20116
2023-02-20 02:46:52 - progress_bar.py[line:274] - INFO: epoch 001:   7700 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=7690, lr=4.86049e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20127
2023-02-20 02:47:03 - progress_bar.py[line:274] - INFO: epoch 001:   7710 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=7700, lr=4.86013e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20138
2023-02-20 02:47:15 - progress_bar.py[line:274] - INFO: epoch 001:   7720 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=7710, lr=4.85976e-05, gnorm=0.247, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=20149
2023-02-20 02:47:26 - progress_bar.py[line:274] - INFO: epoch 001:   7730 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=113.1, bsz=40, num_updates=7720, lr=4.8594e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20160
2023-02-20 02:47:37 - progress_bar.py[line:274] - INFO: epoch 001:   7740 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.1, bsz=40, num_updates=7730, lr=4.85904e-05, gnorm=0.276, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20172
2023-02-20 02:47:48 - progress_bar.py[line:274] - INFO: epoch 001:   7750 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=113.8, bsz=40, num_updates=7740, lr=4.85868e-05, gnorm=0.318, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20183
2023-02-20 02:47:59 - progress_bar.py[line:274] - INFO: epoch 001:   7760 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.4, bsz=40, num_updates=7750, lr=4.85832e-05, gnorm=0.232, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20194
2023-02-20 02:48:11 - progress_bar.py[line:274] - INFO: epoch 001:   7770 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.2, bsz=40, num_updates=7760, lr=4.85796e-05, gnorm=0.336, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20205
2023-02-20 02:48:22 - progress_bar.py[line:274] - INFO: epoch 001:   7780 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.8, bsz=40, num_updates=7770, lr=4.85759e-05, gnorm=0.235, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20217
2023-02-20 02:48:33 - progress_bar.py[line:274] - INFO: epoch 001:   7790 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.6, bsz=40, num_updates=7780, lr=4.85723e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20228
2023-02-20 02:48:44 - progress_bar.py[line:274] - INFO: epoch 001:   7800 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=112.4, bsz=40, num_updates=7790, lr=4.85687e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20239
2023-02-20 02:48:55 - progress_bar.py[line:274] - INFO: epoch 001:   7810 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=108.1, ups=0.94, wpb=114.7, bsz=40, num_updates=7800, lr=4.85651e-05, gnorm=0.307, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20250
2023-02-20 02:49:06 - progress_bar.py[line:274] - INFO: epoch 001:   7820 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.9, wpb=113.4, bsz=40, num_updates=7810, lr=4.85615e-05, gnorm=0.287, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20261
2023-02-20 02:49:17 - progress_bar.py[line:274] - INFO: epoch 001:   7830 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=113.1, bsz=40, num_updates=7820, lr=4.85578e-05, gnorm=0.272, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20272
2023-02-20 02:49:28 - progress_bar.py[line:274] - INFO: epoch 001:   7840 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.88, wpb=113.2, bsz=40, num_updates=7830, lr=4.85542e-05, gnorm=0.334, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=20283
2023-02-20 02:49:39 - progress_bar.py[line:274] - INFO: epoch 001:   7850 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.91, wpb=114.2, bsz=40, num_updates=7840, lr=4.85506e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20294
2023-02-20 02:49:51 - progress_bar.py[line:274] - INFO: epoch 001:   7860 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=112.1, bsz=40, num_updates=7850, lr=4.8547e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20305
2023-02-20 02:50:02 - progress_bar.py[line:274] - INFO: epoch 001:   7870 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113.2, bsz=40, num_updates=7860, lr=4.85434e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20316
2023-02-20 02:50:13 - progress_bar.py[line:274] - INFO: epoch 001:   7880 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.4, bsz=40, num_updates=7870, lr=4.85398e-05, gnorm=0.361, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20327
2023-02-20 02:50:24 - progress_bar.py[line:274] - INFO: epoch 001:   7890 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=114.1, bsz=40, num_updates=7880, lr=4.85361e-05, gnorm=0.271, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20339
2023-02-20 02:50:35 - progress_bar.py[line:274] - INFO: epoch 001:   7900 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.87, wpb=113.4, bsz=40, num_updates=7890, lr=4.85325e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20350
2023-02-20 02:50:47 - progress_bar.py[line:274] - INFO: epoch 001:   7910 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.87, wpb=113.6, bsz=40, num_updates=7900, lr=4.85289e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20362
2023-02-20 02:50:58 - progress_bar.py[line:274] - INFO: epoch 001:   7920 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114, bsz=40, num_updates=7910, lr=4.85253e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20373
2023-02-20 02:51:09 - progress_bar.py[line:274] - INFO: epoch 001:   7930 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=114.1, bsz=40, num_updates=7920, lr=4.85217e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20384
2023-02-20 02:51:21 - progress_bar.py[line:274] - INFO: epoch 001:   7940 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.4, bsz=40, num_updates=7930, lr=4.8518e-05, gnorm=0.242, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20396
2023-02-20 02:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   7950 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=112.7, bsz=40, num_updates=7940, lr=4.85144e-05, gnorm=0.34, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20407
2023-02-20 02:51:43 - progress_bar.py[line:274] - INFO: epoch 001:   7960 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=7950, lr=4.85108e-05, gnorm=0.257, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=20418
2023-02-20 02:51:54 - progress_bar.py[line:274] - INFO: epoch 001:   7970 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=7960, lr=4.85072e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20429
2023-02-20 02:52:05 - progress_bar.py[line:274] - INFO: epoch 001:   7980 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.2, bsz=40, num_updates=7970, lr=4.85036e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20440
2023-02-20 02:52:17 - progress_bar.py[line:274] - INFO: epoch 001:   7990 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.87, wpb=113.2, bsz=40, num_updates=7980, lr=4.85e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20452
2023-02-20 02:52:28 - progress_bar.py[line:274] - INFO: epoch 001:   8000 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.87, wpb=113.9, bsz=40, num_updates=7990, lr=4.84963e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20463
2023-02-20 02:52:39 - progress_bar.py[line:274] - INFO: epoch 001:   8010 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.9, wpb=114, bsz=40, num_updates=8000, lr=4.84927e-05, gnorm=0.192, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20474
2023-02-20 02:52:39 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 02:52:41 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 02:52:41 - train.py[line:551] - INFO: load:1.12 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 02:54:43 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 02:54:43 - train.py[line:551] - INFO: load:1.15 valid_run:121.88 task_valid:118.70 collect_output:2.12
2023-02-20 02:56:42 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 02:56:42 - train.py[line:551] - INFO: load:1.17 valid_run:241.59 task_valid:234.13 collect_output:5.32
2023-02-20 02:58:44 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 02:58:44 - train.py[line:551] - INFO: load:1.20 valid_run:363.42 task_valid:350.24 collect_output:9.97
2023-02-20 03:00:46 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 03:00:46 - train.py[line:551] - INFO: load:1.23 valid_run:485.18 task_valid:463.58 collect_output:17.34
2023-02-20 03:02:46 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 03:02:46 - train.py[line:551] - INFO: load:1.26 valid_run:605.51 task_valid:580.59 collect_output:19.58
2023-02-20 03:04:49 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 03:04:49 - train.py[line:551] - INFO: load:1.28 valid_run:728.30 task_valid:698.99 collect_output:22.89
2023-02-20 03:06:52 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 03:06:52 - train.py[line:551] - INFO: load:1.31 valid_run:851.29 task_valid:816.82 collect_output:27.00
2023-02-20 03:08:54 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 03:08:54 - train.py[line:551] - INFO: load:1.34 valid_run:973.22 task_valid:933.10 collect_output:31.59
2023-02-20 03:10:58 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 03:10:58 - train.py[line:551] - INFO: load:1.37 valid_run:1096.96 task_valid:1049.96 collect_output:37.43
2023-02-20 03:13:00 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 03:13:00 - train.py[line:551] - INFO: load:1.39 valid_run:1218.54 task_valid:1162.32 collect_output:45.59
2023-02-20 03:15:00 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 03:15:00 - train.py[line:551] - INFO: load:1.42 valid_run:1338.52 task_valid:1277.61 collect_output:49.23
2023-02-20 03:17:02 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 03:17:02 - train.py[line:551] - INFO: load:1.45 valid_run:1460.10 task_valid:1394.31 collect_output:53.05
2023-02-20 03:19:00 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 03:19:00 - train.py[line:551] - INFO: load:1.48 valid_run:1578.92 task_valid:1507.90 collect_output:57.24
2023-02-20 03:21:01 - train.py[line:549] - INFO: 2800 / 6234
2023-02-20 03:21:01 - train.py[line:551] - INFO: load:1.50 valid_run:1699.81 task_valid:1625.36 collect_output:59.62
2023-02-20 03:23:02 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 03:23:02 - train.py[line:551] - INFO: load:1.53 valid_run:1820.86 task_valid:1741.31 collect_output:63.65
2023-02-20 03:25:04 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 03:25:04 - train.py[line:551] - INFO: load:1.56 valid_run:1942.30 task_valid:1855.21 collect_output:70.13
2023-02-20 03:27:05 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 03:27:05 - train.py[line:551] - INFO: load:1.59 valid_run:2063.62 task_valid:1971.12 collect_output:74.49
2023-02-20 03:29:06 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 03:29:06 - train.py[line:551] - INFO: load:1.61 valid_run:2184.40 task_valid:2088.73 collect_output:76.61
2023-02-20 03:31:07 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 03:31:07 - train.py[line:551] - INFO: load:1.64 valid_run:2305.56 task_valid:2205.46 collect_output:79.98
2023-02-20 03:33:08 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 03:33:08 - train.py[line:551] - INFO: load:1.67 valid_run:2425.84 task_valid:2321.84 collect_output:82.83
2023-02-20 03:35:10 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 03:35:10 - train.py[line:551] - INFO: load:1.70 valid_run:2547.60 task_valid:2438.27 collect_output:87.11
2023-02-20 03:37:12 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 03:37:12 - train.py[line:551] - INFO: load:1.72 valid_run:2669.60 task_valid:2556.96 collect_output:89.37
2023-02-20 03:39:12 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 03:39:12 - train.py[line:551] - INFO: load:1.75 valid_run:2789.89 task_valid:2671.08 collect_output:94.49
2023-02-20 03:41:12 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 03:41:12 - train.py[line:551] - INFO: load:1.78 valid_run:2909.71 task_valid:2787.01 collect_output:97.32
2023-02-20 03:43:14 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 03:43:14 - train.py[line:551] - INFO: load:1.81 valid_run:3031.37 task_valid:2903.01 collect_output:101.91
2023-02-20 03:45:17 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 03:45:17 - train.py[line:551] - INFO: load:1.83 valid_run:3154.33 task_valid:3018.83 collect_output:108.01
2023-02-20 03:47:16 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 03:47:16 - train.py[line:551] - INFO: load:1.86 valid_run:3273.99 task_valid:3132.87 collect_output:112.58
2023-02-20 03:49:18 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 03:49:18 - train.py[line:551] - INFO: load:1.89 valid_run:3395.67 task_valid:3252.02 collect_output:114.05
2023-02-20 03:51:20 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 03:51:20 - train.py[line:551] - INFO: load:1.92 valid_run:3517.57 task_valid:3367.26 collect_output:119.65
2023-02-20 03:53:22 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 03:53:22 - train.py[line:551] - INFO: load:1.94 valid_run:3639.31 task_valid:3485.47 collect_output:122.11
2023-02-20 03:55:23 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 03:55:23 - train.py[line:551] - INFO: load:1.97 valid_run:3760.29 task_valid:3603.76 collect_output:123.76

====================================================================================================
SGG eval:     R @ 50: 0.6539;     R @ 100: 0.6846;     R @ 500: 0.7063;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4284;    mR @ 100: 0.4635;    mR @ 500: 0.5224;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.5625) (covering:0.4429) (eating:0.8235) (flying in:0.5000) (growing on:0.3750) (hanging from:0.5806) (lying on:0.3000) (mounted on:0.0000) (painted on:0.0000) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7245) (standing on:0.4043) (using:0.6000) (walking in:0.0000) (walking on:0.7432) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-20 03:55:53 - train.py[line:487] - INFO: 0.6845957983193277

====================================================================================================
SGG eval:     R @ 50: 0.6539;     R @ 100: 0.6846;     R @ 500: 0.7063;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4284;    mR @ 100: 0.4635;    mR @ 500: 0.5224;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.5625) (covering:0.4429) (eating:0.8235) (flying in:0.5000) (growing on:0.3750) (hanging from:0.5806) (lying on:0.3000) (mounted on:0.0000) (painted on:0.0000) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7245) (standing on:0.4043) (using:0.6000) (walking in:0.0000) (walking on:0.7432) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-20 03:55:53 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 03:55:54 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.262 | loss_v1 0 | loss_v2 0 | nll_loss 0.094 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.684596 | ppl 1.07 | vqa_score 0.5113 | wps 118.3 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.684596
2023-02-20 03:55:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2023-02-20 03:55:54 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_8000.pt
2023-02-20 03:55:59 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_8000.pt
2023-02-20 03:56:05 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.6845957983193277) (writing took 11.549573427066207 seconds)
2023-02-20 03:56:16 - progress_bar.py[line:274] - INFO: epoch 001:   8020 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=0.3, ups=0, wpb=114.2, bsz=40, num_updates=8010, lr=4.84891e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24291
2023-02-20 03:56:27 - progress_bar.py[line:274] - INFO: epoch 001:   8030 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.5, ups=0.93, wpb=114.5, bsz=40, num_updates=8020, lr=4.84855e-05, gnorm=0.215, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24302
2023-02-20 03:56:38 - progress_bar.py[line:274] - INFO: epoch 001:   8040 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.9, bsz=40, num_updates=8030, lr=4.84819e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24313
2023-02-20 03:56:50 - progress_bar.py[line:274] - INFO: epoch 001:   8050 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.9, wpb=112.6, bsz=40, num_updates=8040, lr=4.84782e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24324
2023-02-20 03:57:01 - progress_bar.py[line:274] - INFO: epoch 001:   8060 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.8, bsz=40, num_updates=8050, lr=4.84746e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24335
2023-02-20 03:57:12 - progress_bar.py[line:274] - INFO: epoch 001:   8070 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.6, bsz=40, num_updates=8060, lr=4.8471e-05, gnorm=0.283, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24347
2023-02-20 03:57:23 - progress_bar.py[line:274] - INFO: epoch 001:   8080 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=8070, lr=4.84674e-05, gnorm=0.325, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24358
2023-02-20 03:57:26 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 03:57:36 - progress_bar.py[line:274] - INFO: epoch 001:   8091 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=91, ups=0.8, wpb=113.9, bsz=40, num_updates=8080, lr=4.84638e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=24370
2023-02-20 03:57:46 - progress_bar.py[line:274] - INFO: epoch 001:   8101 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.93, wpb=112.5, bsz=40, num_updates=8090, lr=4.84602e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24381
2023-02-20 03:57:57 - progress_bar.py[line:274] - INFO: epoch 001:   8111 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.5, ups=0.93, wpb=113.1, bsz=40, num_updates=8100, lr=4.84565e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24392
2023-02-20 03:58:08 - progress_bar.py[line:274] - INFO: epoch 001:   8121 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.9, wpb=112, bsz=40, num_updates=8110, lr=4.84529e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24403
2023-02-20 03:58:20 - progress_bar.py[line:274] - INFO: epoch 001:   8131 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=112.8, bsz=40, num_updates=8120, lr=4.84493e-05, gnorm=0.226, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24414
2023-02-20 03:58:31 - progress_bar.py[line:274] - INFO: epoch 001:   8141 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113.2, bsz=40, num_updates=8130, lr=4.84457e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24426
2023-02-20 03:58:42 - progress_bar.py[line:274] - INFO: epoch 001:   8151 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.87, wpb=113.7, bsz=40, num_updates=8140, lr=4.84421e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24437
2023-02-20 03:58:53 - progress_bar.py[line:274] - INFO: epoch 001:   8161 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=113.8, bsz=40, num_updates=8150, lr=4.84384e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24448
2023-02-20 03:59:04 - progress_bar.py[line:274] - INFO: epoch 001:   8171 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.9, ups=0.93, wpb=113.3, bsz=40, num_updates=8160, lr=4.84348e-05, gnorm=0.268, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24459
2023-02-20 03:59:15 - progress_bar.py[line:274] - INFO: epoch 001:   8181 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.1, ups=0.93, wpb=113.6, bsz=40, num_updates=8170, lr=4.84312e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24470
2023-02-20 03:59:26 - progress_bar.py[line:274] - INFO: epoch 001:   8191 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.3, bsz=40, num_updates=8180, lr=4.84276e-05, gnorm=0.248, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24481
2023-02-20 03:59:37 - progress_bar.py[line:274] - INFO: epoch 001:   8201 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.6, ups=0.92, wpb=114, bsz=40, num_updates=8190, lr=4.8424e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24491
2023-02-20 03:59:48 - progress_bar.py[line:274] - INFO: epoch 001:   8211 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.89, wpb=113.1, bsz=40, num_updates=8200, lr=4.84204e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24503
2023-02-20 03:59:59 - progress_bar.py[line:274] - INFO: epoch 001:   8221 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=8210, lr=4.84167e-05, gnorm=0.257, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24514
2023-02-20 04:00:10 - progress_bar.py[line:274] - INFO: epoch 001:   8231 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.6, bsz=40, num_updates=8220, lr=4.84131e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24525
2023-02-20 04:00:21 - progress_bar.py[line:274] - INFO: epoch 001:   8241 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.93, wpb=112.5, bsz=40, num_updates=8230, lr=4.84095e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24536
2023-02-20 04:00:32 - progress_bar.py[line:274] - INFO: epoch 001:   8251 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.92, wpb=113.2, bsz=40, num_updates=8240, lr=4.84059e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24547
2023-02-20 04:00:43 - progress_bar.py[line:274] - INFO: epoch 001:   8261 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=114.2, bsz=40, num_updates=8250, lr=4.84023e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24558
2023-02-20 04:00:54 - progress_bar.py[line:274] - INFO: epoch 001:   8271 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.3, ups=0.92, wpb=113.6, bsz=40, num_updates=8260, lr=4.83987e-05, gnorm=0.194, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24569
2023-02-20 04:01:05 - progress_bar.py[line:274] - INFO: epoch 001:   8281 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.9, bsz=40, num_updates=8270, lr=4.8395e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24580
2023-02-20 04:01:16 - progress_bar.py[line:274] - INFO: epoch 001:   8291 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=112.6, bsz=40, num_updates=8280, lr=4.83914e-05, gnorm=0.318, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24591
2023-02-20 04:01:28 - progress_bar.py[line:274] - INFO: epoch 001:   8301 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.4, bsz=40, num_updates=8290, lr=4.83878e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24603
2023-02-20 04:01:39 - progress_bar.py[line:274] - INFO: epoch 001:   8311 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.9, wpb=114.1, bsz=40, num_updates=8300, lr=4.83842e-05, gnorm=0.164, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24614
2023-02-20 04:01:50 - progress_bar.py[line:274] - INFO: epoch 001:   8321 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.89, wpb=113.1, bsz=40, num_updates=8310, lr=4.83806e-05, gnorm=0.184, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24625
2023-02-20 04:02:01 - progress_bar.py[line:274] - INFO: epoch 001:   8331 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.3, bsz=40, num_updates=8320, lr=4.83769e-05, gnorm=0.327, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24636
2023-02-20 04:02:12 - progress_bar.py[line:274] - INFO: epoch 001:   8341 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=114.2, bsz=40, num_updates=8330, lr=4.83733e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24647
2023-02-20 04:02:24 - progress_bar.py[line:274] - INFO: epoch 001:   8351 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.3, bsz=40, num_updates=8340, lr=4.83697e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24659
2023-02-20 04:02:35 - progress_bar.py[line:274] - INFO: epoch 001:   8361 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=8350, lr=4.83661e-05, gnorm=0.216, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24670
2023-02-20 04:02:46 - progress_bar.py[line:274] - INFO: epoch 001:   8371 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.8, bsz=40, num_updates=8360, lr=4.83625e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24681
2023-02-20 04:02:57 - progress_bar.py[line:274] - INFO: epoch 001:   8381 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.91, wpb=114.9, bsz=40, num_updates=8370, lr=4.83589e-05, gnorm=0.379, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24692
2023-02-20 04:03:08 - progress_bar.py[line:274] - INFO: epoch 001:   8391 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.9, wpb=113.4, bsz=40, num_updates=8380, lr=4.83552e-05, gnorm=0.31, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24703
2023-02-20 04:03:19 - progress_bar.py[line:274] - INFO: epoch 001:   8401 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=112.1, bsz=40, num_updates=8390, lr=4.83516e-05, gnorm=0.242, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24714
2023-02-20 04:03:30 - progress_bar.py[line:274] - INFO: epoch 001:   8411 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.2, bsz=40, num_updates=8400, lr=4.8348e-05, gnorm=0.32, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24725
2023-02-20 04:03:41 - progress_bar.py[line:274] - INFO: epoch 001:   8421 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=8410, lr=4.83444e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=24736
2023-02-20 04:03:53 - progress_bar.py[line:274] - INFO: epoch 001:   8431 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.9, wpb=113.6, bsz=40, num_updates=8420, lr=4.83408e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24748
2023-02-20 04:04:04 - progress_bar.py[line:274] - INFO: epoch 001:   8441 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.1, bsz=40, num_updates=8430, lr=4.83371e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24759
2023-02-20 04:04:15 - progress_bar.py[line:274] - INFO: epoch 001:   8451 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.92, wpb=113.2, bsz=40, num_updates=8440, lr=4.83335e-05, gnorm=0.254, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24770
2023-02-20 04:04:26 - progress_bar.py[line:274] - INFO: epoch 001:   8461 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.92, wpb=113.8, bsz=40, num_updates=8450, lr=4.83299e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24781
2023-02-20 04:04:37 - progress_bar.py[line:274] - INFO: epoch 001:   8471 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=113.3, bsz=40, num_updates=8460, lr=4.83263e-05, gnorm=0.271, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24792
2023-02-20 04:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   8481 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.1, bsz=40, num_updates=8470, lr=4.83227e-05, gnorm=0.239, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24803
2023-02-20 04:05:00 - progress_bar.py[line:274] - INFO: epoch 001:   8491 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=113.3, bsz=40, num_updates=8480, lr=4.83191e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24814
2023-02-20 04:05:11 - progress_bar.py[line:274] - INFO: epoch 001:   8501 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.1, ups=0.92, wpb=113.4, bsz=40, num_updates=8490, lr=4.83154e-05, gnorm=0.183, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24825
2023-02-20 04:05:22 - progress_bar.py[line:274] - INFO: epoch 001:   8511 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=111.9, bsz=40, num_updates=8500, lr=4.83118e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24837
2023-02-20 04:05:33 - progress_bar.py[line:274] - INFO: epoch 001:   8521 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=8510, lr=4.83082e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24848
2023-02-20 04:05:44 - progress_bar.py[line:274] - INFO: epoch 001:   8531 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.1, ups=0.93, wpb=113.9, bsz=40, num_updates=8520, lr=4.83046e-05, gnorm=0.309, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24859
2023-02-20 04:05:55 - progress_bar.py[line:274] - INFO: epoch 001:   8541 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=113.3, bsz=40, num_updates=8530, lr=4.8301e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24870
2023-02-20 04:06:07 - progress_bar.py[line:274] - INFO: epoch 001:   8551 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=112.8, bsz=40, num_updates=8540, lr=4.82973e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24882
2023-02-20 04:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   8561 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.87, wpb=112, bsz=40, num_updates=8550, lr=4.82937e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24893
2023-02-20 04:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   8571 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=8560, lr=4.82901e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24904
2023-02-20 04:06:40 - progress_bar.py[line:274] - INFO: epoch 001:   8581 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.92, wpb=113, bsz=40, num_updates=8570, lr=4.82865e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24915
2023-02-20 04:06:52 - progress_bar.py[line:274] - INFO: epoch 001:   8591 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.9, bsz=40, num_updates=8580, lr=4.82829e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24926
2023-02-20 04:06:57 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 04:07:04 - progress_bar.py[line:274] - INFO: epoch 001:   8602 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=92.4, ups=0.82, wpb=113, bsz=40, num_updates=8590, lr=4.82793e-05, gnorm=0.248, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=24939
2023-02-20 04:07:15 - progress_bar.py[line:274] - INFO: epoch 001:   8612 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.6, ups=0.92, wpb=114.9, bsz=40, num_updates=8600, lr=4.82756e-05, gnorm=0.305, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24950
2023-02-20 04:07:26 - progress_bar.py[line:274] - INFO: epoch 001:   8622 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.4, bsz=40, num_updates=8610, lr=4.8272e-05, gnorm=0.33, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24961
2023-02-20 04:07:37 - progress_bar.py[line:274] - INFO: epoch 001:   8632 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.1, bsz=40, num_updates=8620, lr=4.82684e-05, gnorm=0.213, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24972
2023-02-20 04:07:48 - progress_bar.py[line:274] - INFO: epoch 001:   8642 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.5, bsz=40, num_updates=8630, lr=4.82648e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24983
2023-02-20 04:08:00 - progress_bar.py[line:274] - INFO: epoch 001:   8652 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.4, bsz=40, num_updates=8640, lr=4.82612e-05, gnorm=0.328, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24995
2023-02-20 04:08:11 - progress_bar.py[line:274] - INFO: epoch 001:   8662 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.4, bsz=40, num_updates=8650, lr=4.82575e-05, gnorm=0.196, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25006
2023-02-20 04:08:22 - progress_bar.py[line:274] - INFO: epoch 001:   8672 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=113.3, bsz=40, num_updates=8660, lr=4.82539e-05, gnorm=0.256, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25017
2023-02-20 04:08:34 - progress_bar.py[line:274] - INFO: epoch 001:   8682 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.88, wpb=114, bsz=40, num_updates=8670, lr=4.82503e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25028
2023-02-20 04:08:45 - progress_bar.py[line:274] - INFO: epoch 001:   8692 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.6, bsz=40, num_updates=8680, lr=4.82467e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25039
2023-02-20 04:08:55 - progress_bar.py[line:274] - INFO: epoch 001:   8702 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=107, ups=0.94, wpb=113.6, bsz=40, num_updates=8690, lr=4.82431e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25050
2023-02-20 04:09:06 - progress_bar.py[line:274] - INFO: epoch 001:   8712 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.89, wpb=114.4, bsz=40, num_updates=8700, lr=4.82395e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25061
2023-02-20 04:09:18 - progress_bar.py[line:274] - INFO: epoch 001:   8722 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=113.2, bsz=40, num_updates=8710, lr=4.82358e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25073
2023-02-20 04:09:29 - progress_bar.py[line:274] - INFO: epoch 001:   8732 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.1, ups=0.92, wpb=112.8, bsz=40, num_updates=8720, lr=4.82322e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25084
2023-02-20 04:09:40 - progress_bar.py[line:274] - INFO: epoch 001:   8742 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=114, bsz=40, num_updates=8730, lr=4.82286e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25095
2023-02-20 04:09:51 - progress_bar.py[line:274] - INFO: epoch 001:   8752 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.2, bsz=40, num_updates=8740, lr=4.8225e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25106
2023-02-20 04:10:02 - progress_bar.py[line:274] - INFO: epoch 001:   8762 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=113.1, bsz=40, num_updates=8750, lr=4.82214e-05, gnorm=0.184, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25117
2023-02-20 04:10:14 - progress_bar.py[line:274] - INFO: epoch 001:   8772 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.6, bsz=40, num_updates=8760, lr=4.82177e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25129
2023-02-20 04:10:25 - progress_bar.py[line:274] - INFO: epoch 001:   8782 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.6, bsz=40, num_updates=8770, lr=4.82141e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25140
2023-02-20 04:10:36 - progress_bar.py[line:274] - INFO: epoch 001:   8792 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112, bsz=40, num_updates=8780, lr=4.82105e-05, gnorm=0.3, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25151
2023-02-20 04:10:48 - progress_bar.py[line:274] - INFO: epoch 001:   8802 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.6, bsz=40, num_updates=8790, lr=4.82069e-05, gnorm=0.29, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25163
2023-02-20 04:10:59 - progress_bar.py[line:274] - INFO: epoch 001:   8812 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.5, bsz=40, num_updates=8800, lr=4.82033e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25174
2023-02-20 04:11:10 - progress_bar.py[line:274] - INFO: epoch 001:   8822 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.7, ups=0.91, wpb=113.1, bsz=40, num_updates=8810, lr=4.81997e-05, gnorm=0.183, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25185
2023-02-20 04:11:21 - progress_bar.py[line:274] - INFO: epoch 001:   8832 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.7, ups=0.86, wpb=113.4, bsz=40, num_updates=8820, lr=4.8196e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=12, gb_free=11.3, ema_decay=0.9999, wall=25196
2023-02-20 04:11:32 - progress_bar.py[line:274] - INFO: epoch 001:   8842 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.92, wpb=114.2, bsz=40, num_updates=8830, lr=4.81924e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25207
2023-02-20 04:11:44 - progress_bar.py[line:274] - INFO: epoch 001:   8852 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.88, wpb=113.9, bsz=40, num_updates=8840, lr=4.81888e-05, gnorm=0.258, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25219
2023-02-20 04:11:55 - progress_bar.py[line:274] - INFO: epoch 001:   8862 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.3, bsz=40, num_updates=8850, lr=4.81852e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25230
2023-02-20 04:12:06 - progress_bar.py[line:274] - INFO: epoch 001:   8872 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.4, bsz=40, num_updates=8860, lr=4.81816e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25241
2023-02-20 04:12:17 - progress_bar.py[line:274] - INFO: epoch 001:   8882 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.4, bsz=40, num_updates=8870, lr=4.8178e-05, gnorm=0.265, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25252
2023-02-20 04:12:29 - progress_bar.py[line:274] - INFO: epoch 001:   8892 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.87, wpb=114.6, bsz=40, num_updates=8880, lr=4.81743e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25264
2023-02-20 04:12:40 - progress_bar.py[line:274] - INFO: epoch 001:   8902 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.3, bsz=40, num_updates=8890, lr=4.81707e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25275
2023-02-20 04:12:51 - progress_bar.py[line:274] - INFO: epoch 001:   8912 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.7, bsz=40, num_updates=8900, lr=4.81671e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25286
2023-02-20 04:13:03 - progress_bar.py[line:274] - INFO: epoch 001:   8922 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.8, bsz=40, num_updates=8910, lr=4.81635e-05, gnorm=0.2, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25297
2023-02-20 04:13:14 - progress_bar.py[line:274] - INFO: epoch 001:   8932 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.4, bsz=40, num_updates=8920, lr=4.81599e-05, gnorm=0.185, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25308
2023-02-20 04:13:25 - progress_bar.py[line:274] - INFO: epoch 001:   8942 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.2, bsz=40, num_updates=8930, lr=4.81562e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25320
2023-02-20 04:13:36 - progress_bar.py[line:274] - INFO: epoch 001:   8952 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.6, ups=0.9, wpb=113.6, bsz=40, num_updates=8940, lr=4.81526e-05, gnorm=0.191, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25331
2023-02-20 04:13:47 - progress_bar.py[line:274] - INFO: epoch 001:   8962 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.4, bsz=40, num_updates=8950, lr=4.8149e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25342
2023-02-20 04:13:58 - progress_bar.py[line:274] - INFO: epoch 001:   8972 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.8, bsz=40, num_updates=8960, lr=4.81454e-05, gnorm=0.198, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25353
2023-02-20 04:14:10 - progress_bar.py[line:274] - INFO: epoch 001:   8982 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114, bsz=40, num_updates=8970, lr=4.81418e-05, gnorm=0.161, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25364
2023-02-20 04:14:21 - progress_bar.py[line:274] - INFO: epoch 001:   8992 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=111.8, bsz=40, num_updates=8980, lr=4.81382e-05, gnorm=0.305, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25376
2023-02-20 04:14:32 - progress_bar.py[line:274] - INFO: epoch 001:   9002 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.9, wpb=112.7, bsz=40, num_updates=8990, lr=4.81345e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25387
2023-02-20 04:14:43 - progress_bar.py[line:274] - INFO: epoch 001:   9012 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107.3, ups=0.94, wpb=113.6, bsz=40, num_updates=9000, lr=4.81309e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25397
2023-02-20 04:14:54 - progress_bar.py[line:274] - INFO: epoch 001:   9022 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.6, bsz=40, num_updates=9010, lr=4.81273e-05, gnorm=0.191, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25408
2023-02-20 04:15:05 - progress_bar.py[line:274] - INFO: epoch 001:   9032 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=114.3, bsz=40, num_updates=9020, lr=4.81237e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25420
2023-02-20 04:15:16 - progress_bar.py[line:274] - INFO: epoch 001:   9042 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.91, wpb=114.3, bsz=40, num_updates=9030, lr=4.81201e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25431
2023-02-20 04:15:27 - progress_bar.py[line:274] - INFO: epoch 001:   9052 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=112.4, bsz=40, num_updates=9040, lr=4.81164e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25442
2023-02-20 04:15:38 - progress_bar.py[line:274] - INFO: epoch 001:   9062 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=115.5, nsentences=40, sample_size=115.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102, ups=0.88, wpb=115.5, bsz=40, num_updates=9050, lr=4.81128e-05, gnorm=0.208, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25453
2023-02-20 04:15:50 - progress_bar.py[line:274] - INFO: epoch 001:   9072 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.1, bsz=40, num_updates=9060, lr=4.81092e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25465
2023-02-20 04:16:01 - progress_bar.py[line:274] - INFO: epoch 001:   9082 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.89, wpb=113, bsz=40, num_updates=9070, lr=4.81056e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25476
2023-02-20 04:16:12 - progress_bar.py[line:274] - INFO: epoch 001:   9092 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.9, wpb=114.8, bsz=40, num_updates=9080, lr=4.8102e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25487
2023-02-20 04:16:24 - progress_bar.py[line:274] - INFO: epoch 001:   9102 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.5, bsz=40, num_updates=9090, lr=4.80984e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25498
2023-02-20 04:16:35 - progress_bar.py[line:274] - INFO: epoch 001:   9112 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.89, wpb=115, bsz=40, num_updates=9100, lr=4.80947e-05, gnorm=0.256, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25510
2023-02-20 04:16:46 - progress_bar.py[line:274] - INFO: epoch 001:   9122 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.1, bsz=40, num_updates=9110, lr=4.80911e-05, gnorm=0.442, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25521
2023-02-20 04:16:57 - progress_bar.py[line:274] - INFO: epoch 001:   9132 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.91, wpb=113.8, bsz=40, num_updates=9120, lr=4.80875e-05, gnorm=0.257, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25532
2023-02-20 04:17:08 - progress_bar.py[line:274] - INFO: epoch 001:   9142 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.3, bsz=40, num_updates=9130, lr=4.80839e-05, gnorm=0.231, clip=0, loss_scale=2048, train_wall=11, gb_free=10, ema_decay=0.9999, wall=25543
2023-02-20 04:17:20 - progress_bar.py[line:274] - INFO: epoch 001:   9152 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.3, bsz=40, num_updates=9140, lr=4.80803e-05, gnorm=0.255, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25555
2023-02-20 04:17:31 - progress_bar.py[line:274] - INFO: epoch 001:   9162 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=114.1, bsz=40, num_updates=9150, lr=4.80766e-05, gnorm=0.231, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25566
2023-02-20 04:17:42 - progress_bar.py[line:274] - INFO: epoch 001:   9172 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.6, bsz=40, num_updates=9160, lr=4.8073e-05, gnorm=0.203, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25577
2023-02-20 04:17:53 - progress_bar.py[line:274] - INFO: epoch 001:   9182 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=113.6, bsz=40, num_updates=9170, lr=4.80694e-05, gnorm=0.23, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25588
2023-02-20 04:18:05 - progress_bar.py[line:274] - INFO: epoch 001:   9192 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=113.5, bsz=40, num_updates=9180, lr=4.80658e-05, gnorm=0.283, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25600
2023-02-20 04:18:16 - progress_bar.py[line:274] - INFO: epoch 001:   9202 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.8, bsz=40, num_updates=9190, lr=4.80622e-05, gnorm=0.291, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25611
2023-02-20 04:18:27 - progress_bar.py[line:274] - INFO: epoch 001:   9212 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.92, wpb=114.2, bsz=40, num_updates=9200, lr=4.80586e-05, gnorm=0.191, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25622
2023-02-20 04:18:38 - progress_bar.py[line:274] - INFO: epoch 001:   9222 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.89, wpb=114.3, bsz=40, num_updates=9210, lr=4.80549e-05, gnorm=0.365, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25633
2023-02-20 04:18:49 - progress_bar.py[line:274] - INFO: epoch 001:   9232 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.91, wpb=113, bsz=40, num_updates=9220, lr=4.80513e-05, gnorm=0.289, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25644
2023-02-20 04:19:00 - progress_bar.py[line:274] - INFO: epoch 001:   9242 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.7, bsz=40, num_updates=9230, lr=4.80477e-05, gnorm=0.208, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25655
2023-02-20 04:19:12 - progress_bar.py[line:274] - INFO: epoch 001:   9252 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.87, wpb=114.7, bsz=40, num_updates=9240, lr=4.80441e-05, gnorm=0.252, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25667
2023-02-20 04:19:23 - progress_bar.py[line:274] - INFO: epoch 001:   9262 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.2, ups=0.9, wpb=114.3, bsz=40, num_updates=9250, lr=4.80405e-05, gnorm=0.153, clip=0, loss_scale=2048, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25678
2023-02-20 04:19:34 - progress_bar.py[line:274] - INFO: epoch 001:   9272 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.4, bsz=40, num_updates=9260, lr=4.80368e-05, gnorm=0.17, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25689
2023-02-20 04:19:45 - progress_bar.py[line:274] - INFO: epoch 001:   9282 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=112.6, bsz=40, num_updates=9270, lr=4.80332e-05, gnorm=0.257, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25700
2023-02-20 04:19:56 - progress_bar.py[line:274] - INFO: epoch 001:   9292 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=114, bsz=40, num_updates=9280, lr=4.80296e-05, gnorm=0.201, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25711
2023-02-20 04:20:07 - progress_bar.py[line:274] - INFO: epoch 001:   9302 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.91, wpb=112.5, bsz=40, num_updates=9290, lr=4.8026e-05, gnorm=0.252, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25722
2023-02-20 04:20:18 - progress_bar.py[line:274] - INFO: epoch 001:   9312 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.8, bsz=40, num_updates=9300, lr=4.80224e-05, gnorm=0.227, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25733
2023-02-20 04:20:30 - progress_bar.py[line:274] - INFO: epoch 001:   9322 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.3, bsz=40, num_updates=9310, lr=4.80188e-05, gnorm=0.213, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25744
2023-02-20 04:20:41 - progress_bar.py[line:274] - INFO: epoch 001:   9332 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=115.1, nsentences=40, sample_size=115.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=115.1, bsz=40, num_updates=9320, lr=4.80151e-05, gnorm=0.256, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25756
2023-02-20 04:20:52 - progress_bar.py[line:274] - INFO: epoch 001:   9342 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=9330, lr=4.80115e-05, gnorm=0.216, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25767
2023-02-20 04:21:03 - progress_bar.py[line:274] - INFO: epoch 001:   9352 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.8, ups=0.93, wpb=114, bsz=40, num_updates=9340, lr=4.80079e-05, gnorm=0.204, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25778
2023-02-20 04:21:14 - progress_bar.py[line:274] - INFO: epoch 001:   9362 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.3, bsz=40, num_updates=9350, lr=4.80043e-05, gnorm=0.268, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25789
2023-02-20 04:21:25 - progress_bar.py[line:274] - INFO: epoch 001:   9372 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.87, wpb=113.8, bsz=40, num_updates=9360, lr=4.80007e-05, gnorm=0.237, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25800
2023-02-20 04:21:37 - progress_bar.py[line:274] - INFO: epoch 001:   9382 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=113.5, bsz=40, num_updates=9370, lr=4.7997e-05, gnorm=0.295, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25811
2023-02-20 04:21:47 - progress_bar.py[line:274] - INFO: epoch 001:   9392 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.7, ups=0.92, wpb=115.2, bsz=40, num_updates=9380, lr=4.79934e-05, gnorm=0.249, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25822
2023-02-20 04:21:58 - progress_bar.py[line:274] - INFO: epoch 001:   9402 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.3, ups=0.92, wpb=113.8, bsz=40, num_updates=9390, lr=4.79898e-05, gnorm=0.209, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25833
2023-02-20 04:22:10 - progress_bar.py[line:274] - INFO: epoch 001:   9412 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=9400, lr=4.79862e-05, gnorm=0.279, clip=0, loss_scale=2048, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25845
2023-02-20 04:22:21 - progress_bar.py[line:274] - INFO: epoch 001:   9422 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.4, bsz=40, num_updates=9410, lr=4.79826e-05, gnorm=0.225, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25856
2023-02-20 04:22:32 - progress_bar.py[line:274] - INFO: epoch 001:   9432 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.8, ups=0.92, wpb=114, bsz=40, num_updates=9420, lr=4.7979e-05, gnorm=0.253, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25867
2023-02-20 04:22:43 - progress_bar.py[line:274] - INFO: epoch 001:   9442 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.88, wpb=112, bsz=40, num_updates=9430, lr=4.79753e-05, gnorm=0.168, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25878
2023-02-20 04:22:54 - progress_bar.py[line:274] - INFO: epoch 001:   9452 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=9440, lr=4.79717e-05, gnorm=0.252, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25889
2023-02-20 04:23:05 - progress_bar.py[line:274] - INFO: epoch 001:   9462 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.92, wpb=112.4, bsz=40, num_updates=9450, lr=4.79681e-05, gnorm=0.289, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25900
2023-02-20 04:23:16 - progress_bar.py[line:274] - INFO: epoch 001:   9472 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.93, wpb=113, bsz=40, num_updates=9460, lr=4.79645e-05, gnorm=0.19, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25911
2023-02-20 04:23:27 - progress_bar.py[line:274] - INFO: epoch 001:   9482 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=9470, lr=4.79609e-05, gnorm=0.246, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25922
2023-02-20 04:23:37 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 04:23:39 - progress_bar.py[line:274] - INFO: epoch 001:   9493 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=93.3, ups=0.82, wpb=114.2, bsz=40, num_updates=9480, lr=4.79572e-05, gnorm=0.305, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=25934
2023-02-20 04:23:51 - progress_bar.py[line:274] - INFO: epoch 001:   9503 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.87, wpb=114, bsz=40, num_updates=9490, lr=4.79536e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25946
2023-02-20 04:24:02 - progress_bar.py[line:274] - INFO: epoch 001:   9513 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=111.8, bsz=40, num_updates=9500, lr=4.795e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25957
2023-02-20 04:24:13 - progress_bar.py[line:274] - INFO: epoch 001:   9523 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.88, wpb=115, bsz=40, num_updates=9510, lr=4.79464e-05, gnorm=0.514, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25968
2023-02-20 04:24:24 - progress_bar.py[line:274] - INFO: epoch 001:   9533 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.5, bsz=40, num_updates=9520, lr=4.79428e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25979
2023-02-20 04:24:35 - progress_bar.py[line:274] - INFO: epoch 001:   9543 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.91, wpb=114.4, bsz=40, num_updates=9530, lr=4.79392e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25990
2023-02-20 04:24:46 - progress_bar.py[line:274] - INFO: epoch 001:   9553 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.91, wpb=112.6, bsz=40, num_updates=9540, lr=4.79355e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26001
2023-02-20 04:24:58 - progress_bar.py[line:274] - INFO: epoch 001:   9563 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.2, bsz=40, num_updates=9550, lr=4.79319e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26013
2023-02-20 04:25:09 - progress_bar.py[line:274] - INFO: epoch 001:   9573 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.92, wpb=114.4, bsz=40, num_updates=9560, lr=4.79283e-05, gnorm=0.2, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26024
2023-02-20 04:25:20 - progress_bar.py[line:274] - INFO: epoch 001:   9583 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.9, bsz=40, num_updates=9570, lr=4.79247e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26035
2023-02-20 04:25:31 - progress_bar.py[line:274] - INFO: epoch 001:   9593 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.5, bsz=40, num_updates=9580, lr=4.79211e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26046
2023-02-20 04:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   9603 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.91, wpb=113.8, bsz=40, num_updates=9590, lr=4.79175e-05, gnorm=0.288, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26057
2023-02-20 04:25:53 - progress_bar.py[line:274] - INFO: epoch 001:   9613 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.5, ups=0.93, wpb=114.1, bsz=40, num_updates=9600, lr=4.79138e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26068
2023-02-20 04:26:04 - progress_bar.py[line:274] - INFO: epoch 001:   9623 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.5, bsz=40, num_updates=9610, lr=4.79102e-05, gnorm=0.319, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26079
2023-02-20 04:26:15 - progress_bar.py[line:274] - INFO: epoch 001:   9633 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=106.3, ups=0.93, wpb=114.5, bsz=40, num_updates=9620, lr=4.79066e-05, gnorm=0.194, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26090
2023-02-20 04:26:26 - progress_bar.py[line:274] - INFO: epoch 001:   9643 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.5, bsz=40, num_updates=9630, lr=4.7903e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26101
2023-02-20 04:26:37 - progress_bar.py[line:274] - INFO: epoch 001:   9653 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.5, ups=0.93, wpb=113.6, bsz=40, num_updates=9640, lr=4.78994e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26112
2023-02-20 04:26:48 - progress_bar.py[line:274] - INFO: epoch 001:   9663 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=113.3, bsz=40, num_updates=9650, lr=4.78957e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26123
2023-02-20 04:26:59 - progress_bar.py[line:274] - INFO: epoch 001:   9673 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=9660, lr=4.78921e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26134
2023-02-20 04:27:10 - progress_bar.py[line:274] - INFO: epoch 001:   9683 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.3, ups=0.91, wpb=115.2, bsz=40, num_updates=9670, lr=4.78885e-05, gnorm=0.197, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26145
2023-02-20 04:27:21 - progress_bar.py[line:274] - INFO: epoch 001:   9693 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=114.2, bsz=40, num_updates=9680, lr=4.78849e-05, gnorm=0.266, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26156
2023-02-20 04:27:32 - progress_bar.py[line:274] - INFO: epoch 001:   9703 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.92, wpb=113.6, bsz=40, num_updates=9690, lr=4.78813e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26167
2023-02-20 04:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   9713 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.5, bsz=40, num_updates=9700, lr=4.78777e-05, gnorm=0.17, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26178
2023-02-20 04:27:54 - progress_bar.py[line:274] - INFO: epoch 001:   9723 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=112.6, bsz=40, num_updates=9710, lr=4.7874e-05, gnorm=0.19, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26189
2023-02-20 04:28:06 - progress_bar.py[line:274] - INFO: epoch 001:   9733 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.88, wpb=114.7, bsz=40, num_updates=9720, lr=4.78704e-05, gnorm=0.187, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26200
2023-02-20 04:28:17 - progress_bar.py[line:274] - INFO: epoch 001:   9743 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=9730, lr=4.78668e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26212
2023-02-20 04:28:28 - progress_bar.py[line:274] - INFO: epoch 001:   9753 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=9740, lr=4.78632e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=26223
2023-02-20 04:28:39 - progress_bar.py[line:274] - INFO: epoch 001:   9763 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.2, ups=0.91, wpb=115, bsz=40, num_updates=9750, lr=4.78596e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26234
2023-02-20 04:28:50 - progress_bar.py[line:274] - INFO: epoch 001:   9773 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=113.3, bsz=40, num_updates=9760, lr=4.78559e-05, gnorm=0.162, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26245
2023-02-20 04:29:01 - progress_bar.py[line:274] - INFO: epoch 001:   9783 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.7, bsz=40, num_updates=9770, lr=4.78523e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26256
2023-02-20 04:29:13 - progress_bar.py[line:274] - INFO: epoch 001:   9793 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=9780, lr=4.78487e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26268
2023-02-20 04:29:24 - progress_bar.py[line:274] - INFO: epoch 001:   9803 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.88, wpb=113.9, bsz=40, num_updates=9790, lr=4.78451e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26279
2023-02-20 04:29:35 - progress_bar.py[line:274] - INFO: epoch 001:   9813 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.91, wpb=114.1, bsz=40, num_updates=9800, lr=4.78415e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26290
2023-02-20 04:29:46 - progress_bar.py[line:274] - INFO: epoch 001:   9823 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.88, wpb=112.4, bsz=40, num_updates=9810, lr=4.78379e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26301
2023-02-20 04:29:58 - progress_bar.py[line:274] - INFO: epoch 001:   9833 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.5, bsz=40, num_updates=9820, lr=4.78342e-05, gnorm=0.194, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=26313
2023-02-20 04:30:09 - progress_bar.py[line:274] - INFO: epoch 001:   9843 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.92, wpb=113.3, bsz=40, num_updates=9830, lr=4.78306e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26324
2023-02-20 04:30:20 - progress_bar.py[line:274] - INFO: epoch 001:   9853 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=9840, lr=4.7827e-05, gnorm=0.378, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26335
2023-02-20 04:30:31 - progress_bar.py[line:274] - INFO: epoch 001:   9863 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.87, wpb=112.9, bsz=40, num_updates=9850, lr=4.78234e-05, gnorm=0.288, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=26346
2023-02-20 04:30:42 - progress_bar.py[line:274] - INFO: epoch 001:   9873 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113.1, bsz=40, num_updates=9860, lr=4.78198e-05, gnorm=0.254, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26357
2023-02-20 04:30:54 - progress_bar.py[line:274] - INFO: epoch 001:   9883 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.1, bsz=40, num_updates=9870, lr=4.78161e-05, gnorm=0.288, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26369
2023-02-20 04:31:05 - progress_bar.py[line:274] - INFO: epoch 001:   9893 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.92, wpb=112.3, bsz=40, num_updates=9880, lr=4.78125e-05, gnorm=0.167, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26380
2023-02-20 04:31:16 - progress_bar.py[line:274] - INFO: epoch 001:   9903 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.2, bsz=40, num_updates=9890, lr=4.78089e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26391
2023-02-20 04:31:27 - progress_bar.py[line:274] - INFO: epoch 001:   9913 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113, bsz=40, num_updates=9900, lr=4.78053e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26402
2023-02-20 04:31:38 - progress_bar.py[line:274] - INFO: epoch 001:   9923 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113, bsz=40, num_updates=9910, lr=4.78017e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26413
2023-02-20 04:31:50 - progress_bar.py[line:274] - INFO: epoch 001:   9933 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.7, bsz=40, num_updates=9920, lr=4.77981e-05, gnorm=0.179, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26424
2023-02-20 04:32:01 - progress_bar.py[line:274] - INFO: epoch 001:   9943 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.89, wpb=113.3, bsz=40, num_updates=9930, lr=4.77944e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=26436
2023-02-20 04:32:12 - progress_bar.py[line:274] - INFO: epoch 001:   9953 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.88, wpb=113.9, bsz=40, num_updates=9940, lr=4.77908e-05, gnorm=0.316, clip=0, loss_scale=1024, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=26447
2023-02-20 04:32:24 - progress_bar.py[line:274] - INFO: epoch 001:   9963 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113, bsz=40, num_updates=9950, lr=4.77872e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26458
2023-02-20 04:32:35 - progress_bar.py[line:274] - INFO: epoch 001:   9973 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.7, bsz=40, num_updates=9960, lr=4.77836e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26470
2023-02-20 04:32:46 - progress_bar.py[line:274] - INFO: epoch 001:   9983 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.9, wpb=114, bsz=40, num_updates=9970, lr=4.778e-05, gnorm=0.162, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26481
2023-02-20 04:32:57 - progress_bar.py[line:274] - INFO: epoch 001:   9993 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.6, bsz=40, num_updates=9980, lr=4.77763e-05, gnorm=0.236, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26492
2023-02-20 04:33:09 - progress_bar.py[line:274] - INFO: epoch 001:  10003 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=9990, lr=4.77727e-05, gnorm=0.27, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26503
2023-02-20 04:33:19 - progress_bar.py[line:274] - INFO: epoch 001:  10013 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.5, bsz=40, num_updates=10000, lr=4.77691e-05, gnorm=0.222, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26514
2023-02-20 04:33:19 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 04:33:21 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 04:33:21 - train.py[line:551] - INFO: load:0.85 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 04:35:23 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 04:35:23 - train.py[line:551] - INFO: load:0.87 valid_run:122.31 task_valid:119.15 collect_output:1.98
2023-02-20 04:37:23 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 04:37:23 - train.py[line:551] - INFO: load:0.90 valid_run:242.42 task_valid:235.19 collect_output:4.89
2023-02-20 04:39:25 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 04:39:25 - train.py[line:551] - INFO: load:0.93 valid_run:364.51 task_valid:351.62 collect_output:9.36
2023-02-20 04:41:27 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 04:41:27 - train.py[line:551] - INFO: load:0.96 valid_run:486.31 task_valid:465.18 collect_output:16.54
2023-02-20 04:43:27 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 04:43:27 - train.py[line:551] - INFO: load:0.98 valid_run:606.65 task_valid:582.33 collect_output:18.67
2023-02-20 04:45:30 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 04:45:30 - train.py[line:551] - INFO: load:1.01 valid_run:729.41 task_valid:700.89 collect_output:21.83
2023-02-20 04:47:33 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 04:47:33 - train.py[line:551] - INFO: load:1.04 valid_run:852.35 task_valid:818.69 collect_output:25.96
2023-02-20 04:49:35 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 04:49:35 - train.py[line:551] - INFO: load:1.06 valid_run:974.05 task_valid:934.93 collect_output:30.39
2023-02-20 04:51:39 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 04:51:39 - train.py[line:551] - INFO: load:1.09 valid_run:1097.71 task_valid:1051.76 collect_output:36.18
2023-02-20 04:53:40 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 04:53:40 - train.py[line:551] - INFO: load:1.12 valid_run:1219.34 task_valid:1164.17 collect_output:44.36
2023-02-20 04:55:41 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 04:55:41 - train.py[line:551] - INFO: load:1.14 valid_run:1339.33 task_valid:1279.59 collect_output:47.89
2023-02-20 04:57:42 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 04:57:42 - train.py[line:551] - INFO: load:1.17 valid_run:1460.74 task_valid:1396.29 collect_output:51.59
2023-02-20 04:59:41 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 04:59:41 - train.py[line:551] - INFO: load:1.20 valid_run:1579.46 task_valid:1509.90 collect_output:55.65
2023-02-20 05:01:42 - train.py[line:549] - INFO: 2800 / 6234
2023-02-20 05:01:42 - train.py[line:551] - INFO: load:1.22 valid_run:1700.29 task_valid:1627.47 collect_output:57.89
2023-02-20 05:03:43 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 05:03:43 - train.py[line:551] - INFO: load:1.25 valid_run:1821.14 task_valid:1743.38 collect_output:61.80
2023-02-20 05:05:44 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 05:05:44 - train.py[line:551] - INFO: load:1.28 valid_run:1942.08 task_valid:1857.11 collect_output:67.98
2023-02-20 05:07:45 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 05:07:45 - train.py[line:551] - INFO: load:1.30 valid_run:2063.36 task_valid:1973.00 collect_output:72.34
2023-02-20 05:09:46 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 05:09:46 - train.py[line:551] - INFO: load:1.33 valid_run:2183.92 task_valid:2090.71 collect_output:74.15
2023-02-20 05:11:47 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 05:11:47 - train.py[line:551] - INFO: load:1.36 valid_run:2305.00 task_valid:2207.48 collect_output:77.42
2023-02-20 05:13:47 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 05:13:47 - train.py[line:551] - INFO: load:1.38 valid_run:2425.30 task_valid:2323.91 collect_output:80.26
2023-02-20 05:15:49 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 05:15:49 - train.py[line:551] - INFO: load:1.41 valid_run:2546.88 task_valid:2440.31 collect_output:84.41
2023-02-20 05:17:51 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 05:17:51 - train.py[line:551] - INFO: load:1.44 valid_run:2668.78 task_valid:2559.08 collect_output:86.50
2023-02-20 05:19:51 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 05:19:51 - train.py[line:551] - INFO: load:1.46 valid_run:2789.05 task_valid:2673.15 collect_output:91.67
2023-02-20 05:21:51 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 05:21:51 - train.py[line:551] - INFO: load:1.49 valid_run:2908.59 task_valid:2788.99 collect_output:94.33
2023-02-20 05:23:52 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 05:23:52 - train.py[line:551] - INFO: load:1.52 valid_run:3030.12 task_valid:2905.02 collect_output:98.74
2023-02-20 05:25:55 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 05:25:55 - train.py[line:551] - INFO: load:1.54 valid_run:3152.94 task_valid:3020.74 collect_output:104.82
2023-02-20 05:27:55 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 05:27:55 - train.py[line:551] - INFO: load:1.57 valid_run:3272.44 task_valid:3134.62 collect_output:109.41
2023-02-20 05:29:56 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 05:29:56 - train.py[line:551] - INFO: load:1.60 valid_run:3394.14 task_valid:3253.78 collect_output:110.92
2023-02-20 05:31:58 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 05:31:58 - train.py[line:551] - INFO: load:1.63 valid_run:3515.82 task_valid:3369.08 collect_output:116.26
2023-02-20 05:34:00 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 05:34:00 - train.py[line:551] - INFO: load:1.65 valid_run:3637.68 task_valid:3487.53 collect_output:118.64
2023-02-20 05:36:01 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 05:36:01 - train.py[line:551] - INFO: load:1.68 valid_run:3758.82 task_valid:3605.98 collect_output:120.31

====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6939;     R @ 500: 0.7153;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4309;    mR @ 100: 0.4603;    mR @ 500: 0.5214;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.5625) (covering:0.4429) (eating:0.8235) (flying in:0.5000) (growing on:0.2500) (hanging from:0.6387) (lying on:0.3000) (mounted on:0.0000) (painted on:0.0000) (parked on:1.0000) (playing:0.0000) (riding:0.9761) (says:0.0000) (sitting on:0.7381) (standing on:0.4243) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6581;     R @ 100: 0.6939;     R @ 500: 0.7153;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4309;    mR @ 100: 0.4603;    mR @ 500: 0.5214;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.5625) (covering:0.4429) (eating:0.8235) (flying in:0.5000) (growing on:0.2500) (hanging from:0.6387) (lying on:0.3000) (mounted on:0.0000) (painted on:0.0000) (parked on:1.0000) (playing:0.0000) (riding:0.9761) (says:0.0000) (sitting on:0.7381) (standing on:0.4243) (using:0.6000) (walking in:0.0000) (walking on:0.7027) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-20 05:36:32 - train.py[line:487] - INFO: 0.6938624649859944
2023-02-20 05:36:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 05:36:32 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.248 | loss_v1 0 | loss_v2 0 | nll_loss 0.081 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.693862 | ppl 1.06 | vqa_score 0.5236 | wps 118.3 | wpb 72 | bsz 24 | num_updates 10000 | best_R@100 0.693862
2023-02-20 05:36:32 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-02-20 05:36:32 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_10000.pt
2023-02-20 05:36:38 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_10000.pt
2023-02-20 05:36:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.6938624649859944) (writing took 11.47732262685895 seconds)
2023-02-20 05:36:55 - progress_bar.py[line:274] - INFO: epoch 001:  10023 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=113.7, bsz=40, num_updates=10010, lr=4.77655e-05, gnorm=0.229, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30329
2023-02-20 05:37:06 - progress_bar.py[line:274] - INFO: epoch 001:  10033 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.88, wpb=114.7, bsz=40, num_updates=10020, lr=4.77619e-05, gnorm=0.261, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30341
2023-02-20 05:37:17 - progress_bar.py[line:274] - INFO: epoch 001:  10043 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.91, wpb=114.7, bsz=40, num_updates=10030, lr=4.77583e-05, gnorm=0.196, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30352
2023-02-20 05:37:28 - progress_bar.py[line:274] - INFO: epoch 001:  10053 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=114.3, bsz=40, num_updates=10040, lr=4.77546e-05, gnorm=0.207, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30363
2023-02-20 05:37:39 - progress_bar.py[line:274] - INFO: epoch 001:  10063 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=113.1, bsz=40, num_updates=10050, lr=4.7751e-05, gnorm=0.326, clip=10, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30374
2023-02-20 05:37:50 - progress_bar.py[line:274] - INFO: epoch 001:  10073 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.9, wpb=113.3, bsz=40, num_updates=10060, lr=4.77474e-05, gnorm=0.168, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30385
2023-02-20 05:37:55 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 05:38:03 - progress_bar.py[line:274] - INFO: epoch 001:  10084 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=91.7, ups=0.81, wpb=113.2, bsz=40, num_updates=10070, lr=4.77438e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30398
2023-02-20 05:38:13 - progress_bar.py[line:274] - INFO: epoch 001:  10094 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106, ups=0.93, wpb=113.9, bsz=40, num_updates=10080, lr=4.77402e-05, gnorm=0.307, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30408
2023-02-20 05:38:25 - progress_bar.py[line:274] - INFO: epoch 001:  10104 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.88, wpb=114.6, bsz=40, num_updates=10090, lr=4.77365e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30420
2023-02-20 05:38:36 - progress_bar.py[line:274] - INFO: epoch 001:  10114 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.92, wpb=112.9, bsz=40, num_updates=10100, lr=4.77329e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30431
2023-02-20 05:38:47 - progress_bar.py[line:274] - INFO: epoch 001:  10124 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.3, ups=0.92, wpb=113.7, bsz=40, num_updates=10110, lr=4.77293e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30441
2023-02-20 05:38:58 - progress_bar.py[line:274] - INFO: epoch 001:  10134 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=113.4, bsz=40, num_updates=10120, lr=4.77257e-05, gnorm=0.289, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30453
2023-02-20 05:39:10 - progress_bar.py[line:274] - INFO: epoch 001:  10144 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.87, wpb=114.1, bsz=40, num_updates=10130, lr=4.77221e-05, gnorm=0.19, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30464
2023-02-20 05:39:21 - progress_bar.py[line:274] - INFO: epoch 001:  10154 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.9, bsz=40, num_updates=10140, lr=4.77185e-05, gnorm=0.228, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30476
2023-02-20 05:39:32 - progress_bar.py[line:274] - INFO: epoch 001:  10164 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114, bsz=40, num_updates=10150, lr=4.77148e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30487
2023-02-20 05:39:43 - progress_bar.py[line:274] - INFO: epoch 001:  10174 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.91, wpb=114.2, bsz=40, num_updates=10160, lr=4.77112e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30498
2023-02-20 05:39:55 - progress_bar.py[line:274] - INFO: epoch 001:  10184 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114.2, bsz=40, num_updates=10170, lr=4.77076e-05, gnorm=0.177, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30509
2023-02-20 05:40:06 - progress_bar.py[line:274] - INFO: epoch 001:  10194 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=10180, lr=4.7704e-05, gnorm=0.201, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30521
2023-02-20 05:40:17 - progress_bar.py[line:274] - INFO: epoch 001:  10204 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.3, bsz=40, num_updates=10190, lr=4.77004e-05, gnorm=0.181, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30531
2023-02-20 05:40:28 - progress_bar.py[line:274] - INFO: epoch 001:  10214 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=112.8, bsz=40, num_updates=10200, lr=4.76967e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30543
2023-02-20 05:40:39 - progress_bar.py[line:274] - INFO: epoch 001:  10224 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.6, ups=0.91, wpb=113.3, bsz=40, num_updates=10210, lr=4.76931e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30554
2023-02-20 05:40:50 - progress_bar.py[line:274] - INFO: epoch 001:  10234 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.87, wpb=113.4, bsz=40, num_updates=10220, lr=4.76895e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30565
2023-02-20 05:41:01 - progress_bar.py[line:274] - INFO: epoch 001:  10244 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.6, ups=0.93, wpb=113.7, bsz=40, num_updates=10230, lr=4.76859e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30576
2023-02-20 05:41:12 - progress_bar.py[line:274] - INFO: epoch 001:  10254 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.91, wpb=113.2, bsz=40, num_updates=10240, lr=4.76823e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30587
2023-02-20 05:41:23 - progress_bar.py[line:274] - INFO: epoch 001:  10264 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.9, wpb=114.2, bsz=40, num_updates=10250, lr=4.76787e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30598
2023-02-20 05:41:34 - progress_bar.py[line:274] - INFO: epoch 001:  10274 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107, ups=0.94, wpb=113.4, bsz=40, num_updates=10260, lr=4.7675e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30609
2023-02-20 05:41:45 - progress_bar.py[line:274] - INFO: epoch 001:  10284 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.7, bsz=40, num_updates=10270, lr=4.76714e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30620
2023-02-20 05:41:57 - progress_bar.py[line:274] - INFO: epoch 001:  10294 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=114, bsz=40, num_updates=10280, lr=4.76678e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30631
2023-02-20 05:42:08 - progress_bar.py[line:274] - INFO: epoch 001:  10304 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.3, bsz=40, num_updates=10290, lr=4.76642e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=30643
2023-02-20 05:42:19 - progress_bar.py[line:274] - INFO: epoch 001:  10314 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=113.4, bsz=40, num_updates=10300, lr=4.76606e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30654
2023-02-20 05:42:30 - progress_bar.py[line:274] - INFO: epoch 001:  10324 / 14203 loss=0.166, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.7, ups=0.92, wpb=113.2, bsz=40, num_updates=10310, lr=4.7657e-05, gnorm=0.144, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30665
2023-02-20 05:42:42 - progress_bar.py[line:274] - INFO: epoch 001:  10334 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=112.6, bsz=40, num_updates=10320, lr=4.76533e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30676
2023-02-20 05:42:53 - progress_bar.py[line:274] - INFO: epoch 001:  10344 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.89, wpb=113.8, bsz=40, num_updates=10330, lr=4.76497e-05, gnorm=0.17, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30688
2023-02-20 05:43:04 - progress_bar.py[line:274] - INFO: epoch 001:  10354 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.6, ups=0.92, wpb=113.8, bsz=40, num_updates=10340, lr=4.76461e-05, gnorm=0.335, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30699
2023-02-20 05:43:15 - progress_bar.py[line:274] - INFO: epoch 001:  10364 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.8, bsz=40, num_updates=10350, lr=4.76425e-05, gnorm=0.266, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30710
2023-02-20 05:43:26 - progress_bar.py[line:274] - INFO: epoch 001:  10374 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.5, bsz=40, num_updates=10360, lr=4.76389e-05, gnorm=0.181, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=30721
2023-02-20 05:43:37 - progress_bar.py[line:274] - INFO: epoch 001:  10384 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=112.3, bsz=40, num_updates=10370, lr=4.76352e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30732
2023-02-20 05:43:48 - progress_bar.py[line:274] - INFO: epoch 001:  10394 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.91, wpb=114.6, bsz=40, num_updates=10380, lr=4.76316e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30743
2023-02-20 05:43:59 - progress_bar.py[line:274] - INFO: epoch 001:  10404 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.1, bsz=40, num_updates=10390, lr=4.7628e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30754
2023-02-20 05:44:10 - progress_bar.py[line:274] - INFO: epoch 001:  10414 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=10400, lr=4.76244e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30765
2023-02-20 05:44:21 - progress_bar.py[line:274] - INFO: epoch 001:  10424 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.1, bsz=40, num_updates=10410, lr=4.76208e-05, gnorm=0.266, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30776
2023-02-20 05:44:32 - progress_bar.py[line:274] - INFO: epoch 001:  10434 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=113.9, bsz=40, num_updates=10420, lr=4.76172e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30787
2023-02-20 05:44:43 - progress_bar.py[line:274] - INFO: epoch 001:  10444 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.91, wpb=113.7, bsz=40, num_updates=10430, lr=4.76135e-05, gnorm=0.165, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30798
2023-02-20 05:44:55 - progress_bar.py[line:274] - INFO: epoch 001:  10454 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=10440, lr=4.76099e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=30809
2023-02-20 05:45:06 - progress_bar.py[line:274] - INFO: epoch 001:  10464 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.88, wpb=113.1, bsz=40, num_updates=10450, lr=4.76063e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30821
2023-02-20 05:45:17 - progress_bar.py[line:274] - INFO: epoch 001:  10474 / 14203 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.2, bsz=40, num_updates=10460, lr=4.76027e-05, gnorm=0.287, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30832
2023-02-20 05:45:28 - progress_bar.py[line:274] - INFO: epoch 001:  10484 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=113.8, bsz=40, num_updates=10470, lr=4.75991e-05, gnorm=0.186, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30843
2023-02-20 05:45:39 - progress_bar.py[line:274] - INFO: epoch 001:  10494 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.92, wpb=112.4, bsz=40, num_updates=10480, lr=4.75954e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30854
2023-02-20 05:45:50 - progress_bar.py[line:274] - INFO: epoch 001:  10504 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=113.5, bsz=40, num_updates=10490, lr=4.75918e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30865
2023-02-20 05:46:02 - progress_bar.py[line:274] - INFO: epoch 001:  10514 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.87, wpb=114, bsz=40, num_updates=10500, lr=4.75882e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30877
2023-02-20 05:46:13 - progress_bar.py[line:274] - INFO: epoch 001:  10524 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.92, wpb=113, bsz=40, num_updates=10510, lr=4.75846e-05, gnorm=0.192, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30888
2023-02-20 05:46:24 - progress_bar.py[line:274] - INFO: epoch 001:  10534 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=114, bsz=40, num_updates=10520, lr=4.7581e-05, gnorm=0.307, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30899
2023-02-20 05:46:35 - progress_bar.py[line:274] - INFO: epoch 001:  10544 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.3, bsz=40, num_updates=10530, lr=4.75774e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30910
2023-02-20 05:46:46 - progress_bar.py[line:274] - INFO: epoch 001:  10554 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113, bsz=40, num_updates=10540, lr=4.75737e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30921
2023-02-20 05:46:57 - progress_bar.py[line:274] - INFO: epoch 001:  10564 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.89, wpb=114.5, bsz=40, num_updates=10550, lr=4.75701e-05, gnorm=0.184, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30932
2023-02-20 05:47:08 - progress_bar.py[line:274] - INFO: epoch 001:  10574 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.4, bsz=40, num_updates=10560, lr=4.75665e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30943
2023-02-20 05:47:19 - progress_bar.py[line:274] - INFO: epoch 001:  10584 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.5, bsz=40, num_updates=10570, lr=4.75629e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30954
2023-02-20 05:47:30 - progress_bar.py[line:274] - INFO: epoch 001:  10594 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.92, wpb=114.3, bsz=40, num_updates=10580, lr=4.75593e-05, gnorm=0.286, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30965
2023-02-20 05:47:42 - progress_bar.py[line:274] - INFO: epoch 001:  10604 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.3, bsz=40, num_updates=10590, lr=4.75556e-05, gnorm=0.283, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30976
2023-02-20 05:47:53 - progress_bar.py[line:274] - INFO: epoch 001:  10614 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=113.8, bsz=40, num_updates=10600, lr=4.7552e-05, gnorm=0.273, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30988
2023-02-20 05:48:04 - progress_bar.py[line:274] - INFO: epoch 001:  10624 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=10610, lr=4.75484e-05, gnorm=0.269, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30999
2023-02-20 05:48:16 - progress_bar.py[line:274] - INFO: epoch 001:  10634 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.4, bsz=40, num_updates=10620, lr=4.75448e-05, gnorm=0.188, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31010
2023-02-20 05:48:26 - progress_bar.py[line:274] - INFO: epoch 001:  10644 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.92, wpb=113.3, bsz=40, num_updates=10630, lr=4.75412e-05, gnorm=0.207, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31021
2023-02-20 05:48:38 - progress_bar.py[line:274] - INFO: epoch 001:  10654 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.7, bsz=40, num_updates=10640, lr=4.75376e-05, gnorm=0.252, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31033
2023-02-20 05:48:49 - progress_bar.py[line:274] - INFO: epoch 001:  10664 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.88, wpb=114.6, bsz=40, num_updates=10650, lr=4.75339e-05, gnorm=0.25, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31044
2023-02-20 05:49:00 - progress_bar.py[line:274] - INFO: epoch 001:  10674 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102, ups=0.89, wpb=114.4, bsz=40, num_updates=10660, lr=4.75303e-05, gnorm=0.181, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31055
2023-02-20 05:49:11 - progress_bar.py[line:274] - INFO: epoch 001:  10684 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.5, ups=0.9, wpb=114.4, bsz=40, num_updates=10670, lr=4.75267e-05, gnorm=0.162, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31066
2023-02-20 05:49:23 - progress_bar.py[line:274] - INFO: epoch 001:  10694 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.5, bsz=40, num_updates=10680, lr=4.75231e-05, gnorm=0.258, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31078
2023-02-20 05:49:24 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 05:49:34 - progress_bar.py[line:274] - INFO: epoch 001:  10705 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.86, wpb=113.7, bsz=40, num_updates=10690, lr=4.75195e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=31089
2023-02-20 05:49:46 - progress_bar.py[line:274] - INFO: epoch 001:  10715 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.3, bsz=40, num_updates=10700, lr=4.75158e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31101
2023-02-20 05:49:57 - progress_bar.py[line:274] - INFO: epoch 001:  10725 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.6, ups=0.92, wpb=113.7, bsz=40, num_updates=10710, lr=4.75122e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31111
2023-02-20 05:50:08 - progress_bar.py[line:274] - INFO: epoch 001:  10735 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.4, bsz=40, num_updates=10720, lr=4.75086e-05, gnorm=0.267, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31123
2023-02-20 05:50:19 - progress_bar.py[line:274] - INFO: epoch 001:  10745 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.89, wpb=114, bsz=40, num_updates=10730, lr=4.7505e-05, gnorm=0.24, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31134
2023-02-20 05:50:30 - progress_bar.py[line:274] - INFO: epoch 001:  10755 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.4, bsz=40, num_updates=10740, lr=4.75014e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31145
2023-02-20 05:50:42 - progress_bar.py[line:274] - INFO: epoch 001:  10765 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=113.1, bsz=40, num_updates=10750, lr=4.74978e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31156
2023-02-20 05:50:53 - progress_bar.py[line:274] - INFO: epoch 001:  10775 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=10760, lr=4.74941e-05, gnorm=0.325, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31168
2023-02-20 05:51:04 - progress_bar.py[line:274] - INFO: epoch 001:  10785 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.9, bsz=40, num_updates=10770, lr=4.74905e-05, gnorm=0.236, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31179
2023-02-20 05:51:15 - progress_bar.py[line:274] - INFO: epoch 001:  10795 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.9, wpb=112.6, bsz=40, num_updates=10780, lr=4.74869e-05, gnorm=0.248, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31190
2023-02-20 05:51:25 - progress_bar.py[line:274] - INFO: epoch 001:  10805 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=108.4, ups=0.96, wpb=113.4, bsz=40, num_updates=10790, lr=4.74833e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=31200
2023-02-20 05:51:36 - progress_bar.py[line:274] - INFO: epoch 001:  10815 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=113.7, bsz=40, num_updates=10800, lr=4.74797e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31211
2023-02-20 05:51:48 - progress_bar.py[line:274] - INFO: epoch 001:  10825 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.5, bsz=40, num_updates=10810, lr=4.7476e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31223
2023-02-20 05:51:59 - progress_bar.py[line:274] - INFO: epoch 001:  10835 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=113.1, bsz=40, num_updates=10820, lr=4.74724e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31234
2023-02-20 05:52:10 - progress_bar.py[line:274] - INFO: epoch 001:  10845 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=113.6, bsz=40, num_updates=10830, lr=4.74688e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31245
2023-02-20 05:52:21 - progress_bar.py[line:274] - INFO: epoch 001:  10855 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.92, wpb=112.2, bsz=40, num_updates=10840, lr=4.74652e-05, gnorm=0.176, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31256
2023-02-20 05:52:32 - progress_bar.py[line:274] - INFO: epoch 001:  10865 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=112.8, bsz=40, num_updates=10850, lr=4.74616e-05, gnorm=0.298, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31267
2023-02-20 05:52:43 - progress_bar.py[line:274] - INFO: epoch 001:  10875 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.4, bsz=40, num_updates=10860, lr=4.7458e-05, gnorm=0.176, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31278
2023-02-20 05:52:54 - progress_bar.py[line:274] - INFO: epoch 001:  10885 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.2, ups=0.92, wpb=113.5, bsz=40, num_updates=10870, lr=4.74543e-05, gnorm=0.12, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31289
2023-02-20 05:53:05 - progress_bar.py[line:274] - INFO: epoch 001:  10895 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.1, ups=0.91, wpb=115, bsz=40, num_updates=10880, lr=4.74507e-05, gnorm=0.299, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31300
2023-02-20 05:53:17 - progress_bar.py[line:274] - INFO: epoch 001:  10905 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=10890, lr=4.74471e-05, gnorm=0.226, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31311
2023-02-20 05:53:28 - progress_bar.py[line:274] - INFO: epoch 001:  10915 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=112.9, bsz=40, num_updates=10900, lr=4.74435e-05, gnorm=0.191, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31322
2023-02-20 05:53:39 - progress_bar.py[line:274] - INFO: epoch 001:  10925 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=115.4, nsentences=40, sample_size=115.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.88, wpb=115.4, bsz=40, num_updates=10910, lr=4.74399e-05, gnorm=0.271, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31334
2023-02-20 05:53:50 - progress_bar.py[line:274] - INFO: epoch 001:  10935 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.88, wpb=114.7, bsz=40, num_updates=10920, lr=4.74362e-05, gnorm=0.258, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31345
2023-02-20 05:54:01 - progress_bar.py[line:274] - INFO: epoch 001:  10945 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.6, ups=0.92, wpb=113.9, bsz=40, num_updates=10930, lr=4.74326e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31356
2023-02-20 05:54:12 - progress_bar.py[line:274] - INFO: epoch 001:  10955 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.3, bsz=40, num_updates=10940, lr=4.7429e-05, gnorm=0.162, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31367
2023-02-20 05:54:23 - progress_bar.py[line:274] - INFO: epoch 001:  10965 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.4, bsz=40, num_updates=10950, lr=4.74254e-05, gnorm=0.242, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31378
2023-02-20 05:54:35 - progress_bar.py[line:274] - INFO: epoch 001:  10975 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.1, bsz=40, num_updates=10960, lr=4.74218e-05, gnorm=0.314, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31390
2023-02-20 05:54:46 - progress_bar.py[line:274] - INFO: epoch 001:  10985 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.5, bsz=40, num_updates=10970, lr=4.74182e-05, gnorm=0.197, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31401
2023-02-20 05:54:57 - progress_bar.py[line:274] - INFO: epoch 001:  10995 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.5, bsz=40, num_updates=10980, lr=4.74145e-05, gnorm=0.235, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31412
2023-02-20 05:55:08 - progress_bar.py[line:274] - INFO: epoch 001:  11005 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=113.5, bsz=40, num_updates=10990, lr=4.74109e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31423
2023-02-20 05:55:19 - progress_bar.py[line:274] - INFO: epoch 001:  11015 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=113.6, bsz=40, num_updates=11000, lr=4.74073e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31434
2023-02-20 05:55:31 - progress_bar.py[line:274] - INFO: epoch 001:  11025 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.3, bsz=40, num_updates=11010, lr=4.74037e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31445
2023-02-20 05:55:41 - progress_bar.py[line:274] - INFO: epoch 001:  11035 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.93, wpb=113.5, bsz=40, num_updates=11020, lr=4.74001e-05, gnorm=0.202, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31456
2023-02-20 05:55:52 - progress_bar.py[line:274] - INFO: epoch 001:  11045 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.9, ups=0.93, wpb=113.9, bsz=40, num_updates=11030, lr=4.73965e-05, gnorm=0.191, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31467
2023-02-20 05:56:03 - progress_bar.py[line:274] - INFO: epoch 001:  11055 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.5, ups=0.91, wpb=114.3, bsz=40, num_updates=11040, lr=4.73928e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31478
2023-02-20 05:56:14 - progress_bar.py[line:274] - INFO: epoch 001:  11065 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114, bsz=40, num_updates=11050, lr=4.73892e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31489
2023-02-20 05:56:26 - progress_bar.py[line:274] - INFO: epoch 001:  11075 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=11060, lr=4.73856e-05, gnorm=0.183, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31501
2023-02-20 05:56:37 - progress_bar.py[line:274] - INFO: epoch 001:  11085 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.2, bsz=40, num_updates=11070, lr=4.7382e-05, gnorm=0.276, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31512
2023-02-20 05:56:48 - progress_bar.py[line:274] - INFO: epoch 001:  11095 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.4, bsz=40, num_updates=11080, lr=4.73784e-05, gnorm=0.266, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31523
2023-02-20 05:56:59 - progress_bar.py[line:274] - INFO: epoch 001:  11105 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.87, wpb=114.5, bsz=40, num_updates=11090, lr=4.73747e-05, gnorm=0.231, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31534
2023-02-20 05:57:10 - progress_bar.py[line:274] - INFO: epoch 001:  11115 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.92, wpb=112.4, bsz=40, num_updates=11100, lr=4.73711e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31545
2023-02-20 05:57:21 - progress_bar.py[line:274] - INFO: epoch 001:  11125 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.6, ups=0.91, wpb=113.2, bsz=40, num_updates=11110, lr=4.73675e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31556
2023-02-20 05:57:32 - progress_bar.py[line:274] - INFO: epoch 001:  11135 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.9, ups=0.93, wpb=112.7, bsz=40, num_updates=11120, lr=4.73639e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31567
2023-02-20 05:57:43 - progress_bar.py[line:274] - INFO: epoch 001:  11145 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.7, bsz=40, num_updates=11130, lr=4.73603e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31578
2023-02-20 05:57:54 - progress_bar.py[line:274] - INFO: epoch 001:  11155 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.5, bsz=40, num_updates=11140, lr=4.73567e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31589
2023-02-20 05:58:06 - progress_bar.py[line:274] - INFO: epoch 001:  11165 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.89, wpb=113, bsz=40, num_updates=11150, lr=4.7353e-05, gnorm=0.362, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31601
2023-02-20 05:58:16 - progress_bar.py[line:274] - INFO: epoch 001:  11175 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.8, ups=0.93, wpb=114.8, bsz=40, num_updates=11160, lr=4.73494e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31611
2023-02-20 05:58:28 - progress_bar.py[line:274] - INFO: epoch 001:  11185 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=114.2, bsz=40, num_updates=11170, lr=4.73458e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31623
2023-02-20 05:58:39 - progress_bar.py[line:274] - INFO: epoch 001:  11195 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.8, ups=0.92, wpb=114.2, bsz=40, num_updates=11180, lr=4.73422e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31634
2023-02-20 05:58:50 - progress_bar.py[line:274] - INFO: epoch 001:  11205 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.91, wpb=111.8, bsz=40, num_updates=11190, lr=4.73386e-05, gnorm=0.21, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31645
2023-02-20 05:59:01 - progress_bar.py[line:274] - INFO: epoch 001:  11215 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.2, bsz=40, num_updates=11200, lr=4.73349e-05, gnorm=0.208, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31656
2023-02-20 05:59:12 - progress_bar.py[line:274] - INFO: epoch 001:  11225 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.7, ups=0.94, wpb=112.2, bsz=40, num_updates=11210, lr=4.73313e-05, gnorm=0.2, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31667
2023-02-20 05:59:23 - progress_bar.py[line:274] - INFO: epoch 001:  11235 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.92, wpb=113.1, bsz=40, num_updates=11220, lr=4.73277e-05, gnorm=0.364, clip=10, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31677
2023-02-20 05:59:34 - progress_bar.py[line:274] - INFO: epoch 001:  11245 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.5, bsz=40, num_updates=11230, lr=4.73241e-05, gnorm=0.28, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31689
2023-02-20 05:59:45 - progress_bar.py[line:274] - INFO: epoch 001:  11255 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113.1, bsz=40, num_updates=11240, lr=4.73205e-05, gnorm=0.24, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=31700
2023-02-20 05:59:56 - progress_bar.py[line:274] - INFO: epoch 001:  11265 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.89, wpb=114.8, bsz=40, num_updates=11250, lr=4.73169e-05, gnorm=0.193, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31711
2023-02-20 06:00:07 - progress_bar.py[line:274] - INFO: epoch 001:  11275 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=11260, lr=4.73132e-05, gnorm=0.206, clip=0, loss_scale=2048, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=31722
2023-02-20 06:00:19 - progress_bar.py[line:274] - INFO: epoch 001:  11285 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.9, ups=0.86, wpb=112.2, bsz=40, num_updates=11270, lr=4.73096e-05, gnorm=0.211, clip=0, loss_scale=2048, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=31734
2023-02-20 06:00:30 - progress_bar.py[line:274] - INFO: epoch 001:  11295 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.2, ups=0.91, wpb=112.8, bsz=40, num_updates=11280, lr=4.7306e-05, gnorm=0.181, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31745
2023-02-20 06:00:41 - progress_bar.py[line:274] - INFO: epoch 001:  11305 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.88, wpb=114.5, bsz=40, num_updates=11290, lr=4.73024e-05, gnorm=0.203, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31756
2023-02-20 06:00:52 - progress_bar.py[line:274] - INFO: epoch 001:  11315 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.89, wpb=113.5, bsz=40, num_updates=11300, lr=4.72988e-05, gnorm=0.164, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31767
2023-02-20 06:00:58 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 06:01:05 - progress_bar.py[line:274] - INFO: epoch 001:  11326 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=93.9, ups=0.82, wpb=114.7, bsz=40, num_updates=11310, lr=4.72951e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=31779
2023-02-20 06:01:15 - progress_bar.py[line:274] - INFO: epoch 001:  11336 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.92, wpb=113, bsz=40, num_updates=11320, lr=4.72915e-05, gnorm=0.196, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31790
2023-02-20 06:01:27 - progress_bar.py[line:274] - INFO: epoch 001:  11346 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.1, bsz=40, num_updates=11330, lr=4.72879e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31802
2023-02-20 06:01:38 - progress_bar.py[line:274] - INFO: epoch 001:  11356 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.87, wpb=112.6, bsz=40, num_updates=11340, lr=4.72843e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31813
2023-02-20 06:01:50 - progress_bar.py[line:274] - INFO: epoch 001:  11366 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.1, bsz=40, num_updates=11350, lr=4.72807e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31825
2023-02-20 06:02:01 - progress_bar.py[line:274] - INFO: epoch 001:  11376 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.1, bsz=40, num_updates=11360, lr=4.72771e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31836
2023-02-20 06:02:12 - progress_bar.py[line:274] - INFO: epoch 001:  11386 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.6, bsz=40, num_updates=11370, lr=4.72734e-05, gnorm=0.176, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31847
2023-02-20 06:02:23 - progress_bar.py[line:274] - INFO: epoch 001:  11396 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104, ups=0.91, wpb=114, bsz=40, num_updates=11380, lr=4.72698e-05, gnorm=0.523, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31858
2023-02-20 06:02:35 - progress_bar.py[line:274] - INFO: epoch 001:  11406 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.7, bsz=40, num_updates=11390, lr=4.72662e-05, gnorm=0.168, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31870
2023-02-20 06:02:46 - progress_bar.py[line:274] - INFO: epoch 001:  11416 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.87, wpb=114.6, bsz=40, num_updates=11400, lr=4.72626e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31881
2023-02-20 06:02:57 - progress_bar.py[line:274] - INFO: epoch 001:  11426 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.92, wpb=113.9, bsz=40, num_updates=11410, lr=4.7259e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31892
2023-02-20 06:03:08 - progress_bar.py[line:274] - INFO: epoch 001:  11436 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=11420, lr=4.72553e-05, gnorm=0.176, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31903
2023-02-20 06:03:20 - progress_bar.py[line:274] - INFO: epoch 001:  11446 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.2, bsz=40, num_updates=11430, lr=4.72517e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31914
2023-02-20 06:03:31 - progress_bar.py[line:274] - INFO: epoch 001:  11456 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.3, ups=0.9, wpb=113.2, bsz=40, num_updates=11440, lr=4.72481e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31926
2023-02-20 06:03:42 - progress_bar.py[line:274] - INFO: epoch 001:  11466 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.9, wpb=114, bsz=40, num_updates=11450, lr=4.72445e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31937
2023-02-20 06:03:53 - progress_bar.py[line:274] - INFO: epoch 001:  11476 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.2, bsz=40, num_updates=11460, lr=4.72409e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31948
2023-02-20 06:04:04 - progress_bar.py[line:274] - INFO: epoch 001:  11486 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=115.4, nsentences=40, sample_size=115.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107.2, ups=0.93, wpb=115.4, bsz=40, num_updates=11470, lr=4.72373e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31959
2023-02-20 06:04:15 - progress_bar.py[line:274] - INFO: epoch 001:  11496 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.87, wpb=113.9, bsz=40, num_updates=11480, lr=4.72336e-05, gnorm=0.267, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31970
2023-02-20 06:04:26 - progress_bar.py[line:274] - INFO: epoch 001:  11506 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112.9, bsz=40, num_updates=11490, lr=4.723e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31981
2023-02-20 06:04:38 - progress_bar.py[line:274] - INFO: epoch 001:  11516 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.87, wpb=114.7, bsz=40, num_updates=11500, lr=4.72264e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31993
2023-02-20 06:04:49 - progress_bar.py[line:274] - INFO: epoch 001:  11526 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.6, bsz=40, num_updates=11510, lr=4.72228e-05, gnorm=0.305, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32004
2023-02-20 06:05:00 - progress_bar.py[line:274] - INFO: epoch 001:  11536 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.91, wpb=113.1, bsz=40, num_updates=11520, lr=4.72192e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32015
2023-02-20 06:05:11 - progress_bar.py[line:274] - INFO: epoch 001:  11546 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.8, ups=0.92, wpb=114.2, bsz=40, num_updates=11530, lr=4.72155e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32026
2023-02-20 06:05:22 - progress_bar.py[line:274] - INFO: epoch 001:  11556 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.2, bsz=40, num_updates=11540, lr=4.72119e-05, gnorm=0.23, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32037
2023-02-20 06:05:34 - progress_bar.py[line:274] - INFO: epoch 001:  11566 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.9, ups=0.89, wpb=113.9, bsz=40, num_updates=11550, lr=4.72083e-05, gnorm=0.214, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32048
2023-02-20 06:05:45 - progress_bar.py[line:274] - INFO: epoch 001:  11576 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.9, wpb=113.6, bsz=40, num_updates=11560, lr=4.72047e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32060
2023-02-20 06:05:56 - progress_bar.py[line:274] - INFO: epoch 001:  11586 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=11570, lr=4.72011e-05, gnorm=0.16, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32071
2023-02-20 06:06:07 - progress_bar.py[line:274] - INFO: epoch 001:  11596 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=114.4, bsz=40, num_updates=11580, lr=4.71975e-05, gnorm=0.192, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32082
2023-02-20 06:06:18 - progress_bar.py[line:274] - INFO: epoch 001:  11606 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.9, wpb=113.2, bsz=40, num_updates=11590, lr=4.71938e-05, gnorm=0.141, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32093
2023-02-20 06:06:29 - progress_bar.py[line:274] - INFO: epoch 001:  11616 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=11600, lr=4.71902e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32104
2023-02-20 06:06:40 - progress_bar.py[line:274] - INFO: epoch 001:  11626 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.91, wpb=113.2, bsz=40, num_updates=11610, lr=4.71866e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32115
2023-02-20 06:06:51 - progress_bar.py[line:274] - INFO: epoch 001:  11636 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.88, wpb=114.3, bsz=40, num_updates=11620, lr=4.7183e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32126
2023-02-20 06:07:03 - progress_bar.py[line:274] - INFO: epoch 001:  11646 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.5, bsz=40, num_updates=11630, lr=4.71794e-05, gnorm=0.267, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32138
2023-02-20 06:07:14 - progress_bar.py[line:274] - INFO: epoch 001:  11656 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.88, wpb=114.4, bsz=40, num_updates=11640, lr=4.71758e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32149
2023-02-20 06:07:25 - progress_bar.py[line:274] - INFO: epoch 001:  11666 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.9, wpb=112.4, bsz=40, num_updates=11650, lr=4.71721e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32160
2023-02-20 06:07:36 - progress_bar.py[line:274] - INFO: epoch 001:  11676 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=113.9, bsz=40, num_updates=11660, lr=4.71685e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32171
2023-02-20 06:07:48 - progress_bar.py[line:274] - INFO: epoch 001:  11686 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.2, bsz=40, num_updates=11670, lr=4.71649e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32183
2023-02-20 06:07:59 - progress_bar.py[line:274] - INFO: epoch 001:  11696 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.92, wpb=113, bsz=40, num_updates=11680, lr=4.71613e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32193
2023-02-20 06:08:10 - progress_bar.py[line:274] - INFO: epoch 001:  11706 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.8, bsz=40, num_updates=11690, lr=4.71577e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32205
2023-02-20 06:08:21 - progress_bar.py[line:274] - INFO: epoch 001:  11716 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.9, wpb=113.3, bsz=40, num_updates=11700, lr=4.7154e-05, gnorm=0.172, clip=0, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=32216
2023-02-20 06:08:32 - progress_bar.py[line:274] - INFO: epoch 001:  11726 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=113.7, bsz=40, num_updates=11710, lr=4.71504e-05, gnorm=0.274, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=32227
2023-02-20 06:08:44 - progress_bar.py[line:274] - INFO: epoch 001:  11736 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.87, wpb=113.9, bsz=40, num_updates=11720, lr=4.71468e-05, gnorm=0.14, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32239
2023-02-20 06:08:55 - progress_bar.py[line:274] - INFO: epoch 001:  11746 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.91, wpb=113.8, bsz=40, num_updates=11730, lr=4.71432e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32250
2023-02-20 06:09:06 - progress_bar.py[line:274] - INFO: epoch 001:  11756 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.87, wpb=114.3, bsz=40, num_updates=11740, lr=4.71396e-05, gnorm=0.257, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32261
2023-02-20 06:09:17 - progress_bar.py[line:274] - INFO: epoch 001:  11766 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.91, wpb=114.9, bsz=40, num_updates=11750, lr=4.7136e-05, gnorm=0.202, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32272
2023-02-20 06:09:28 - progress_bar.py[line:274] - INFO: epoch 001:  11776 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=113.2, bsz=40, num_updates=11760, lr=4.71323e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32283
2023-02-20 06:09:40 - progress_bar.py[line:274] - INFO: epoch 001:  11786 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.2, bsz=40, num_updates=11770, lr=4.71287e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32295
2023-02-20 06:09:51 - progress_bar.py[line:274] - INFO: epoch 001:  11796 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.2, bsz=40, num_updates=11780, lr=4.71251e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32306
2023-02-20 06:10:02 - progress_bar.py[line:274] - INFO: epoch 001:  11806 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105, ups=0.92, wpb=114.4, bsz=40, num_updates=11790, lr=4.71215e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32317
2023-02-20 06:10:13 - progress_bar.py[line:274] - INFO: epoch 001:  11816 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=115.1, nsentences=40, sample_size=115.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.9, wpb=115.1, bsz=40, num_updates=11800, lr=4.71179e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32328
2023-02-20 06:10:24 - progress_bar.py[line:274] - INFO: epoch 001:  11826 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.4, bsz=40, num_updates=11810, lr=4.71142e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32339
2023-02-20 06:10:36 - progress_bar.py[line:274] - INFO: epoch 001:  11836 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=113.7, bsz=40, num_updates=11820, lr=4.71106e-05, gnorm=0.177, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32351
2023-02-20 06:10:47 - progress_bar.py[line:274] - INFO: epoch 001:  11846 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.86, wpb=113.8, bsz=40, num_updates=11830, lr=4.7107e-05, gnorm=0.206, clip=0, loss_scale=2048, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=32362
2023-02-20 06:10:58 - progress_bar.py[line:274] - INFO: epoch 001:  11856 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.93, wpb=111.6, bsz=40, num_updates=11840, lr=4.71034e-05, gnorm=0.21, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32373
2023-02-20 06:11:09 - progress_bar.py[line:274] - INFO: epoch 001:  11866 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.6, bsz=40, num_updates=11850, lr=4.70998e-05, gnorm=0.256, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32384
2023-02-20 06:11:19 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 06:11:21 - progress_bar.py[line:274] - INFO: epoch 001:  11877 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=92.3, ups=0.82, wpb=112.6, bsz=40, num_updates=11860, lr=4.70962e-05, gnorm=0.175, clip=0, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=32396
2023-02-20 06:11:33 - progress_bar.py[line:274] - INFO: epoch 001:  11887 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.3, bsz=40, num_updates=11870, lr=4.70925e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32408
2023-02-20 06:11:44 - progress_bar.py[line:274] - INFO: epoch 001:  11897 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=11880, lr=4.70889e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32418
2023-02-20 06:11:55 - progress_bar.py[line:274] - INFO: epoch 001:  11907 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=11890, lr=4.70853e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32430
2023-02-20 06:12:06 - progress_bar.py[line:274] - INFO: epoch 001:  11917 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.3, bsz=40, num_updates=11900, lr=4.70817e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32441
2023-02-20 06:12:17 - progress_bar.py[line:274] - INFO: epoch 001:  11927 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.4, bsz=40, num_updates=11910, lr=4.70781e-05, gnorm=0.191, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32452
2023-02-20 06:12:28 - progress_bar.py[line:274] - INFO: epoch 001:  11937 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.92, wpb=113.5, bsz=40, num_updates=11920, lr=4.70744e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32463
2023-02-20 06:12:39 - progress_bar.py[line:274] - INFO: epoch 001:  11947 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=115.6, nsentences=40, sample_size=115.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.89, wpb=115.6, bsz=40, num_updates=11930, lr=4.70708e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32474
2023-02-20 06:12:50 - progress_bar.py[line:274] - INFO: epoch 001:  11957 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.9, wpb=114.7, bsz=40, num_updates=11940, lr=4.70672e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32485
2023-02-20 06:13:02 - progress_bar.py[line:274] - INFO: epoch 001:  11967 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.9, wpb=113.5, bsz=40, num_updates=11950, lr=4.70636e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32496
2023-02-20 06:13:12 - progress_bar.py[line:274] - INFO: epoch 001:  11977 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.1, ups=0.92, wpb=113.4, bsz=40, num_updates=11960, lr=4.706e-05, gnorm=0.177, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32507
2023-02-20 06:13:24 - progress_bar.py[line:274] - INFO: epoch 001:  11987 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.3, bsz=40, num_updates=11970, lr=4.70564e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32519
2023-02-20 06:13:35 - progress_bar.py[line:274] - INFO: epoch 001:  11997 / 14203 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.4, bsz=40, num_updates=11980, lr=4.70527e-05, gnorm=0.305, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32530
2023-02-20 06:13:46 - progress_bar.py[line:274] - INFO: epoch 001:  12007 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.88, wpb=112.2, bsz=40, num_updates=11990, lr=4.70491e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32541
2023-02-20 06:13:57 - progress_bar.py[line:274] - INFO: epoch 001:  12017 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=112.5, bsz=40, num_updates=12000, lr=4.70455e-05, gnorm=0.283, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32552
2023-02-20 06:13:57 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 06:13:59 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 06:13:59 - train.py[line:551] - INFO: load:1.10 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 06:16:00 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 06:16:00 - train.py[line:551] - INFO: load:1.13 valid_run:121.72 task_valid:118.72 collect_output:1.92
2023-02-20 06:18:00 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 06:18:00 - train.py[line:551] - INFO: load:1.15 valid_run:241.45 task_valid:234.05 collect_output:5.29
2023-02-20 06:20:02 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 06:20:02 - train.py[line:551] - INFO: load:1.18 valid_run:363.27 task_valid:350.17 collect_output:9.93
2023-02-20 06:22:04 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 06:22:04 - train.py[line:551] - INFO: load:1.21 valid_run:485.00 task_valid:463.40 collect_output:17.41
2023-02-20 06:24:04 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 06:24:04 - train.py[line:551] - INFO: load:1.23 valid_run:605.13 task_valid:580.18 collect_output:19.75
2023-02-20 06:26:07 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 06:26:07 - train.py[line:551] - INFO: load:1.26 valid_run:727.88 task_valid:698.58 collect_output:23.07
2023-02-20 06:28:10 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 06:28:10 - train.py[line:551] - INFO: load:1.29 valid_run:850.73 task_valid:816.35 collect_output:27.12
2023-02-20 06:30:12 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 06:30:12 - train.py[line:551] - INFO: load:1.31 valid_run:972.44 task_valid:932.52 collect_output:31.64
2023-02-20 06:32:15 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 06:32:15 - train.py[line:551] - INFO: load:1.34 valid_run:1096.04 task_valid:1049.45 collect_output:37.29
2023-02-20 06:34:17 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 06:34:17 - train.py[line:551] - INFO: load:1.37 valid_run:1217.54 task_valid:1161.67 collect_output:45.54
2023-02-20 06:36:17 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 06:36:17 - train.py[line:551] - INFO: load:1.39 valid_run:1337.33 task_valid:1276.79 collect_output:49.18
2023-02-20 06:38:18 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 06:38:18 - train.py[line:551] - INFO: load:1.42 valid_run:1458.79 task_valid:1393.40 collect_output:53.03
2023-02-20 06:40:17 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 06:40:17 - train.py[line:551] - INFO: load:1.45 valid_run:1577.44 task_valid:1506.78 collect_output:57.28
2023-02-20 06:42:18 - train.py[line:549] - INFO: 2800 / 6234
2023-02-20 06:42:18 - train.py[line:551] - INFO: load:1.48 valid_run:1698.14 task_valid:1624.09 collect_output:59.65
2023-02-20 06:44:19 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 06:44:19 - train.py[line:551] - INFO: load:1.50 valid_run:1818.92 task_valid:1739.78 collect_output:63.69
2023-02-20 06:46:19 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 06:46:19 - train.py[line:551] - INFO: load:1.53 valid_run:1939.81 task_valid:1853.24 collect_output:70.10
2023-02-20 06:48:21 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 06:48:21 - train.py[line:551] - INFO: load:1.55 valid_run:2060.89 task_valid:1968.91 collect_output:74.51
2023-02-20 06:50:21 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 06:50:21 - train.py[line:551] - INFO: load:1.58 valid_run:2181.36 task_valid:2086.43 collect_output:76.43
2023-02-20 06:52:22 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 06:52:22 - train.py[line:551] - INFO: load:1.61 valid_run:2302.51 task_valid:2203.10 collect_output:79.84
2023-02-20 06:54:23 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 06:54:23 - train.py[line:551] - INFO: load:1.63 valid_run:2422.73 task_valid:2319.32 collect_output:82.83
2023-02-20 06:56:24 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 06:56:24 - train.py[line:551] - INFO: load:1.66 valid_run:2544.27 task_valid:2435.38 collect_output:87.31
2023-02-20 06:58:26 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 06:58:26 - train.py[line:551] - INFO: load:1.69 valid_run:2665.89 task_valid:2553.89 collect_output:89.40
2023-02-20 07:00:26 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 07:00:26 - train.py[line:551] - INFO: load:1.71 valid_run:2785.97 task_valid:2667.79 collect_output:94.57
2023-02-20 07:02:26 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 07:02:26 - train.py[line:551] - INFO: load:1.74 valid_run:2905.77 task_valid:2783.56 collect_output:97.58
2023-02-20 07:04:27 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 07:04:27 - train.py[line:551] - INFO: load:1.76 valid_run:3027.21 task_valid:2899.39 collect_output:102.19
2023-02-20 07:06:30 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 07:06:30 - train.py[line:551] - INFO: load:1.79 valid_run:3149.95 task_valid:3015.01 collect_output:108.29
2023-02-20 07:08:30 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 07:08:30 - train.py[line:551] - INFO: load:1.81 valid_run:3269.38 task_valid:3128.75 collect_output:112.97
2023-02-20 07:10:31 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 07:10:31 - train.py[line:551] - INFO: load:1.84 valid_run:3390.86 task_valid:3247.73 collect_output:114.48
2023-02-20 07:12:33 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 07:12:33 - train.py[line:551] - INFO: load:1.87 valid_run:3512.40 task_valid:3362.93 collect_output:119.72
2023-02-20 07:14:35 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 07:14:35 - train.py[line:551] - INFO: load:1.90 valid_run:3634.01 task_valid:3480.99 collect_output:122.28
2023-02-20 07:16:35 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 07:16:35 - train.py[line:551] - INFO: load:1.92 valid_run:3754.87 task_valid:3599.05 collect_output:124.06

====================================================================================================
SGG eval:     R @ 50: 0.6514;     R @ 100: 0.6912;     R @ 500: 0.7123;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4087;    mR @ 100: 0.4900;    mR @ 500: 0.5267;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.8125) (covering:0.3714) (eating:0.8235) (flying in:0.7727) (growing on:0.2500) (hanging from:0.6129) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9794) (says:0.0000) (sitting on:0.7211) (standing on:0.4243) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================

2023-02-20 07:17:06 - train.py[line:487] - INFO: 0.6911867074102368

====================================================================================================
SGG eval:     R @ 50: 0.6514;     R @ 100: 0.6912;     R @ 500: 0.7123;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4087;    mR @ 100: 0.4900;    mR @ 500: 0.5267;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.8125) (covering:0.3714) (eating:0.8235) (flying in:0.7727) (growing on:0.2500) (hanging from:0.6129) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9794) (says:0.0000) (sitting on:0.7211) (standing on:0.4243) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================

2023-02-20 07:17:06 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 07:17:06 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.26 | loss_v1 0 | loss_v2 0 | nll_loss 0.092 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.691187 | ppl 1.07 | vqa_score 0.5203 | wps 118.4 | wpb 72 | bsz 24 | num_updates 12000 | best_R@100 0.693862
2023-02-20 07:17:06 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 12000 updates
2023-02-20 07:17:06 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_12000.pt
2023-02-20 07:17:12 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_12000.pt
2023-02-20 07:17:15 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 0.6911867074102368) (writing took 8.18703232333064 seconds)
2023-02-20 07:17:25 - progress_bar.py[line:274] - INFO: epoch 001:  12027 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=0.3, ups=0, wpb=114.3, bsz=40, num_updates=12010, lr=4.70419e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36360
2023-02-20 07:17:37 - progress_bar.py[line:274] - INFO: epoch 001:  12037 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=113.5, bsz=40, num_updates=12020, lr=4.70383e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=36372
2023-02-20 07:17:48 - progress_bar.py[line:274] - INFO: epoch 001:  12047 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.3, bsz=40, num_updates=12030, lr=4.70346e-05, gnorm=0.21, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36383
2023-02-20 07:17:59 - progress_bar.py[line:274] - INFO: epoch 001:  12057 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.9, bsz=40, num_updates=12040, lr=4.7031e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36394
2023-02-20 07:18:10 - progress_bar.py[line:274] - INFO: epoch 001:  12067 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112, bsz=40, num_updates=12050, lr=4.70274e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36405
2023-02-20 07:18:21 - progress_bar.py[line:274] - INFO: epoch 001:  12077 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113.1, bsz=40, num_updates=12060, lr=4.70238e-05, gnorm=0.267, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=36416
2023-02-20 07:18:33 - progress_bar.py[line:274] - INFO: epoch 001:  12087 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=12070, lr=4.70202e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=36427
2023-02-20 07:18:44 - progress_bar.py[line:274] - INFO: epoch 001:  12097 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.87, wpb=113.1, bsz=40, num_updates=12080, lr=4.70166e-05, gnorm=0.258, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=36439
2023-02-20 07:18:55 - progress_bar.py[line:274] - INFO: epoch 001:  12107 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.9, wpb=113.5, bsz=40, num_updates=12090, lr=4.70129e-05, gnorm=0.17, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36450
2023-02-20 07:19:06 - progress_bar.py[line:274] - INFO: epoch 001:  12117 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.9, wpb=114.5, bsz=40, num_updates=12100, lr=4.70093e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36461
2023-02-20 07:19:18 - progress_bar.py[line:274] - INFO: epoch 001:  12127 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.88, wpb=114.4, bsz=40, num_updates=12110, lr=4.70057e-05, gnorm=0.163, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36472
2023-02-20 07:19:29 - progress_bar.py[line:274] - INFO: epoch 001:  12137 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.89, wpb=114.1, bsz=40, num_updates=12120, lr=4.70021e-05, gnorm=0.197, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=36484
2023-02-20 07:19:40 - progress_bar.py[line:274] - INFO: epoch 001:  12147 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=114.1, bsz=40, num_updates=12130, lr=4.69985e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=36495
2023-02-20 07:19:51 - progress_bar.py[line:274] - INFO: epoch 001:  12157 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=114.2, bsz=40, num_updates=12140, lr=4.69948e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=36506
2023-02-20 07:20:02 - progress_bar.py[line:274] - INFO: epoch 001:  12167 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.9, bsz=40, num_updates=12150, lr=4.69912e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36517
2023-02-20 07:20:13 - progress_bar.py[line:274] - INFO: epoch 001:  12177 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.8, ups=0.9, wpb=113.7, bsz=40, num_updates=12160, lr=4.69876e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36528
2023-02-20 07:20:25 - progress_bar.py[line:274] - INFO: epoch 001:  12187 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.4, bsz=40, num_updates=12170, lr=4.6984e-05, gnorm=0.231, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36540
2023-02-20 07:20:36 - progress_bar.py[line:274] - INFO: epoch 001:  12197 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=113.9, bsz=40, num_updates=12180, lr=4.69804e-05, gnorm=0.257, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36551
2023-02-20 07:20:47 - progress_bar.py[line:274] - INFO: epoch 001:  12207 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.9, bsz=40, num_updates=12190, lr=4.69768e-05, gnorm=0.318, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36562
2023-02-20 07:20:59 - progress_bar.py[line:274] - INFO: epoch 001:  12217 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.8, bsz=40, num_updates=12200, lr=4.69731e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36574
2023-02-20 07:21:10 - progress_bar.py[line:274] - INFO: epoch 001:  12227 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.5, bsz=40, num_updates=12210, lr=4.69695e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36585
2023-02-20 07:21:21 - progress_bar.py[line:274] - INFO: epoch 001:  12237 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.9, wpb=114, bsz=40, num_updates=12220, lr=4.69659e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36596
2023-02-20 07:21:33 - progress_bar.py[line:274] - INFO: epoch 001:  12247 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.88, wpb=113.4, bsz=40, num_updates=12230, lr=4.69623e-05, gnorm=0.168, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36607
2023-02-20 07:21:43 - progress_bar.py[line:274] - INFO: epoch 001:  12257 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.4, ups=0.92, wpb=113.9, bsz=40, num_updates=12240, lr=4.69587e-05, gnorm=0.214, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36618
2023-02-20 07:21:55 - progress_bar.py[line:274] - INFO: epoch 001:  12267 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.89, wpb=113.3, bsz=40, num_updates=12250, lr=4.6955e-05, gnorm=0.159, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36630
2023-02-20 07:22:06 - progress_bar.py[line:274] - INFO: epoch 001:  12277 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.7, ups=0.9, wpb=113.7, bsz=40, num_updates=12260, lr=4.69514e-05, gnorm=0.213, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36641
2023-02-20 07:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  12287 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=113, bsz=40, num_updates=12270, lr=4.69478e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=36652
2023-02-20 07:22:28 - progress_bar.py[line:274] - INFO: epoch 001:  12297 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.9, wpb=113.4, bsz=40, num_updates=12280, lr=4.69442e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=36663
2023-02-20 07:22:39 - progress_bar.py[line:274] - INFO: epoch 001:  12307 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=112.3, bsz=40, num_updates=12290, lr=4.69406e-05, gnorm=0.27, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36674
2023-02-20 07:22:50 - progress_bar.py[line:274] - INFO: epoch 001:  12317 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.87, wpb=113.1, bsz=40, num_updates=12300, lr=4.6937e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36685
2023-02-20 07:23:02 - progress_bar.py[line:274] - INFO: epoch 001:  12327 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.8, bsz=40, num_updates=12310, lr=4.69333e-05, gnorm=0.235, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=36697
2023-02-20 07:23:13 - progress_bar.py[line:274] - INFO: epoch 001:  12337 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.2, bsz=40, num_updates=12320, lr=4.69297e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=36708
2023-02-20 07:23:25 - progress_bar.py[line:274] - INFO: epoch 001:  12347 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.5, bsz=40, num_updates=12330, lr=4.69261e-05, gnorm=0.208, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36719
2023-02-20 07:23:36 - progress_bar.py[line:274] - INFO: epoch 001:  12357 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.9, wpb=114.4, bsz=40, num_updates=12340, lr=4.69225e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=36730
2023-02-20 07:23:47 - progress_bar.py[line:274] - INFO: epoch 001:  12367 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=12350, lr=4.69189e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=36742
2023-02-20 07:23:58 - progress_bar.py[line:274] - INFO: epoch 001:  12377 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.91, wpb=114.3, bsz=40, num_updates=12360, lr=4.69153e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36753
2023-02-20 07:24:09 - progress_bar.py[line:274] - INFO: epoch 001:  12387 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.92, wpb=113.4, bsz=40, num_updates=12370, lr=4.69116e-05, gnorm=0.198, clip=0, loss_scale=2048, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=36764
2023-02-20 07:24:20 - progress_bar.py[line:274] - INFO: epoch 001:  12397 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114.6, bsz=40, num_updates=12380, lr=4.6908e-05, gnorm=0.216, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36775
2023-02-20 07:24:32 - progress_bar.py[line:274] - INFO: epoch 001:  12407 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.87, wpb=113.2, bsz=40, num_updates=12390, lr=4.69044e-05, gnorm=0.315, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36787
2023-02-20 07:24:43 - progress_bar.py[line:274] - INFO: epoch 001:  12417 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=12400, lr=4.69008e-05, gnorm=0.334, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=36798
2023-02-20 07:24:48 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 07:24:55 - progress_bar.py[line:274] - INFO: epoch 001:  12428 / 14203 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=93.2, ups=0.83, wpb=113, bsz=40, num_updates=12410, lr=4.68972e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=36810
2023-02-20 07:25:06 - progress_bar.py[line:274] - INFO: epoch 001:  12438 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.9, wpb=113.2, bsz=40, num_updates=12420, lr=4.68935e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=36821
2023-02-20 07:25:17 - progress_bar.py[line:274] - INFO: epoch 001:  12448 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.8, ups=0.93, wpb=113.5, bsz=40, num_updates=12430, lr=4.68899e-05, gnorm=0.185, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36832
2023-02-20 07:25:28 - progress_bar.py[line:274] - INFO: epoch 001:  12458 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=112.2, bsz=40, num_updates=12440, lr=4.68863e-05, gnorm=0.263, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36843
2023-02-20 07:25:39 - progress_bar.py[line:274] - INFO: epoch 001:  12468 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=12450, lr=4.68827e-05, gnorm=0.201, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=36854
2023-02-20 07:25:50 - progress_bar.py[line:274] - INFO: epoch 001:  12478 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.1, ups=0.93, wpb=114.3, bsz=40, num_updates=12460, lr=4.68791e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36865
2023-02-20 07:26:01 - progress_bar.py[line:274] - INFO: epoch 001:  12488 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.2, bsz=40, num_updates=12470, lr=4.68755e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=36876
2023-02-20 07:26:13 - progress_bar.py[line:274] - INFO: epoch 001:  12498 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=113.2, bsz=40, num_updates=12480, lr=4.68718e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36887
2023-02-20 07:26:23 - progress_bar.py[line:274] - INFO: epoch 001:  12508 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.3, ups=0.94, wpb=113, bsz=40, num_updates=12490, lr=4.68682e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36898
2023-02-20 07:26:34 - progress_bar.py[line:274] - INFO: epoch 001:  12518 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=114.2, bsz=40, num_updates=12500, lr=4.68646e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=36909
2023-02-20 07:26:45 - progress_bar.py[line:274] - INFO: epoch 001:  12528 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=112.6, bsz=40, num_updates=12510, lr=4.6861e-05, gnorm=0.325, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36920
2023-02-20 07:26:57 - progress_bar.py[line:274] - INFO: epoch 001:  12538 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=113.1, bsz=40, num_updates=12520, lr=4.68574e-05, gnorm=0.161, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36932
2023-02-20 07:27:08 - progress_bar.py[line:274] - INFO: epoch 001:  12548 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.91, wpb=115.2, bsz=40, num_updates=12530, lr=4.68537e-05, gnorm=0.144, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=36942
2023-02-20 07:27:19 - progress_bar.py[line:274] - INFO: epoch 001:  12558 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=12540, lr=4.68501e-05, gnorm=0.19, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36954
2023-02-20 07:27:30 - progress_bar.py[line:274] - INFO: epoch 001:  12568 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.9, bsz=40, num_updates=12550, lr=4.68465e-05, gnorm=0.29, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36965
2023-02-20 07:27:41 - progress_bar.py[line:274] - INFO: epoch 001:  12578 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.88, wpb=113.5, bsz=40, num_updates=12560, lr=4.68429e-05, gnorm=0.159, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=36976
2023-02-20 07:27:53 - progress_bar.py[line:274] - INFO: epoch 001:  12588 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.9, wpb=112.8, bsz=40, num_updates=12570, lr=4.68393e-05, gnorm=0.156, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=36987
2023-02-20 07:28:04 - progress_bar.py[line:274] - INFO: epoch 001:  12598 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.88, wpb=114.2, bsz=40, num_updates=12580, lr=4.68357e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=36999
2023-02-20 07:28:15 - progress_bar.py[line:274] - INFO: epoch 001:  12608 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.89, wpb=114.2, bsz=40, num_updates=12590, lr=4.6832e-05, gnorm=0.199, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37010
2023-02-20 07:28:26 - progress_bar.py[line:274] - INFO: epoch 001:  12618 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=12600, lr=4.68284e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37021
2023-02-20 07:28:37 - progress_bar.py[line:274] - INFO: epoch 001:  12628 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.9, wpb=114.1, bsz=40, num_updates=12610, lr=4.68248e-05, gnorm=0.187, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37032
2023-02-20 07:28:48 - progress_bar.py[line:274] - INFO: epoch 001:  12638 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=113.1, bsz=40, num_updates=12620, lr=4.68212e-05, gnorm=0.171, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37043
2023-02-20 07:29:00 - progress_bar.py[line:274] - INFO: epoch 001:  12648 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.9, bsz=40, num_updates=12630, lr=4.68176e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37055
2023-02-20 07:29:11 - progress_bar.py[line:274] - INFO: epoch 001:  12658 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.89, wpb=113.8, bsz=40, num_updates=12640, lr=4.68139e-05, gnorm=0.149, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37066
2023-02-20 07:29:22 - progress_bar.py[line:274] - INFO: epoch 001:  12668 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.3, bsz=40, num_updates=12650, lr=4.68103e-05, gnorm=0.213, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37077
2023-02-20 07:29:34 - progress_bar.py[line:274] - INFO: epoch 001:  12678 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.88, wpb=114.9, bsz=40, num_updates=12660, lr=4.68067e-05, gnorm=0.155, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37089
2023-02-20 07:29:45 - progress_bar.py[line:274] - INFO: epoch 001:  12688 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.9, bsz=40, num_updates=12670, lr=4.68031e-05, gnorm=0.168, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37100
2023-02-20 07:29:56 - progress_bar.py[line:274] - INFO: epoch 001:  12698 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=113.2, bsz=40, num_updates=12680, lr=4.67995e-05, gnorm=0.17, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37111
2023-02-20 07:30:07 - progress_bar.py[line:274] - INFO: epoch 001:  12708 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.4, bsz=40, num_updates=12690, lr=4.67959e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37122
2023-02-20 07:30:18 - progress_bar.py[line:274] - INFO: epoch 001:  12718 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.3, ups=0.89, wpb=114.7, bsz=40, num_updates=12700, lr=4.67922e-05, gnorm=0.17, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37133
2023-02-20 07:30:29 - progress_bar.py[line:274] - INFO: epoch 001:  12728 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.93, wpb=113, bsz=40, num_updates=12710, lr=4.67886e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37144
2023-02-20 07:30:41 - progress_bar.py[line:274] - INFO: epoch 001:  12738 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.89, wpb=113.4, bsz=40, num_updates=12720, lr=4.6785e-05, gnorm=0.136, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37155
2023-02-20 07:30:52 - progress_bar.py[line:274] - INFO: epoch 001:  12748 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=114, bsz=40, num_updates=12730, lr=4.67814e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37167
2023-02-20 07:31:03 - progress_bar.py[line:274] - INFO: epoch 001:  12758 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.2, ups=0.94, wpb=113.3, bsz=40, num_updates=12740, lr=4.67778e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=37177
2023-02-20 07:31:13 - progress_bar.py[line:274] - INFO: epoch 001:  12768 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.5, ups=0.93, wpb=113.7, bsz=40, num_updates=12750, lr=4.67741e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37188
2023-02-20 07:31:25 - progress_bar.py[line:274] - INFO: epoch 001:  12778 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=113, bsz=40, num_updates=12760, lr=4.67705e-05, gnorm=0.216, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37199
2023-02-20 07:31:36 - progress_bar.py[line:274] - INFO: epoch 001:  12788 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=12770, lr=4.67669e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37211
2023-02-20 07:31:47 - progress_bar.py[line:274] - INFO: epoch 001:  12798 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.3, bsz=40, num_updates=12780, lr=4.67633e-05, gnorm=0.162, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37222
2023-02-20 07:31:58 - progress_bar.py[line:274] - INFO: epoch 001:  12808 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.92, wpb=114.3, bsz=40, num_updates=12790, lr=4.67597e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37233
2023-02-20 07:32:09 - progress_bar.py[line:274] - INFO: epoch 001:  12818 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.4, bsz=40, num_updates=12800, lr=4.67561e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37244
2023-02-20 07:32:20 - progress_bar.py[line:274] - INFO: epoch 001:  12828 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.92, wpb=113.3, bsz=40, num_updates=12810, lr=4.67524e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37255
2023-02-20 07:32:31 - progress_bar.py[line:274] - INFO: epoch 001:  12838 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.9, wpb=114.2, bsz=40, num_updates=12820, lr=4.67488e-05, gnorm=0.178, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37266
2023-02-20 07:32:42 - progress_bar.py[line:274] - INFO: epoch 001:  12848 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.87, wpb=114.9, bsz=40, num_updates=12830, lr=4.67452e-05, gnorm=0.236, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37277
2023-02-20 07:32:53 - progress_bar.py[line:274] - INFO: epoch 001:  12858 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.92, wpb=113.2, bsz=40, num_updates=12840, lr=4.67416e-05, gnorm=0.182, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37288
2023-02-20 07:33:05 - progress_bar.py[line:274] - INFO: epoch 001:  12868 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.9, ups=0.9, wpb=113.7, bsz=40, num_updates=12850, lr=4.6738e-05, gnorm=0.158, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37299
2023-02-20 07:33:16 - progress_bar.py[line:274] - INFO: epoch 001:  12878 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.6, bsz=40, num_updates=12860, lr=4.67343e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37311
2023-02-20 07:33:27 - progress_bar.py[line:274] - INFO: epoch 001:  12888 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.93, wpb=113.3, bsz=40, num_updates=12870, lr=4.67307e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37321
2023-02-20 07:33:38 - progress_bar.py[line:274] - INFO: epoch 001:  12898 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.5, bsz=40, num_updates=12880, lr=4.67271e-05, gnorm=0.247, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37332
2023-02-20 07:33:49 - progress_bar.py[line:274] - INFO: epoch 001:  12908 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.9, wpb=114.4, bsz=40, num_updates=12890, lr=4.67235e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37344
2023-02-20 07:34:00 - progress_bar.py[line:274] - INFO: epoch 001:  12918 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.92, wpb=113.6, bsz=40, num_updates=12900, lr=4.67199e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37354
2023-02-20 07:34:11 - progress_bar.py[line:274] - INFO: epoch 001:  12928 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.87, wpb=113.6, bsz=40, num_updates=12910, lr=4.67163e-05, gnorm=0.231, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=37366
2023-02-20 07:34:22 - progress_bar.py[line:274] - INFO: epoch 001:  12938 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.88, wpb=114.8, bsz=40, num_updates=12920, lr=4.67126e-05, gnorm=0.223, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37377
2023-02-20 07:34:34 - progress_bar.py[line:274] - INFO: epoch 001:  12948 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.89, wpb=114.5, bsz=40, num_updates=12930, lr=4.6709e-05, gnorm=0.257, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37388
2023-02-20 07:34:44 - progress_bar.py[line:274] - INFO: epoch 001:  12958 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.92, wpb=112.1, bsz=40, num_updates=12940, lr=4.67054e-05, gnorm=0.172, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37399
2023-02-20 07:34:55 - progress_bar.py[line:274] - INFO: epoch 001:  12968 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.94, wpb=112, bsz=40, num_updates=12950, lr=4.67018e-05, gnorm=0.278, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37410
2023-02-20 07:35:07 - progress_bar.py[line:274] - INFO: epoch 001:  12978 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.4, bsz=40, num_updates=12960, lr=4.66982e-05, gnorm=0.186, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37421
2023-02-20 07:35:18 - progress_bar.py[line:274] - INFO: epoch 001:  12988 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=115.3, nsentences=40, sample_size=115.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.91, wpb=115.3, bsz=40, num_updates=12970, lr=4.66945e-05, gnorm=0.236, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37432
2023-02-20 07:35:29 - progress_bar.py[line:274] - INFO: epoch 001:  12998 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.92, wpb=113.2, bsz=40, num_updates=12980, lr=4.66909e-05, gnorm=0.239, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37443
2023-02-20 07:35:40 - progress_bar.py[line:274] - INFO: epoch 001:  13008 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102, ups=0.91, wpb=112.6, bsz=40, num_updates=12990, lr=4.66873e-05, gnorm=0.22, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37454
2023-02-20 07:35:51 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 07:35:52 - progress_bar.py[line:274] - INFO: epoch 001:  13019 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=93, ups=0.82, wpb=113.7, bsz=40, num_updates=13000, lr=4.66837e-05, gnorm=0.113, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=37467
2023-02-20 07:36:03 - progress_bar.py[line:274] - INFO: epoch 001:  13029 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.88, wpb=113.8, bsz=40, num_updates=13010, lr=4.66801e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=37478
2023-02-20 07:36:14 - progress_bar.py[line:274] - INFO: epoch 001:  13039 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=112.4, bsz=40, num_updates=13020, lr=4.66765e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37489
2023-02-20 07:36:25 - progress_bar.py[line:274] - INFO: epoch 001:  13049 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.9, wpb=114, bsz=40, num_updates=13030, lr=4.66728e-05, gnorm=0.325, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37500
2023-02-20 07:36:36 - progress_bar.py[line:274] - INFO: epoch 001:  13059 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=13040, lr=4.66692e-05, gnorm=0.169, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37511
2023-02-20 07:36:47 - progress_bar.py[line:274] - INFO: epoch 001:  13069 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=13050, lr=4.66656e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37522
2023-02-20 07:36:59 - progress_bar.py[line:274] - INFO: epoch 001:  13079 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.89, wpb=113.8, bsz=40, num_updates=13060, lr=4.6662e-05, gnorm=0.199, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37533
2023-02-20 07:37:10 - progress_bar.py[line:274] - INFO: epoch 001:  13089 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=114.2, bsz=40, num_updates=13070, lr=4.66584e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37545
2023-02-20 07:37:21 - progress_bar.py[line:274] - INFO: epoch 001:  13099 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.8, ups=0.92, wpb=114.2, bsz=40, num_updates=13080, lr=4.66548e-05, gnorm=0.272, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37556
2023-02-20 07:37:32 - progress_bar.py[line:274] - INFO: epoch 001:  13109 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.8, bsz=40, num_updates=13090, lr=4.66511e-05, gnorm=0.208, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37566
2023-02-20 07:37:43 - progress_bar.py[line:274] - INFO: epoch 001:  13119 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=112.8, bsz=40, num_updates=13100, lr=4.66475e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37578
2023-02-20 07:37:54 - progress_bar.py[line:274] - INFO: epoch 001:  13129 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=13110, lr=4.66439e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37588
2023-02-20 07:38:05 - progress_bar.py[line:274] - INFO: epoch 001:  13139 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.92, wpb=113.7, bsz=40, num_updates=13120, lr=4.66403e-05, gnorm=0.191, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37599
2023-02-20 07:38:16 - progress_bar.py[line:274] - INFO: epoch 001:  13149 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.2, bsz=40, num_updates=13130, lr=4.66367e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37611
2023-02-20 07:38:27 - progress_bar.py[line:274] - INFO: epoch 001:  13159 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.2, bsz=40, num_updates=13140, lr=4.6633e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37622
2023-02-20 07:38:38 - progress_bar.py[line:274] - INFO: epoch 001:  13169 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.9, bsz=40, num_updates=13150, lr=4.66294e-05, gnorm=0.208, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37633
2023-02-20 07:38:50 - progress_bar.py[line:274] - INFO: epoch 001:  13179 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.9, wpb=113.9, bsz=40, num_updates=13160, lr=4.66258e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=37644
2023-02-20 07:39:00 - progress_bar.py[line:274] - INFO: epoch 001:  13189 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.92, wpb=113.1, bsz=40, num_updates=13170, lr=4.66222e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37655
2023-02-20 07:39:12 - progress_bar.py[line:274] - INFO: epoch 001:  13199 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.8, bsz=40, num_updates=13180, lr=4.66186e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37667
2023-02-20 07:39:23 - progress_bar.py[line:274] - INFO: epoch 001:  13209 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=13190, lr=4.6615e-05, gnorm=0.327, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37678
2023-02-20 07:39:34 - progress_bar.py[line:274] - INFO: epoch 001:  13219 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=114.1, bsz=40, num_updates=13200, lr=4.66113e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37689
2023-02-20 07:39:45 - progress_bar.py[line:274] - INFO: epoch 001:  13229 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=111.7, bsz=40, num_updates=13210, lr=4.66077e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37700
2023-02-20 07:39:57 - progress_bar.py[line:274] - INFO: epoch 001:  13239 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.1, bsz=40, num_updates=13220, lr=4.66041e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=37712
2023-02-20 07:40:08 - progress_bar.py[line:274] - INFO: epoch 001:  13249 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.88, wpb=114.9, bsz=40, num_updates=13230, lr=4.66005e-05, gnorm=0.21, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=37723
2023-02-20 07:40:19 - progress_bar.py[line:274] - INFO: epoch 001:  13259 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=114.1, bsz=40, num_updates=13240, lr=4.65969e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37734
2023-02-20 07:40:30 - progress_bar.py[line:274] - INFO: epoch 001:  13269 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=13250, lr=4.65932e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=37745
2023-02-20 07:40:42 - progress_bar.py[line:274] - INFO: epoch 001:  13279 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=113.4, bsz=40, num_updates=13260, lr=4.65896e-05, gnorm=0.158, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37757
2023-02-20 07:40:53 - progress_bar.py[line:274] - INFO: epoch 001:  13289 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.91, wpb=113.7, bsz=40, num_updates=13270, lr=4.6586e-05, gnorm=0.199, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37768
2023-02-20 07:41:04 - progress_bar.py[line:274] - INFO: epoch 001:  13299 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.6, ups=0.92, wpb=114, bsz=40, num_updates=13280, lr=4.65824e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37779
2023-02-20 07:41:15 - progress_bar.py[line:274] - INFO: epoch 001:  13309 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=113.8, bsz=40, num_updates=13290, lr=4.65788e-05, gnorm=0.198, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37790
2023-02-20 07:41:26 - progress_bar.py[line:274] - INFO: epoch 001:  13319 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.5, ups=0.91, wpb=113.1, bsz=40, num_updates=13300, lr=4.65752e-05, gnorm=0.163, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37801
2023-02-20 07:41:37 - progress_bar.py[line:274] - INFO: epoch 001:  13329 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=112.7, bsz=40, num_updates=13310, lr=4.65715e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37812
2023-02-20 07:41:49 - progress_bar.py[line:274] - INFO: epoch 001:  13339 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.3, bsz=40, num_updates=13320, lr=4.65679e-05, gnorm=0.2, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=37824
2023-02-20 07:42:00 - progress_bar.py[line:274] - INFO: epoch 001:  13349 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.1, bsz=40, num_updates=13330, lr=4.65643e-05, gnorm=0.192, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37835
2023-02-20 07:42:12 - progress_bar.py[line:274] - INFO: epoch 001:  13359 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.6, bsz=40, num_updates=13340, lr=4.65607e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=37846
2023-02-20 07:42:23 - progress_bar.py[line:274] - INFO: epoch 001:  13369 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.87, wpb=113.5, bsz=40, num_updates=13350, lr=4.65571e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=37858
2023-02-20 07:42:34 - progress_bar.py[line:274] - INFO: epoch 001:  13379 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113, bsz=40, num_updates=13360, lr=4.65534e-05, gnorm=0.213, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37869
2023-02-20 07:42:46 - progress_bar.py[line:274] - INFO: epoch 001:  13389 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.88, wpb=114.2, bsz=40, num_updates=13370, lr=4.65498e-05, gnorm=0.16, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37881
2023-02-20 07:42:57 - progress_bar.py[line:274] - INFO: epoch 001:  13399 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.9, wpb=114.3, bsz=40, num_updates=13380, lr=4.65462e-05, gnorm=0.148, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=37892
2023-02-20 07:43:08 - progress_bar.py[line:274] - INFO: epoch 001:  13409 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.2, ups=0.87, wpb=113.8, bsz=40, num_updates=13390, lr=4.65426e-05, gnorm=0.199, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37903
2023-02-20 07:43:19 - progress_bar.py[line:274] - INFO: epoch 001:  13419 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.5, ups=0.92, wpb=114.1, bsz=40, num_updates=13400, lr=4.6539e-05, gnorm=0.131, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37914
2023-02-20 07:43:31 - progress_bar.py[line:274] - INFO: epoch 001:  13429 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.88, wpb=114, bsz=40, num_updates=13410, lr=4.65354e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37925
2023-02-20 07:43:42 - progress_bar.py[line:274] - INFO: epoch 001:  13439 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=13420, lr=4.65317e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37937
2023-02-20 07:43:53 - progress_bar.py[line:274] - INFO: epoch 001:  13449 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=115.3, nsentences=40, sample_size=115.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.9, wpb=115.3, bsz=40, num_updates=13430, lr=4.65281e-05, gnorm=0.165, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37948
2023-02-20 07:44:04 - progress_bar.py[line:274] - INFO: epoch 001:  13459 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.4, ups=0.92, wpb=113.7, bsz=40, num_updates=13440, lr=4.65245e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37959
2023-02-20 07:44:15 - progress_bar.py[line:274] - INFO: epoch 001:  13469 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.9, bsz=40, num_updates=13450, lr=4.65209e-05, gnorm=0.171, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=37970
2023-02-20 07:44:26 - progress_bar.py[line:274] - INFO: epoch 001:  13479 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=115.5, nsentences=40, sample_size=115.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.88, wpb=115.5, bsz=40, num_updates=13460, lr=4.65173e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37981
2023-02-20 07:44:38 - progress_bar.py[line:274] - INFO: epoch 001:  13489 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.5, bsz=40, num_updates=13470, lr=4.65136e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37992
2023-02-20 07:44:48 - progress_bar.py[line:274] - INFO: epoch 001:  13499 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.92, wpb=112.7, bsz=40, num_updates=13480, lr=4.651e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38003
2023-02-20 07:45:00 - progress_bar.py[line:274] - INFO: epoch 001:  13509 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=13490, lr=4.65064e-05, gnorm=0.156, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38015
2023-02-20 07:45:11 - progress_bar.py[line:274] - INFO: epoch 001:  13519 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.9, wpb=114.6, bsz=40, num_updates=13500, lr=4.65028e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38026
2023-02-20 07:45:22 - progress_bar.py[line:274] - INFO: epoch 001:  13529 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=112.9, bsz=40, num_updates=13510, lr=4.64992e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38037
2023-02-20 07:45:33 - progress_bar.py[line:274] - INFO: epoch 001:  13539 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.6, ups=0.94, wpb=113.4, bsz=40, num_updates=13520, lr=4.64956e-05, gnorm=0.188, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38047
2023-02-20 07:45:44 - progress_bar.py[line:274] - INFO: epoch 001:  13549 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.6, ups=0.87, wpb=112.4, bsz=40, num_updates=13530, lr=4.64919e-05, gnorm=0.132, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38059
2023-02-20 07:45:55 - progress_bar.py[line:274] - INFO: epoch 001:  13559 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.7, ups=0.91, wpb=114.7, bsz=40, num_updates=13540, lr=4.64883e-05, gnorm=0.213, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38070
2023-02-20 07:46:06 - progress_bar.py[line:274] - INFO: epoch 001:  13569 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.7, ups=0.92, wpb=113.1, bsz=40, num_updates=13550, lr=4.64847e-05, gnorm=0.211, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38081
2023-02-20 07:46:17 - progress_bar.py[line:274] - INFO: epoch 001:  13579 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.9, ups=0.91, wpb=112.6, bsz=40, num_updates=13560, lr=4.64811e-05, gnorm=0.125, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38092
2023-02-20 07:46:28 - progress_bar.py[line:274] - INFO: epoch 001:  13589 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=113.5, bsz=40, num_updates=13570, lr=4.64775e-05, gnorm=0.268, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=38103
2023-02-20 07:46:39 - progress_bar.py[line:274] - INFO: epoch 001:  13599 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=13580, lr=4.64738e-05, gnorm=0.215, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38114
2023-02-20 07:46:50 - progress_bar.py[line:274] - INFO: epoch 001:  13609 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.3, bsz=40, num_updates=13590, lr=4.64702e-05, gnorm=0.251, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38125
2023-02-20 07:47:01 - progress_bar.py[line:274] - INFO: epoch 001:  13619 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.6, bsz=40, num_updates=13600, lr=4.64666e-05, gnorm=0.217, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38136
2023-02-20 07:47:13 - progress_bar.py[line:274] - INFO: epoch 001:  13629 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.5, bsz=40, num_updates=13610, lr=4.6463e-05, gnorm=0.208, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38148
2023-02-20 07:47:24 - progress_bar.py[line:274] - INFO: epoch 001:  13639 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.88, wpb=113.4, bsz=40, num_updates=13620, lr=4.64594e-05, gnorm=0.143, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38159
2023-02-20 07:47:35 - progress_bar.py[line:274] - INFO: epoch 001:  13649 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=114.6, bsz=40, num_updates=13630, lr=4.64558e-05, gnorm=0.199, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38170
2023-02-20 07:47:47 - progress_bar.py[line:274] - INFO: epoch 001:  13659 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.4, bsz=40, num_updates=13640, lr=4.64521e-05, gnorm=0.256, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38182
2023-02-20 07:47:58 - progress_bar.py[line:274] - INFO: epoch 001:  13669 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.5, ups=0.93, wpb=113.6, bsz=40, num_updates=13650, lr=4.64485e-05, gnorm=0.203, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38193
2023-02-20 07:48:09 - progress_bar.py[line:274] - INFO: epoch 001:  13679 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=113.8, bsz=40, num_updates=13660, lr=4.64449e-05, gnorm=0.328, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38204
2023-02-20 07:48:20 - progress_bar.py[line:274] - INFO: epoch 001:  13689 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=113.3, bsz=40, num_updates=13670, lr=4.64413e-05, gnorm=0.277, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38215
2023-02-20 07:48:31 - progress_bar.py[line:274] - INFO: epoch 001:  13699 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.9, wpb=113.6, bsz=40, num_updates=13680, lr=4.64377e-05, gnorm=0.211, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38226
2023-02-20 07:48:42 - progress_bar.py[line:274] - INFO: epoch 001:  13709 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.9, ups=0.89, wpb=113.9, bsz=40, num_updates=13690, lr=4.64341e-05, gnorm=0.189, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38237
2023-02-20 07:48:53 - progress_bar.py[line:274] - INFO: epoch 001:  13719 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.7, ups=0.88, wpb=113.2, bsz=40, num_updates=13700, lr=4.64304e-05, gnorm=0.178, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38248
2023-02-20 07:49:05 - progress_bar.py[line:274] - INFO: epoch 001:  13729 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.6, bsz=40, num_updates=13710, lr=4.64268e-05, gnorm=0.259, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38260
2023-02-20 07:49:16 - progress_bar.py[line:274] - INFO: epoch 001:  13739 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.87, wpb=114.9, bsz=40, num_updates=13720, lr=4.64232e-05, gnorm=0.183, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38271
2023-02-20 07:49:27 - progress_bar.py[line:274] - INFO: epoch 001:  13749 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114, bsz=40, num_updates=13730, lr=4.64196e-05, gnorm=0.216, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=38282
2023-02-20 07:49:39 - progress_bar.py[line:274] - INFO: epoch 001:  13759 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=113.3, bsz=40, num_updates=13740, lr=4.6416e-05, gnorm=0.212, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=38293
2023-02-20 07:49:50 - progress_bar.py[line:274] - INFO: epoch 001:  13769 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=114.2, bsz=40, num_updates=13750, lr=4.64123e-05, gnorm=0.221, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38304
2023-02-20 07:50:01 - progress_bar.py[line:274] - INFO: epoch 001:  13779 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=113, bsz=40, num_updates=13760, lr=4.64087e-05, gnorm=0.238, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38315
2023-02-20 07:50:11 - progress_bar.py[line:274] - INFO: epoch 001:  13789 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.9, ups=0.94, wpb=112.9, bsz=40, num_updates=13770, lr=4.64051e-05, gnorm=0.235, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38326
2023-02-20 07:50:22 - progress_bar.py[line:274] - INFO: epoch 001:  13799 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=13780, lr=4.64015e-05, gnorm=0.327, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38337
2023-02-20 07:50:34 - progress_bar.py[line:274] - INFO: epoch 001:  13809 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.86, wpb=113.7, bsz=40, num_updates=13790, lr=4.63979e-05, gnorm=0.229, clip=0, loss_scale=2048, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=38349
2023-02-20 07:50:45 - progress_bar.py[line:274] - INFO: epoch 001:  13819 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=13800, lr=4.63943e-05, gnorm=0.187, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38360
2023-02-20 07:50:56 - progress_bar.py[line:274] - INFO: epoch 001:  13829 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.9, wpb=113.4, bsz=40, num_updates=13810, lr=4.63906e-05, gnorm=0.274, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38371
2023-02-20 07:51:07 - progress_bar.py[line:274] - INFO: epoch 001:  13839 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.5, ups=0.92, wpb=114.1, bsz=40, num_updates=13820, lr=4.6387e-05, gnorm=0.179, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38382
2023-02-20 07:51:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 07:51:19 - progress_bar.py[line:274] - INFO: epoch 001:  13850 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96, ups=0.85, wpb=113, bsz=40, num_updates=13830, lr=4.63834e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=38394
2023-02-20 07:51:30 - progress_bar.py[line:274] - INFO: epoch 001:  13860 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.92, wpb=113.3, bsz=40, num_updates=13840, lr=4.63798e-05, gnorm=0.194, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38405
2023-02-20 07:51:41 - progress_bar.py[line:274] - INFO: epoch 001:  13870 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=114.2, bsz=40, num_updates=13850, lr=4.63762e-05, gnorm=0.197, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38416
2023-02-20 07:51:52 - progress_bar.py[line:274] - INFO: epoch 001:  13880 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=113, bsz=40, num_updates=13860, lr=4.63725e-05, gnorm=0.159, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=38427
2023-02-20 07:52:03 - progress_bar.py[line:274] - INFO: epoch 001:  13890 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.92, wpb=112.7, bsz=40, num_updates=13870, lr=4.63689e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38438
2023-02-20 07:52:15 - progress_bar.py[line:274] - INFO: epoch 001:  13900 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=112.9, bsz=40, num_updates=13880, lr=4.63653e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38450
2023-02-20 07:52:26 - progress_bar.py[line:274] - INFO: epoch 001:  13910 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=113.9, bsz=40, num_updates=13890, lr=4.63617e-05, gnorm=0.181, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38460
2023-02-20 07:52:37 - progress_bar.py[line:274] - INFO: epoch 001:  13920 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.9, wpb=113, bsz=40, num_updates=13900, lr=4.63581e-05, gnorm=0.165, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38472
2023-02-20 07:52:47 - progress_bar.py[line:274] - INFO: epoch 001:  13930 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.6, ups=0.94, wpb=113.1, bsz=40, num_updates=13910, lr=4.63545e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38482
2023-02-20 07:52:59 - progress_bar.py[line:274] - INFO: epoch 001:  13940 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.6, ups=0.88, wpb=112.8, bsz=40, num_updates=13920, lr=4.63508e-05, gnorm=0.198, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38494
2023-02-20 07:53:10 - progress_bar.py[line:274] - INFO: epoch 001:  13950 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.9, bsz=40, num_updates=13930, lr=4.63472e-05, gnorm=0.166, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38505
2023-02-20 07:53:21 - progress_bar.py[line:274] - INFO: epoch 001:  13960 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.2, bsz=40, num_updates=13940, lr=4.63436e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38516
2023-02-20 07:53:32 - progress_bar.py[line:274] - INFO: epoch 001:  13970 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=13950, lr=4.634e-05, gnorm=0.201, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38527
2023-02-20 07:53:44 - progress_bar.py[line:274] - INFO: epoch 001:  13980 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.87, wpb=113.5, bsz=40, num_updates=13960, lr=4.63364e-05, gnorm=0.178, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38538
2023-02-20 07:53:55 - progress_bar.py[line:274] - INFO: epoch 001:  13990 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.88, wpb=114.3, bsz=40, num_updates=13970, lr=4.63327e-05, gnorm=0.21, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38550
2023-02-20 07:54:06 - progress_bar.py[line:274] - INFO: epoch 001:  14000 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.5, ups=0.92, wpb=115, bsz=40, num_updates=13980, lr=4.63291e-05, gnorm=0.202, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38561
2023-02-20 07:54:17 - progress_bar.py[line:274] - INFO: epoch 001:  14010 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.89, wpb=113.5, bsz=40, num_updates=13990, lr=4.63255e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=38572
2023-02-20 07:54:28 - progress_bar.py[line:274] - INFO: epoch 001:  14020 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.88, wpb=113.7, bsz=40, num_updates=14000, lr=4.63219e-05, gnorm=0.184, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38583
2023-02-20 07:54:28 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 07:54:30 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 07:54:30 - train.py[line:551] - INFO: load:0.93 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 07:56:32 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 07:56:32 - train.py[line:551] - INFO: load:0.95 valid_run:122.31 task_valid:118.69 collect_output:2.50
2023-02-20 07:58:32 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 07:58:32 - train.py[line:551] - INFO: load:0.98 valid_run:242.19 task_valid:234.05 collect_output:5.95
2023-02-20 08:00:34 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 08:00:34 - train.py[line:551] - INFO: load:1.01 valid_run:364.09 task_valid:350.12 collect_output:10.74
2023-02-20 08:02:36 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 08:02:36 - train.py[line:551] - INFO: load:1.04 valid_run:486.00 task_valid:463.37 collect_output:18.36
2023-02-20 08:04:36 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 08:04:36 - train.py[line:551] - INFO: load:1.06 valid_run:606.28 task_valid:580.28 collect_output:20.70
2023-02-20 08:06:39 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 08:06:39 - train.py[line:551] - INFO: load:1.09 valid_run:728.85 task_valid:698.43 collect_output:24.10
2023-02-20 08:08:42 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 08:08:42 - train.py[line:551] - INFO: load:1.11 valid_run:851.70 task_valid:816.08 collect_output:28.27
2023-02-20 08:10:43 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 08:10:43 - train.py[line:551] - INFO: load:1.14 valid_run:973.35 task_valid:932.10 collect_output:32.86
2023-02-20 08:12:47 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 08:12:47 - train.py[line:551] - INFO: load:1.17 valid_run:1096.97 task_valid:1048.90 collect_output:38.66
2023-02-20 08:14:49 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 08:14:49 - train.py[line:551] - INFO: load:1.19 valid_run:1218.62 task_valid:1161.03 collect_output:47.17
2023-02-20 08:16:49 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 08:16:49 - train.py[line:551] - INFO: load:1.22 valid_run:1338.62 task_valid:1276.32 collect_output:50.86
2023-02-20 08:18:50 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 08:18:50 - train.py[line:551] - INFO: load:1.25 valid_run:1460.18 task_valid:1392.95 collect_output:54.77
2023-02-20 08:20:49 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 08:20:49 - train.py[line:551] - INFO: load:1.27 valid_run:1578.82 task_valid:1506.33 collect_output:59.02
2023-02-20 08:22:50 - train.py[line:549] - INFO: 2800 / 6234
2023-02-20 08:22:50 - train.py[line:551] - INFO: load:1.30 valid_run:1699.60 task_valid:1623.76 collect_output:61.35
2023-02-20 08:24:51 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 08:24:51 - train.py[line:551] - INFO: load:1.33 valid_run:1820.37 task_valid:1739.57 collect_output:65.29
2023-02-20 08:26:52 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 08:26:52 - train.py[line:551] - INFO: load:1.36 valid_run:1941.38 task_valid:1853.20 collect_output:71.65
2023-02-20 08:28:53 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 08:28:53 - train.py[line:551] - INFO: load:1.38 valid_run:2062.40 task_valid:1968.86 collect_output:75.99
2023-02-20 08:30:53 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 08:30:53 - train.py[line:551] - INFO: load:1.41 valid_run:2182.65 task_valid:2086.28 collect_output:77.80
2023-02-20 08:32:54 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 08:32:54 - train.py[line:551] - INFO: load:1.44 valid_run:2303.72 task_valid:2202.94 collect_output:81.19
2023-02-20 08:34:55 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 08:34:55 - train.py[line:551] - INFO: load:1.46 valid_run:2423.84 task_valid:2319.12 collect_output:84.11
2023-02-20 08:36:56 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 08:36:56 - train.py[line:551] - INFO: load:1.49 valid_run:2545.31 task_valid:2435.31 collect_output:88.35
2023-02-20 08:38:58 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 08:38:58 - train.py[line:551] - INFO: load:1.52 valid_run:2667.05 task_valid:2553.90 collect_output:90.49
2023-02-20 08:40:58 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 08:40:58 - train.py[line:551] - INFO: load:1.54 valid_run:2787.16 task_valid:2667.86 collect_output:95.64
2023-02-20 08:42:58 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 08:42:58 - train.py[line:551] - INFO: load:1.57 valid_run:2906.76 task_valid:2783.62 collect_output:98.47
2023-02-20 08:44:59 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 08:44:59 - train.py[line:551] - INFO: load:1.59 valid_run:3028.41 task_valid:2899.57 collect_output:103.16
2023-02-20 08:47:02 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 08:47:02 - train.py[line:551] - INFO: load:1.62 valid_run:3151.36 task_valid:3015.18 collect_output:109.50
2023-02-20 08:49:02 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 08:49:02 - train.py[line:551] - INFO: load:1.64 valid_run:3270.62 task_valid:3128.91 collect_output:114.01
2023-02-20 08:51:04 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 08:51:04 - train.py[line:551] - INFO: load:1.67 valid_run:3392.29 task_valid:3248.07 collect_output:115.50
2023-02-20 08:53:05 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 08:53:05 - train.py[line:551] - INFO: load:1.70 valid_run:3513.83 task_valid:3363.31 collect_output:120.80
2023-02-20 08:55:07 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 08:55:07 - train.py[line:551] - INFO: load:1.72 valid_run:3635.52 task_valid:3481.44 collect_output:123.36
2023-02-20 08:57:08 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 08:57:08 - train.py[line:551] - INFO: load:1.75 valid_run:3756.47 task_valid:3599.58 collect_output:125.16

====================================================================================================
SGG eval:     R @ 50: 0.6402;     R @ 100: 0.6802;     R @ 500: 0.7040;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4096;    mR @ 100: 0.4682;    mR @ 500: 0.5250;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.8125) (covering:0.3000) (eating:0.8235) (flying in:0.5682) (growing on:0.2500) (hanging from:0.5484) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9794) (says:0.0000) (sitting on:0.7228) (standing on:0.4043) (using:0.5500) (walking in:0.0000) (walking on:0.6486) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6402;     R @ 100: 0.6802;     R @ 500: 0.7040;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4096;    mR @ 100: 0.4682;    mR @ 500: 0.5250;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.8125) (covering:0.3000) (eating:0.8235) (flying in:0.5682) (growing on:0.2500) (hanging from:0.5484) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9794) (says:0.0000) (sitting on:0.7228) (standing on:0.4043) (using:0.5500) (walking in:0.0000) (walking on:0.6486) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================

2023-02-20 08:57:39 - train.py[line:487] - INFO: 0.6802018589253882
2023-02-20 08:57:39 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 08:57:39 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.244 | loss_v1 0 | loss_v2 0 | nll_loss 0.079 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.680202 | ppl 1.06 | vqa_score 0.5158 | wps 118.4 | wpb 72 | bsz 24 | num_updates 14000 | best_R@100 0.693862
2023-02-20 08:57:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 14000 updates
2023-02-20 08:57:39 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_14000.pt
2023-02-20 08:57:44 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_14000.pt
2023-02-20 08:57:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 0.6802018589253882) (writing took 8.183986280113459 seconds)
2023-02-20 08:57:58 - progress_bar.py[line:274] - INFO: epoch 001:  14030 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=112.7, bsz=40, num_updates=14010, lr=4.63183e-05, gnorm=0.186, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42393
2023-02-20 08:58:09 - progress_bar.py[line:274] - INFO: epoch 001:  14040 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=112.8, bsz=40, num_updates=14020, lr=4.63147e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42404
2023-02-20 08:58:20 - progress_bar.py[line:274] - INFO: epoch 001:  14050 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.3, bsz=40, num_updates=14030, lr=4.6311e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42415
2023-02-20 08:58:31 - progress_bar.py[line:274] - INFO: epoch 001:  14060 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.6, bsz=40, num_updates=14040, lr=4.63074e-05, gnorm=0.196, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42426
2023-02-20 08:58:42 - progress_bar.py[line:274] - INFO: epoch 001:  14070 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.8, bsz=40, num_updates=14050, lr=4.63038e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42437
2023-02-20 08:58:54 - progress_bar.py[line:274] - INFO: epoch 001:  14080 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.9, wpb=114.1, bsz=40, num_updates=14060, lr=4.63002e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42448
2023-02-20 08:59:05 - progress_bar.py[line:274] - INFO: epoch 001:  14090 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.6, ups=0.91, wpb=114.5, bsz=40, num_updates=14070, lr=4.62966e-05, gnorm=0.144, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42459
2023-02-20 08:59:16 - progress_bar.py[line:274] - INFO: epoch 001:  14100 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.88, wpb=113.1, bsz=40, num_updates=14080, lr=4.62929e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42471
2023-02-20 08:59:27 - progress_bar.py[line:274] - INFO: epoch 001:  14110 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=113.9, bsz=40, num_updates=14090, lr=4.62893e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42482
2023-02-20 08:59:38 - progress_bar.py[line:274] - INFO: epoch 001:  14120 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103, ups=0.9, wpb=113.9, bsz=40, num_updates=14100, lr=4.62857e-05, gnorm=0.257, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=42493
2023-02-20 08:59:49 - progress_bar.py[line:274] - INFO: epoch 001:  14130 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.88, wpb=113.3, bsz=40, num_updates=14110, lr=4.62821e-05, gnorm=0.122, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42504
2023-02-20 09:00:00 - progress_bar.py[line:274] - INFO: epoch 001:  14140 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.9, wpb=111.7, bsz=40, num_updates=14120, lr=4.62785e-05, gnorm=0.163, clip=0, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=42515
2023-02-20 09:00:12 - progress_bar.py[line:274] - INFO: epoch 001:  14150 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=113.2, bsz=40, num_updates=14130, lr=4.62749e-05, gnorm=0.248, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42527
2023-02-20 09:00:23 - progress_bar.py[line:274] - INFO: epoch 001:  14160 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.6, bsz=40, num_updates=14140, lr=4.62712e-05, gnorm=0.2, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42538
2023-02-20 09:00:34 - progress_bar.py[line:274] - INFO: epoch 001:  14170 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.4, bsz=40, num_updates=14150, lr=4.62676e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42549
2023-02-20 09:00:45 - progress_bar.py[line:274] - INFO: epoch 001:  14180 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=113.6, bsz=40, num_updates=14160, lr=4.6264e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=42560
2023-02-20 09:00:57 - progress_bar.py[line:274] - INFO: epoch 001:  14190 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.2, bsz=40, num_updates=14170, lr=4.62604e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42571
2023-02-20 09:01:08 - progress_bar.py[line:274] - INFO: epoch 001:  14200 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.92, wpb=113, bsz=40, num_updates=14180, lr=4.62568e-05, gnorm=0.173, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42582
2023-02-20 09:01:11 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-02-20 09:01:11 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.19 | loss_v1 0 | loss_v2 0 | nll_loss 0.063 | ntokens 113.484 | nsentences 39.998 | sample_size 113.484 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.04 | wps 37.8 | ups 0.33 | wpb 113.5 | bsz 40 | num_updates 14183 | lr 4.62557e-05 | gnorm 0.414 | clip 5.9 | loss_scale 1024 | train_wall 15807 | gb_free 30.5 | ema_decay 0.9999 | wall 42586
2023-02-20 09:01:11 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv slice_id 1 row count 284044 total row count 568088
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_train_NA1_E1.tsv slice_id 0 row count 284044 total row count 568088
2023-02-20 09:01:12 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-20 09:01:13 - trainer.py[line:758] - INFO: begin training epoch 2
2023-02-20 09:01:13 - train.py[line:312] - INFO: Start iterating over samples
2023-02-20 09:01:23 - progress_bar.py[line:274] - INFO: epoch 002:      7 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=103.9, nsentences=36.8, sample_size=103.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=67.9, ups=0.65, wpb=103.9, bsz=36.8, num_updates=14190, lr=4.62531e-05, gnorm=0.265, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42598
2023-02-20 09:01:34 - progress_bar.py[line:274] - INFO: epoch 002:     17 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.6, bsz=40, num_updates=14200, lr=4.62495e-05, gnorm=0.166, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42609
2023-02-20 09:01:45 - progress_bar.py[line:274] - INFO: epoch 002:     27 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.5, bsz=40, num_updates=14210, lr=4.62459e-05, gnorm=0.27, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=42620
2023-02-20 09:01:56 - progress_bar.py[line:274] - INFO: epoch 002:     37 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.9, wpb=114.7, bsz=40, num_updates=14220, lr=4.62423e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42631
2023-02-20 09:02:07 - progress_bar.py[line:274] - INFO: epoch 002:     47 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.7, bsz=40, num_updates=14230, lr=4.62387e-05, gnorm=0.239, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42642
2023-02-20 09:02:18 - progress_bar.py[line:274] - INFO: epoch 002:     57 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.6, ups=0.94, wpb=113.1, bsz=40, num_updates=14240, lr=4.62351e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42653
2023-02-20 09:02:29 - progress_bar.py[line:274] - INFO: epoch 002:     67 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.1, ups=0.94, wpb=113.1, bsz=40, num_updates=14250, lr=4.62314e-05, gnorm=0.239, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42663
2023-02-20 09:02:39 - progress_bar.py[line:274] - INFO: epoch 002:     77 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.1, ups=0.94, wpb=112.9, bsz=40, num_updates=14260, lr=4.62278e-05, gnorm=0.236, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42674
2023-02-20 09:02:50 - progress_bar.py[line:274] - INFO: epoch 002:     87 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.6, bsz=40, num_updates=14270, lr=4.62242e-05, gnorm=0.26, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42685
2023-02-20 09:03:02 - progress_bar.py[line:274] - INFO: epoch 002:     97 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.88, wpb=114.8, bsz=40, num_updates=14280, lr=4.62206e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42697
2023-02-20 09:03:13 - progress_bar.py[line:274] - INFO: epoch 002:    107 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.2, ups=0.87, wpb=112.9, bsz=40, num_updates=14290, lr=4.6217e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42708
2023-02-20 09:03:24 - progress_bar.py[line:274] - INFO: epoch 002:    117 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=14300, lr=4.62133e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42719
2023-02-20 09:03:35 - progress_bar.py[line:274] - INFO: epoch 002:    127 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.6, bsz=40, num_updates=14310, lr=4.62097e-05, gnorm=0.195, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42730
2023-02-20 09:03:46 - progress_bar.py[line:274] - INFO: epoch 002:    137 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.2, ups=0.93, wpb=113.1, bsz=40, num_updates=14320, lr=4.62061e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42741
2023-02-20 09:03:57 - progress_bar.py[line:274] - INFO: epoch 002:    147 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.3, bsz=40, num_updates=14330, lr=4.62025e-05, gnorm=0.164, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42752
2023-02-20 09:04:09 - progress_bar.py[line:274] - INFO: epoch 002:    157 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.87, wpb=114.4, bsz=40, num_updates=14340, lr=4.61989e-05, gnorm=0.179, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42764
2023-02-20 09:04:20 - progress_bar.py[line:274] - INFO: epoch 002:    167 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=113.3, bsz=40, num_updates=14350, lr=4.61953e-05, gnorm=0.133, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42775
2023-02-20 09:04:23 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 09:04:32 - progress_bar.py[line:274] - INFO: epoch 002:    178 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=95.8, ups=0.85, wpb=112.9, bsz=40, num_updates=14360, lr=4.61916e-05, gnorm=0.159, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=42787
2023-02-20 09:04:43 - progress_bar.py[line:274] - INFO: epoch 002:    188 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114, bsz=40, num_updates=14370, lr=4.6188e-05, gnorm=0.213, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42798
2023-02-20 09:04:54 - progress_bar.py[line:274] - INFO: epoch 002:    198 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.6, bsz=40, num_updates=14380, lr=4.61844e-05, gnorm=0.232, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42809
2023-02-20 09:05:06 - progress_bar.py[line:274] - INFO: epoch 002:    208 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.87, wpb=113.2, bsz=40, num_updates=14390, lr=4.61808e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42821
2023-02-20 09:05:17 - progress_bar.py[line:274] - INFO: epoch 002:    218 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.5, ups=0.88, wpb=111.8, bsz=40, num_updates=14400, lr=4.61772e-05, gnorm=0.223, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42832
2023-02-20 09:05:28 - progress_bar.py[line:274] - INFO: epoch 002:    228 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.9, ups=0.93, wpb=112.9, bsz=40, num_updates=14410, lr=4.61736e-05, gnorm=0.228, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42843
2023-02-20 09:05:39 - progress_bar.py[line:274] - INFO: epoch 002:    238 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.5, ups=0.91, wpb=114.3, bsz=40, num_updates=14420, lr=4.61699e-05, gnorm=0.176, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42854
2023-02-20 09:05:50 - progress_bar.py[line:274] - INFO: epoch 002:    248 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=14430, lr=4.61663e-05, gnorm=0.226, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42865
2023-02-20 09:06:02 - progress_bar.py[line:274] - INFO: epoch 002:    258 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.88, wpb=114.9, bsz=40, num_updates=14440, lr=4.61627e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42877
2023-02-20 09:06:13 - progress_bar.py[line:274] - INFO: epoch 002:    268 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=14450, lr=4.61591e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42888
2023-02-20 09:06:24 - progress_bar.py[line:274] - INFO: epoch 002:    278 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.87, wpb=112.2, bsz=40, num_updates=14460, lr=4.61555e-05, gnorm=0.178, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42899
2023-02-20 09:06:36 - progress_bar.py[line:274] - INFO: epoch 002:    288 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.1, bsz=40, num_updates=14470, lr=4.61518e-05, gnorm=0.202, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42910
2023-02-20 09:06:47 - progress_bar.py[line:274] - INFO: epoch 002:    298 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.8, ups=0.92, wpb=114.1, bsz=40, num_updates=14480, lr=4.61482e-05, gnorm=0.26, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42921
2023-02-20 09:06:58 - progress_bar.py[line:274] - INFO: epoch 002:    308 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=14490, lr=4.61446e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42933
2023-02-20 09:07:09 - progress_bar.py[line:274] - INFO: epoch 002:    318 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=112.8, bsz=40, num_updates=14500, lr=4.6141e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42944
2023-02-20 09:07:20 - progress_bar.py[line:274] - INFO: epoch 002:    328 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.88, wpb=113.5, bsz=40, num_updates=14510, lr=4.61374e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=42955
2023-02-20 09:07:31 - progress_bar.py[line:274] - INFO: epoch 002:    338 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.1, bsz=40, num_updates=14520, lr=4.61338e-05, gnorm=0.199, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42966
2023-02-20 09:07:42 - progress_bar.py[line:274] - INFO: epoch 002:    348 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105, ups=0.93, wpb=113, bsz=40, num_updates=14530, lr=4.61301e-05, gnorm=0.151, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42977
2023-02-20 09:07:53 - progress_bar.py[line:274] - INFO: epoch 002:    358 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=14540, lr=4.61265e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42988
2023-02-20 09:08:04 - progress_bar.py[line:274] - INFO: epoch 002:    368 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.91, wpb=114.4, bsz=40, num_updates=14550, lr=4.61229e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42999
2023-02-20 09:08:16 - progress_bar.py[line:274] - INFO: epoch 002:    378 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.5, bsz=40, num_updates=14560, lr=4.61193e-05, gnorm=0.188, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43011
2023-02-20 09:08:27 - progress_bar.py[line:274] - INFO: epoch 002:    388 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.87, wpb=113.6, bsz=40, num_updates=14570, lr=4.61157e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43022
2023-02-20 09:08:38 - progress_bar.py[line:274] - INFO: epoch 002:    398 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.8, ups=0.92, wpb=114.1, bsz=40, num_updates=14580, lr=4.6112e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43033
2023-02-20 09:08:49 - progress_bar.py[line:274] - INFO: epoch 002:    408 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.88, wpb=114, bsz=40, num_updates=14590, lr=4.61084e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43044
2023-02-20 09:09:00 - progress_bar.py[line:274] - INFO: epoch 002:    418 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.91, wpb=112.9, bsz=40, num_updates=14600, lr=4.61048e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43055
2023-02-20 09:09:11 - progress_bar.py[line:274] - INFO: epoch 002:    428 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.91, wpb=113.8, bsz=40, num_updates=14610, lr=4.61012e-05, gnorm=0.172, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43066
2023-02-20 09:09:22 - progress_bar.py[line:274] - INFO: epoch 002:    438 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.9, ups=0.93, wpb=112.8, bsz=40, num_updates=14620, lr=4.60976e-05, gnorm=0.177, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43077
2023-02-20 09:09:33 - progress_bar.py[line:274] - INFO: epoch 002:    448 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.8, bsz=40, num_updates=14630, lr=4.6094e-05, gnorm=0.266, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43088
2023-02-20 09:09:44 - progress_bar.py[line:274] - INFO: epoch 002:    458 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=14640, lr=4.60903e-05, gnorm=0.188, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43099
2023-02-20 09:09:55 - progress_bar.py[line:274] - INFO: epoch 002:    468 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.4, ups=0.92, wpb=113.8, bsz=40, num_updates=14650, lr=4.60867e-05, gnorm=0.232, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43110
2023-02-20 09:10:07 - progress_bar.py[line:274] - INFO: epoch 002:    478 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.7, bsz=40, num_updates=14660, lr=4.60831e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43122
2023-02-20 09:10:18 - progress_bar.py[line:274] - INFO: epoch 002:    488 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.6, bsz=40, num_updates=14670, lr=4.60795e-05, gnorm=0.256, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43133
2023-02-20 09:10:29 - progress_bar.py[line:274] - INFO: epoch 002:    498 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.5, bsz=40, num_updates=14680, lr=4.60759e-05, gnorm=0.171, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=43144
2023-02-20 09:10:40 - progress_bar.py[line:274] - INFO: epoch 002:    508 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.1, ups=0.89, wpb=114.1, bsz=40, num_updates=14690, lr=4.60722e-05, gnorm=0.14, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=43155
2023-02-20 09:10:52 - progress_bar.py[line:274] - INFO: epoch 002:    518 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.4, bsz=40, num_updates=14700, lr=4.60686e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43166
2023-02-20 09:11:03 - progress_bar.py[line:274] - INFO: epoch 002:    528 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.89, wpb=113.4, bsz=40, num_updates=14710, lr=4.6065e-05, gnorm=0.2, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43178
2023-02-20 09:11:14 - progress_bar.py[line:274] - INFO: epoch 002:    538 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=14720, lr=4.60614e-05, gnorm=0.184, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43189
2023-02-20 09:11:25 - progress_bar.py[line:274] - INFO: epoch 002:    548 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.4, bsz=40, num_updates=14730, lr=4.60578e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43200
2023-02-20 09:11:36 - progress_bar.py[line:274] - INFO: epoch 002:    558 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.3, bsz=40, num_updates=14740, lr=4.60542e-05, gnorm=0.178, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43211
2023-02-20 09:11:48 - progress_bar.py[line:274] - INFO: epoch 002:    568 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.87, wpb=113.4, bsz=40, num_updates=14750, lr=4.60505e-05, gnorm=0.166, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43223
2023-02-20 09:11:59 - progress_bar.py[line:274] - INFO: epoch 002:    578 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.89, wpb=114.9, bsz=40, num_updates=14760, lr=4.60469e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43234
2023-02-20 09:12:10 - progress_bar.py[line:274] - INFO: epoch 002:    588 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.6, ups=0.89, wpb=112.7, bsz=40, num_updates=14770, lr=4.60433e-05, gnorm=0.132, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=43245
2023-02-20 09:12:22 - progress_bar.py[line:274] - INFO: epoch 002:    598 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=114, bsz=40, num_updates=14780, lr=4.60397e-05, gnorm=0.188, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43256
2023-02-20 09:12:32 - progress_bar.py[line:274] - INFO: epoch 002:    608 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=106.6, ups=0.94, wpb=113.4, bsz=40, num_updates=14790, lr=4.60361e-05, gnorm=0.239, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43267
2023-02-20 09:12:43 - progress_bar.py[line:274] - INFO: epoch 002:    618 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.2, ups=0.89, wpb=113.4, bsz=40, num_updates=14800, lr=4.60324e-05, gnorm=0.162, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43278
2023-02-20 09:12:55 - progress_bar.py[line:274] - INFO: epoch 002:    628 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=14810, lr=4.60288e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43290
2023-02-20 09:13:06 - progress_bar.py[line:274] - INFO: epoch 002:    638 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.9, ups=0.87, wpb=112.4, bsz=40, num_updates=14820, lr=4.60252e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43301
2023-02-20 09:13:17 - progress_bar.py[line:274] - INFO: epoch 002:    648 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=115.5, nsentences=40, sample_size=115.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.9, wpb=115.5, bsz=40, num_updates=14830, lr=4.60216e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43312
2023-02-20 09:13:28 - progress_bar.py[line:274] - INFO: epoch 002:    658 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.91, wpb=112.1, bsz=40, num_updates=14840, lr=4.6018e-05, gnorm=0.27, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43323
2023-02-20 09:13:40 - progress_bar.py[line:274] - INFO: epoch 002:    668 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=114, bsz=40, num_updates=14850, lr=4.60144e-05, gnorm=0.186, clip=0, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=43334
2023-02-20 09:13:51 - progress_bar.py[line:274] - INFO: epoch 002:    678 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.8, bsz=40, num_updates=14860, lr=4.60107e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43346
2023-02-20 09:14:02 - progress_bar.py[line:274] - INFO: epoch 002:    688 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.9, wpb=112.3, bsz=40, num_updates=14870, lr=4.60071e-05, gnorm=0.22, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43357
2023-02-20 09:14:13 - progress_bar.py[line:274] - INFO: epoch 002:    698 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.9, wpb=113.1, bsz=40, num_updates=14880, lr=4.60035e-05, gnorm=0.12, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43368
2023-02-20 09:14:25 - progress_bar.py[line:274] - INFO: epoch 002:    708 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.5, ups=0.87, wpb=111.9, bsz=40, num_updates=14890, lr=4.59999e-05, gnorm=0.264, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43379
2023-02-20 09:14:36 - progress_bar.py[line:274] - INFO: epoch 002:    718 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=112.7, bsz=40, num_updates=14900, lr=4.59963e-05, gnorm=0.337, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43391
2023-02-20 09:14:47 - progress_bar.py[line:274] - INFO: epoch 002:    728 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=113.1, bsz=40, num_updates=14910, lr=4.59926e-05, gnorm=0.193, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43402
2023-02-20 09:14:59 - progress_bar.py[line:274] - INFO: epoch 002:    738 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.3, bsz=40, num_updates=14920, lr=4.5989e-05, gnorm=0.185, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43413
2023-02-20 09:15:10 - progress_bar.py[line:274] - INFO: epoch 002:    748 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.88, wpb=113.3, bsz=40, num_updates=14930, lr=4.59854e-05, gnorm=0.208, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43425
2023-02-20 09:15:21 - progress_bar.py[line:274] - INFO: epoch 002:    758 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=115.3, nsentences=40, sample_size=115.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.89, wpb=115.3, bsz=40, num_updates=14940, lr=4.59818e-05, gnorm=0.281, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43436
2023-02-20 09:15:32 - progress_bar.py[line:274] - INFO: epoch 002:    768 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.6, bsz=40, num_updates=14950, lr=4.59782e-05, gnorm=0.211, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43447
2023-02-20 09:15:43 - progress_bar.py[line:274] - INFO: epoch 002:    778 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.88, wpb=112, bsz=40, num_updates=14960, lr=4.59746e-05, gnorm=0.163, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43458
2023-02-20 09:15:55 - progress_bar.py[line:274] - INFO: epoch 002:    788 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=14970, lr=4.59709e-05, gnorm=0.216, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43469
2023-02-20 09:16:06 - progress_bar.py[line:274] - INFO: epoch 002:    798 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113.2, bsz=40, num_updates=14980, lr=4.59673e-05, gnorm=0.195, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43480
2023-02-20 09:16:16 - progress_bar.py[line:274] - INFO: epoch 002:    808 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=14990, lr=4.59637e-05, gnorm=0.261, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43491
2023-02-20 09:16:28 - progress_bar.py[line:274] - INFO: epoch 002:    818 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.6, bsz=40, num_updates=15000, lr=4.59601e-05, gnorm=0.292, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43502
2023-02-20 09:16:39 - progress_bar.py[line:274] - INFO: epoch 002:    828 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=113.3, bsz=40, num_updates=15010, lr=4.59565e-05, gnorm=0.144, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43514
2023-02-20 09:16:50 - progress_bar.py[line:274] - INFO: epoch 002:    838 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.87, wpb=113.7, bsz=40, num_updates=15020, lr=4.59528e-05, gnorm=0.207, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43525
2023-02-20 09:17:01 - progress_bar.py[line:274] - INFO: epoch 002:    848 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=115.1, nsentences=40, sample_size=115.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.1, ups=0.9, wpb=115.1, bsz=40, num_updates=15030, lr=4.59492e-05, gnorm=0.205, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43536
2023-02-20 09:17:12 - progress_bar.py[line:274] - INFO: epoch 002:    858 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=112.1, bsz=40, num_updates=15040, lr=4.59456e-05, gnorm=0.206, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43547
2023-02-20 09:17:23 - progress_bar.py[line:274] - INFO: epoch 002:    868 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.5, bsz=40, num_updates=15050, lr=4.5942e-05, gnorm=0.229, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43558
2023-02-20 09:17:35 - progress_bar.py[line:274] - INFO: epoch 002:    878 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.6, bsz=40, num_updates=15060, lr=4.59384e-05, gnorm=0.18, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43569
2023-02-20 09:17:45 - progress_bar.py[line:274] - INFO: epoch 002:    888 / 14203 loss=0.164, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.6, ups=0.92, wpb=112.9, bsz=40, num_updates=15070, lr=4.59348e-05, gnorm=0.129, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43580
2023-02-20 09:17:57 - progress_bar.py[line:274] - INFO: epoch 002:    898 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=112.9, bsz=40, num_updates=15080, lr=4.59311e-05, gnorm=0.286, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43591
2023-02-20 09:18:07 - progress_bar.py[line:274] - INFO: epoch 002:    908 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.92, wpb=112.7, bsz=40, num_updates=15090, lr=4.59275e-05, gnorm=0.215, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43602
2023-02-20 09:18:18 - progress_bar.py[line:274] - INFO: epoch 002:    918 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.7, bsz=40, num_updates=15100, lr=4.59239e-05, gnorm=0.191, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43613
2023-02-20 09:18:30 - progress_bar.py[line:274] - INFO: epoch 002:    928 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=15110, lr=4.59203e-05, gnorm=0.26, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=43624
2023-02-20 09:18:41 - progress_bar.py[line:274] - INFO: epoch 002:    938 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.89, wpb=113.5, bsz=40, num_updates=15120, lr=4.59167e-05, gnorm=0.169, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43636
2023-02-20 09:18:52 - progress_bar.py[line:274] - INFO: epoch 002:    948 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=15130, lr=4.59131e-05, gnorm=0.155, clip=0, loss_scale=2048, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=43647
2023-02-20 09:19:03 - progress_bar.py[line:274] - INFO: epoch 002:    958 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.92, wpb=113.6, bsz=40, num_updates=15140, lr=4.59094e-05, gnorm=0.199, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43658
2023-02-20 09:19:14 - progress_bar.py[line:274] - INFO: epoch 002:    968 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.92, wpb=113.2, bsz=40, num_updates=15150, lr=4.59058e-05, gnorm=0.187, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43669
2023-02-20 09:19:25 - progress_bar.py[line:274] - INFO: epoch 002:    978 / 14203 loss=0.165, loss_v1=0, loss_v2=0, nll_loss=0.041, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.87, wpb=113.4, bsz=40, num_updates=15160, lr=4.59022e-05, gnorm=0.123, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43680
2023-02-20 09:19:36 - progress_bar.py[line:274] - INFO: epoch 002:    988 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.3, ups=0.93, wpb=113.5, bsz=40, num_updates=15170, lr=4.58986e-05, gnorm=0.144, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=43691
2023-02-20 09:19:48 - progress_bar.py[line:274] - INFO: epoch 002:    998 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.7, bsz=40, num_updates=15180, lr=4.5895e-05, gnorm=0.19, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=43702
2023-02-20 09:19:59 - progress_bar.py[line:274] - INFO: epoch 002:   1008 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.4, bsz=40, num_updates=15190, lr=4.58913e-05, gnorm=0.176, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43714
2023-02-20 09:20:10 - progress_bar.py[line:274] - INFO: epoch 002:   1018 / 14203 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.6, ups=0.92, wpb=113.7, bsz=40, num_updates=15200, lr=4.58877e-05, gnorm=0.262, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43724
2023-02-20 09:20:21 - progress_bar.py[line:274] - INFO: epoch 002:   1028 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=112.9, bsz=40, num_updates=15210, lr=4.58841e-05, gnorm=0.213, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43736
2023-02-20 09:20:32 - progress_bar.py[line:274] - INFO: epoch 002:   1038 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.87, wpb=114, bsz=40, num_updates=15220, lr=4.58805e-05, gnorm=0.228, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43747
2023-02-20 09:20:44 - progress_bar.py[line:274] - INFO: epoch 002:   1048 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.88, wpb=114.4, bsz=40, num_updates=15230, lr=4.58769e-05, gnorm=0.325, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43759
2023-02-20 09:20:55 - progress_bar.py[line:274] - INFO: epoch 002:   1058 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.9, bsz=40, num_updates=15240, lr=4.58733e-05, gnorm=0.19, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=43770
2023-02-20 09:21:06 - progress_bar.py[line:274] - INFO: epoch 002:   1068 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=112.9, bsz=40, num_updates=15250, lr=4.58696e-05, gnorm=0.157, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43781
2023-02-20 09:21:18 - progress_bar.py[line:274] - INFO: epoch 002:   1078 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.2, ups=0.88, wpb=112.7, bsz=40, num_updates=15260, lr=4.5866e-05, gnorm=0.154, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43792
2023-02-20 09:21:29 - progress_bar.py[line:274] - INFO: epoch 002:   1088 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.89, wpb=113, bsz=40, num_updates=15270, lr=4.58624e-05, gnorm=0.15, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43804
2023-02-20 09:21:40 - progress_bar.py[line:274] - INFO: epoch 002:   1098 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=114, bsz=40, num_updates=15280, lr=4.58588e-05, gnorm=0.32, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43815
2023-02-20 09:21:51 - progress_bar.py[line:274] - INFO: epoch 002:   1108 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.89, wpb=114.8, bsz=40, num_updates=15290, lr=4.58552e-05, gnorm=0.213, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43826
2023-02-20 09:22:02 - progress_bar.py[line:274] - INFO: epoch 002:   1118 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.8, ups=0.94, wpb=112.6, bsz=40, num_updates=15300, lr=4.58515e-05, gnorm=0.199, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43837
2023-02-20 09:22:13 - progress_bar.py[line:274] - INFO: epoch 002:   1128 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.5, bsz=40, num_updates=15310, lr=4.58479e-05, gnorm=0.19, clip=0, loss_scale=2048, train_wall=11, gb_free=10, ema_decay=0.9999, wall=43848
2023-02-20 09:22:24 - progress_bar.py[line:274] - INFO: epoch 002:   1138 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.9, ups=0.93, wpb=114, bsz=40, num_updates=15320, lr=4.58443e-05, gnorm=0.238, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43859
2023-02-20 09:22:35 - progress_bar.py[line:274] - INFO: epoch 002:   1148 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=113.1, bsz=40, num_updates=15330, lr=4.58407e-05, gnorm=0.211, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43870
2023-02-20 09:22:46 - progress_bar.py[line:274] - INFO: epoch 002:   1158 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.88, wpb=113.9, bsz=40, num_updates=15340, lr=4.58371e-05, gnorm=0.21, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43881
2023-02-20 09:22:57 - progress_bar.py[line:274] - INFO: epoch 002:   1168 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=113.9, bsz=40, num_updates=15350, lr=4.58335e-05, gnorm=0.264, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43892
2023-02-20 09:23:09 - progress_bar.py[line:274] - INFO: epoch 002:   1178 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.7, bsz=40, num_updates=15360, lr=4.58298e-05, gnorm=0.226, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43904
2023-02-20 09:23:20 - progress_bar.py[line:274] - INFO: epoch 002:   1188 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.6, bsz=40, num_updates=15370, lr=4.58262e-05, gnorm=0.24, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43915
2023-02-20 09:23:31 - progress_bar.py[line:274] - INFO: epoch 002:   1198 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.6, bsz=40, num_updates=15380, lr=4.58226e-05, gnorm=0.201, clip=0, loss_scale=4096, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43926
2023-02-20 09:23:42 - progress_bar.py[line:274] - INFO: epoch 002:   1208 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.8, bsz=40, num_updates=15390, lr=4.5819e-05, gnorm=0.202, clip=0, loss_scale=4096, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43937
2023-02-20 09:23:54 - progress_bar.py[line:274] - INFO: epoch 002:   1218 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.9, wpb=113.3, bsz=40, num_updates=15400, lr=4.58154e-05, gnorm=0.158, clip=0, loss_scale=4096, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43948
2023-02-20 09:24:05 - progress_bar.py[line:274] - INFO: epoch 002:   1228 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=15410, lr=4.58117e-05, gnorm=0.217, clip=0, loss_scale=4096, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43959
2023-02-20 09:24:16 - progress_bar.py[line:274] - INFO: epoch 002:   1238 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.8, ups=0.9, wpb=113.8, bsz=40, num_updates=15420, lr=4.58081e-05, gnorm=0.131, clip=0, loss_scale=4096, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43971
2023-02-20 09:24:23 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0
2023-02-20 09:24:28 - progress_bar.py[line:274] - INFO: epoch 002:   1249 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=93.9, ups=0.83, wpb=113.6, bsz=40, num_updates=15430, lr=4.58045e-05, gnorm=0.199, clip=0, loss_scale=2048, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=43983
2023-02-20 09:24:39 - progress_bar.py[line:274] - INFO: epoch 002:   1259 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=15440, lr=4.58009e-05, gnorm=0.271, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43994
2023-02-20 09:24:50 - progress_bar.py[line:274] - INFO: epoch 002:   1269 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.89, wpb=113.5, bsz=40, num_updates=15450, lr=4.57973e-05, gnorm=0.206, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44005
2023-02-20 09:25:02 - progress_bar.py[line:274] - INFO: epoch 002:   1279 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.5, bsz=40, num_updates=15460, lr=4.57937e-05, gnorm=0.197, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44017
2023-02-20 09:25:13 - progress_bar.py[line:274] - INFO: epoch 002:   1289 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.4, bsz=40, num_updates=15470, lr=4.579e-05, gnorm=0.239, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44028
2023-02-20 09:25:24 - progress_bar.py[line:274] - INFO: epoch 002:   1299 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.89, wpb=113.1, bsz=40, num_updates=15480, lr=4.57864e-05, gnorm=0.153, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44039
2023-02-20 09:25:35 - progress_bar.py[line:274] - INFO: epoch 002:   1309 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=114.3, bsz=40, num_updates=15490, lr=4.57828e-05, gnorm=0.25, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44050
2023-02-20 09:25:47 - progress_bar.py[line:274] - INFO: epoch 002:   1319 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=113.4, bsz=40, num_updates=15500, lr=4.57792e-05, gnorm=0.162, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44061
2023-02-20 09:25:58 - progress_bar.py[line:274] - INFO: epoch 002:   1329 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=113.1, bsz=40, num_updates=15510, lr=4.57756e-05, gnorm=0.236, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=44073
2023-02-20 09:26:09 - progress_bar.py[line:274] - INFO: epoch 002:   1339 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.2, ups=0.93, wpb=113.4, bsz=40, num_updates=15520, lr=4.57719e-05, gnorm=0.201, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44083
2023-02-20 09:26:20 - progress_bar.py[line:274] - INFO: epoch 002:   1349 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.2, bsz=40, num_updates=15530, lr=4.57683e-05, gnorm=0.224, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44095
2023-02-20 09:26:31 - progress_bar.py[line:274] - INFO: epoch 002:   1359 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.89, wpb=113.9, bsz=40, num_updates=15540, lr=4.57647e-05, gnorm=0.22, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44106
2023-02-20 09:26:42 - progress_bar.py[line:274] - INFO: epoch 002:   1369 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=112.2, bsz=40, num_updates=15550, lr=4.57611e-05, gnorm=0.259, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44117
2023-02-20 09:26:53 - progress_bar.py[line:274] - INFO: epoch 002:   1379 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=113.1, bsz=40, num_updates=15560, lr=4.57575e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44128
2023-02-20 09:27:05 - progress_bar.py[line:274] - INFO: epoch 002:   1389 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.5, ups=0.87, wpb=112.7, bsz=40, num_updates=15570, lr=4.57539e-05, gnorm=0.178, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44140
2023-02-20 09:27:11 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 09:27:17 - progress_bar.py[line:274] - INFO: epoch 002:   1400 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=93.5, ups=0.82, wpb=114.3, bsz=40, num_updates=15580, lr=4.57502e-05, gnorm=0.185, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=44152
2023-02-20 09:27:28 - progress_bar.py[line:274] - INFO: epoch 002:   1410 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.88, wpb=114.1, bsz=40, num_updates=15590, lr=4.57466e-05, gnorm=0.237, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44163
2023-02-20 09:27:40 - progress_bar.py[line:274] - INFO: epoch 002:   1420 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.9, bsz=40, num_updates=15600, lr=4.5743e-05, gnorm=0.163, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44174
2023-02-20 09:27:51 - progress_bar.py[line:274] - INFO: epoch 002:   1430 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=113.2, bsz=40, num_updates=15610, lr=4.57394e-05, gnorm=0.201, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44186
2023-02-20 09:28:02 - progress_bar.py[line:274] - INFO: epoch 002:   1440 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.9, wpb=113.7, bsz=40, num_updates=15620, lr=4.57358e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44197
2023-02-20 09:28:13 - progress_bar.py[line:274] - INFO: epoch 002:   1450 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.6, bsz=40, num_updates=15630, lr=4.57321e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44208
2023-02-20 09:28:24 - progress_bar.py[line:274] - INFO: epoch 002:   1460 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.8, ups=0.93, wpb=114.3, bsz=40, num_updates=15640, lr=4.57285e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44218
2023-02-20 09:28:35 - progress_bar.py[line:274] - INFO: epoch 002:   1470 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.4, bsz=40, num_updates=15650, lr=4.57249e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=44230
2023-02-20 09:28:46 - progress_bar.py[line:274] - INFO: epoch 002:   1480 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.4, bsz=40, num_updates=15660, lr=4.57213e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44241
2023-02-20 09:28:57 - progress_bar.py[line:274] - INFO: epoch 002:   1490 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.87, wpb=113.8, bsz=40, num_updates=15670, lr=4.57177e-05, gnorm=0.215, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44252
2023-02-20 09:29:09 - progress_bar.py[line:274] - INFO: epoch 002:   1500 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.9, bsz=40, num_updates=15680, lr=4.57141e-05, gnorm=0.179, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44264
2023-02-20 09:29:20 - progress_bar.py[line:274] - INFO: epoch 002:   1510 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.88, wpb=114.2, bsz=40, num_updates=15690, lr=4.57104e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44275
2023-02-20 09:29:31 - progress_bar.py[line:274] - INFO: epoch 002:   1520 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=15700, lr=4.57068e-05, gnorm=0.231, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44286
2023-02-20 09:29:42 - progress_bar.py[line:274] - INFO: epoch 002:   1530 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.9, ups=0.92, wpb=114.5, bsz=40, num_updates=15710, lr=4.57032e-05, gnorm=0.198, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44297
2023-02-20 09:29:54 - progress_bar.py[line:274] - INFO: epoch 002:   1540 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113, bsz=40, num_updates=15720, lr=4.56996e-05, gnorm=0.256, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44309
2023-02-20 09:30:05 - progress_bar.py[line:274] - INFO: epoch 002:   1550 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.5, bsz=40, num_updates=15730, lr=4.5696e-05, gnorm=0.344, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44320
2023-02-20 09:30:16 - progress_bar.py[line:274] - INFO: epoch 002:   1560 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.9, wpb=112.6, bsz=40, num_updates=15740, lr=4.56923e-05, gnorm=0.35, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44331
2023-02-20 09:30:27 - progress_bar.py[line:274] - INFO: epoch 002:   1570 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.9, bsz=40, num_updates=15750, lr=4.56887e-05, gnorm=0.213, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44342
2023-02-20 09:30:38 - progress_bar.py[line:274] - INFO: epoch 002:   1580 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=115.3, nsentences=40, sample_size=115.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.9, wpb=115.3, bsz=40, num_updates=15760, lr=4.56851e-05, gnorm=0.24, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44353
2023-02-20 09:30:49 - progress_bar.py[line:274] - INFO: epoch 002:   1590 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=112.4, bsz=40, num_updates=15770, lr=4.56815e-05, gnorm=0.168, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44364
2023-02-20 09:31:00 - progress_bar.py[line:274] - INFO: epoch 002:   1600 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.9, wpb=113.8, bsz=40, num_updates=15780, lr=4.56779e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44375
2023-02-20 09:31:11 - progress_bar.py[line:274] - INFO: epoch 002:   1610 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.1, bsz=40, num_updates=15790, lr=4.56743e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44386
2023-02-20 09:31:22 - progress_bar.py[line:274] - INFO: epoch 002:   1620 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=15800, lr=4.56706e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44397
2023-02-20 09:31:33 - progress_bar.py[line:274] - INFO: epoch 002:   1630 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.5, bsz=40, num_updates=15810, lr=4.5667e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44408
2023-02-20 09:31:44 - progress_bar.py[line:274] - INFO: epoch 002:   1640 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.9, ups=0.92, wpb=113.3, bsz=40, num_updates=15820, lr=4.56634e-05, gnorm=0.197, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44419
2023-02-20 09:31:55 - progress_bar.py[line:274] - INFO: epoch 002:   1650 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=113.2, bsz=40, num_updates=15830, lr=4.56598e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44430
2023-02-20 09:32:06 - progress_bar.py[line:274] - INFO: epoch 002:   1660 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=112.8, bsz=40, num_updates=15840, lr=4.56562e-05, gnorm=0.272, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44441
2023-02-20 09:32:17 - progress_bar.py[line:274] - INFO: epoch 002:   1670 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113, bsz=40, num_updates=15850, lr=4.56526e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44452
2023-02-20 09:32:28 - progress_bar.py[line:274] - INFO: epoch 002:   1680 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=15860, lr=4.56489e-05, gnorm=0.23, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44463
2023-02-20 09:32:40 - progress_bar.py[line:274] - INFO: epoch 002:   1690 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.3, bsz=40, num_updates=15870, lr=4.56453e-05, gnorm=0.162, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44475
2023-02-20 09:32:51 - progress_bar.py[line:274] - INFO: epoch 002:   1700 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=113.3, bsz=40, num_updates=15880, lr=4.56417e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44486
2023-02-20 09:33:02 - progress_bar.py[line:274] - INFO: epoch 002:   1710 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.5, bsz=40, num_updates=15890, lr=4.56381e-05, gnorm=0.268, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44497
2023-02-20 09:33:13 - progress_bar.py[line:274] - INFO: epoch 002:   1720 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.88, wpb=114.7, bsz=40, num_updates=15900, lr=4.56345e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44508
2023-02-20 09:33:25 - progress_bar.py[line:274] - INFO: epoch 002:   1730 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.88, wpb=114.6, bsz=40, num_updates=15910, lr=4.56308e-05, gnorm=0.285, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44520
2023-02-20 09:33:36 - progress_bar.py[line:274] - INFO: epoch 002:   1740 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.5, bsz=40, num_updates=15920, lr=4.56272e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44531
2023-02-20 09:33:47 - progress_bar.py[line:274] - INFO: epoch 002:   1750 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.88, wpb=113.3, bsz=40, num_updates=15930, lr=4.56236e-05, gnorm=0.141, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44542
2023-02-20 09:33:58 - progress_bar.py[line:274] - INFO: epoch 002:   1760 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.8, ups=0.92, wpb=114.2, bsz=40, num_updates=15940, lr=4.562e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44553
2023-02-20 09:34:09 - progress_bar.py[line:274] - INFO: epoch 002:   1770 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.7, ups=0.92, wpb=113.1, bsz=40, num_updates=15950, lr=4.56164e-05, gnorm=0.13, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44564
2023-02-20 09:34:20 - progress_bar.py[line:274] - INFO: epoch 002:   1780 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.88, wpb=112.9, bsz=40, num_updates=15960, lr=4.56128e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44575
2023-02-20 09:34:32 - progress_bar.py[line:274] - INFO: epoch 002:   1790 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.4, bsz=40, num_updates=15970, lr=4.56091e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44587
2023-02-20 09:34:43 - progress_bar.py[line:274] - INFO: epoch 002:   1800 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105, ups=0.93, wpb=113.2, bsz=40, num_updates=15980, lr=4.56055e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44597
2023-02-20 09:34:54 - progress_bar.py[line:274] - INFO: epoch 002:   1810 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99, ups=0.87, wpb=114.1, bsz=40, num_updates=15990, lr=4.56019e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44609
2023-02-20 09:35:05 - progress_bar.py[line:274] - INFO: epoch 002:   1820 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.87, wpb=112.7, bsz=40, num_updates=16000, lr=4.55983e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44620
2023-02-20 09:35:05 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 09:35:07 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 09:35:07 - train.py[line:551] - INFO: load:1.09 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 09:35:23 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.97 GiB (GPU 0; 39.59 GiB total capacity; 8.00 GiB already allocated; 1.91 GiB free; 35.19 GiB reserved in total by PyTorch)
2023-02-20 09:35:23 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8192 MB |   12376 MB |    6492 TB |    6492 TB |
|       from large pool |    8047 MB |   12231 MB |    6489 TB |    6489 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8192 MB |   12376 MB |    6492 TB |    6492 TB |
|       from large pool |    8047 MB |   12231 MB |    6489 TB |    6489 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   36032 MB |   36088 MB |  213460 MB |  177428 MB |
|       from large pool |   35886 MB |   35886 MB |  213050 MB |  177164 MB |
|       from small pool |     146 MB |     202 MB |     410 MB |     264 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27839 MB |   27839 MB |    6522 TB |    6522 TB |
|       from large pool |   27838 MB |   27838 MB |    6520 TB |    6520 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |  298715 K  |  298711 K  |
|       from large pool |     563    |     575    |  112091 K  |  112090 K  |
|       from small pool |    3106    |    3116    |  186624 K  |  186621 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |  298715 K  |  298711 K  |
|       from large pool |     563    |     575    |  112091 K  |  112090 K  |
|       from small pool |    3106    |    3116    |  186624 K  |  186621 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     175    |     203    |     599    |     424    |
|       from large pool |     102    |     102    |     394    |     292    |
|       from small pool |      73    |     101    |     205    |     132    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     125    |     135    |  190841 K  |  190841 K  |
|       from large pool |      90    |      92    |   39548 K  |   39548 K  |
|       from small pool |      35    |      47    |  151293 K  |  151293 K  |
|===========================================================================|

2023-02-20 09:35:23 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-02-20 09:35:23 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-02-20 09:37:11 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 09:37:11 - train.py[line:551] - INFO: load:1.12 valid_run:123.90 task_valid:119.13 collect_output:2.63
2023-02-20 09:39:11 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 09:39:11 - train.py[line:551] - INFO: load:1.15 valid_run:243.96 task_valid:234.77 collect_output:5.96
2023-02-20 09:41:14 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 09:41:14 - train.py[line:551] - INFO: load:1.18 valid_run:366.59 task_valid:351.12 collect_output:11.16
2023-02-20 09:43:16 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 09:43:16 - train.py[line:551] - INFO: load:1.21 valid_run:488.50 task_valid:464.60 collect_output:18.53
2023-02-20 09:45:16 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 09:45:16 - train.py[line:551] - INFO: load:1.24 valid_run:608.87 task_valid:581.58 collect_output:20.88
2023-02-20 09:47:19 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 09:47:19 - train.py[line:551] - INFO: load:1.26 valid_run:731.68 task_valid:699.94 collect_output:24.27
2023-02-20 09:49:22 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 09:49:22 - train.py[line:551] - INFO: load:1.29 valid_run:854.60 task_valid:817.73 collect_output:28.36
2023-02-20 09:51:24 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 09:51:24 - train.py[line:551] - INFO: load:1.32 valid_run:976.38 task_valid:933.91 collect_output:32.92
2023-02-20 09:53:28 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 09:53:28 - train.py[line:551] - INFO: load:1.35 valid_run:1100.32 task_valid:1050.87 collect_output:38.85
2023-02-20 09:55:30 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 09:55:30 - train.py[line:551] - INFO: load:1.37 valid_run:1222.35 task_valid:1163.40 collect_output:47.32
2023-02-20 09:57:30 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 09:57:30 - train.py[line:551] - INFO: load:1.40 valid_run:1342.47 task_valid:1278.85 collect_output:50.95
2023-02-20 09:59:32 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 09:59:32 - train.py[line:551] - INFO: load:1.43 valid_run:1464.05 task_valid:1395.52 collect_output:54.81
2023-02-20 10:01:31 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 10:01:31 - train.py[line:551] - INFO: load:1.46 valid_run:1582.94 task_valid:1508.99 collect_output:59.18
2023-02-20 10:03:32 - train.py[line:549] - INFO: 2800 / 6234
2023-02-20 10:03:32 - train.py[line:551] - INFO: load:1.48 valid_run:1703.81 task_valid:1626.43 collect_output:61.56
2023-02-20 10:05:33 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 10:05:33 - train.py[line:551] - INFO: load:1.51 valid_run:1824.75 task_valid:1742.19 collect_output:65.70
2023-02-20 10:07:34 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 10:07:34 - train.py[line:551] - INFO: load:1.54 valid_run:1945.90 task_valid:1855.94 collect_output:72.06
2023-02-20 10:09:35 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 10:09:35 - train.py[line:551] - INFO: load:1.56 valid_run:2067.09 task_valid:1971.89 collect_output:76.26
2023-02-20 10:11:36 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 10:11:36 - train.py[line:551] - INFO: load:1.59 valid_run:2187.65 task_valid:2089.53 collect_output:78.14
2023-02-20 10:13:37 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 10:13:37 - train.py[line:551] - INFO: load:1.62 valid_run:2308.98 task_valid:2206.29 collect_output:81.65
2023-02-20 10:15:38 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 10:15:38 - train.py[line:551] - INFO: load:1.65 valid_run:2429.44 task_valid:2322.81 collect_output:84.56
2023-02-20 10:17:39 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 10:17:39 - train.py[line:551] - INFO: load:1.67 valid_run:2551.20 task_valid:2439.20 collect_output:88.89
2023-02-20 10:19:41 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 10:19:41 - train.py[line:551] - INFO: load:1.70 valid_run:2673.26 task_valid:2557.90 collect_output:91.21
2023-02-20 10:21:42 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 10:21:42 - train.py[line:551] - INFO: load:1.73 valid_run:2793.60 task_valid:2672.10 collect_output:96.31
2023-02-20 10:23:42 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 10:23:42 - train.py[line:551] - INFO: load:1.75 valid_run:2913.65 task_valid:2788.36 collect_output:99.04
2023-02-20 10:25:44 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 10:25:44 - train.py[line:551] - INFO: load:1.78 valid_run:3035.95 task_valid:2905.36 collect_output:103.25
2023-02-20 10:27:48 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 10:27:48 - train.py[line:551] - INFO: load:1.81 valid_run:3159.11 task_valid:3021.26 collect_output:109.46
2023-02-20 10:29:47 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 10:29:47 - train.py[line:551] - INFO: load:1.83 valid_run:3278.87 task_valid:3135.46 collect_output:113.97
2023-02-20 10:31:49 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 10:31:49 - train.py[line:551] - INFO: load:1.86 valid_run:3400.66 task_valid:3254.75 collect_output:115.43
2023-02-20 10:33:51 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 10:33:51 - train.py[line:551] - INFO: load:1.89 valid_run:3522.50 task_valid:3370.30 collect_output:120.66
2023-02-20 10:35:53 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 10:35:53 - train.py[line:551] - INFO: load:1.92 valid_run:3644.46 task_valid:3488.83 collect_output:123.05
2023-02-20 10:37:54 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 10:37:54 - train.py[line:551] - INFO: load:1.95 valid_run:3765.55 task_valid:3607.20 collect_output:124.73

====================================================================================================
SGG eval:     R @ 50: 0.6382;     R @ 100: 0.6774;     R @ 500: 0.6960;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3985;    mR @ 100: 0.4737;    mR @ 500: 0.5174;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.8125) (covering:0.3000) (eating:0.8235) (flying in:0.8182) (growing on:0.1250) (hanging from:0.5000) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9804) (says:0.0000) (sitting on:0.7160) (standing on:0.3993) (using:0.5500) (walking in:0.0000) (walking on:0.6757) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6382;     R @ 100: 0.6774;     R @ 500: 0.6960;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3985;    mR @ 100: 0.4737;    mR @ 500: 0.5174;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.8125) (covering:0.3000) (eating:0.8235) (flying in:0.8182) (growing on:0.1250) (hanging from:0.5000) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9804) (says:0.0000) (sitting on:0.7160) (standing on:0.3993) (using:0.5500) (walking in:0.0000) (walking on:0.6757) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================

2023-02-20 10:38:25 - train.py[line:487] - INFO: 0.6774018589253883
2023-02-20 10:38:25 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 10:38:25 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.247 | loss_v1 0 | loss_v2 0 | nll_loss 0.082 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.677402 | ppl 1.06 | vqa_score 0.5124 | wps 118.1 | wpb 72 | bsz 24 | num_updates 16000 | best_R@100 0.693862
2023-02-20 10:38:25 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 16000 updates
2023-02-20 10:38:25 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_2_16000.pt
2023-02-20 10:38:31 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_2_16000.pt
2023-02-20 10:38:34 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_2_16000.pt (epoch 2 @ 16000 updates, score 0.6774018589253883) (writing took 8.769245008006692 seconds)
2023-02-20 10:38:45 - progress_bar.py[line:274] - INFO: epoch 002:   1830 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=113, bsz=40, num_updates=16010, lr=4.55947e-05, gnorm=0.195, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=48440
2023-02-20 10:38:56 - progress_bar.py[line:274] - INFO: epoch 002:   1840 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=114.5, bsz=40, num_updates=16020, lr=4.5591e-05, gnorm=0.163, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=48451
2023-02-20 10:39:08 - progress_bar.py[line:274] - INFO: epoch 002:   1850 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.2, ups=0.86, wpb=112.7, bsz=40, num_updates=16030, lr=4.55874e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=48463
2023-02-20 10:39:19 - progress_bar.py[line:274] - INFO: epoch 002:   1860 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.5, bsz=40, num_updates=16040, lr=4.55838e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=48474
2023-02-20 10:39:30 - progress_bar.py[line:274] - INFO: epoch 002:   1870 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.1, bsz=40, num_updates=16050, lr=4.55802e-05, gnorm=0.175, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48485
2023-02-20 10:39:42 - progress_bar.py[line:274] - INFO: epoch 002:   1880 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.9, bsz=40, num_updates=16060, lr=4.55766e-05, gnorm=0.208, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=48497
2023-02-20 10:39:53 - progress_bar.py[line:274] - INFO: epoch 002:   1890 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.89, wpb=113.8, bsz=40, num_updates=16070, lr=4.5573e-05, gnorm=0.158, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48508
2023-02-20 10:40:04 - progress_bar.py[line:274] - INFO: epoch 002:   1900 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.88, wpb=112.6, bsz=40, num_updates=16080, lr=4.55693e-05, gnorm=0.172, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48519
2023-02-20 10:40:16 - progress_bar.py[line:274] - INFO: epoch 002:   1910 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.3, bsz=40, num_updates=16090, lr=4.55657e-05, gnorm=0.244, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48531
2023-02-20 10:40:27 - progress_bar.py[line:274] - INFO: epoch 002:   1920 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=16100, lr=4.55621e-05, gnorm=0.224, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=48542
2023-02-20 10:40:39 - progress_bar.py[line:274] - INFO: epoch 002:   1930 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.87, wpb=112.8, bsz=40, num_updates=16110, lr=4.55585e-05, gnorm=0.17, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=48553
2023-02-20 10:40:50 - progress_bar.py[line:274] - INFO: epoch 002:   1940 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.9, bsz=40, num_updates=16120, lr=4.55549e-05, gnorm=0.209, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48565
2023-02-20 10:41:01 - progress_bar.py[line:274] - INFO: epoch 002:   1950 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=16130, lr=4.55512e-05, gnorm=0.168, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48576
2023-02-20 10:41:09 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 10:41:13 - progress_bar.py[line:274] - INFO: epoch 002:   1961 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=93.4, ups=0.83, wpb=112.7, bsz=40, num_updates=16140, lr=4.55476e-05, gnorm=0.162, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=48588
2023-02-20 10:41:25 - progress_bar.py[line:274] - INFO: epoch 002:   1971 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.04, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.8, ups=0.89, wpb=114.7, bsz=40, num_updates=16150, lr=4.5544e-05, gnorm=0.105, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=48599
2023-02-20 10:41:35 - progress_bar.py[line:274] - INFO: epoch 002:   1981 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.5, ups=0.93, wpb=112.2, bsz=40, num_updates=16160, lr=4.55404e-05, gnorm=0.146, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48610
2023-02-20 10:41:46 - progress_bar.py[line:274] - INFO: epoch 002:   1991 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.3, ups=0.89, wpb=114.3, bsz=40, num_updates=16170, lr=4.55368e-05, gnorm=0.167, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48621
2023-02-20 10:41:58 - progress_bar.py[line:274] - INFO: epoch 002:   2001 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.1, ups=0.89, wpb=114.1, bsz=40, num_updates=16180, lr=4.55332e-05, gnorm=0.159, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48633
2023-02-20 10:42:09 - progress_bar.py[line:274] - INFO: epoch 002:   2011 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.041, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=16190, lr=4.55295e-05, gnorm=0.081, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48644
2023-02-20 10:42:20 - progress_bar.py[line:274] - INFO: epoch 002:   2021 / 14203 loss=0.162, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.1, ups=0.9, wpb=114, bsz=40, num_updates=16200, lr=4.55259e-05, gnorm=0.107, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48655
2023-02-20 10:42:31 - progress_bar.py[line:274] - INFO: epoch 002:   2031 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.8, bsz=40, num_updates=16210, lr=4.55223e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=48666
2023-02-20 10:42:43 - progress_bar.py[line:274] - INFO: epoch 002:   2041 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=112.5, bsz=40, num_updates=16220, lr=4.55187e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48678
2023-02-20 10:42:54 - progress_bar.py[line:274] - INFO: epoch 002:   2051 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.2, bsz=40, num_updates=16230, lr=4.55151e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48689
2023-02-20 10:43:05 - progress_bar.py[line:274] - INFO: epoch 002:   2061 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.88, wpb=113.1, bsz=40, num_updates=16240, lr=4.55114e-05, gnorm=0.138, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48700
2023-02-20 10:43:16 - progress_bar.py[line:274] - INFO: epoch 002:   2071 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=112.1, bsz=40, num_updates=16250, lr=4.55078e-05, gnorm=0.229, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=48711
2023-02-20 10:43:27 - progress_bar.py[line:274] - INFO: epoch 002:   2081 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.1, ups=0.92, wpb=112.9, bsz=40, num_updates=16260, lr=4.55042e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48722
2023-02-20 10:43:38 - progress_bar.py[line:274] - INFO: epoch 002:   2091 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.9, wpb=114.2, bsz=40, num_updates=16270, lr=4.55006e-05, gnorm=0.271, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48733
2023-02-20 10:43:49 - progress_bar.py[line:274] - INFO: epoch 002:   2101 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=106.9, ups=0.93, wpb=114.5, bsz=40, num_updates=16280, lr=4.5497e-05, gnorm=0.157, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=48744
2023-02-20 10:44:00 - progress_bar.py[line:274] - INFO: epoch 002:   2111 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=16290, lr=4.54934e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48755
2023-02-20 10:44:11 - progress_bar.py[line:274] - INFO: epoch 002:   2121 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.89, wpb=113, bsz=40, num_updates=16300, lr=4.54897e-05, gnorm=0.169, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=48766
2023-02-20 10:44:22 - progress_bar.py[line:274] - INFO: epoch 002:   2131 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.5, bsz=40, num_updates=16310, lr=4.54861e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48777
2023-02-20 10:44:33 - progress_bar.py[line:274] - INFO: epoch 002:   2141 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.8, bsz=40, num_updates=16320, lr=4.54825e-05, gnorm=0.232, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48788
2023-02-20 10:44:44 - progress_bar.py[line:274] - INFO: epoch 002:   2151 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.4, ups=0.92, wpb=112.4, bsz=40, num_updates=16330, lr=4.54789e-05, gnorm=0.136, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48799
2023-02-20 10:44:56 - progress_bar.py[line:274] - INFO: epoch 002:   2161 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=16340, lr=4.54753e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48810
2023-02-20 10:45:07 - progress_bar.py[line:274] - INFO: epoch 002:   2171 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.5, ups=0.91, wpb=114.1, bsz=40, num_updates=16350, lr=4.54716e-05, gnorm=0.164, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=48821
2023-02-20 10:45:18 - progress_bar.py[line:274] - INFO: epoch 002:   2181 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114.1, bsz=40, num_updates=16360, lr=4.5468e-05, gnorm=0.196, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=48833
2023-02-20 10:45:29 - progress_bar.py[line:274] - INFO: epoch 002:   2191 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.9, ups=0.9, wpb=114.3, bsz=40, num_updates=16370, lr=4.54644e-05, gnorm=0.14, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=48844
2023-02-20 10:45:40 - progress_bar.py[line:274] - INFO: epoch 002:   2201 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.4, ups=0.92, wpb=112.8, bsz=40, num_updates=16380, lr=4.54608e-05, gnorm=0.182, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48855
2023-02-20 10:45:51 - progress_bar.py[line:274] - INFO: epoch 002:   2211 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=115.1, nsentences=40, sample_size=115.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.9, wpb=115.1, bsz=40, num_updates=16390, lr=4.54572e-05, gnorm=0.242, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48866
2023-02-20 10:46:02 - progress_bar.py[line:274] - INFO: epoch 002:   2221 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.92, wpb=112.5, bsz=40, num_updates=16400, lr=4.54536e-05, gnorm=0.198, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48877
2023-02-20 10:46:13 - progress_bar.py[line:274] - INFO: epoch 002:   2231 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.9, wpb=114.3, bsz=40, num_updates=16410, lr=4.54499e-05, gnorm=0.248, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48888
2023-02-20 10:46:24 - progress_bar.py[line:274] - INFO: epoch 002:   2241 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.7, ups=0.93, wpb=113.3, bsz=40, num_updates=16420, lr=4.54463e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48899
2023-02-20 10:46:35 - progress_bar.py[line:274] - INFO: epoch 002:   2251 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.93, wpb=113.5, bsz=40, num_updates=16430, lr=4.54427e-05, gnorm=0.178, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=48909
2023-02-20 10:46:46 - progress_bar.py[line:274] - INFO: epoch 002:   2261 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=113.7, bsz=40, num_updates=16440, lr=4.54391e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=48921
2023-02-20 10:46:57 - progress_bar.py[line:274] - INFO: epoch 002:   2271 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.6, bsz=40, num_updates=16450, lr=4.54355e-05, gnorm=0.215, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48932
2023-02-20 10:47:08 - progress_bar.py[line:274] - INFO: epoch 002:   2281 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.5, bsz=40, num_updates=16460, lr=4.54319e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48943
2023-02-20 10:47:19 - progress_bar.py[line:274] - INFO: epoch 002:   2291 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.2, bsz=40, num_updates=16470, lr=4.54282e-05, gnorm=0.2, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48954
2023-02-20 10:47:31 - progress_bar.py[line:274] - INFO: epoch 002:   2301 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.88, wpb=113.9, bsz=40, num_updates=16480, lr=4.54246e-05, gnorm=0.227, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=48966
2023-02-20 10:47:42 - progress_bar.py[line:274] - INFO: epoch 002:   2311 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.9, ups=0.92, wpb=114.6, bsz=40, num_updates=16490, lr=4.5421e-05, gnorm=0.21, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=48977
2023-02-20 10:47:53 - progress_bar.py[line:274] - INFO: epoch 002:   2321 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=112.9, bsz=40, num_updates=16500, lr=4.54174e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=48988
2023-02-20 10:48:05 - progress_bar.py[line:274] - INFO: epoch 002:   2331 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.9, ups=0.85, wpb=113.7, bsz=40, num_updates=16510, lr=4.54138e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=49000
2023-02-20 10:48:16 - progress_bar.py[line:274] - INFO: epoch 002:   2341 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=16520, lr=4.54101e-05, gnorm=0.254, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49011
2023-02-20 10:48:27 - progress_bar.py[line:274] - INFO: epoch 002:   2351 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104, ups=0.92, wpb=113.6, bsz=40, num_updates=16530, lr=4.54065e-05, gnorm=0.14, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49022
2023-02-20 10:48:38 - progress_bar.py[line:274] - INFO: epoch 002:   2361 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.5, bsz=40, num_updates=16540, lr=4.54029e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49033
2023-02-20 10:48:50 - progress_bar.py[line:274] - INFO: epoch 002:   2371 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=111.8, bsz=40, num_updates=16550, lr=4.53993e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49044
2023-02-20 10:49:01 - progress_bar.py[line:274] - INFO: epoch 002:   2381 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=114.1, bsz=40, num_updates=16560, lr=4.53957e-05, gnorm=0.192, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49056
2023-02-20 10:49:12 - progress_bar.py[line:274] - INFO: epoch 002:   2391 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.5, ups=0.92, wpb=114.4, bsz=40, num_updates=16570, lr=4.53921e-05, gnorm=0.249, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49067
2023-02-20 10:49:23 - progress_bar.py[line:274] - INFO: epoch 002:   2401 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=114, bsz=40, num_updates=16580, lr=4.53884e-05, gnorm=0.182, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49078
2023-02-20 10:49:34 - progress_bar.py[line:274] - INFO: epoch 002:   2411 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.88, wpb=114.6, bsz=40, num_updates=16590, lr=4.53848e-05, gnorm=0.183, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49089
2023-02-20 10:49:45 - progress_bar.py[line:274] - INFO: epoch 002:   2421 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=16600, lr=4.53812e-05, gnorm=0.184, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49100
2023-02-20 10:49:57 - progress_bar.py[line:274] - INFO: epoch 002:   2431 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=112.7, bsz=40, num_updates=16610, lr=4.53776e-05, gnorm=0.215, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=49111
2023-02-20 10:50:08 - progress_bar.py[line:274] - INFO: epoch 002:   2441 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.1, bsz=40, num_updates=16620, lr=4.5374e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49123
2023-02-20 10:50:19 - progress_bar.py[line:274] - INFO: epoch 002:   2451 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.6, ups=0.93, wpb=112.9, bsz=40, num_updates=16630, lr=4.53703e-05, gnorm=0.134, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49133
2023-02-20 10:50:30 - progress_bar.py[line:274] - INFO: epoch 002:   2461 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.6, bsz=40, num_updates=16640, lr=4.53667e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49145
2023-02-20 10:50:41 - progress_bar.py[line:274] - INFO: epoch 002:   2471 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.89, wpb=114.6, bsz=40, num_updates=16650, lr=4.53631e-05, gnorm=0.151, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49156
2023-02-20 10:50:52 - progress_bar.py[line:274] - INFO: epoch 002:   2481 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.9, wpb=113.6, bsz=40, num_updates=16660, lr=4.53595e-05, gnorm=0.154, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49167
2023-02-20 10:51:03 - progress_bar.py[line:274] - INFO: epoch 002:   2491 / 14203 loss=0.166, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.3, ups=0.9, wpb=113.1, bsz=40, num_updates=16670, lr=4.53559e-05, gnorm=0.156, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49178
2023-02-20 10:51:14 - progress_bar.py[line:274] - INFO: epoch 002:   2501 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.9, ups=0.91, wpb=114.2, bsz=40, num_updates=16680, lr=4.53523e-05, gnorm=0.213, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49189
2023-02-20 10:51:26 - progress_bar.py[line:274] - INFO: epoch 002:   2511 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.8, ups=0.88, wpb=113.1, bsz=40, num_updates=16690, lr=4.53486e-05, gnorm=0.204, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49200
2023-02-20 10:51:37 - progress_bar.py[line:274] - INFO: epoch 002:   2521 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.6, bsz=40, num_updates=16700, lr=4.5345e-05, gnorm=0.179, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49212
2023-02-20 10:51:48 - progress_bar.py[line:274] - INFO: epoch 002:   2531 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.1, ups=0.88, wpb=113.4, bsz=40, num_updates=16710, lr=4.53414e-05, gnorm=0.148, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49223
2023-02-20 10:51:59 - progress_bar.py[line:274] - INFO: epoch 002:   2541 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=114.1, bsz=40, num_updates=16720, lr=4.53378e-05, gnorm=0.194, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49234
2023-02-20 10:52:11 - progress_bar.py[line:274] - INFO: epoch 002:   2551 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.87, wpb=113.6, bsz=40, num_updates=16730, lr=4.53342e-05, gnorm=0.236, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49245
2023-02-20 10:52:22 - progress_bar.py[line:274] - INFO: epoch 002:   2561 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=16740, lr=4.53305e-05, gnorm=0.235, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49257
2023-02-20 10:52:33 - progress_bar.py[line:274] - INFO: epoch 002:   2571 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.9, wpb=114.7, bsz=40, num_updates=16750, lr=4.53269e-05, gnorm=0.195, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49268
2023-02-20 10:52:44 - progress_bar.py[line:274] - INFO: epoch 002:   2581 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.4, bsz=40, num_updates=16760, lr=4.53233e-05, gnorm=0.173, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49279
2023-02-20 10:52:55 - progress_bar.py[line:274] - INFO: epoch 002:   2591 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.9, wpb=114.9, bsz=40, num_updates=16770, lr=4.53197e-05, gnorm=0.154, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49290
2023-02-20 10:53:06 - progress_bar.py[line:274] - INFO: epoch 002:   2601 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=112.9, bsz=40, num_updates=16780, lr=4.53161e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49301
2023-02-20 10:53:17 - progress_bar.py[line:274] - INFO: epoch 002:   2611 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.93, wpb=111.9, bsz=40, num_updates=16790, lr=4.53125e-05, gnorm=0.241, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49312
2023-02-20 10:53:28 - progress_bar.py[line:274] - INFO: epoch 002:   2621 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=112.6, bsz=40, num_updates=16800, lr=4.53088e-05, gnorm=0.179, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49323
2023-02-20 10:53:39 - progress_bar.py[line:274] - INFO: epoch 002:   2631 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=114, bsz=40, num_updates=16810, lr=4.53052e-05, gnorm=0.151, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49334
2023-02-20 10:53:50 - progress_bar.py[line:274] - INFO: epoch 002:   2641 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105, ups=0.92, wpb=114.4, bsz=40, num_updates=16820, lr=4.53016e-05, gnorm=0.145, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=49345
2023-02-20 10:54:01 - progress_bar.py[line:274] - INFO: epoch 002:   2651 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.8, bsz=40, num_updates=16830, lr=4.5298e-05, gnorm=0.173, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49356
2023-02-20 10:54:13 - progress_bar.py[line:274] - INFO: epoch 002:   2661 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.88, wpb=114.6, bsz=40, num_updates=16840, lr=4.52944e-05, gnorm=0.198, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49368
2023-02-20 10:54:24 - progress_bar.py[line:274] - INFO: epoch 002:   2671 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=113.1, bsz=40, num_updates=16850, lr=4.52907e-05, gnorm=0.222, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49379
2023-02-20 10:54:35 - progress_bar.py[line:274] - INFO: epoch 002:   2681 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.5, ups=0.91, wpb=113, bsz=40, num_updates=16860, lr=4.52871e-05, gnorm=0.144, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49390
2023-02-20 10:54:46 - progress_bar.py[line:274] - INFO: epoch 002:   2691 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=113.2, bsz=40, num_updates=16870, lr=4.52835e-05, gnorm=0.246, clip=0, loss_scale=2048, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=49401
2023-02-20 10:54:58 - progress_bar.py[line:274] - INFO: epoch 002:   2701 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.8, bsz=40, num_updates=16880, lr=4.52799e-05, gnorm=0.201, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49412
2023-02-20 10:55:09 - progress_bar.py[line:274] - INFO: epoch 002:   2711 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=16890, lr=4.52763e-05, gnorm=0.193, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49424
2023-02-20 10:55:20 - progress_bar.py[line:274] - INFO: epoch 002:   2721 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=113.9, bsz=40, num_updates=16900, lr=4.52727e-05, gnorm=0.2, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49435
2023-02-20 10:55:31 - progress_bar.py[line:274] - INFO: epoch 002:   2731 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.92, wpb=114, bsz=40, num_updates=16910, lr=4.5269e-05, gnorm=0.221, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49446
2023-02-20 10:55:42 - progress_bar.py[line:274] - INFO: epoch 002:   2741 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98, ups=0.87, wpb=113.2, bsz=40, num_updates=16920, lr=4.52654e-05, gnorm=0.153, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49457
2023-02-20 10:55:54 - progress_bar.py[line:274] - INFO: epoch 002:   2751 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.89, wpb=113.6, bsz=40, num_updates=16930, lr=4.52618e-05, gnorm=0.162, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49468
2023-02-20 10:56:04 - progress_bar.py[line:274] - INFO: epoch 002:   2761 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.9, ups=0.93, wpb=114, bsz=40, num_updates=16940, lr=4.52582e-05, gnorm=0.155, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49479
2023-02-20 10:56:16 - progress_bar.py[line:274] - INFO: epoch 002:   2771 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=16950, lr=4.52546e-05, gnorm=0.152, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49490
2023-02-20 10:56:27 - progress_bar.py[line:274] - INFO: epoch 002:   2781 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.9, wpb=113.2, bsz=40, num_updates=16960, lr=4.52509e-05, gnorm=0.185, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49502
2023-02-20 10:56:38 - progress_bar.py[line:274] - INFO: epoch 002:   2791 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=113.3, bsz=40, num_updates=16970, lr=4.52473e-05, gnorm=0.158, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49513
2023-02-20 10:56:49 - progress_bar.py[line:274] - INFO: epoch 002:   2801 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=16980, lr=4.52437e-05, gnorm=0.167, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49524
2023-02-20 10:57:00 - progress_bar.py[line:274] - INFO: epoch 002:   2811 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.88, wpb=114.4, bsz=40, num_updates=16990, lr=4.52401e-05, gnorm=0.147, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49535
2023-02-20 10:57:12 - progress_bar.py[line:274] - INFO: epoch 002:   2821 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.87, wpb=113.2, bsz=40, num_updates=17000, lr=4.52365e-05, gnorm=0.198, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49547
2023-02-20 10:57:23 - progress_bar.py[line:274] - INFO: epoch 002:   2831 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.87, wpb=114.8, bsz=40, num_updates=17010, lr=4.52329e-05, gnorm=0.229, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49558
2023-02-20 10:57:35 - progress_bar.py[line:274] - INFO: epoch 002:   2841 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.87, wpb=113.3, bsz=40, num_updates=17020, lr=4.52292e-05, gnorm=0.155, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49570
2023-02-20 10:57:46 - progress_bar.py[line:274] - INFO: epoch 002:   2851 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.1, ups=0.9, wpb=114.1, bsz=40, num_updates=17030, lr=4.52256e-05, gnorm=0.143, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49581
2023-02-20 10:57:57 - progress_bar.py[line:274] - INFO: epoch 002:   2861 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=17040, lr=4.5222e-05, gnorm=0.204, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49592
2023-02-20 10:58:08 - progress_bar.py[line:274] - INFO: epoch 002:   2871 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=113.3, bsz=40, num_updates=17050, lr=4.52184e-05, gnorm=0.185, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49603
2023-02-20 10:58:19 - progress_bar.py[line:274] - INFO: epoch 002:   2881 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.89, wpb=113.4, bsz=40, num_updates=17060, lr=4.52148e-05, gnorm=0.158, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49614
2023-02-20 10:58:30 - progress_bar.py[line:274] - INFO: epoch 002:   2891 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.4, bsz=40, num_updates=17070, lr=4.52111e-05, gnorm=0.18, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49625
2023-02-20 10:58:42 - progress_bar.py[line:274] - INFO: epoch 002:   2901 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.88, wpb=112.2, bsz=40, num_updates=17080, lr=4.52075e-05, gnorm=0.226, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49637
2023-02-20 10:58:53 - progress_bar.py[line:274] - INFO: epoch 002:   2911 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.2, ups=0.9, wpb=113, bsz=40, num_updates=17090, lr=4.52039e-05, gnorm=0.174, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49648
2023-02-20 10:59:04 - progress_bar.py[line:274] - INFO: epoch 002:   2921 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=112.9, bsz=40, num_updates=17100, lr=4.52003e-05, gnorm=0.212, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49659
2023-02-20 10:59:15 - progress_bar.py[line:274] - INFO: epoch 002:   2931 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.88, wpb=114.8, bsz=40, num_updates=17110, lr=4.51967e-05, gnorm=0.174, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49670
2023-02-20 10:59:26 - progress_bar.py[line:274] - INFO: epoch 002:   2941 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.3, ups=0.91, wpb=115.2, bsz=40, num_updates=17120, lr=4.51931e-05, gnorm=0.192, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49681
2023-02-20 10:59:38 - progress_bar.py[line:274] - INFO: epoch 002:   2951 / 14203 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.87, wpb=111.9, bsz=40, num_updates=17130, lr=4.51894e-05, gnorm=0.252, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49692
2023-02-20 10:59:50 - progress_bar.py[line:274] - INFO: epoch 002:   2961 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=93.7, ups=0.83, wpb=112.8, bsz=40, num_updates=17140, lr=4.51858e-05, gnorm=0.207, clip=0, loss_scale=2048, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=49704
2023-02-20 11:00:01 - progress_bar.py[line:274] - INFO: epoch 002:   2971 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.3, ups=0.86, wpb=112.7, bsz=40, num_updates=17150, lr=4.51822e-05, gnorm=0.14, clip=0, loss_scale=2048, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=49716
2023-02-20 11:00:12 - progress_bar.py[line:274] - INFO: epoch 002:   2981 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.8, bsz=40, num_updates=17160, lr=4.51786e-05, gnorm=0.167, clip=0, loss_scale=4096, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49727
2023-02-20 11:00:24 - progress_bar.py[line:274] - INFO: epoch 002:   2991 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.89, wpb=113.6, bsz=40, num_updates=17170, lr=4.5175e-05, gnorm=0.141, clip=0, loss_scale=4096, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49739
2023-02-20 11:00:35 - progress_bar.py[line:274] - INFO: epoch 002:   3001 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=115.4, nsentences=40, sample_size=115.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.89, wpb=115.4, bsz=40, num_updates=17180, lr=4.51714e-05, gnorm=0.188, clip=0, loss_scale=4096, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49750
2023-02-20 11:00:46 - progress_bar.py[line:274] - INFO: epoch 002:   3011 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=112.9, bsz=40, num_updates=17190, lr=4.51677e-05, gnorm=0.204, clip=0, loss_scale=4096, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49761
2023-02-20 11:00:57 - progress_bar.py[line:274] - INFO: epoch 002:   3021 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=113, bsz=40, num_updates=17200, lr=4.51641e-05, gnorm=0.214, clip=0, loss_scale=4096, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=49772
2023-02-20 11:01:08 - progress_bar.py[line:274] - INFO: epoch 002:   3031 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.7, ups=0.92, wpb=114.4, bsz=40, num_updates=17210, lr=4.51605e-05, gnorm=0.217, clip=0, loss_scale=4096, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49783
2023-02-20 11:01:19 - progress_bar.py[line:274] - INFO: epoch 002:   3041 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.8, ups=0.89, wpb=114.1, bsz=40, num_updates=17220, lr=4.51569e-05, gnorm=0.155, clip=0, loss_scale=4096, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49794
2023-02-20 11:01:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0
2023-02-20 11:01:31 - progress_bar.py[line:274] - INFO: epoch 002:   3052 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=94.9, ups=0.83, wpb=114.2, bsz=40, num_updates=17230, lr=4.51533e-05, gnorm=0.178, clip=0, loss_scale=2048, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=49806
2023-02-20 11:01:42 - progress_bar.py[line:274] - INFO: epoch 002:   3062 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.91, wpb=113.3, bsz=40, num_updates=17240, lr=4.51496e-05, gnorm=0.238, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49817
2023-02-20 11:01:53 - progress_bar.py[line:274] - INFO: epoch 002:   3072 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=106, ups=0.93, wpb=114.2, bsz=40, num_updates=17250, lr=4.5146e-05, gnorm=0.191, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49828
2023-02-20 11:02:04 - progress_bar.py[line:274] - INFO: epoch 002:   3082 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=17260, lr=4.51424e-05, gnorm=0.245, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49839
2023-02-20 11:02:15 - progress_bar.py[line:274] - INFO: epoch 002:   3092 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.2, ups=0.91, wpb=114, bsz=40, num_updates=17270, lr=4.51388e-05, gnorm=0.251, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49850
2023-02-20 11:02:26 - progress_bar.py[line:274] - INFO: epoch 002:   3102 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=106.2, ups=0.93, wpb=114.3, bsz=40, num_updates=17280, lr=4.51352e-05, gnorm=0.137, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49861
2023-02-20 11:02:37 - progress_bar.py[line:274] - INFO: epoch 002:   3112 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.5, ups=0.93, wpb=114.6, bsz=40, num_updates=17290, lr=4.51316e-05, gnorm=0.257, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49871
2023-02-20 11:02:48 - progress_bar.py[line:274] - INFO: epoch 002:   3122 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=112.9, bsz=40, num_updates=17300, lr=4.51279e-05, gnorm=0.212, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49883
2023-02-20 11:02:59 - progress_bar.py[line:274] - INFO: epoch 002:   3132 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.87, wpb=112.8, bsz=40, num_updates=17310, lr=4.51243e-05, gnorm=0.192, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49894
2023-02-20 11:03:10 - progress_bar.py[line:274] - INFO: epoch 002:   3142 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.3, ups=0.92, wpb=113.3, bsz=40, num_updates=17320, lr=4.51207e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49905
2023-02-20 11:03:21 - progress_bar.py[line:274] - INFO: epoch 002:   3152 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103, ups=0.91, wpb=113.7, bsz=40, num_updates=17330, lr=4.51171e-05, gnorm=0.179, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49916
2023-02-20 11:03:32 - progress_bar.py[line:274] - INFO: epoch 002:   3162 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=17340, lr=4.51135e-05, gnorm=0.16, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49927
2023-02-20 11:03:44 - progress_bar.py[line:274] - INFO: epoch 002:   3172 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.1, ups=0.88, wpb=113.5, bsz=40, num_updates=17350, lr=4.51098e-05, gnorm=0.234, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49939
2023-02-20 11:03:55 - progress_bar.py[line:274] - INFO: epoch 002:   3182 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.7, ups=0.89, wpb=113.1, bsz=40, num_updates=17360, lr=4.51062e-05, gnorm=0.194, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49950
2023-02-20 11:04:06 - progress_bar.py[line:274] - INFO: epoch 002:   3192 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=113.8, bsz=40, num_updates=17370, lr=4.51026e-05, gnorm=0.237, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49961
2023-02-20 11:04:17 - progress_bar.py[line:274] - INFO: epoch 002:   3202 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.9, ups=0.92, wpb=113.3, bsz=40, num_updates=17380, lr=4.5099e-05, gnorm=0.203, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49972
2023-02-20 11:04:28 - progress_bar.py[line:274] - INFO: epoch 002:   3212 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.6, bsz=40, num_updates=17390, lr=4.50954e-05, gnorm=0.231, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49983
2023-02-20 11:04:40 - progress_bar.py[line:274] - INFO: epoch 002:   3222 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98, ups=0.87, wpb=112.7, bsz=40, num_updates=17400, lr=4.50918e-05, gnorm=0.274, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49995
2023-02-20 11:04:51 - progress_bar.py[line:274] - INFO: epoch 002:   3232 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.8, ups=0.87, wpb=113.6, bsz=40, num_updates=17410, lr=4.50881e-05, gnorm=0.118, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50006
2023-02-20 11:05:03 - progress_bar.py[line:274] - INFO: epoch 002:   3242 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=17420, lr=4.50845e-05, gnorm=0.245, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50018
2023-02-20 11:05:14 - progress_bar.py[line:274] - INFO: epoch 002:   3252 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=17430, lr=4.50809e-05, gnorm=0.141, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50029
2023-02-20 11:05:25 - progress_bar.py[line:274] - INFO: epoch 002:   3262 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106, ups=0.93, wpb=114.3, bsz=40, num_updates=17440, lr=4.50773e-05, gnorm=0.235, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50040
2023-02-20 11:05:36 - progress_bar.py[line:274] - INFO: epoch 002:   3272 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.87, wpb=113, bsz=40, num_updates=17450, lr=4.50737e-05, gnorm=0.188, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50051
2023-02-20 11:05:48 - progress_bar.py[line:274] - INFO: epoch 002:   3282 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.5, bsz=40, num_updates=17460, lr=4.507e-05, gnorm=0.162, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50062
2023-02-20 11:05:59 - progress_bar.py[line:274] - INFO: epoch 002:   3292 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.6, ups=0.87, wpb=112, bsz=40, num_updates=17470, lr=4.50664e-05, gnorm=0.358, clip=10, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50074
2023-02-20 11:06:05 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 11:06:12 - progress_bar.py[line:274] - INFO: epoch 002:   3303 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=90.2, ups=0.8, wpb=112.4, bsz=40, num_updates=17480, lr=4.50628e-05, gnorm=0.219, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=50086
2023-02-20 11:06:23 - progress_bar.py[line:274] - INFO: epoch 002:   3313 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.6, ups=0.89, wpb=112.8, bsz=40, num_updates=17490, lr=4.50592e-05, gnorm=0.202, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50098
2023-02-20 11:06:34 - progress_bar.py[line:274] - INFO: epoch 002:   3323 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.8, ups=0.89, wpb=114, bsz=40, num_updates=17500, lr=4.50556e-05, gnorm=0.155, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50109
2023-02-20 11:06:45 - progress_bar.py[line:274] - INFO: epoch 002:   3333 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.7, bsz=40, num_updates=17510, lr=4.5052e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50120
2023-02-20 11:06:56 - progress_bar.py[line:274] - INFO: epoch 002:   3343 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105, ups=0.92, wpb=114.4, bsz=40, num_updates=17520, lr=4.50483e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50131
2023-02-20 11:07:08 - progress_bar.py[line:274] - INFO: epoch 002:   3353 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.86, wpb=114, bsz=40, num_updates=17530, lr=4.50447e-05, gnorm=0.199, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=50143
2023-02-20 11:07:19 - progress_bar.py[line:274] - INFO: epoch 002:   3363 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.3, ups=0.93, wpb=113.4, bsz=40, num_updates=17540, lr=4.50411e-05, gnorm=0.179, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50154
2023-02-20 11:07:30 - progress_bar.py[line:274] - INFO: epoch 002:   3373 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=114.1, bsz=40, num_updates=17550, lr=4.50375e-05, gnorm=0.238, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50165
2023-02-20 11:07:41 - progress_bar.py[line:274] - INFO: epoch 002:   3383 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.2, bsz=40, num_updates=17560, lr=4.50339e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50176
2023-02-20 11:07:52 - progress_bar.py[line:274] - INFO: epoch 002:   3393 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=115.6, nsentences=40, sample_size=115.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.9, wpb=115.6, bsz=40, num_updates=17570, lr=4.50302e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50187
2023-02-20 11:08:04 - progress_bar.py[line:274] - INFO: epoch 002:   3403 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=112.6, bsz=40, num_updates=17580, lr=4.50266e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50198
2023-02-20 11:08:15 - progress_bar.py[line:274] - INFO: epoch 002:   3413 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.88, wpb=114.6, bsz=40, num_updates=17590, lr=4.5023e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50210
2023-02-20 11:08:26 - progress_bar.py[line:274] - INFO: epoch 002:   3423 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=17600, lr=4.50194e-05, gnorm=0.218, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50221
2023-02-20 11:08:38 - progress_bar.py[line:274] - INFO: epoch 002:   3433 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.9, ups=0.88, wpb=112.3, bsz=40, num_updates=17610, lr=4.50158e-05, gnorm=0.14, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50232
2023-02-20 11:08:49 - progress_bar.py[line:274] - INFO: epoch 002:   3443 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.5, ups=0.92, wpb=113.9, bsz=40, num_updates=17620, lr=4.50122e-05, gnorm=0.232, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50243
2023-02-20 11:09:00 - progress_bar.py[line:274] - INFO: epoch 002:   3453 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.89, wpb=113.9, bsz=40, num_updates=17630, lr=4.50085e-05, gnorm=0.158, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50255
2023-02-20 11:09:11 - progress_bar.py[line:274] - INFO: epoch 002:   3463 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.88, wpb=114.5, bsz=40, num_updates=17640, lr=4.50049e-05, gnorm=0.177, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=50266
2023-02-20 11:09:22 - progress_bar.py[line:274] - INFO: epoch 002:   3473 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.8, ups=0.93, wpb=113.5, bsz=40, num_updates=17650, lr=4.50013e-05, gnorm=0.17, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50277
2023-02-20 11:09:33 - progress_bar.py[line:274] - INFO: epoch 002:   3483 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=113.2, bsz=40, num_updates=17660, lr=4.49977e-05, gnorm=0.226, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50288
2023-02-20 11:09:44 - progress_bar.py[line:274] - INFO: epoch 002:   3493 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.92, wpb=112.1, bsz=40, num_updates=17670, lr=4.49941e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50299
2023-02-20 11:09:55 - progress_bar.py[line:274] - INFO: epoch 002:   3503 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112, bsz=40, num_updates=17680, lr=4.49904e-05, gnorm=0.194, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50310
2023-02-20 11:10:07 - progress_bar.py[line:274] - INFO: epoch 002:   3513 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98, ups=0.86, wpb=113.6, bsz=40, num_updates=17690, lr=4.49868e-05, gnorm=0.165, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=50322
2023-02-20 11:10:18 - progress_bar.py[line:274] - INFO: epoch 002:   3523 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.88, wpb=114.6, bsz=40, num_updates=17700, lr=4.49832e-05, gnorm=0.196, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50333
2023-02-20 11:10:29 - progress_bar.py[line:274] - INFO: epoch 002:   3533 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=17710, lr=4.49796e-05, gnorm=0.169, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50344
2023-02-20 11:10:40 - progress_bar.py[line:274] - INFO: epoch 002:   3543 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=17720, lr=4.4976e-05, gnorm=0.188, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50355
2023-02-20 11:10:52 - progress_bar.py[line:274] - INFO: epoch 002:   3553 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.9, wpb=113.5, bsz=40, num_updates=17730, lr=4.49724e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50366
2023-02-20 11:11:03 - progress_bar.py[line:274] - INFO: epoch 002:   3563 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=112.4, bsz=40, num_updates=17740, lr=4.49687e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50378
2023-02-20 11:11:14 - progress_bar.py[line:274] - INFO: epoch 002:   3573 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.88, wpb=114.3, bsz=40, num_updates=17750, lr=4.49651e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50389
2023-02-20 11:11:26 - progress_bar.py[line:274] - INFO: epoch 002:   3583 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98.5, ups=0.87, wpb=113.3, bsz=40, num_updates=17760, lr=4.49615e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50401
2023-02-20 11:11:37 - progress_bar.py[line:274] - INFO: epoch 002:   3593 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.88, wpb=113.7, bsz=40, num_updates=17770, lr=4.49579e-05, gnorm=0.181, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50412
2023-02-20 11:11:48 - progress_bar.py[line:274] - INFO: epoch 002:   3603 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.9, wpb=112.4, bsz=40, num_updates=17780, lr=4.49543e-05, gnorm=0.174, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=50423
2023-02-20 11:11:59 - progress_bar.py[line:274] - INFO: epoch 002:   3613 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.1, ups=0.93, wpb=113, bsz=40, num_updates=17790, lr=4.49506e-05, gnorm=0.166, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50434
2023-02-20 11:12:11 - progress_bar.py[line:274] - INFO: epoch 002:   3623 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=115.4, nsentences=40, sample_size=115.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.6, ups=0.87, wpb=115.4, bsz=40, num_updates=17800, lr=4.4947e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50445
2023-02-20 11:12:22 - progress_bar.py[line:274] - INFO: epoch 002:   3633 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=113.2, bsz=40, num_updates=17810, lr=4.49434e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50457
2023-02-20 11:12:33 - progress_bar.py[line:274] - INFO: epoch 002:   3643 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=17820, lr=4.49398e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=50468
2023-02-20 11:12:44 - progress_bar.py[line:274] - INFO: epoch 002:   3653 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=114.2, bsz=40, num_updates=17830, lr=4.49362e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50479
2023-02-20 11:12:55 - progress_bar.py[line:274] - INFO: epoch 002:   3663 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.88, wpb=113.7, bsz=40, num_updates=17840, lr=4.49326e-05, gnorm=0.157, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=50490
2023-02-20 11:13:07 - progress_bar.py[line:274] - INFO: epoch 002:   3673 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.3, ups=0.9, wpb=114.5, bsz=40, num_updates=17850, lr=4.49289e-05, gnorm=0.189, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50501
2023-02-20 11:13:18 - progress_bar.py[line:274] - INFO: epoch 002:   3683 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=113.3, bsz=40, num_updates=17860, lr=4.49253e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=50513
2023-02-20 11:13:29 - progress_bar.py[line:274] - INFO: epoch 002:   3693 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.89, wpb=114, bsz=40, num_updates=17870, lr=4.49217e-05, gnorm=0.192, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=50524
2023-02-20 11:13:40 - progress_bar.py[line:274] - INFO: epoch 002:   3703 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.88, wpb=114.5, bsz=40, num_updates=17880, lr=4.49181e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50535
2023-02-20 11:13:51 - progress_bar.py[line:274] - INFO: epoch 002:   3713 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.9, bsz=40, num_updates=17890, lr=4.49145e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50546
2023-02-20 11:14:02 - progress_bar.py[line:274] - INFO: epoch 002:   3723 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.91, wpb=114.2, bsz=40, num_updates=17900, lr=4.49109e-05, gnorm=0.24, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50557
2023-02-20 11:14:13 - progress_bar.py[line:274] - INFO: epoch 002:   3733 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.92, wpb=112.6, bsz=40, num_updates=17910, lr=4.49072e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=50568
2023-02-20 11:14:24 - progress_bar.py[line:274] - INFO: epoch 002:   3743 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.8, ups=0.92, wpb=113.2, bsz=40, num_updates=17920, lr=4.49036e-05, gnorm=0.19, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50579
2023-02-20 11:14:35 - progress_bar.py[line:274] - INFO: epoch 002:   3753 / 14203 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.9, wpb=113.4, bsz=40, num_updates=17930, lr=4.49e-05, gnorm=0.211, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50590
2023-02-20 11:14:46 - progress_bar.py[line:274] - INFO: epoch 002:   3763 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.4, ups=0.91, wpb=114.2, bsz=40, num_updates=17940, lr=4.48964e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50601
2023-02-20 11:14:58 - progress_bar.py[line:274] - INFO: epoch 002:   3773 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.6, bsz=40, num_updates=17950, lr=4.48928e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50612
2023-02-20 11:15:09 - progress_bar.py[line:274] - INFO: epoch 002:   3783 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=17960, lr=4.48891e-05, gnorm=0.183, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50624
2023-02-20 11:15:20 - progress_bar.py[line:274] - INFO: epoch 002:   3793 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.9, wpb=114.1, bsz=40, num_updates=17970, lr=4.48855e-05, gnorm=0.144, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50635
2023-02-20 11:15:31 - progress_bar.py[line:274] - INFO: epoch 002:   3803 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=17980, lr=4.48819e-05, gnorm=0.152, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50646
2023-02-20 11:15:42 - progress_bar.py[line:274] - INFO: epoch 002:   3813 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.89, wpb=113, bsz=40, num_updates=17990, lr=4.48783e-05, gnorm=0.127, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=50657
2023-02-20 11:15:53 - progress_bar.py[line:274] - INFO: epoch 002:   3823 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=113.9, bsz=40, num_updates=18000, lr=4.48747e-05, gnorm=0.184, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50668
2023-02-20 11:15:53 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 11:15:55 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 11:15:55 - train.py[line:551] - INFO: load:0.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 11:17:59 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 11:17:59 - train.py[line:551] - INFO: load:0.90 valid_run:124.77 task_valid:121.24 collect_output:2.36
2023-02-20 11:20:00 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 11:20:00 - train.py[line:551] - INFO: load:0.93 valid_run:245.44 task_valid:237.44 collect_output:5.72
2023-02-20 11:22:03 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 11:22:03 - train.py[line:551] - INFO: load:0.96 valid_run:368.02 task_valid:353.73 collect_output:10.97
2023-02-20 11:24:05 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 11:24:05 - train.py[line:551] - INFO: load:0.99 valid_run:489.89 task_valid:467.33 collect_output:18.19
2023-02-20 11:26:05 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 11:26:05 - train.py[line:551] - INFO: load:1.01 valid_run:610.42 task_valid:584.68 collect_output:20.33
2023-02-20 11:28:09 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 11:28:09 - train.py[line:551] - INFO: load:1.04 valid_run:733.94 task_valid:704.25 collect_output:23.18
2023-02-20 11:30:12 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 11:30:12 - train.py[line:551] - INFO: load:1.07 valid_run:857.28 task_valid:822.83 collect_output:26.88
2023-02-20 11:32:15 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 11:32:15 - train.py[line:551] - INFO: load:1.10 valid_run:980.04 task_valid:940.41 collect_output:30.98
2023-02-20 11:34:19 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 11:34:19 - train.py[line:551] - INFO: load:1.12 valid_run:1104.14 task_valid:1058.04 collect_output:36.37
2023-02-20 11:36:24 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 11:36:24 - train.py[line:551] - INFO: load:1.15 valid_run:1228.54 task_valid:1172.69 collect_output:45.01
2023-02-20 11:38:24 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 11:38:24 - train.py[line:551] - INFO: load:1.18 valid_run:1348.92 task_valid:1288.57 collect_output:48.42
2023-02-20 11:40:26 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 11:40:26 - train.py[line:551] - INFO: load:1.21 valid_run:1470.95 task_valid:1406.18 collect_output:51.80
2023-02-20 11:42:26 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 11:42:26 - train.py[line:551] - INFO: load:1.23 valid_run:1590.16 task_valid:1520.33 collect_output:55.78
2023-02-20 11:44:27 - train.py[line:549] - INFO: 2800 / 6234
2023-02-20 11:44:27 - train.py[line:551] - INFO: load:1.26 valid_run:1711.28 task_valid:1638.16 collect_output:58.01
2023-02-20 11:46:28 - train.py[line:549] - INFO: 3000 / 6234
2023-02-20 11:46:28 - train.py[line:551] - INFO: load:1.28 valid_run:1832.27 task_valid:1754.29 collect_output:61.85
2023-02-20 11:48:29 - train.py[line:549] - INFO: 3200 / 6234
2023-02-20 11:48:29 - train.py[line:551] - INFO: load:1.31 valid_run:1953.41 task_valid:1868.25 collect_output:67.97
2023-02-20 11:50:30 - train.py[line:549] - INFO: 3400 / 6234
2023-02-20 11:50:30 - train.py[line:551] - INFO: load:1.34 valid_run:2074.84 task_valid:1984.32 collect_output:72.30
2023-02-20 11:52:31 - train.py[line:549] - INFO: 3600 / 6234
2023-02-20 11:52:31 - train.py[line:551] - INFO: load:1.36 valid_run:2195.61 task_valid:2102.29 collect_output:74.07
2023-02-20 11:54:33 - train.py[line:549] - INFO: 3800 / 6234
2023-02-20 11:54:33 - train.py[line:551] - INFO: load:1.39 valid_run:2316.92 task_valid:2219.27 collect_output:77.38
2023-02-20 11:56:33 - train.py[line:549] - INFO: 4000 / 6234
2023-02-20 11:56:33 - train.py[line:551] - INFO: load:1.42 valid_run:2437.66 task_valid:2336.29 collect_output:80.02
2023-02-20 11:58:35 - train.py[line:549] - INFO: 4200 / 6234
2023-02-20 11:58:35 - train.py[line:551] - INFO: load:1.44 valid_run:2559.59 task_valid:2453.35 collect_output:83.87
2023-02-20 12:00:38 - train.py[line:549] - INFO: 4400 / 6234
2023-02-20 12:00:38 - train.py[line:551] - INFO: load:1.47 valid_run:2681.73 task_valid:2572.50 collect_output:85.78
2023-02-20 12:02:39 - train.py[line:549] - INFO: 4600 / 6234
2023-02-20 12:02:39 - train.py[line:551] - INFO: load:1.49 valid_run:2802.73 task_valid:2687.82 collect_output:90.39
2023-02-20 12:04:39 - train.py[line:549] - INFO: 4800 / 6234
2023-02-20 12:04:39 - train.py[line:551] - INFO: load:1.52 valid_run:2923.10 task_valid:2804.93 collect_output:92.57
2023-02-20 12:06:41 - train.py[line:549] - INFO: 5000 / 6234
2023-02-20 12:06:41 - train.py[line:551] - INFO: load:1.55 valid_run:3045.25 task_valid:2922.01 collect_output:96.57
2023-02-20 12:08:45 - train.py[line:549] - INFO: 5200 / 6234
2023-02-20 12:08:45 - train.py[line:551] - INFO: load:1.57 valid_run:3168.41 task_valid:3038.98 collect_output:101.71
2023-02-20 12:10:45 - train.py[line:549] - INFO: 5400 / 6234
2023-02-20 12:10:45 - train.py[line:551] - INFO: load:1.60 valid_run:3288.71 task_valid:3154.62 collect_output:105.27
2023-02-20 12:12:48 - train.py[line:549] - INFO: 5600 / 6234
2023-02-20 12:12:48 - train.py[line:551] - INFO: load:1.62 valid_run:3411.59 task_valid:3275.19 collect_output:106.48
2023-02-20 12:14:51 - train.py[line:549] - INFO: 5800 / 6234
2023-02-20 12:14:51 - train.py[line:551] - INFO: load:1.65 valid_run:3534.17 task_valid:3392.12 collect_output:111.04
2023-02-20 12:16:53 - train.py[line:549] - INFO: 6000 / 6234
2023-02-20 12:16:53 - train.py[line:551] - INFO: load:1.67 valid_run:3656.32 task_valid:3510.73 collect_output:113.56
2023-02-20 12:18:55 - train.py[line:549] - INFO: 6200 / 6234
2023-02-20 12:18:55 - train.py[line:551] - INFO: load:1.70 valid_run:3778.04 task_valid:3629.62 collect_output:115.35

====================================================================================================
SGG eval:     R @ 50: 0.6214;     R @ 100: 0.6562;     R @ 500: 0.6755;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4029;    mR @ 100: 0.4813;    mR @ 500: 0.5180;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.8125) (covering:0.3000) (eating:0.8235) (flying in:0.9091) (growing on:0.2500) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:1.0000) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.6831) (standing on:0.3843) (using:0.5500) (walking in:0.0000) (walking on:0.7027) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6214;     R @ 100: 0.6562;     R @ 500: 0.6755;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4029;    mR @ 100: 0.4813;    mR @ 500: 0.5180;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.8125) (covering:0.3000) (eating:0.8235) (flying in:0.9091) (growing on:0.2500) (hanging from:0.4032) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:1.0000) (playing:0.0000) (riding:0.9500) (says:0.0000) (sitting on:0.6831) (standing on:0.3843) (using:0.5500) (walking in:0.0000) (walking on:0.7027) (watching:0.5139) 
--------------------------------------------------------
====================================================================================================

2023-02-20 12:19:25 - train.py[line:487] - INFO: 0.6562321619556913
2023-02-20 12:19:25 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-20 12:19:26 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.255 | loss_v1 0 | loss_v2 0 | nll_loss 0.088 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.656232 | ppl 1.06 | vqa_score 0.5056 | wps 117.7 | wpb 72 | bsz 24 | num_updates 18000 | best_R@100 0.693862
2023-02-20 12:19:26 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 18000 updates
2023-02-20 12:19:26 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_2_18000.pt
2023-02-20 12:19:32 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_2_18000.pt
2023-02-20 12:19:34 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS_contrastive-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_2_18000.pt (epoch 2 @ 18000 updates, score 0.6562321619556913) (writing took 8.844407537952065 seconds)
2023-02-20 12:19:46 - progress_bar.py[line:274] - INFO: epoch 002:   3833 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=0.3, ups=0, wpb=113.2, bsz=40, num_updates=18010, lr=4.48711e-05, gnorm=0.227, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54501
2023-02-20 12:19:57 - progress_bar.py[line:274] - INFO: epoch 002:   3843 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=114.1, bsz=40, num_updates=18020, lr=4.48674e-05, gnorm=0.204, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54512
2023-02-20 12:20:08 - progress_bar.py[line:274] - INFO: epoch 002:   3853 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=114.1, bsz=40, num_updates=18030, lr=4.48638e-05, gnorm=0.155, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54523
2023-02-20 12:20:20 - progress_bar.py[line:274] - INFO: epoch 002:   3863 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.88, wpb=114.8, bsz=40, num_updates=18040, lr=4.48602e-05, gnorm=0.136, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54534
2023-02-20 12:20:31 - progress_bar.py[line:274] - INFO: epoch 002:   3873 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.1, ups=0.87, wpb=113.7, bsz=40, num_updates=18050, lr=4.48566e-05, gnorm=0.156, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54546
2023-02-20 12:20:42 - progress_bar.py[line:274] - INFO: epoch 002:   3883 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.6, bsz=40, num_updates=18060, lr=4.4853e-05, gnorm=0.207, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54557
2023-02-20 12:20:53 - progress_bar.py[line:274] - INFO: epoch 002:   3893 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.3, ups=0.92, wpb=113.9, bsz=40, num_updates=18070, lr=4.48493e-05, gnorm=0.15, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54568
2023-02-20 12:21:04 - progress_bar.py[line:274] - INFO: epoch 002:   3903 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=18080, lr=4.48457e-05, gnorm=0.183, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54579
2023-02-20 12:21:16 - progress_bar.py[line:274] - INFO: epoch 002:   3913 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.88, wpb=112.7, bsz=40, num_updates=18090, lr=4.48421e-05, gnorm=0.215, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54591
2023-02-20 12:21:27 - progress_bar.py[line:274] - INFO: epoch 002:   3923 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.6, bsz=40, num_updates=18100, lr=4.48385e-05, gnorm=0.229, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=54602
2023-02-20 12:21:39 - progress_bar.py[line:274] - INFO: epoch 002:   3933 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=97.9, ups=0.87, wpb=112.9, bsz=40, num_updates=18110, lr=4.48349e-05, gnorm=0.193, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54613
2023-02-20 12:21:49 - progress_bar.py[line:274] - INFO: epoch 002:   3943 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.7, ups=0.94, wpb=112.5, bsz=40, num_updates=18120, lr=4.48313e-05, gnorm=0.182, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54624
2023-02-20 12:22:00 - progress_bar.py[line:274] - INFO: epoch 002:   3953 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.6, ups=0.93, wpb=112.4, bsz=40, num_updates=18130, lr=4.48276e-05, gnorm=0.24, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54635
2023-02-20 12:22:11 - progress_bar.py[line:274] - INFO: epoch 002:   3963 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=113.3, bsz=40, num_updates=18140, lr=4.4824e-05, gnorm=0.253, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54646
2023-02-20 12:22:22 - progress_bar.py[line:274] - INFO: epoch 002:   3973 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.91, wpb=114.9, bsz=40, num_updates=18150, lr=4.48204e-05, gnorm=0.232, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54657
2023-02-20 12:22:33 - progress_bar.py[line:274] - INFO: epoch 002:   3983 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.9, wpb=113.5, bsz=40, num_updates=18160, lr=4.48168e-05, gnorm=0.203, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54668
2023-02-20 12:22:44 - progress_bar.py[line:274] - INFO: epoch 002:   3993 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=114.1, bsz=40, num_updates=18170, lr=4.48132e-05, gnorm=0.23, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54679
2023-02-20 12:22:56 - progress_bar.py[line:274] - INFO: epoch 002:   4003 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.2, ups=0.89, wpb=113.5, bsz=40, num_updates=18180, lr=4.48095e-05, gnorm=0.275, clip=10, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54691
2023-02-20 12:23:07 - progress_bar.py[line:274] - INFO: epoch 002:   4013 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.1, ups=0.88, wpb=114.8, bsz=40, num_updates=18190, lr=4.48059e-05, gnorm=0.18, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54702
2023-02-20 12:23:18 - progress_bar.py[line:274] - INFO: epoch 002:   4023 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.8, ups=0.93, wpb=114.8, bsz=40, num_updates=18200, lr=4.48023e-05, gnorm=0.164, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54713
2023-02-20 12:23:29 - progress_bar.py[line:274] - INFO: epoch 002:   4033 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.88, wpb=113.5, bsz=40, num_updates=18210, lr=4.47987e-05, gnorm=0.215, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54724
2023-02-20 12:23:40 - progress_bar.py[line:274] - INFO: epoch 002:   4043 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=113.4, bsz=40, num_updates=18220, lr=4.47951e-05, gnorm=0.213, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54735
2023-02-20 12:23:51 - progress_bar.py[line:274] - INFO: epoch 002:   4053 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.89, wpb=115.2, bsz=40, num_updates=18230, lr=4.47915e-05, gnorm=0.201, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54746
2023-02-20 12:24:03 - progress_bar.py[line:274] - INFO: epoch 002:   4063 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=18240, lr=4.47878e-05, gnorm=0.193, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54758
2023-02-20 12:24:14 - progress_bar.py[line:274] - INFO: epoch 002:   4073 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.9, wpb=114.5, bsz=40, num_updates=18250, lr=4.47842e-05, gnorm=0.173, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54769
2023-02-20 12:24:25 - progress_bar.py[line:274] - INFO: epoch 002:   4083 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.5, ups=0.93, wpb=114.7, bsz=40, num_updates=18260, lr=4.47806e-05, gnorm=0.154, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54780
2023-02-20 12:24:36 - progress_bar.py[line:274] - INFO: epoch 002:   4093 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.9, wpb=114.5, bsz=40, num_updates=18270, lr=4.4777e-05, gnorm=0.161, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54791
2023-02-20 12:24:47 - progress_bar.py[line:274] - INFO: epoch 002:   4103 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=18280, lr=4.47734e-05, gnorm=0.181, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54802
2023-02-20 12:24:58 - progress_bar.py[line:274] - INFO: epoch 002:   4113 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.6, ups=0.92, wpb=114.3, bsz=40, num_updates=18290, lr=4.47697e-05, gnorm=0.139, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54813
2023-02-20 12:25:09 - progress_bar.py[line:274] - INFO: epoch 002:   4123 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103, ups=0.91, wpb=112.6, bsz=40, num_updates=18300, lr=4.47661e-05, gnorm=0.182, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54824
2023-02-20 12:25:19 - progress_bar.py[line:274] - INFO: epoch 002:   4133 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.4, ups=0.95, wpb=112.6, bsz=40, num_updates=18310, lr=4.47625e-05, gnorm=0.153, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54834
2023-02-20 12:25:31 - progress_bar.py[line:274] - INFO: epoch 002:   4143 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.89, wpb=114.2, bsz=40, num_updates=18320, lr=4.47589e-05, gnorm=0.172, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54846
2023-02-20 12:25:42 - progress_bar.py[line:274] - INFO: epoch 002:   4153 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.87, wpb=114, bsz=40, num_updates=18330, lr=4.47553e-05, gnorm=0.193, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54857
2023-02-20 12:25:54 - progress_bar.py[line:274] - INFO: epoch 002:   4163 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.87, wpb=113.8, bsz=40, num_updates=18340, lr=4.47517e-05, gnorm=0.216, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=54869
2023-02-20 12:26:05 - progress_bar.py[line:274] - INFO: epoch 002:   4173 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.2, ups=0.88, wpb=112.6, bsz=40, num_updates=18350, lr=4.4748e-05, gnorm=0.142, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54880
2023-02-20 12:26:16 - progress_bar.py[line:274] - INFO: epoch 002:   4183 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=112.4, bsz=40, num_updates=18360, lr=4.47444e-05, gnorm=0.164, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54891
2023-02-20 12:26:28 - progress_bar.py[line:274] - INFO: epoch 002:   4193 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.5, bsz=40, num_updates=18370, lr=4.47408e-05, gnorm=0.169, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54903
2023-02-20 12:26:39 - progress_bar.py[line:274] - INFO: epoch 002:   4203 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=112.3, bsz=40, num_updates=18380, lr=4.47372e-05, gnorm=0.226, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54914
2023-02-20 12:26:50 - progress_bar.py[line:274] - INFO: epoch 002:   4213 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.88, wpb=113.9, bsz=40, num_updates=18390, lr=4.47336e-05, gnorm=0.226, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54925
2023-02-20 12:27:01 - progress_bar.py[line:274] - INFO: epoch 002:   4223 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.5, ups=0.9, wpb=114.4, bsz=40, num_updates=18400, lr=4.47299e-05, gnorm=0.182, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54936
2023-02-20 12:27:12 - progress_bar.py[line:274] - INFO: epoch 002:   4233 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.1, ups=0.93, wpb=113, bsz=40, num_updates=18410, lr=4.47263e-05, gnorm=0.258, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54947
2023-02-20 12:27:24 - progress_bar.py[line:274] - INFO: epoch 002:   4243 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100, ups=0.88, wpb=113.7, bsz=40, num_updates=18420, lr=4.47227e-05, gnorm=0.122, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54958
2023-02-20 12:27:35 - progress_bar.py[line:274] - INFO: epoch 002:   4253 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=113.8, bsz=40, num_updates=18430, lr=4.47191e-05, gnorm=0.167, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=54970
2023-02-20 12:27:46 - progress_bar.py[line:274] - INFO: epoch 002:   4263 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.89, wpb=114.5, bsz=40, num_updates=18440, lr=4.47155e-05, gnorm=0.152, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=54981
2023-02-20 12:27:57 - progress_bar.py[line:274] - INFO: epoch 002:   4273 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.4, ups=0.88, wpb=113.9, bsz=40, num_updates=18450, lr=4.47119e-05, gnorm=0.133, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54992
2023-02-20 12:28:09 - progress_bar.py[line:274] - INFO: epoch 002:   4283 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.7, bsz=40, num_updates=18460, lr=4.47082e-05, gnorm=0.198, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55003
2023-02-20 12:28:20 - progress_bar.py[line:274] - INFO: epoch 002:   4293 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.3, ups=0.88, wpb=112.9, bsz=40, num_updates=18470, lr=4.47046e-05, gnorm=0.134, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55015
2023-02-20 12:28:31 - progress_bar.py[line:274] - INFO: epoch 002:   4303 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.8, bsz=40, num_updates=18480, lr=4.4701e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55026
2023-02-20 12:28:42 - progress_bar.py[line:274] - INFO: epoch 002:   4313 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.7, nsentences=40, sample_size=114.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.9, wpb=114.7, bsz=40, num_updates=18490, lr=4.46974e-05, gnorm=0.251, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55037
2023-02-20 12:28:54 - progress_bar.py[line:274] - INFO: epoch 002:   4323 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113, bsz=40, num_updates=18500, lr=4.46938e-05, gnorm=0.195, clip=0, loss_scale=4096, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55049
2023-02-20 12:29:05 - progress_bar.py[line:274] - INFO: epoch 002:   4333 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.3, bsz=40, num_updates=18510, lr=4.46902e-05, gnorm=0.174, clip=0, loss_scale=4096, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55060
2023-02-20 12:29:06 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0
2023-02-20 12:29:17 - progress_bar.py[line:274] - INFO: epoch 002:   4344 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=94.7, ups=0.84, wpb=113.3, bsz=40, num_updates=18520, lr=4.46865e-05, gnorm=0.134, clip=0, loss_scale=2048, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=55072
2023-02-20 12:29:28 - progress_bar.py[line:274] - INFO: epoch 002:   4354 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102, ups=0.9, wpb=113, bsz=40, num_updates=18530, lr=4.46829e-05, gnorm=0.153, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55083
2023-02-20 12:29:39 - progress_bar.py[line:274] - INFO: epoch 002:   4364 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.9, wpb=113.6, bsz=40, num_updates=18540, lr=4.46793e-05, gnorm=0.195, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55094
2023-02-20 12:29:50 - progress_bar.py[line:274] - INFO: epoch 002:   4374 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.87, wpb=114, bsz=40, num_updates=18550, lr=4.46757e-05, gnorm=0.174, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55105
2023-02-20 12:30:02 - progress_bar.py[line:274] - INFO: epoch 002:   4384 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.88, wpb=112.3, bsz=40, num_updates=18560, lr=4.46721e-05, gnorm=0.159, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=55117
2023-02-20 12:30:13 - progress_bar.py[line:274] - INFO: epoch 002:   4394 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=115, nsentences=40, sample_size=115, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.89, wpb=115, bsz=40, num_updates=18570, lr=4.46684e-05, gnorm=0.22, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55128
2023-02-20 12:30:24 - progress_bar.py[line:274] - INFO: epoch 002:   4404 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.91, wpb=114.1, bsz=40, num_updates=18580, lr=4.46648e-05, gnorm=0.21, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55139
2023-02-20 12:30:35 - progress_bar.py[line:274] - INFO: epoch 002:   4414 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.8, ups=0.88, wpb=113.4, bsz=40, num_updates=18590, lr=4.46612e-05, gnorm=0.2, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55150
2023-02-20 12:30:46 - progress_bar.py[line:274] - INFO: epoch 002:   4424 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.1, bsz=40, num_updates=18600, lr=4.46576e-05, gnorm=0.164, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55161
2023-02-20 12:30:58 - progress_bar.py[line:274] - INFO: epoch 002:   4434 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.87, wpb=114.2, bsz=40, num_updates=18610, lr=4.4654e-05, gnorm=0.176, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55173
2023-02-20 12:31:09 - progress_bar.py[line:274] - INFO: epoch 002:   4444 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.89, wpb=114.2, bsz=40, num_updates=18620, lr=4.46504e-05, gnorm=0.14, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55184
2023-02-20 12:31:20 - progress_bar.py[line:274] - INFO: epoch 002:   4454 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=113.6, bsz=40, num_updates=18630, lr=4.46467e-05, gnorm=0.174, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55195
2023-02-20 12:31:31 - progress_bar.py[line:274] - INFO: epoch 002:   4464 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=18640, lr=4.46431e-05, gnorm=0.17, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55206
2023-02-20 12:31:43 - progress_bar.py[line:274] - INFO: epoch 002:   4474 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=113, bsz=40, num_updates=18650, lr=4.46395e-05, gnorm=0.285, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55217
2023-02-20 12:31:54 - progress_bar.py[line:274] - INFO: epoch 002:   4484 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.4, bsz=40, num_updates=18660, lr=4.46359e-05, gnorm=0.19, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=55229
2023-02-20 12:32:05 - progress_bar.py[line:274] - INFO: epoch 002:   4494 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=113.9, bsz=40, num_updates=18670, lr=4.46323e-05, gnorm=0.253, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55240
2023-02-20 12:32:16 - progress_bar.py[line:274] - INFO: epoch 002:   4504 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.6, bsz=40, num_updates=18680, lr=4.46286e-05, gnorm=0.222, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55251
2023-02-20 12:32:27 - progress_bar.py[line:274] - INFO: epoch 002:   4514 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.4, ups=0.9, wpb=113.9, bsz=40, num_updates=18690, lr=4.4625e-05, gnorm=0.126, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55262
2023-02-20 12:32:38 - progress_bar.py[line:274] - INFO: epoch 002:   4524 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.7, ups=0.93, wpb=113, bsz=40, num_updates=18700, lr=4.46214e-05, gnorm=0.171, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=55273
2023-02-20 12:32:49 - progress_bar.py[line:274] - INFO: epoch 002:   4534 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.5, ups=0.91, wpb=114.6, bsz=40, num_updates=18710, lr=4.46178e-05, gnorm=0.162, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55284
2023-02-20 12:33:00 - progress_bar.py[line:274] - INFO: epoch 002:   4544 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.88, wpb=114.2, bsz=40, num_updates=18720, lr=4.46142e-05, gnorm=0.157, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55295
2023-02-20 12:33:12 - progress_bar.py[line:274] - INFO: epoch 002:   4554 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=18730, lr=4.46106e-05, gnorm=0.184, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55306
2023-02-20 12:33:22 - progress_bar.py[line:274] - INFO: epoch 002:   4564 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.92, wpb=113.1, bsz=40, num_updates=18740, lr=4.46069e-05, gnorm=0.17, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55317
2023-02-20 12:33:34 - progress_bar.py[line:274] - INFO: epoch 002:   4574 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.6, bsz=40, num_updates=18750, lr=4.46033e-05, gnorm=0.217, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55329
2023-02-20 12:33:45 - progress_bar.py[line:274] - INFO: epoch 002:   4584 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.9, ups=0.91, wpb=114.6, bsz=40, num_updates=18760, lr=4.45997e-05, gnorm=0.192, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=55339
2023-02-20 12:33:56 - progress_bar.py[line:274] - INFO: epoch 002:   4594 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.88, wpb=113.8, bsz=40, num_updates=18770, lr=4.45961e-05, gnorm=0.182, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55351
2023-02-20 12:34:07 - progress_bar.py[line:274] - INFO: epoch 002:   4604 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=113.2, bsz=40, num_updates=18780, lr=4.45925e-05, gnorm=0.249, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55362
2023-02-20 12:34:19 - progress_bar.py[line:274] - INFO: epoch 002:   4614 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.89, wpb=114, bsz=40, num_updates=18790, lr=4.45888e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=55373
2023-02-20 12:34:30 - progress_bar.py[line:274] - INFO: epoch 002:   4624 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.86, wpb=113.3, bsz=40, num_updates=18800, lr=4.45852e-05, gnorm=0.177, clip=0, loss_scale=2048, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=55385
2023-02-20 12:34:42 - progress_bar.py[line:274] - INFO: epoch 002:   4634 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.6, bsz=40, num_updates=18810, lr=4.45816e-05, gnorm=0.174, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55396
2023-02-20 12:34:53 - progress_bar.py[line:274] - INFO: epoch 002:   4644 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.6, ups=0.88, wpb=114.6, bsz=40, num_updates=18820, lr=4.4578e-05, gnorm=0.172, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55408
2023-02-20 12:35:04 - progress_bar.py[line:274] - INFO: epoch 002:   4654 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.039, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=18830, lr=4.45744e-05, gnorm=0.115, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55419
2023-02-20 12:35:15 - progress_bar.py[line:274] - INFO: epoch 002:   4664 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103, ups=0.91, wpb=113.8, bsz=40, num_updates=18840, lr=4.45708e-05, gnorm=0.207, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55430
2023-02-20 12:35:26 - progress_bar.py[line:274] - INFO: epoch 002:   4674 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114.8, nsentences=40, sample_size=114.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.6, ups=0.89, wpb=114.8, bsz=40, num_updates=18850, lr=4.45671e-05, gnorm=0.198, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55441
2023-02-20 12:35:38 - progress_bar.py[line:274] - INFO: epoch 002:   4684 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=18860, lr=4.45635e-05, gnorm=0.229, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55453
2023-02-20 12:35:49 - progress_bar.py[line:274] - INFO: epoch 002:   4694 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.2, bsz=40, num_updates=18870, lr=4.45599e-05, gnorm=0.206, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55464
2023-02-20 12:36:00 - progress_bar.py[line:274] - INFO: epoch 002:   4704 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=105.5, ups=0.93, wpb=113.4, bsz=40, num_updates=18880, lr=4.45563e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55474
2023-02-20 12:36:04 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-20 12:36:12 - progress_bar.py[line:274] - INFO: epoch 002:   4715 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=91, ups=0.8, wpb=113.9, bsz=40, num_updates=18890, lr=4.45527e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=55487
2023-02-20 12:36:23 - progress_bar.py[line:274] - INFO: epoch 002:   4725 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.1, ups=0.92, wpb=113.4, bsz=40, num_updates=18900, lr=4.4549e-05, gnorm=0.169, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55498
2023-02-20 12:36:34 - progress_bar.py[line:274] - INFO: epoch 002:   4735 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=113.1, bsz=40, num_updates=18910, lr=4.45454e-05, gnorm=0.272, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=55509
2023-02-20 12:36:45 - progress_bar.py[line:274] - INFO: epoch 002:   4745 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=18920, lr=4.45418e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55520
2023-02-20 12:36:57 - progress_bar.py[line:274] - INFO: epoch 002:   4755 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.2, ups=0.88, wpb=113.3, bsz=40, num_updates=18930, lr=4.45382e-05, gnorm=0.188, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55532
2023-02-20 12:37:08 - progress_bar.py[line:274] - INFO: epoch 002:   4765 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=113.5, bsz=40, num_updates=18940, lr=4.45346e-05, gnorm=0.332, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55543
2023-02-20 12:37:19 - progress_bar.py[line:274] - INFO: epoch 002:   4775 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.7, ups=0.9, wpb=114, bsz=40, num_updates=18950, lr=4.4531e-05, gnorm=0.183, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55554
2023-02-20 12:37:30 - progress_bar.py[line:274] - INFO: epoch 002:   4785 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.1, bsz=40, num_updates=18960, lr=4.45273e-05, gnorm=0.149, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55565
2023-02-20 12:37:41 - progress_bar.py[line:274] - INFO: epoch 002:   4795 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.5, ups=0.89, wpb=114.9, bsz=40, num_updates=18970, lr=4.45237e-05, gnorm=0.198, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55576
2023-02-20 12:37:53 - progress_bar.py[line:274] - INFO: epoch 002:   4805 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.88, wpb=113.7, bsz=40, num_updates=18980, lr=4.45201e-05, gnorm=0.209, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55588
2023-02-20 12:38:04 - progress_bar.py[line:274] - INFO: epoch 002:   4815 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.7, ups=0.88, wpb=113.4, bsz=40, num_updates=18990, lr=4.45165e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55599
2023-02-20 12:38:15 - progress_bar.py[line:274] - INFO: epoch 002:   4825 / 14203 loss=0.168, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.9, wpb=112.3, bsz=40, num_updates=19000, lr=4.45129e-05, gnorm=0.159, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55610
2023-02-20 12:38:26 - progress_bar.py[line:274] - INFO: epoch 002:   4835 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.9, ups=0.89, wpb=112.4, bsz=40, num_updates=19010, lr=4.45092e-05, gnorm=0.271, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55621
2023-02-20 12:38:38 - progress_bar.py[line:274] - INFO: epoch 002:   4845 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.86, wpb=114, bsz=40, num_updates=19020, lr=4.45056e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=55633
2023-02-20 12:38:49 - progress_bar.py[line:274] - INFO: epoch 002:   4855 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.2, bsz=40, num_updates=19030, lr=4.4502e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55644
2023-02-20 12:39:00 - progress_bar.py[line:274] - INFO: epoch 002:   4865 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.89, wpb=113.6, bsz=40, num_updates=19040, lr=4.44984e-05, gnorm=0.203, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55655
2023-02-20 12:39:12 - progress_bar.py[line:274] - INFO: epoch 002:   4875 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.5, ups=0.89, wpb=114, bsz=40, num_updates=19050, lr=4.44948e-05, gnorm=0.149, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55666
2023-02-20 12:39:23 - progress_bar.py[line:274] - INFO: epoch 002:   4885 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.8, ups=0.9, wpb=113.6, bsz=40, num_updates=19060, lr=4.44912e-05, gnorm=0.185, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55677
2023-02-20 12:39:34 - progress_bar.py[line:274] - INFO: epoch 002:   4895 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.89, wpb=112.5, bsz=40, num_updates=19070, lr=4.44875e-05, gnorm=0.155, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55689
2023-02-20 12:39:45 - progress_bar.py[line:274] - INFO: epoch 002:   4905 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.2, bsz=40, num_updates=19080, lr=4.44839e-05, gnorm=0.179, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55700
2023-02-20 12:39:56 - progress_bar.py[line:274] - INFO: epoch 002:   4915 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.9, bsz=40, num_updates=19090, lr=4.44803e-05, gnorm=0.188, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55711
2023-02-20 12:40:07 - progress_bar.py[line:274] - INFO: epoch 002:   4925 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=112.6, bsz=40, num_updates=19100, lr=4.44767e-05, gnorm=0.18, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55722
2023-02-20 12:40:18 - progress_bar.py[line:274] - INFO: epoch 002:   4935 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=114.1, bsz=40, num_updates=19110, lr=4.44731e-05, gnorm=0.168, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55733
2023-02-20 12:40:29 - progress_bar.py[line:274] - INFO: epoch 002:   4945 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.9, wpb=112.7, bsz=40, num_updates=19120, lr=4.44694e-05, gnorm=0.225, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55744
2023-02-20 12:40:41 - progress_bar.py[line:274] - INFO: epoch 002:   4955 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.7, ups=0.87, wpb=114.2, bsz=40, num_updates=19130, lr=4.44658e-05, gnorm=0.152, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55756
2023-02-20 12:40:52 - progress_bar.py[line:274] - INFO: epoch 002:   4965 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=113.1, bsz=40, num_updates=19140, lr=4.44622e-05, gnorm=0.173, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55767
2023-02-20 12:41:03 - progress_bar.py[line:274] - INFO: epoch 002:   4975 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=113.6, bsz=40, num_updates=19150, lr=4.44586e-05, gnorm=0.22, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55778
2023-02-20 12:41:14 - progress_bar.py[line:274] - INFO: epoch 002:   4985 / 14203 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=113.4, bsz=40, num_updates=19160, lr=4.4455e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55789
2023-02-20 12:41:25 - progress_bar.py[line:274] - INFO: epoch 002:   4995 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107.9, ups=0.94, wpb=114.5, bsz=40, num_updates=19170, lr=4.44514e-05, gnorm=0.23, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55800
2023-02-20 12:41:36 - progress_bar.py[line:274] - INFO: epoch 002:   5005 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.89, wpb=113.1, bsz=40, num_updates=19180, lr=4.44477e-05, gnorm=0.206, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=55811
2023-02-20 12:41:47 - progress_bar.py[line:274] - INFO: epoch 002:   5015 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.4, ups=0.93, wpb=112.2, bsz=40, num_updates=19190, lr=4.44441e-05, gnorm=0.178, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55822
2023-02-20 12:41:58 - progress_bar.py[line:274] - INFO: epoch 002:   5025 / 14203 loss=0.171, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.3, ups=0.92, wpb=113.8, bsz=40, num_updates=19200, lr=4.44405e-05, gnorm=0.186, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55833
2023-02-20 12:42:09 - progress_bar.py[line:274] - INFO: epoch 002:   5035 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=19210, lr=4.44369e-05, gnorm=0.235, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55844
2023-02-20 12:42:20 - progress_bar.py[line:274] - INFO: epoch 002:   5045 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.8, ups=0.87, wpb=112.4, bsz=40, num_updates=19220, lr=4.44333e-05, gnorm=0.236, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55855
2023-02-20 12:42:31 - progress_bar.py[line:274] - INFO: epoch 002:   5055 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.4, ups=0.92, wpb=112.7, bsz=40, num_updates=19230, lr=4.44297e-05, gnorm=0.157, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55866
2023-02-20 12:42:42 - progress_bar.py[line:274] - INFO: epoch 002:   5065 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=113.9, bsz=40, num_updates=19240, lr=4.4426e-05, gnorm=0.157, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55877
2023-02-20 12:42:53 - progress_bar.py[line:274] - INFO: epoch 002:   5075 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103, ups=0.9, wpb=114, bsz=40, num_updates=19250, lr=4.44224e-05, gnorm=0.144, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55888
2023-02-20 12:43:04 - progress_bar.py[line:274] - INFO: epoch 002:   5085 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.6, ups=0.92, wpb=113.1, bsz=40, num_updates=19260, lr=4.44188e-05, gnorm=0.231, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55899
2023-02-20 12:43:16 - progress_bar.py[line:274] - INFO: epoch 002:   5095 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=113, bsz=40, num_updates=19270, lr=4.44152e-05, gnorm=0.25, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=55910
2023-02-20 12:43:27 - progress_bar.py[line:274] - INFO: epoch 002:   5105 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113, bsz=40, num_updates=19280, lr=4.44116e-05, gnorm=0.212, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55922
2023-02-20 12:43:38 - progress_bar.py[line:274] - INFO: epoch 002:   5115 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.89, wpb=113.9, bsz=40, num_updates=19290, lr=4.44079e-05, gnorm=0.185, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55933
2023-02-20 12:43:49 - progress_bar.py[line:274] - INFO: epoch 002:   5125 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.9, ups=0.88, wpb=113.8, bsz=40, num_updates=19300, lr=4.44043e-05, gnorm=0.168, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55944
2023-02-20 12:44:01 - progress_bar.py[line:274] - INFO: epoch 002:   5135 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.6, ups=0.89, wpb=114, bsz=40, num_updates=19310, lr=4.44007e-05, gnorm=0.186, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55955
2023-02-20 12:44:12 - progress_bar.py[line:274] - INFO: epoch 002:   5145 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.9, wpb=112.6, bsz=40, num_updates=19320, lr=4.43971e-05, gnorm=0.193, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55967
2023-02-20 12:44:23 - progress_bar.py[line:274] - INFO: epoch 002:   5155 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.9, wpb=113.2, bsz=40, num_updates=19330, lr=4.43935e-05, gnorm=0.178, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55978
2023-02-20 12:44:34 - progress_bar.py[line:274] - INFO: epoch 002:   5165 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.2, ups=0.87, wpb=114.1, bsz=40, num_updates=19340, lr=4.43899e-05, gnorm=0.141, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55989
2023-02-20 12:44:45 - progress_bar.py[line:274] - INFO: epoch 002:   5175 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.6, bsz=40, num_updates=19350, lr=4.43862e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56000
2023-02-20 12:44:56 - progress_bar.py[line:274] - INFO: epoch 002:   5185 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.5, bsz=40, num_updates=19360, lr=4.43826e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=56011
2023-02-20 12:45:08 - progress_bar.py[line:274] - INFO: epoch 002:   5195 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.87, wpb=113.3, bsz=40, num_updates=19370, lr=4.4379e-05, gnorm=0.187, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56023
2023-02-20 12:45:19 - progress_bar.py[line:274] - INFO: epoch 002:   5205 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.89, wpb=113.5, bsz=40, num_updates=19380, lr=4.43754e-05, gnorm=0.185, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56034
2023-02-20 12:45:30 - progress_bar.py[line:274] - INFO: epoch 002:   5215 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103, ups=0.9, wpb=113.8, bsz=40, num_updates=19390, lr=4.43718e-05, gnorm=0.146, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56045
2023-02-20 12:45:41 - progress_bar.py[line:274] - INFO: epoch 002:   5225 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=113.9, bsz=40, num_updates=19400, lr=4.43681e-05, gnorm=0.236, clip=0, loss_scale=2048, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=56056
2023-02-20 12:45:52 - progress_bar.py[line:274] - INFO: epoch 002:   5235 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.91, wpb=113.6, bsz=40, num_updates=19410, lr=4.43645e-05, gnorm=0.212, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56067
2023-02-20 12:46:03 - progress_bar.py[line:274] - INFO: epoch 002:   5245 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=107.5, ups=0.94, wpb=114, bsz=40, num_updates=19420, lr=4.43609e-05, gnorm=0.169, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=56078
2023-02-20 12:46:14 - progress_bar.py[line:274] - INFO: epoch 002:   5255 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=115.1, nsentences=40, sample_size=115.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.89, wpb=115.1, bsz=40, num_updates=19430, lr=4.43573e-05, gnorm=0.235, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56089
2023-02-20 12:46:26 - progress_bar.py[line:274] - INFO: epoch 002:   5265 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.87, wpb=113, bsz=40, num_updates=19440, lr=4.43537e-05, gnorm=0.179, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56101
2023-02-20 12:46:37 - progress_bar.py[line:274] - INFO: epoch 002:   5275 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.89, wpb=114, bsz=40, num_updates=19450, lr=4.43501e-05, gnorm=0.162, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56112
2023-02-20 12:46:48 - progress_bar.py[line:274] - INFO: epoch 002:   5285 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.5, ups=0.9, wpb=114.6, bsz=40, num_updates=19460, lr=4.43464e-05, gnorm=0.133, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=56123
2023-02-20 12:46:59 - progress_bar.py[line:274] - INFO: epoch 002:   5295 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.89, wpb=112.9, bsz=40, num_updates=19470, lr=4.43428e-05, gnorm=0.167, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56134
2023-02-20 12:47:10 - progress_bar.py[line:274] - INFO: epoch 002:   5305 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.5, ups=0.89, wpb=113.5, bsz=40, num_updates=19480, lr=4.43392e-05, gnorm=0.125, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56145
2023-02-20 12:47:22 - progress_bar.py[line:274] - INFO: epoch 002:   5315 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.87, wpb=113.7, bsz=40, num_updates=19490, lr=4.43356e-05, gnorm=0.229, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56157
2023-02-20 12:47:33 - progress_bar.py[line:274] - INFO: epoch 002:   5325 / 14203 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.92, wpb=112.9, bsz=40, num_updates=19500, lr=4.4332e-05, gnorm=0.274, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56168
2023-02-20 12:47:44 - progress_bar.py[line:274] - INFO: epoch 002:   5335 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.91, wpb=113.9, bsz=40, num_updates=19510, lr=4.43283e-05, gnorm=0.224, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=56179
2023-02-20 12:47:55 - progress_bar.py[line:274] - INFO: epoch 002:   5345 / 14203 loss=0.17, loss_v1=0, loss_v2=0, nll_loss=0.047, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.89, wpb=113.3, bsz=40, num_updates=19520, lr=4.43247e-05, gnorm=0.142, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56190
2023-02-20 12:48:06 - progress_bar.py[line:274] - INFO: epoch 002:   5355 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=19530, lr=4.43211e-05, gnorm=0.15, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56201
2023-02-20 12:48:17 - progress_bar.py[line:274] - INFO: epoch 002:   5365 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.9, wpb=112.7, bsz=40, num_updates=19540, lr=4.43175e-05, gnorm=0.188, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56212
2023-02-20 12:48:29 - progress_bar.py[line:274] - INFO: epoch 002:   5375 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.042, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=19550, lr=4.43139e-05, gnorm=0.12, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56223
2023-02-20 12:48:40 - progress_bar.py[line:274] - INFO: epoch 002:   5385 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=112.9, bsz=40, num_updates=19560, lr=4.43103e-05, gnorm=0.238, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56235
2023-02-20 12:48:51 - progress_bar.py[line:274] - INFO: epoch 002:   5395 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=113.2, bsz=40, num_updates=19570, lr=4.43066e-05, gnorm=0.211, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56246
2023-02-20 12:49:02 - progress_bar.py[line:274] - INFO: epoch 002:   5405 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.9, wpb=114.4, bsz=40, num_updates=19580, lr=4.4303e-05, gnorm=0.14, clip=0, loss_scale=2048, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=56257
2023-02-20 12:49:13 - progress_bar.py[line:274] - INFO: epoch 002:   5415 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.88, wpb=113.2, bsz=40, num_updates=19590, lr=4.42994e-05, gnorm=0.197, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56268
2023-02-20 12:49:24 - progress_bar.py[line:274] - INFO: epoch 002:   5425 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.8, ups=0.92, wpb=113.2, bsz=40, num_updates=19600, lr=4.42958e-05, gnorm=0.218, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=56279
2023-02-20 12:49:36 - progress_bar.py[line:274] - INFO: epoch 002:   5435 / 14203 loss=0.181, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.9, wpb=114.2, bsz=40, num_updates=19610, lr=4.42922e-05, gnorm=0.225, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56290
2023-02-20 12:49:47 - progress_bar.py[line:274] - INFO: epoch 002:   5445 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.5, bsz=40, num_updates=19620, lr=4.42885e-05, gnorm=0.176, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=56302
2023-02-20 12:49:58 - progress_bar.py[line:274] - INFO: epoch 002:   5455 / 14203 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=112.3, bsz=40, num_updates=19630, lr=4.42849e-05, gnorm=0.176, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56313
2023-02-20 12:50:09 - progress_bar.py[line:274] - INFO: epoch 002:   5465 / 14203 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.89, wpb=114.4, bsz=40, num_updates=19640, lr=4.42813e-05, gnorm=0.243, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56324
2023-02-20 12:50:20 - progress_bar.py[line:274] - INFO: epoch 002:   5475 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.89, wpb=113.6, bsz=40, num_updates=19650, lr=4.42777e-05, gnorm=0.134, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56335
2023-02-20 12:50:31 - progress_bar.py[line:274] - INFO: epoch 002:   5485 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.8, bsz=40, num_updates=19660, lr=4.42741e-05, gnorm=0.13, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56346
2023-02-20 12:50:42 - progress_bar.py[line:274] - INFO: epoch 002:   5495 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.4, ups=0.9, wpb=113.2, bsz=40, num_updates=19670, lr=4.42705e-05, gnorm=0.136, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56357
2023-02-20 12:50:54 - progress_bar.py[line:274] - INFO: epoch 002:   5505 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.89, wpb=114.3, bsz=40, num_updates=19680, lr=4.42668e-05, gnorm=0.199, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56369
2023-02-20 12:51:05 - progress_bar.py[line:274] - INFO: epoch 002:   5515 / 14203 loss=0.186, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.89, wpb=112.3, bsz=40, num_updates=19690, lr=4.42632e-05, gnorm=0.186, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56380
2023-02-20 12:51:16 - progress_bar.py[line:274] - INFO: epoch 002:   5525 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.8, ups=0.91, wpb=114.4, bsz=40, num_updates=19700, lr=4.42596e-05, gnorm=0.235, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56391
2023-02-20 12:51:27 - progress_bar.py[line:274] - INFO: epoch 002:   5535 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.2, ups=0.9, wpb=113.2, bsz=40, num_updates=19710, lr=4.4256e-05, gnorm=0.121, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56402
2023-02-20 12:51:38 - progress_bar.py[line:274] - INFO: epoch 002:   5545 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.9, wpb=114.2, bsz=40, num_updates=19720, lr=4.42524e-05, gnorm=0.217, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56413
2023-02-20 12:51:49 - progress_bar.py[line:274] - INFO: epoch 002:   5555 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.043, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.7, ups=0.89, wpb=113.9, bsz=40, num_updates=19730, lr=4.42487e-05, gnorm=0.12, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56424
2023-02-20 12:52:01 - progress_bar.py[line:274] - INFO: epoch 002:   5565 / 14203 loss=0.167, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=98, ups=0.87, wpb=112.6, bsz=40, num_updates=19740, lr=4.42451e-05, gnorm=0.124, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56436
2023-02-20 12:52:12 - progress_bar.py[line:274] - INFO: epoch 002:   5575 / 14203 loss=0.182, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.87, wpb=114.4, bsz=40, num_updates=19750, lr=4.42415e-05, gnorm=0.216, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=56447
2023-02-20 12:52:24 - progress_bar.py[line:274] - INFO: epoch 002:   5585 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.3, ups=0.88, wpb=113.9, bsz=40, num_updates=19760, lr=4.42379e-05, gnorm=0.179, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56459
2023-02-20 12:52:35 - progress_bar.py[line:274] - INFO: epoch 002:   5595 / 14203 loss=0.176, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.87, wpb=114.1, bsz=40, num_updates=19770, lr=4.42343e-05, gnorm=0.175, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56470
2023-02-20 12:52:46 - progress_bar.py[line:274] - INFO: epoch 002:   5605 / 14203 loss=0.184, loss_v1=0, loss_v2=0, nll_loss=0.05, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.7, bsz=40, num_updates=19780, lr=4.42307e-05, gnorm=0.174, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56481
2023-02-20 12:52:58 - progress_bar.py[line:274] - INFO: epoch 002:   5615 / 14203 loss=0.179, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.88, wpb=113.1, bsz=40, num_updates=19790, lr=4.4227e-05, gnorm=0.211, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=56493
2023-02-20 12:53:09 - progress_bar.py[line:274] - INFO: epoch 002:   5625 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.9, wpb=113.1, bsz=40, num_updates=19800, lr=4.42234e-05, gnorm=0.164, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56504
2023-02-20 12:53:20 - progress_bar.py[line:274] - INFO: epoch 002:   5635 / 14203 loss=0.175, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=104.1, ups=0.92, wpb=113.5, bsz=40, num_updates=19810, lr=4.42198e-05, gnorm=0.173, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=56515
2023-02-20 12:53:31 - progress_bar.py[line:274] - INFO: epoch 002:   5645 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.052, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112.3, bsz=40, num_updates=19820, lr=4.42162e-05, gnorm=0.165, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56526
2023-02-20 12:53:42 - progress_bar.py[line:274] - INFO: epoch 002:   5655 / 14203 loss=0.169, loss_v1=0, loss_v2=0, nll_loss=0.041, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101, ups=0.89, wpb=113.2, bsz=40, num_updates=19830, lr=4.42126e-05, gnorm=0.126, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56537
2023-02-20 12:53:53 - progress_bar.py[line:274] - INFO: epoch 002:   5665 / 14203 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.4, ups=0.94, wpb=113.2, bsz=40, num_updates=19840, lr=4.42089e-05, gnorm=0.162, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56548
2023-02-20 12:54:04 - progress_bar.py[line:274] - INFO: epoch 002:   5675 / 14203 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.3, bsz=40, num_updates=19850, lr=4.42053e-05, gnorm=0.159, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56559
2023-02-20 12:54:15 - progress_bar.py[line:274] - INFO: epoch 002:   5685 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=113.1, bsz=40, num_updates=19860, lr=4.42017e-05, gnorm=0.19, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=56570
2023-02-20 12:54:27 - progress_bar.py[line:274] - INFO: epoch 002:   5695 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.046, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=99.5, ups=0.88, wpb=113.1, bsz=40, num_updates=19870, lr=4.41981e-05, gnorm=0.16, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56581
2023-02-20 12:54:38 - progress_bar.py[line:274] - INFO: epoch 002:   5705 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.049, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.6, ups=0.88, wpb=114.1, bsz=40, num_updates=19880, lr=4.41945e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=56593
2023-02-20 12:54:49 - progress_bar.py[line:274] - INFO: epoch 002:   5715 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=100.9, ups=0.89, wpb=113.1, bsz=40, num_updates=19890, lr=4.41909e-05, gnorm=0.142, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56604
2023-02-20 12:55:00 - progress_bar.py[line:274] - INFO: epoch 002:   5725 / 14203 loss=0.174, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=111.8, bsz=40, num_updates=19900, lr=4.41872e-05, gnorm=0.202, clip=0, loss_scale=2048, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56615
2023-02-20 12:55:11 - progress_bar.py[line:274] - INFO: epoch 002:   5735 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=102.8, ups=0.91, wpb=113.1, bsz=40, num_updates=19910, lr=4.41836e-05, gnorm=0.139, clip=0, loss_scale=4096, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56626
2023-02-20 12:55:22 - progress_bar.py[line:274] - INFO: epoch 002:   5745 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=19920, lr=4.418e-05, gnorm=0.164, clip=0, loss_scale=4096, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56637
2023-02-20 12:55:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2048.0
2023-02-20 12:55:35 - progress_bar.py[line:274] - INFO: epoch 002:   5756 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=91.8, ups=0.81, wpb=113.4, bsz=40, num_updates=19930, lr=4.41764e-05, gnorm=0.166, clip=0, loss_scale=2048, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=56650
2023-02-20 12:55:46 - progress_bar.py[line:274] - INFO: epoch 002:   5766 / 14203 loss=0.178, loss_v1=0, loss_v2=0, nll_loss=0.048, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=101.3, ups=0.89, wpb=113.6, bsz=40, num_updates=19940, lr=4.41728e-05, gnorm=0.196, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56661
2023-02-20 12:55:57 - progress_bar.py[line:274] - INFO: epoch 002:   5776 / 14203 loss=0.183, loss_v1=0, loss_v2=0, nll_loss=0.053, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.9, bsz=40, num_updates=19950, lr=4.41692e-05, gnorm=0.169, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=56672
2023-02-20 12:56:08 - progress_bar.py[line:274] - INFO: epoch 002:   5786 / 14203 loss=0.173, loss_v1=0, loss_v2=0, nll_loss=0.045, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=103.2, ups=0.91, wpb=112.9, bsz=40, num_updates=19960, lr=4.41655e-05, gnorm=0.121, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56683
2023-02-20 12:56:19 - progress_bar.py[line:274] - INFO: epoch 002:   5796 / 14203 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=106.1, ups=0.93, wpb=114.1, bsz=40, num_updates=19970, lr=4.41619e-05, gnorm=0.187, clip=0, loss_scale=2048, train_wall=11, gb_free=11, ema_decay=0.9999, wall=56694
2023-02-20 12:56:29 - progress_bar.py[line:274] - INFO: epoch 002:   5806 / 14203 loss=0.172, loss_v1=0, loss_v2=0, nll_loss=0.044, ntokens=115.2, nsentences=40, sample_size=115.2, sample_size_v1=0, sample_size_v2=0, ppl=1.03, wps=108.4, ups=0.94, wpb=115.2, bsz=40, num_updates=19980, lr=4.41583e-05, gnorm=0.17, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56704
2023-02-20 12:56:40 - progress_bar.py[line:274] - INFO: epoch 002:   5816 / 14203 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.1, bsz=40, num_updates=19990, lr=4.41547e-05, gnorm=0.3, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56715
2023-02-20 12:56:52 - progress_bar.py[line:274] - INFO: epoch 002:   5826 / 14203 loss=0.177, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=20000, lr=4.41511e-05, gnorm=0.181, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56727
2023-02-20 12:56:52 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-20 12:56:53 - train.py[line:549] - INFO: 0 / 6234
2023-02-20 12:56:53 - train.py[line:551] - INFO: load:1.19 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-20 12:58:56 - train.py[line:549] - INFO: 200 / 6234
2023-02-20 12:58:56 - train.py[line:551] - INFO: load:1.22 valid_run:122.42 task_valid:119.29 collect_output:2.04
2023-02-20 13:00:56 - train.py[line:549] - INFO: 400 / 6234
2023-02-20 13:00:56 - train.py[line:551] - INFO: load:1.24 valid_run:242.43 task_valid:234.81 collect_output:5.52
2023-02-20 13:02:58 - train.py[line:549] - INFO: 600 / 6234
2023-02-20 13:02:58 - train.py[line:551] - INFO: load:1.27 valid_run:364.48 task_valid:351.13 collect_output:10.22
2023-02-20 13:05:00 - train.py[line:549] - INFO: 800 / 6234
2023-02-20 13:05:00 - train.py[line:551] - INFO: load:1.29 valid_run:486.50 task_valid:464.85 collect_output:17.49
2023-02-20 13:07:01 - train.py[line:549] - INFO: 1000 / 6234
2023-02-20 13:07:01 - train.py[line:551] - INFO: load:1.32 valid_run:607.15 task_valid:582.29 collect_output:19.67
2023-02-20 13:09:04 - train.py[line:549] - INFO: 1200 / 6234
2023-02-20 13:09:04 - train.py[line:551] - INFO: load:1.34 valid_run:730.03 task_valid:700.93 collect_output:22.89
2023-02-20 13:11:07 - train.py[line:549] - INFO: 1400 / 6234
2023-02-20 13:11:07 - train.py[line:551] - INFO: load:1.37 valid_run:853.19 task_valid:819.08 collect_output:26.89
2023-02-20 13:13:09 - train.py[line:549] - INFO: 1600 / 6234
2023-02-20 13:13:09 - train.py[line:551] - INFO: load:1.39 valid_run:975.21 task_valid:935.70 collect_output:31.26
2023-02-20 13:15:13 - train.py[line:549] - INFO: 1800 / 6234
2023-02-20 13:15:13 - train.py[line:551] - INFO: load:1.42 valid_run:1099.07 task_valid:1053.02 collect_output:36.76
2023-02-20 13:17:15 - train.py[line:549] - INFO: 2000 / 6234
2023-02-20 13:17:15 - train.py[line:551] - INFO: load:1.44 valid_run:1221.38 task_valid:1166.05 collect_output:44.98
2023-02-20 13:19:16 - train.py[line:549] - INFO: 2200 / 6234
2023-02-20 13:19:16 - train.py[line:551] - INFO: load:1.47 valid_run:1341.99 task_valid:1281.71 collect_output:48.89
2023-02-20 13:21:18 - train.py[line:549] - INFO: 2400 / 6234
2023-02-20 13:21:18 - train.py[line:551] - INFO: load:1.49 valid_run:1463.78 task_valid:1398.64 collect_output:52.71
2023-02-20 13:23:17 - train.py[line:549] - INFO: 2600 / 6234
2023-02-20 13:23:17 - train.py[line:551] - INFO: load:1.52 valid_run:1582.95 task_valid:1512.52 collect_output:56.95
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3256859
Killing subprocess 3256861
Main process received SIGINT, exiting
