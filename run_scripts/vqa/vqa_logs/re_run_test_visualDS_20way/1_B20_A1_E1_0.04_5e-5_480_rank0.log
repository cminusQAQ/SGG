2023-01-04 21:12:19 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-04 21:12:19 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-04 21:12:19 - utils.py[line:261] - INFO: Start init
2023-01-04 21:12:19 - utils.py[line:261] - INFO: Start init
2023-01-04 21:12:20 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-04 21:12:20 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-04 21:12:21 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.2023-01-04 21:12:21 - utils.py[line:274] - INFO: initialized host node4 as rank 0

single-machine distributed training is initialized.
2023-01-04 21:12:28 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/re_run_test_visualDS_20way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='none', distill_alpha=0.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/re_run_test_visualDS_20way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-01-04 21:12:28 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-01-04 21:12:28 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-04 21:12:32 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-01-04 21:12:33 - train.py[line:118] - INFO: task: VqaGenTask
2023-01-04 21:12:33 - train.py[line:119] - INFO: model: OFAModel
2023-01-04 21:12:33 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-01-04 21:12:33 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-01-04 21:12:33 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-04 21:12:33 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-01-04 21:12:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-01-04 21:12:34 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-01-04 21:12:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-01-04 21:12:36 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-01-04 21:12:37 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-01-04 21:12:38 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-04 21:12:38 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-04 21:12:38 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-04 21:12:38 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-04 21:12:38 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-01-04 21:12:38 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-01-04 21:12:38 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-01-04 21:12:47 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-04 21:12:47 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-04 21:12:47 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-01-04 21:12:47 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-01-04 21:12:48 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-01-04 21:12:48 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv slice_id 1 row count 2316893 total row count 4633786file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv slice_id 0 row count 2316893 total row count 4633786

2023-01-04 21:12:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 115845, warmup steps 4633, warmup_factor 0.0002158428663932657
Total steps 115845, warmup steps 4633, warmup_factor 0.0002158428663932657
2023-01-04 21:12:57 - trainer.py[line:758] - INFO: begin training epoch 1
2023-01-04 21:12:57 - train.py[line:312] - INFO: Start iterating over samples
2023-01-04 21:13:03 - trainer.py[line:1409] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 30.00 MiB (GPU 1; 39.59 GiB total capacity; 26.21 GiB already allocated; 11.19 MiB free; 26.38 GiB reserved in total by PyTorch)
2023-01-04 21:13:03 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-04 21:13:03 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   26843 MB |   26843 MB |   51375 MB |   24531 MB |
|       from large pool |   26725 MB |   26725 MB |   51195 MB |   24470 MB |
|       from small pool |     118 MB |     118 MB |     179 MB |      61 MB |
|---------------------------------------------------------------------------|
| Active memory         |   26843 MB |   26843 MB |   51375 MB |   24531 MB |
|       from large pool |   26725 MB |   26725 MB |   51195 MB |   24470 MB |
|       from small pool |     118 MB |     118 MB |     179 MB |      61 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   27012 MB |   27012 MB |   27012 MB |       0 B  |
|       from large pool |   26892 MB |   26892 MB |   26892 MB |       0 B  |
|       from small pool |     120 MB |     120 MB |     120 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  172136 KB |     859 MB |   17279 MB |   17111 MB |
|       from large pool |  170756 KB |     857 MB |   17135 MB |   16968 MB |
|       from small pool |    1380 KB |       2 MB |     143 MB |     142 MB |
|---------------------------------------------------------------------------|
| Allocations           |    3759    |    3759    |    5239    |    1480    |
|       from large pool |     910    |     910    |    1421    |     511    |
|       from small pool |    2849    |    2849    |    3818    |     969    |
|---------------------------------------------------------------------------|
| Active allocs         |    3759    |    3759    |    5239    |    1480    |
|       from large pool |     910    |     910    |    1421    |     511    |
|       from small pool |    2849    |    2849    |    3818    |     969    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     237    |     237    |     237    |       0    |
|       from large pool |     177    |     177    |     177    |       0    |
|       from small pool |      60    |      60    |      60    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |     114    |     644    |     552    |
|       from large pool |      90    |     110    |     540    |     450    |
|       from small pool |       2    |       5    |     104    |     102    |
|===========================================================================|

2023-01-04 21:13:03 - trainer.py[line:877] - WARNING: attempting to recover from OOM in forward/backward pass
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1644247
Killing subprocess 1644248
Main process received SIGINT, exiting
2023-01-04 21:16:52 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-04 21:16:52 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-04 21:16:52 - utils.py[line:261] - INFO: Start init
2023-01-04 21:16:52 - utils.py[line:261] - INFO: Start init
2023-01-04 21:16:53 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-04 21:16:53 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-04 21:16:53 - utils.py[line:274] - INFO: initialized host node4 as rank 1
2023-01-04 21:16:53 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
single-machine distributed training is initialized.
2023-01-04 21:17:00 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/re_run_test_visualDS_20way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='none', distill_alpha=0.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/re_run_test_visualDS_20way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'none', 'distill_alpha': 0.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-01-04 21:17:00 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-01-04 21:17:00 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-04 21:17:05 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-01-04 21:17:05 - train.py[line:118] - INFO: task: VqaGenTask
2023-01-04 21:17:05 - train.py[line:119] - INFO: model: OFAModel
2023-01-04 21:17:05 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-01-04 21:17:05 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-01-04 21:17:05 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-04 21:17:05 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-01-04 21:17:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-01-04 21:17:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-01-04 21:17:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-01-04 21:17:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-01-04 21:17:06 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-01-04 21:17:07 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-01-04 21:17:08 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-01-04 21:17:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-01-04 21:17:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-01-04 21:17:10 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-04 21:17:10 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-04 21:17:10 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-04 21:17:10 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-04 21:17:10 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-01-04 21:17:10 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-01-04 21:17:10 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-01-04 21:17:13 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-04 21:17:13 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-04 21:17:14 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-01-04 21:17:14 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-01-04 21:17:14 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-01-04 21:17:15 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv slice_id 1 row count 2316893 total row count 4633786
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_train_NA1_E0.tsv slice_id 0 row count 2316893 total row count 4633786
2023-01-04 21:17:18 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 115845, warmup steps 4633, warmup_factor 0.0002158428663932657
Total steps 115845, warmup steps 4633, warmup_factor 0.0002158428663932657
2023-01-04 21:17:19 - trainer.py[line:758] - INFO: begin training epoch 1
2023-01-04 21:17:19 - train.py[line:312] - INFO: Start iterating over samples
2023-01-04 21:17:37 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 115845 loss=1.282, loss_v1=0, loss_v2=0, nll_loss=1.143, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=89.7, ups=0.82, wpb=109.9, bsz=40, num_updates=10, lr=1.07921e-07, gnorm=12.939, clip=100, loss_scale=128, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=27
2023-01-04 21:17:49 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 115845 loss=1.287, loss_v1=0, loss_v2=0, nll_loss=1.148, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=2.22, wps=96, ups=0.88, wpb=109.6, bsz=40, num_updates=20, lr=2.15843e-07, gnorm=13.347, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38
2023-01-04 21:18:00 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.1, ups=0.9, wpb=109, bsz=40, num_updates=30, lr=3.23764e-07, gnorm=13.306, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50
2023-01-04 21:18:11 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 115845 loss=1.289, loss_v1=0, loss_v2=0, nll_loss=1.149, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=2.22, wps=98.1, ups=0.9, wpb=108.7, bsz=40, num_updates=40, lr=4.31686e-07, gnorm=12.936, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=61
2023-01-04 21:18:22 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.9, wpb=108.8, bsz=40, num_updates=50, lr=5.39607e-07, gnorm=12.164, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=72
2023-01-04 21:18:34 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 115845 loss=1.339, loss_v1=0, loss_v2=0, nll_loss=1.224, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=2.34, wps=97.8, ups=0.9, wpb=108.3, bsz=40, num_updates=60, lr=6.47529e-07, gnorm=13.189, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=83
2023-01-04 21:18:45 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 115845 loss=1.245, loss_v1=0, loss_v2=0, nll_loss=1.138, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=2.2, wps=96.1, ups=0.88, wpb=109.6, bsz=40, num_updates=70, lr=7.5545e-07, gnorm=10.388, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=95
2023-01-04 21:18:56 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 115845 loss=1.237, loss_v1=0, loss_v2=0, nll_loss=1.138, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=2.2, wps=95.8, ups=0.88, wpb=108.3, bsz=40, num_updates=80, lr=8.63371e-07, gnorm=10.255, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=106
2023-01-04 21:19:08 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 115845 loss=1.119, loss_v1=0, loss_v2=0, nll_loss=1.02, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=2.03, wps=97, ups=0.89, wpb=109.4, bsz=40, num_updates=90, lr=9.71293e-07, gnorm=8.754, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=117
2023-01-04 21:19:19 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 115845 loss=1.077, loss_v1=0, loss_v2=0, nll_loss=0.983, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.98, wps=101.3, ups=0.93, wpb=109, bsz=40, num_updates=100, lr=1.07921e-06, gnorm=7.529, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=128
2023-01-04 21:19:30 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 115845 loss=1.089, loss_v1=0, loss_v2=0, nll_loss=1.004, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=2.01, wps=93.7, ups=0.86, wpb=108.4, bsz=40, num_updates=110, lr=1.18714e-06, gnorm=7.227, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=140
2023-01-04 21:19:41 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 115845 loss=0.964, loss_v1=0, loss_v2=0, nll_loss=0.872, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.83, wps=100.3, ups=0.91, wpb=109.9, bsz=40, num_updates=120, lr=1.29506e-06, gnorm=6.206, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=151
2023-01-04 21:19:53 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 115845 loss=0.917, loss_v1=0, loss_v2=0, nll_loss=0.823, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=97.2, ups=0.89, wpb=109.7, bsz=40, num_updates=130, lr=1.40298e-06, gnorm=5.341, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=162
2023-01-04 21:20:04 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.9, wpb=108.1, bsz=40, num_updates=140, lr=1.5109e-06, gnorm=5.192, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=174
2023-01-04 21:20:15 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.3, ups=0.89, wpb=108.7, bsz=40, num_updates=150, lr=1.61882e-06, gnorm=4.773, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=185
2023-01-04 21:20:27 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 115845 loss=0.912, loss_v1=0, loss_v2=0, nll_loss=0.836, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=160, lr=1.72674e-06, gnorm=4.409, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=196
2023-01-04 21:20:38 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.3, ups=0.92, wpb=110.4, bsz=40, num_updates=170, lr=1.83466e-06, gnorm=3.903, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=207
2023-01-04 21:20:49 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 115845 loss=0.898, loss_v1=0, loss_v2=0, nll_loss=0.819, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.76, wps=95.4, ups=0.89, wpb=107.2, bsz=40, num_updates=180, lr=1.94259e-06, gnorm=3.938, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=219
2023-01-04 21:21:00 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 115845 loss=0.853, loss_v1=0, loss_v2=0, nll_loss=0.776, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=99.6, ups=0.91, wpb=109.6, bsz=40, num_updates=190, lr=2.05051e-06, gnorm=3.819, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=230
2023-01-04 21:21:11 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 115845 loss=0.881, loss_v1=0, loss_v2=0, nll_loss=0.807, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=96.6, ups=0.89, wpb=108.6, bsz=40, num_updates=200, lr=2.15843e-06, gnorm=3.551, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=241
2023-01-04 21:21:23 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 115845 loss=0.854, loss_v1=0, loss_v2=0, nll_loss=0.781, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=210, lr=2.26635e-06, gnorm=3.255, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=252
2023-01-04 21:21:34 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 115845 loss=0.842, loss_v1=0, loss_v2=0, nll_loss=0.768, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=96.6, ups=0.88, wpb=110.1, bsz=40, num_updates=220, lr=2.37427e-06, gnorm=3.416, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=264
2023-01-04 21:21:46 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 115845 loss=0.797, loss_v1=0, loss_v2=0, nll_loss=0.718, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=95.2, ups=0.87, wpb=109.2, bsz=40, num_updates=230, lr=2.48219e-06, gnorm=2.926, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=275
2023-01-04 21:21:57 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99, ups=0.91, wpb=108.8, bsz=40, num_updates=240, lr=2.59011e-06, gnorm=2.927, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=286
2023-01-04 21:22:08 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.8, nsentences=40, sample_size=106.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.91, wpb=106.8, bsz=40, num_updates=250, lr=2.69804e-06, gnorm=3.022, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=297
2023-01-04 21:22:19 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.9, wpb=108.1, bsz=40, num_updates=260, lr=2.80596e-06, gnorm=3.037, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=309
2023-01-04 21:22:30 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 115845 loss=0.743, loss_v1=0, loss_v2=0, nll_loss=0.653, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=104, ups=0.94, wpb=110.3, bsz=40, num_updates=270, lr=2.91388e-06, gnorm=2.74, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=320
2023-01-04 21:22:41 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 115845 loss=0.867, loss_v1=0, loss_v2=0, nll_loss=0.794, ntokens=107.1, nsentences=40, sample_size=107.1, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=99.6, ups=0.93, wpb=107.1, bsz=40, num_updates=280, lr=3.0218e-06, gnorm=3.174, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=331
2023-01-04 21:22:52 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 115845 loss=0.771, loss_v1=0, loss_v2=0, nll_loss=0.692, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=98.5, ups=0.9, wpb=109.5, bsz=40, num_updates=290, lr=3.12972e-06, gnorm=2.751, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=342
2023-01-04 21:23:03 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.9, wpb=109.2, bsz=40, num_updates=300, lr=3.23764e-06, gnorm=2.564, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=353
2023-01-04 21:23:15 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 115845 loss=0.775, loss_v1=0, loss_v2=0, nll_loss=0.7, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=96.9, ups=0.88, wpb=110.3, bsz=40, num_updates=310, lr=3.34556e-06, gnorm=2.401, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=364
2023-01-04 21:23:26 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.8, ups=0.89, wpb=107.3, bsz=40, num_updates=320, lr=3.45349e-06, gnorm=2.462, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=376
2023-01-04 21:23:37 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.9, wpb=108.5, bsz=40, num_updates=330, lr=3.56141e-06, gnorm=2.481, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=387
2023-01-04 21:23:49 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 115845 loss=0.764, loss_v1=0, loss_v2=0, nll_loss=0.682, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=340, lr=3.66933e-06, gnorm=2.375, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=398
2023-01-04 21:24:01 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 115845 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.641, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=95.1, ups=0.87, wpb=109.1, bsz=40, num_updates=350, lr=3.77725e-06, gnorm=2.39, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=410
2023-01-04 21:24:11 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 115845 loss=0.785, loss_v1=0, loss_v2=0, nll_loss=0.707, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=101.3, ups=0.93, wpb=108.9, bsz=40, num_updates=360, lr=3.88517e-06, gnorm=2.225, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=421
2023-01-04 21:24:23 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 115845 loss=0.743, loss_v1=0, loss_v2=0, nll_loss=0.656, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=97, ups=0.9, wpb=108.3, bsz=40, num_updates=370, lr=3.99309e-06, gnorm=2.29, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=432
2023-01-04 21:24:34 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 115845 loss=0.768, loss_v1=0, loss_v2=0, nll_loss=0.684, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=96.8, ups=0.89, wpb=108.5, bsz=40, num_updates=380, lr=4.10101e-06, gnorm=2.251, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=444
2023-01-04 21:24:45 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 115845 loss=0.777, loss_v1=0, loss_v2=0, nll_loss=0.697, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=97, ups=0.89, wpb=108.6, bsz=40, num_updates=390, lr=4.20894e-06, gnorm=2.374, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=455
2023-01-04 21:24:57 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 115845 loss=0.697, loss_v1=0, loss_v2=0, nll_loss=0.61, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=98.5, ups=0.89, wpb=110.6, bsz=40, num_updates=400, lr=4.31686e-06, gnorm=2.372, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=467
2023-01-04 21:25:08 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.89, wpb=108.1, bsz=40, num_updates=410, lr=4.42478e-06, gnorm=2.167, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=478
2023-01-04 21:25:20 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 115845 loss=0.77, loss_v1=0, loss_v2=0, nll_loss=0.684, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=97.8, ups=0.9, wpb=108.4, bsz=40, num_updates=420, lr=4.5327e-06, gnorm=2.258, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=489
2023-01-04 21:25:30 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 115845 loss=0.759, loss_v1=0, loss_v2=0, nll_loss=0.678, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=100.4, ups=0.92, wpb=109.4, bsz=40, num_updates=430, lr=4.64062e-06, gnorm=2.499, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=500
2023-01-04 21:25:42 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 115845 loss=0.756, loss_v1=0, loss_v2=0, nll_loss=0.67, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=95.9, ups=0.88, wpb=108.8, bsz=40, num_updates=440, lr=4.74854e-06, gnorm=2.382, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=512
2023-01-04 21:25:53 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 115845 loss=0.759, loss_v1=0, loss_v2=0, nll_loss=0.678, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=96.1, ups=0.88, wpb=109, bsz=40, num_updates=450, lr=4.85646e-06, gnorm=2.134, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=523
2023-01-04 21:26:05 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 115845 loss=0.784, loss_v1=0, loss_v2=0, nll_loss=0.705, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=97.8, ups=0.9, wpb=108.3, bsz=40, num_updates=460, lr=4.96439e-06, gnorm=2.126, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=534
2023-01-04 21:26:16 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 115845 loss=0.741, loss_v1=0, loss_v2=0, nll_loss=0.654, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=98.5, ups=0.9, wpb=109.5, bsz=40, num_updates=470, lr=5.07231e-06, gnorm=2.03, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=546
2023-01-04 21:26:27 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 115845 loss=0.72, loss_v1=0, loss_v2=0, nll_loss=0.634, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=101.6, ups=0.92, wpb=110.5, bsz=40, num_updates=480, lr=5.18023e-06, gnorm=2.054, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=557
2023-01-04 21:26:38 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 115845 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.671, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=96.1, ups=0.88, wpb=108.7, bsz=40, num_updates=490, lr=5.28815e-06, gnorm=2.128, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=568
2023-01-04 21:26:49 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=500, lr=5.39607e-06, gnorm=2.259, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=579
2023-01-04 21:27:00 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.9, ups=0.92, wpb=109.7, bsz=40, num_updates=510, lr=5.50399e-06, gnorm=2.178, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=590
2023-01-04 21:27:12 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 115845 loss=0.75, loss_v1=0, loss_v2=0, nll_loss=0.668, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=95.2, ups=0.88, wpb=107.9, bsz=40, num_updates=520, lr=5.61191e-06, gnorm=2.253, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=602
2023-01-04 21:27:23 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 115845 loss=0.734, loss_v1=0, loss_v2=0, nll_loss=0.643, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=98.5, ups=0.91, wpb=108.5, bsz=40, num_updates=530, lr=5.71984e-06, gnorm=2.196, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=613
2023-01-04 21:27:34 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 115845 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.558, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=102.1, ups=0.93, wpb=109.5, bsz=40, num_updates=540, lr=5.82776e-06, gnorm=2.001, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=624
2023-01-04 21:27:45 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 115845 loss=0.731, loss_v1=0, loss_v2=0, nll_loss=0.643, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=96.6, ups=0.87, wpb=110.6, bsz=40, num_updates=550, lr=5.93568e-06, gnorm=2.15, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=635
2023-01-04 21:27:57 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 115845 loss=0.716, loss_v1=0, loss_v2=0, nll_loss=0.626, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=96.7, ups=0.89, wpb=109.3, bsz=40, num_updates=560, lr=6.0436e-06, gnorm=2.301, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=646
2023-01-04 21:28:08 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 115845 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.64, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=100.5, ups=0.92, wpb=109.5, bsz=40, num_updates=570, lr=6.15152e-06, gnorm=2.016, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=657
2023-01-04 21:28:19 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 115845 loss=0.693, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=98.7, ups=0.91, wpb=108.7, bsz=40, num_updates=580, lr=6.25944e-06, gnorm=2.085, clip=100, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=669
2023-01-04 21:28:30 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 115845 loss=0.724, loss_v1=0, loss_v2=0, nll_loss=0.634, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=100.4, ups=0.92, wpb=109.4, bsz=40, num_updates=590, lr=6.36736e-06, gnorm=2.124, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=680
2023-01-04 21:28:42 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.6, ups=0.86, wpb=109.6, bsz=40, num_updates=600, lr=6.47529e-06, gnorm=1.995, clip=100, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=691
2023-01-04 21:28:53 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 115845 loss=0.716, loss_v1=0, loss_v2=0, nll_loss=0.622, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=98.5, ups=0.91, wpb=108.7, bsz=40, num_updates=610, lr=6.58321e-06, gnorm=2.108, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=702
2023-01-04 21:29:04 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 115845 loss=0.69, loss_v1=0, loss_v2=0, nll_loss=0.596, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=100.7, ups=0.92, wpb=109.6, bsz=40, num_updates=620, lr=6.69113e-06, gnorm=2.033, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=713
2023-01-04 21:29:15 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.89, wpb=109.9, bsz=40, num_updates=630, lr=6.79905e-06, gnorm=2.145, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=725
2023-01-04 21:29:26 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.9, wpb=111, bsz=40, num_updates=640, lr=6.90697e-06, gnorm=2.126, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=736
2023-01-04 21:29:38 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.6, ups=0.89, wpb=107.6, bsz=40, num_updates=650, lr=7.01489e-06, gnorm=2.194, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=747
2023-01-04 21:29:49 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 115845 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=96.3, ups=0.88, wpb=109, bsz=40, num_updates=660, lr=7.12281e-06, gnorm=2.146, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=759
2023-01-04 21:30:00 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 115845 loss=0.699, loss_v1=0, loss_v2=0, nll_loss=0.603, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=100.6, ups=0.92, wpb=109.9, bsz=40, num_updates=670, lr=7.23074e-06, gnorm=1.984, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=770
2023-01-04 21:30:11 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 115845 loss=0.683, loss_v1=0, loss_v2=0, nll_loss=0.586, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=99.6, ups=0.91, wpb=109.6, bsz=40, num_updates=680, lr=7.33866e-06, gnorm=2.019, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=781
2023-01-04 21:30:22 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.2, ups=0.91, wpb=109.4, bsz=40, num_updates=690, lr=7.44658e-06, gnorm=2.097, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=792
2023-01-04 21:30:33 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 115845 loss=0.698, loss_v1=0, loss_v2=0, nll_loss=0.6, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=98.1, ups=0.9, wpb=109.2, bsz=40, num_updates=700, lr=7.5545e-06, gnorm=2.189, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=803
2023-01-04 21:30:45 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101, ups=0.92, wpb=110.3, bsz=40, num_updates=710, lr=7.66242e-06, gnorm=1.919, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=814
2023-01-04 21:30:55 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.6, ups=0.93, wpb=109.2, bsz=40, num_updates=720, lr=7.77034e-06, gnorm=1.995, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=825
2023-01-04 21:31:07 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 115845 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=730, lr=7.87826e-06, gnorm=1.957, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=836
2023-01-04 21:31:18 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 115845 loss=0.69, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=740, lr=7.98619e-06, gnorm=2.103, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=848
2023-01-04 21:31:29 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 115845 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.542, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=97.3, ups=0.88, wpb=110.4, bsz=40, num_updates=750, lr=8.09411e-06, gnorm=2.092, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=859
2023-01-04 21:31:41 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.5, ups=0.88, wpb=109.3, bsz=40, num_updates=760, lr=8.20203e-06, gnorm=1.841, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=871
2023-01-04 21:31:52 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 115845 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.575, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=102.2, ups=0.92, wpb=111.5, bsz=40, num_updates=770, lr=8.30995e-06, gnorm=1.998, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=882
2023-01-04 21:32:03 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.9, wpb=108.6, bsz=40, num_updates=780, lr=8.41787e-06, gnorm=1.971, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=893
2023-01-04 21:32:15 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 115845 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=790, lr=8.52579e-06, gnorm=1.921, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=904
2023-01-04 21:32:26 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.7, ups=0.9, wpb=108, bsz=40, num_updates=800, lr=8.63371e-06, gnorm=2.1, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=916
2023-01-04 21:32:37 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 115845 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=99.5, ups=0.92, wpb=108.6, bsz=40, num_updates=810, lr=8.74164e-06, gnorm=2.275, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=927
2023-01-04 21:32:48 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 115845 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.3, ups=0.9, wpb=109.3, bsz=40, num_updates=820, lr=8.84956e-06, gnorm=1.885, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=938
2023-01-04 21:32:59 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 115845 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.59, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=99, ups=0.92, wpb=107.8, bsz=40, num_updates=830, lr=8.95748e-06, gnorm=2.308, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=949
2023-01-04 21:33:10 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 115845 loss=0.659, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=98.4, ups=0.91, wpb=108.5, bsz=40, num_updates=840, lr=9.0654e-06, gnorm=1.966, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=960
2023-01-04 21:33:21 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.1, ups=0.92, wpb=109.3, bsz=40, num_updates=850, lr=9.17332e-06, gnorm=1.94, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=971
2023-01-04 21:33:33 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 115845 loss=0.683, loss_v1=0, loss_v2=0, nll_loss=0.579, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=99, ups=0.9, wpb=109.9, bsz=40, num_updates=860, lr=9.28124e-06, gnorm=1.977, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=982
2023-01-04 21:33:44 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 115845 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.556, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=96.8, ups=0.89, wpb=108.2, bsz=40, num_updates=870, lr=9.38916e-06, gnorm=2.066, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=994
2023-01-04 21:33:55 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.87, wpb=110.8, bsz=40, num_updates=880, lr=9.49709e-06, gnorm=1.891, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1005
2023-01-04 21:34:06 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.7, ups=0.91, wpb=111.1, bsz=40, num_updates=890, lr=9.60501e-06, gnorm=1.768, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1016
2023-01-04 21:34:18 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 115845 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.541, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=97.7, ups=0.9, wpb=108.3, bsz=40, num_updates=900, lr=9.71293e-06, gnorm=1.809, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1027
2023-01-04 21:34:29 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 115845 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=98.1, ups=0.9, wpb=108.9, bsz=40, num_updates=910, lr=9.82085e-06, gnorm=1.882, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=1039
2023-01-04 21:34:41 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 115845 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.572, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=94.4, ups=0.87, wpb=107.9, bsz=40, num_updates=920, lr=9.92877e-06, gnorm=2.303, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1050
2023-01-04 21:34:52 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 115845 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.518, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=95.3, ups=0.88, wpb=107.7, bsz=40, num_updates=930, lr=1.00367e-05, gnorm=2.002, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1062
2023-01-04 21:35:03 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 115845 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.543, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=95.9, ups=0.88, wpb=108.5, bsz=40, num_updates=940, lr=1.01446e-05, gnorm=2.043, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1073
2023-01-04 21:35:14 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 115845 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.539, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=102.6, ups=0.94, wpb=108.8, bsz=40, num_updates=950, lr=1.02525e-05, gnorm=1.829, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1084
2023-01-04 21:35:25 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.1, ups=0.92, wpb=109.3, bsz=40, num_updates=960, lr=1.03605e-05, gnorm=1.867, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1095
2023-01-04 21:35:37 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.88, wpb=107.9, bsz=40, num_updates=970, lr=1.04684e-05, gnorm=1.812, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1106
2023-01-04 21:35:48 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=980, lr=1.05763e-05, gnorm=1.867, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1118
2023-01-04 21:35:59 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 115845 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.573, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=99.8, ups=0.91, wpb=109.5, bsz=40, num_updates=990, lr=1.06842e-05, gnorm=2.003, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1129
2023-01-04 21:36:10 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 115845 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.538, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=98.8, ups=0.92, wpb=107.8, bsz=40, num_updates=1000, lr=1.07921e-05, gnorm=1.836, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1140
2023-01-04 21:36:21 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 115845 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.534, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=97.8, ups=0.9, wpb=108.4, bsz=40, num_updates=1010, lr=1.09001e-05, gnorm=1.954, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1151
2023-01-04 21:36:32 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 115845 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.567, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=99.3, ups=0.9, wpb=110.2, bsz=40, num_updates=1020, lr=1.1008e-05, gnorm=1.817, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1162
2023-01-04 21:36:44 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 115845 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.552, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=97.8, ups=0.9, wpb=109.3, bsz=40, num_updates=1030, lr=1.11159e-05, gnorm=1.799, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1173
2023-01-04 21:36:55 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 115845 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=96.9, ups=0.89, wpb=108.9, bsz=40, num_updates=1040, lr=1.12238e-05, gnorm=1.881, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1185
2023-01-04 21:37:06 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.3, ups=0.88, wpb=109.4, bsz=40, num_updates=1050, lr=1.13318e-05, gnorm=1.829, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1196
2023-01-04 21:37:18 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.9, wpb=110.4, bsz=40, num_updates=1060, lr=1.14397e-05, gnorm=1.555, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1207
2023-01-04 21:37:29 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 115845 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.546, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=100, ups=0.92, wpb=108.7, bsz=40, num_updates=1070, lr=1.15476e-05, gnorm=1.902, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1218
2023-01-04 21:37:40 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.4, ups=0.91, wpb=110.1, bsz=40, num_updates=1080, lr=1.16555e-05, gnorm=1.805, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1229
2023-01-04 21:37:51 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 115845 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.516, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=99.9, ups=0.91, wpb=109.8, bsz=40, num_updates=1090, lr=1.17634e-05, gnorm=1.848, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1240
2023-01-04 21:38:02 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.91, wpb=110.1, bsz=40, num_updates=1100, lr=1.18714e-05, gnorm=1.993, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1252
2023-01-04 21:38:13 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 115845 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=99, ups=0.91, wpb=109, bsz=40, num_updates=1110, lr=1.19793e-05, gnorm=1.948, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1263
2023-01-04 21:38:24 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.91, wpb=108.2, bsz=40, num_updates=1120, lr=1.20872e-05, gnorm=1.863, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1274
2023-01-04 21:38:35 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 115845 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=97, ups=0.89, wpb=108.8, bsz=40, num_updates=1130, lr=1.21951e-05, gnorm=1.962, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1285
2023-01-04 21:38:47 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 115845 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=99.1, ups=0.91, wpb=109.4, bsz=40, num_updates=1140, lr=1.2303e-05, gnorm=1.784, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1296
2023-01-04 21:38:58 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 115845 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.498, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=98, ups=0.9, wpb=109.5, bsz=40, num_updates=1150, lr=1.2411e-05, gnorm=1.806, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1308
2023-01-04 21:39:09 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 115845 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.1, ups=0.91, wpb=108.3, bsz=40, num_updates=1160, lr=1.25189e-05, gnorm=1.842, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1319
2023-01-04 21:39:20 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 115845 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=1170, lr=1.26268e-05, gnorm=1.621, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1330
2023-01-04 21:39:32 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95, ups=0.87, wpb=108.8, bsz=40, num_updates=1180, lr=1.27347e-05, gnorm=1.848, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1342
2023-01-04 21:39:43 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 115845 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=94.5, ups=0.87, wpb=109.1, bsz=40, num_updates=1190, lr=1.28427e-05, gnorm=1.704, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1353
2023-01-04 21:39:55 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 115845 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.563, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=96.5, ups=0.89, wpb=107.9, bsz=40, num_updates=1200, lr=1.29506e-05, gnorm=1.774, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1364
2023-01-04 21:40:07 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 115845 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=91.1, ups=0.84, wpb=108.4, bsz=40, num_updates=1210, lr=1.30585e-05, gnorm=1.838, clip=100, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=1376
2023-01-04 21:40:18 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.91, wpb=109.2, bsz=40, num_updates=1220, lr=1.31664e-05, gnorm=1.775, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1388
2023-01-04 21:40:29 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 115845 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.511, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=95.6, ups=0.88, wpb=108.2, bsz=40, num_updates=1230, lr=1.32743e-05, gnorm=1.938, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1399
2023-01-04 21:40:40 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.92, wpb=108.5, bsz=40, num_updates=1240, lr=1.33823e-05, gnorm=1.847, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=1410
2023-01-04 21:40:52 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 115845 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.8, ups=0.89, wpb=109.3, bsz=40, num_updates=1250, lr=1.34902e-05, gnorm=1.728, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1421
2023-01-04 21:41:03 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 115845 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=98.8, ups=0.91, wpb=108.8, bsz=40, num_updates=1260, lr=1.35981e-05, gnorm=1.662, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1433
2023-01-04 21:41:14 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.92, wpb=108.3, bsz=40, num_updates=1270, lr=1.3706e-05, gnorm=1.725, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1444
2023-01-04 21:41:25 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 115845 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=98.5, ups=0.9, wpb=110, bsz=40, num_updates=1280, lr=1.38139e-05, gnorm=1.706, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1455
2023-01-04 21:41:37 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 115845 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=96.1, ups=0.88, wpb=108.7, bsz=40, num_updates=1290, lr=1.39219e-05, gnorm=1.895, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1466
2023-01-04 21:41:47 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=103.6, ups=0.95, wpb=109.3, bsz=40, num_updates=1300, lr=1.40298e-05, gnorm=1.829, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1477
2023-01-04 21:41:58 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 115845 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100.6, ups=0.92, wpb=109.4, bsz=40, num_updates=1310, lr=1.41377e-05, gnorm=1.699, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1488
2023-01-04 21:42:09 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=110.2, bsz=40, num_updates=1320, lr=1.42456e-05, gnorm=1.566, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1499
2023-01-04 21:42:20 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.1, ups=0.94, wpb=108.3, bsz=40, num_updates=1330, lr=1.43536e-05, gnorm=1.744, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1510
2023-01-04 21:42:32 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 115845 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.5, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=96.4, ups=0.89, wpb=108.1, bsz=40, num_updates=1340, lr=1.44615e-05, gnorm=1.691, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1521
2023-01-04 21:42:43 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 115845 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=94.6, ups=0.88, wpb=107.3, bsz=40, num_updates=1350, lr=1.45694e-05, gnorm=1.733, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1533
2023-01-04 21:42:54 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 115845 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.513, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=98.3, ups=0.91, wpb=108, bsz=40, num_updates=1360, lr=1.46773e-05, gnorm=2.032, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1544
2023-01-04 21:43:05 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.91, wpb=109.1, bsz=40, num_updates=1370, lr=1.47852e-05, gnorm=1.758, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1555
2023-01-04 21:43:16 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 115845 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.5, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=95.9, ups=0.88, wpb=108.7, bsz=40, num_updates=1380, lr=1.48932e-05, gnorm=1.6, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1566
2023-01-04 21:43:28 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.91, wpb=109.4, bsz=40, num_updates=1390, lr=1.50011e-05, gnorm=1.78, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1577
2023-01-04 21:43:39 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 115845 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=98.7, ups=0.91, wpb=108.8, bsz=40, num_updates=1400, lr=1.5109e-05, gnorm=1.604, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1588
2023-01-04 21:43:50 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.89, wpb=108.6, bsz=40, num_updates=1410, lr=1.52169e-05, gnorm=1.726, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1600
2023-01-04 21:44:01 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 115845 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.503, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=96.1, ups=0.89, wpb=108.5, bsz=40, num_updates=1420, lr=1.53248e-05, gnorm=1.734, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1611
2023-01-04 21:44:13 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 115845 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98, ups=0.9, wpb=108.4, bsz=40, num_updates=1430, lr=1.54328e-05, gnorm=1.855, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1622
2023-01-04 21:44:24 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 115845 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98.6, ups=0.9, wpb=109.5, bsz=40, num_updates=1440, lr=1.55407e-05, gnorm=1.629, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1633
2023-01-04 21:44:34 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 115845 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.8, ups=0.94, wpb=109.1, bsz=40, num_updates=1450, lr=1.56486e-05, gnorm=1.697, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1644
2023-01-04 21:44:46 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 115845 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.7, ups=0.91, wpb=109.9, bsz=40, num_updates=1460, lr=1.57565e-05, gnorm=1.545, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1655
2023-01-04 21:44:57 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 115845 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=1470, lr=1.58645e-05, gnorm=1.649, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1666
2023-01-04 21:45:08 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 115845 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.5, ups=0.91, wpb=109.8, bsz=40, num_updates=1480, lr=1.59724e-05, gnorm=1.793, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1678
2023-01-04 21:45:19 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 115845 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.517, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=97.7, ups=0.89, wpb=109.4, bsz=40, num_updates=1490, lr=1.60803e-05, gnorm=1.75, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1689
2023-01-04 21:45:31 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.1, ups=0.88, wpb=108.9, bsz=40, num_updates=1500, lr=1.61882e-05, gnorm=1.711, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1700
2023-01-04 21:45:42 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 115845 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=95.2, ups=0.88, wpb=107.7, bsz=40, num_updates=1510, lr=1.62961e-05, gnorm=1.691, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1712
2023-01-04 21:45:53 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.91, wpb=107.9, bsz=40, num_updates=1520, lr=1.64041e-05, gnorm=1.726, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1723
2023-01-04 21:46:05 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.9, wpb=109.7, bsz=40, num_updates=1530, lr=1.6512e-05, gnorm=1.69, clip=100, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=1734
2023-01-04 21:46:16 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 115845 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=1540, lr=1.66199e-05, gnorm=1.65, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1746
2023-01-04 21:46:27 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 115845 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99, ups=0.91, wpb=109, bsz=40, num_updates=1550, lr=1.67278e-05, gnorm=1.622, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1757
2023-01-04 21:46:38 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 115845 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.4, ups=0.9, wpb=108.8, bsz=40, num_updates=1560, lr=1.68357e-05, gnorm=1.623, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1768
2023-01-04 21:46:50 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 115845 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98.5, ups=0.91, wpb=108.5, bsz=40, num_updates=1570, lr=1.69437e-05, gnorm=1.565, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1779
2023-01-04 21:47:01 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 115845 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.3, ups=0.92, wpb=110.1, bsz=40, num_updates=1580, lr=1.70516e-05, gnorm=1.667, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1790
2023-01-04 21:47:12 - progress_bar.py[line:274] - INFO: epoch 001:   1590 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.2, ups=0.89, wpb=108.7, bsz=40, num_updates=1590, lr=1.71595e-05, gnorm=1.684, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1802
2023-01-04 21:47:23 - progress_bar.py[line:274] - INFO: epoch 001:   1600 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.5, ups=0.89, wpb=107, bsz=40, num_updates=1600, lr=1.72674e-05, gnorm=1.864, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1813
2023-01-04 21:47:27 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 21:47:35 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=90.5, ups=0.84, wpb=107.6, bsz=40, num_updates=1610, lr=1.73754e-05, gnorm=1.669, clip=100, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1825
2023-01-04 21:47:46 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 115845 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.4, ups=0.91, wpb=109.3, bsz=40, num_updates=1620, lr=1.74833e-05, gnorm=1.471, clip=100, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=1836
2023-01-04 21:47:57 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 115845 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=102, ups=0.94, wpb=108, bsz=40, num_updates=1630, lr=1.75912e-05, gnorm=1.675, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1847
2023-01-04 21:48:09 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 115845 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=95.7, ups=0.88, wpb=108.4, bsz=40, num_updates=1640, lr=1.76991e-05, gnorm=1.75, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1858
2023-01-04 21:48:20 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 115845 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99, ups=0.9, wpb=110, bsz=40, num_updates=1650, lr=1.7807e-05, gnorm=1.573, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1869
2023-01-04 21:48:31 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 115845 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.7, ups=0.92, wpb=108.9, bsz=40, num_updates=1660, lr=1.7915e-05, gnorm=1.573, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1880
2023-01-04 21:48:42 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 115845 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.4, ups=0.92, wpb=109.6, bsz=40, num_updates=1670, lr=1.80229e-05, gnorm=1.721, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1891
2023-01-04 21:48:53 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 115845 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.2, ups=0.92, wpb=110.1, bsz=40, num_updates=1680, lr=1.81308e-05, gnorm=1.616, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1902
2023-01-04 21:49:04 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 115845 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=101.8, ups=0.93, wpb=109.7, bsz=40, num_updates=1690, lr=1.82387e-05, gnorm=1.619, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1913
2023-01-04 21:49:15 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 115845 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.5, ups=0.9, wpb=108.9, bsz=40, num_updates=1700, lr=1.83466e-05, gnorm=1.586, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1924
2023-01-04 21:49:26 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.89, wpb=109, bsz=40, num_updates=1710, lr=1.84546e-05, gnorm=1.76, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1936
2023-01-04 21:49:38 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 115845 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=1720, lr=1.85625e-05, gnorm=1.516, clip=100, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=1947
2023-01-04 21:49:49 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.89, wpb=109.4, bsz=40, num_updates=1730, lr=1.86704e-05, gnorm=1.58, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1959
2023-01-04 21:50:00 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.89, wpb=108.6, bsz=40, num_updates=1740, lr=1.87783e-05, gnorm=1.745, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1970
2023-01-04 21:50:12 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 115845 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=95.5, ups=0.88, wpb=109.1, bsz=40, num_updates=1750, lr=1.88863e-05, gnorm=1.6, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1981
2023-01-04 21:50:23 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.5, ups=0.88, wpb=108.9, bsz=40, num_updates=1760, lr=1.89942e-05, gnorm=1.586, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1993
2023-01-04 21:50:34 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 115845 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.9, ups=0.93, wpb=108.7, bsz=40, num_updates=1770, lr=1.91021e-05, gnorm=1.592, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2004
2023-01-04 21:50:45 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 115845 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=96.8, ups=0.89, wpb=108.9, bsz=40, num_updates=1780, lr=1.921e-05, gnorm=1.639, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2015
2023-01-04 21:50:57 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 115845 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=96, ups=0.87, wpb=110.5, bsz=40, num_updates=1790, lr=1.93179e-05, gnorm=1.468, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2027
2023-01-04 21:51:08 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 115845 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=97.4, ups=0.89, wpb=108.8, bsz=40, num_updates=1800, lr=1.94259e-05, gnorm=1.414, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2038
2023-01-04 21:51:19 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100, ups=0.92, wpb=108.7, bsz=40, num_updates=1810, lr=1.95338e-05, gnorm=1.605, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2049
2023-01-04 21:51:30 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.91, wpb=110, bsz=40, num_updates=1820, lr=1.96417e-05, gnorm=1.568, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2060
2023-01-04 21:51:42 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 115845 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=94, ups=0.87, wpb=107.8, bsz=40, num_updates=1830, lr=1.97496e-05, gnorm=1.549, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2072
2023-01-04 21:51:53 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.91, wpb=108.9, bsz=40, num_updates=1840, lr=1.98575e-05, gnorm=1.682, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2083
2023-01-04 21:52:05 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 115845 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=94.9, ups=0.87, wpb=108.6, bsz=40, num_updates=1850, lr=1.99655e-05, gnorm=1.492, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2094
2023-01-04 21:52:16 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 115845 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=98.2, ups=0.9, wpb=108.6, bsz=40, num_updates=1860, lr=2.00734e-05, gnorm=1.581, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2106
2023-01-04 21:52:27 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 115845 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=96.8, ups=0.9, wpb=107.3, bsz=40, num_updates=1870, lr=2.01813e-05, gnorm=1.59, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2117
2023-01-04 21:52:38 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 115845 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.8, ups=0.89, wpb=108.2, bsz=40, num_updates=1880, lr=2.02892e-05, gnorm=1.624, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2128
2023-01-04 21:52:49 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 115845 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.9, ups=0.92, wpb=109.9, bsz=40, num_updates=1890, lr=2.03972e-05, gnorm=1.561, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2139
2023-01-04 21:53:01 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 115845 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.483, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.4, ups=0.91, wpb=109.7, bsz=40, num_updates=1900, lr=2.05051e-05, gnorm=1.518, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2150
2023-01-04 21:53:12 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 115845 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99, ups=0.89, wpb=111.5, bsz=40, num_updates=1910, lr=2.0613e-05, gnorm=1.469, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2162
2023-01-04 21:53:23 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 115845 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=102.9, ups=0.94, wpb=109.2, bsz=40, num_updates=1920, lr=2.07209e-05, gnorm=1.377, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2172
2023-01-04 21:53:34 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.89, wpb=108.2, bsz=40, num_updates=1930, lr=2.08288e-05, gnorm=1.587, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2184
2023-01-04 21:53:45 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 115845 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=1940, lr=2.09368e-05, gnorm=1.498, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2195
2023-01-04 21:53:57 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.6, ups=0.87, wpb=109.5, bsz=40, num_updates=1950, lr=2.10447e-05, gnorm=1.446, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2207
2023-01-04 21:54:08 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 115845 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=100.2, ups=0.92, wpb=109.1, bsz=40, num_updates=1960, lr=2.11526e-05, gnorm=1.538, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2217
2023-01-04 21:54:19 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 115845 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=94.3, ups=0.87, wpb=108.1, bsz=40, num_updates=1970, lr=2.12605e-05, gnorm=1.394, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2229
2023-01-04 21:54:31 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 115845 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=96.8, ups=0.89, wpb=108.4, bsz=40, num_updates=1980, lr=2.13684e-05, gnorm=1.46, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2240
2023-01-04 21:54:42 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.9, wpb=107.9, bsz=40, num_updates=1990, lr=2.14764e-05, gnorm=1.526, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2251
2023-01-04 21:54:53 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 115845 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=98.6, ups=0.91, wpb=108.7, bsz=40, num_updates=2000, lr=2.15843e-05, gnorm=1.544, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2263
2023-01-04 21:54:53 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-04 21:54:53 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-01-04 21:54:54 - train.py[line:549] - INFO: 0 / 4988
2023-01-04 21:54:54 - train.py[line:551] - INFO: load:0.89 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-04 21:57:29 - train.py[line:549] - INFO: 200 / 4988
2023-01-04 21:57:29 - train.py[line:551] - INFO: load:0.91 valid_run:154.87 task_valid:150.52 collect_output:3.29
2023-01-04 21:59:59 - train.py[line:549] - INFO: 400 / 4988
2023-01-04 21:59:59 - train.py[line:551] - INFO: load:0.93 valid_run:303.62 task_valid:293.70 collect_output:7.82
2023-01-04 22:02:31 - train.py[line:549] - INFO: 600 / 4988
2023-01-04 22:02:31 - train.py[line:551] - INFO: load:0.96 valid_run:456.12 task_valid:436.59 collect_output:16.37
2023-01-04 22:05:00 - train.py[line:549] - INFO: 800 / 4988
2023-01-04 22:05:00 - train.py[line:551] - INFO: load:0.98 valid_run:604.70 task_valid:581.35 collect_output:19.16
2023-01-04 22:07:33 - train.py[line:549] - INFO: 1000 / 4988
2023-01-04 22:07:33 - train.py[line:551] - INFO: load:1.01 valid_run:756.81 task_valid:728.78 collect_output:22.78
2023-01-04 22:10:05 - train.py[line:549] - INFO: 1200 / 4988
2023-01-04 22:10:05 - train.py[line:551] - INFO: load:1.04 valid_run:908.55 task_valid:874.65 collect_output:27.58
2023-01-04 22:12:38 - train.py[line:549] - INFO: 1400 / 4988
2023-01-04 22:12:38 - train.py[line:551] - INFO: load:1.06 valid_run:1061.39 task_valid:1020.88 collect_output:33.09
2023-01-04 22:15:09 - train.py[line:549] - INFO: 1600 / 4988
2023-01-04 22:15:10 - train.py[line:551] - INFO: load:1.09 valid_run:1212.07 task_valid:1162.07 collect_output:41.52
2023-01-04 22:17:39 - train.py[line:549] - INFO: 1800 / 4988
2023-01-04 22:17:39 - train.py[line:551] - INFO: load:1.12 valid_run:1361.36 task_valid:1306.74 collect_output:45.07
2023-01-04 22:20:08 - train.py[line:549] - INFO: 2000 / 4988
2023-01-04 22:20:08 - train.py[line:551] - INFO: load:1.14 valid_run:1510.45 task_valid:1450.76 collect_output:49.08
2023-01-04 22:22:38 - train.py[line:549] - INFO: 2200 / 4988
2023-01-04 22:22:38 - train.py[line:551] - INFO: load:1.17 valid_run:1659.90 task_valid:1595.70 collect_output:52.53
2023-01-04 22:25:08 - train.py[line:549] - INFO: 2400 / 4988
2023-01-04 22:25:08 - train.py[line:551] - INFO: load:1.20 valid_run:1809.49 task_valid:1740.54 collect_output:56.22
2023-01-04 22:27:38 - train.py[line:549] - INFO: 2600 / 4988
2023-01-04 22:27:38 - train.py[line:551] - INFO: load:1.22 valid_run:1959.09 task_valid:1882.33 collect_output:63.00
2023-01-04 22:30:08 - train.py[line:549] - INFO: 2800 / 4988
2023-01-04 22:30:08 - train.py[line:551] - INFO: load:1.25 valid_run:2109.63 task_valid:2027.87 collect_output:66.96
2023-01-04 22:32:39 - train.py[line:549] - INFO: 3000 / 4988
2023-01-04 22:32:39 - train.py[line:551] - INFO: load:1.27 valid_run:2259.63 task_valid:2174.51 collect_output:69.27
2023-01-04 22:35:09 - train.py[line:549] - INFO: 3200 / 4988
2023-01-04 22:35:09 - train.py[line:551] - INFO: load:1.30 valid_run:2409.66 task_valid:2318.67 collect_output:74.10
2023-01-04 22:37:40 - train.py[line:549] - INFO: 3400 / 4988
2023-01-04 22:37:40 - train.py[line:551] - INFO: load:1.33 valid_run:2561.02 task_valid:2464.16 collect_output:78.94
2023-01-04 22:40:11 - train.py[line:549] - INFO: 3600 / 4988
2023-01-04 22:40:11 - train.py[line:551] - INFO: load:1.35 valid_run:2711.72 task_valid:2611.30 collect_output:81.46
2023-01-04 22:42:39 - train.py[line:549] - INFO: 3800 / 4988
2023-01-04 22:42:39 - train.py[line:551] - INFO: load:1.38 valid_run:2859.92 task_valid:2752.97 collect_output:86.93
2023-01-04 22:45:10 - train.py[line:549] - INFO: 4000 / 4988
2023-01-04 22:45:10 - train.py[line:551] - INFO: load:1.40 valid_run:3009.98 task_valid:2898.13 collect_output:90.79
2023-01-04 22:47:42 - train.py[line:549] - INFO: 4200 / 4988
2023-01-04 22:47:42 - train.py[line:551] - INFO: load:1.43 valid_run:3161.61 task_valid:3042.80 collect_output:96.69
2023-01-04 22:50:11 - train.py[line:549] - INFO: 4400 / 4988
2023-01-04 22:50:11 - train.py[line:551] - INFO: load:1.45 valid_run:3310.94 task_valid:3187.36 collect_output:100.41
2023-01-04 22:52:42 - train.py[line:549] - INFO: 4600 / 4988
2023-01-04 22:52:42 - train.py[line:551] - INFO: load:1.48 valid_run:3461.93 task_valid:3333.57 collect_output:104.15
2023-01-04 22:55:13 - train.py[line:549] - INFO: 4800 / 4988
2023-01-04 22:55:13 - train.py[line:551] - INFO: load:1.51 valid_run:3613.18 task_valid:3480.22 collect_output:107.71

====================================================================================================
SGG eval:     R @ 50: 0.2905;     R @ 100: 0.3515;     R @ 500: 0.4103;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1346;    mR @ 100: 0.1879;    mR @ 500: 0.2242;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0488) (covered in:0.0000) (covering:0.1429) (eating:0.4706) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2083) (playing:0.0000) (riding:0.4167) (says:0.0000) (sitting on:0.4310) (standing on:0.4900) (using:0.1500) (walking in:0.0000) (walking on:0.1757) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-04 22:57:44 - train.py[line:487] - INFO: 0.3515
2023-01-04 22:57:44 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.2905;     R @ 100: 0.3515;     R @ 500: 0.4103;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1346;    mR @ 100: 0.1879;    mR @ 500: 0.2242;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0488) (covered in:0.0000) (covering:0.1429) (eating:0.4706) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2083) (playing:0.0000) (riding:0.4167) (says:0.0000) (sitting on:0.4310) (standing on:0.4900) (using:0.1500) (walking in:0.0000) (walking on:0.1757) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-04 22:57:44 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.374 | loss_v1 0 | loss_v2 0 | nll_loss 0.225 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.3515 | ppl 1.17 | vqa_score 0.2061 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 2000
2023-01-04 22:57:44 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-01-04 22:57:44 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-04 22:58:30 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-04 23:01:26 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.3515) (writing took 221.9822432436049 seconds)
2023-01-04 23:01:37 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 115845 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=0.3, ups=0, wpb=108.2, bsz=40, num_updates=2010, lr=2.16922e-05, gnorm=1.396, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6267
2023-01-04 23:01:49 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 115845 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.2, ups=0.88, wpb=110, bsz=40, num_updates=2020, lr=2.18001e-05, gnorm=1.363, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6278
2023-01-04 23:02:00 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 115845 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=98.5, ups=0.91, wpb=108.4, bsz=40, num_updates=2030, lr=2.19081e-05, gnorm=1.41, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6289
2023-01-04 23:02:11 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 115845 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100, ups=0.91, wpb=110.2, bsz=40, num_updates=2040, lr=2.2016e-05, gnorm=1.587, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6300
2023-01-04 23:02:22 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 115845 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.4, ups=0.9, wpb=111, bsz=40, num_updates=2050, lr=2.21239e-05, gnorm=1.476, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6312
2023-01-04 23:02:33 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 115845 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=100.5, ups=0.91, wpb=110, bsz=40, num_updates=2060, lr=2.22318e-05, gnorm=1.618, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6323
2023-01-04 23:02:44 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 115845 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=97.3, ups=0.89, wpb=108.8, bsz=40, num_updates=2070, lr=2.23397e-05, gnorm=1.479, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6334
2023-01-04 23:02:55 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 115845 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=2080, lr=2.24477e-05, gnorm=1.465, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6345
2023-01-04 23:03:07 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 115845 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=2090, lr=2.25556e-05, gnorm=1.49, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6356
2023-01-04 23:03:17 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101, ups=0.93, wpb=108, bsz=40, num_updates=2100, lr=2.26635e-05, gnorm=1.733, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6367
2023-01-04 23:03:29 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.91, wpb=108.9, bsz=40, num_updates=2110, lr=2.27714e-05, gnorm=1.448, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6378
2023-01-04 23:03:39 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 115845 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.4, ups=0.92, wpb=108.4, bsz=40, num_updates=2120, lr=2.28793e-05, gnorm=1.514, clip=100, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6389
2023-01-04 23:03:47 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 23:03:52 - progress_bar.py[line:274] - INFO: epoch 001:   2132 / 115845 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=89.3, ups=0.81, wpb=110.4, bsz=40, num_updates=2130, lr=2.29873e-05, gnorm=1.447, clip=100, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6402
2023-01-04 23:04:03 - progress_bar.py[line:274] - INFO: epoch 001:   2142 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.9, wpb=108.1, bsz=40, num_updates=2140, lr=2.30952e-05, gnorm=1.433, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6413
2023-01-04 23:04:14 - progress_bar.py[line:274] - INFO: epoch 001:   2152 / 115845 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.8, ups=0.9, wpb=107.2, bsz=40, num_updates=2150, lr=2.32031e-05, gnorm=1.495, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6424
2023-01-04 23:04:25 - progress_bar.py[line:274] - INFO: epoch 001:   2162 / 115845 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.5, ups=0.91, wpb=108.8, bsz=40, num_updates=2160, lr=2.3311e-05, gnorm=1.521, clip=100, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=6435
2023-01-04 23:04:36 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 115845 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.1, ups=0.92, wpb=107.8, bsz=40, num_updates=2170, lr=2.3419e-05, gnorm=1.366, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6446
2023-01-04 23:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 115845 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98, ups=0.9, wpb=108.8, bsz=40, num_updates=2180, lr=2.35269e-05, gnorm=1.514, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6457
2023-01-04 23:04:59 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 115845 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.4, ups=0.9, wpb=107.9, bsz=40, num_updates=2190, lr=2.36348e-05, gnorm=1.444, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6468
2023-01-04 23:05:10 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 115845 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.4, ups=0.88, wpb=110.1, bsz=40, num_updates=2200, lr=2.37427e-05, gnorm=1.444, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6480
2023-01-04 23:05:21 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=2210, lr=2.38506e-05, gnorm=1.397, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6491
2023-01-04 23:05:32 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.92, wpb=107.6, bsz=40, num_updates=2220, lr=2.39586e-05, gnorm=1.39, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6502
2023-01-04 23:05:43 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 115845 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98.8, ups=0.91, wpb=109, bsz=40, num_updates=2230, lr=2.40665e-05, gnorm=1.657, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6513
2023-01-04 23:05:55 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 115845 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=2240, lr=2.41744e-05, gnorm=1.342, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6524
2023-01-04 23:06:06 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.9, wpb=107.6, bsz=40, num_updates=2250, lr=2.42823e-05, gnorm=1.383, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6536
2023-01-04 23:06:17 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=108.1, bsz=40, num_updates=2260, lr=2.43902e-05, gnorm=1.545, clip=100, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6547
2023-01-04 23:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 115845 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=94.2, ups=0.87, wpb=108.5, bsz=40, num_updates=2270, lr=2.44982e-05, gnorm=1.469, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6558
2023-01-04 23:06:40 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 115845 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.4, ups=0.9, wpb=109.1, bsz=40, num_updates=2280, lr=2.46061e-05, gnorm=1.294, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6570
2023-01-04 23:06:51 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.89, wpb=109.3, bsz=40, num_updates=2290, lr=2.4714e-05, gnorm=1.288, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6581
2023-01-04 23:07:02 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 115845 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=95.5, ups=0.88, wpb=108, bsz=40, num_updates=2300, lr=2.48219e-05, gnorm=1.496, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6592
2023-01-04 23:07:14 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 115845 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=95.7, ups=0.88, wpb=108.7, bsz=40, num_updates=2310, lr=2.49299e-05, gnorm=1.365, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6604
2023-01-04 23:07:25 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 115845 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=2320, lr=2.50378e-05, gnorm=1.419, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6615
2023-01-04 23:07:36 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 115845 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=96.7, ups=0.9, wpb=107.9, bsz=40, num_updates=2330, lr=2.51457e-05, gnorm=1.4, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6626
2023-01-04 23:07:47 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 115845 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=97.3, ups=0.9, wpb=108.3, bsz=40, num_updates=2340, lr=2.52536e-05, gnorm=1.463, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6637
2023-01-04 23:07:59 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 115845 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.4, ups=0.91, wpb=109.6, bsz=40, num_updates=2350, lr=2.53615e-05, gnorm=1.383, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6648
2023-01-04 23:08:10 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 115845 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.3, ups=0.91, wpb=111.2, bsz=40, num_updates=2360, lr=2.54695e-05, gnorm=1.277, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6659
2023-01-04 23:08:21 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 115845 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=2370, lr=2.55774e-05, gnorm=1.366, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6671
2023-01-04 23:08:32 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.92, wpb=108.5, bsz=40, num_updates=2380, lr=2.56853e-05, gnorm=1.306, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6682
2023-01-04 23:08:43 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 115845 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=96.5, ups=0.89, wpb=108.7, bsz=40, num_updates=2390, lr=2.57932e-05, gnorm=1.271, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6693
2023-01-04 23:08:54 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 115845 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100, ups=0.92, wpb=108.4, bsz=40, num_updates=2400, lr=2.59011e-05, gnorm=1.315, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6704
2023-01-04 23:09:05 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.9, wpb=108.6, bsz=40, num_updates=2410, lr=2.60091e-05, gnorm=1.336, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6715
2023-01-04 23:09:16 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 115845 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100, ups=0.91, wpb=109.7, bsz=40, num_updates=2420, lr=2.6117e-05, gnorm=1.334, clip=100, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6726
2023-01-04 23:09:27 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 115845 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=106.5, nsentences=40, sample_size=106.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=96.5, ups=0.91, wpb=106.5, bsz=40, num_updates=2430, lr=2.62249e-05, gnorm=1.432, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6737
2023-01-04 23:09:38 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 115845 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.3, ups=0.93, wpb=109.3, bsz=40, num_updates=2440, lr=2.63328e-05, gnorm=1.454, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6748
2023-01-04 23:09:49 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 115845 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.5, ups=0.9, wpb=110.4, bsz=40, num_updates=2450, lr=2.64408e-05, gnorm=1.4, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6759
2023-01-04 23:10:01 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 115845 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.9, ups=0.89, wpb=109.4, bsz=40, num_updates=2460, lr=2.65487e-05, gnorm=1.407, clip=100, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6770
2023-01-04 23:10:12 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.91, wpb=109.2, bsz=40, num_updates=2470, lr=2.66566e-05, gnorm=1.264, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6781
2023-01-04 23:10:23 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 115845 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.9, ups=0.9, wpb=109.6, bsz=40, num_updates=2480, lr=2.67645e-05, gnorm=1.218, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6793
2023-01-04 23:10:34 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 115845 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.5, ups=0.9, wpb=108.4, bsz=40, num_updates=2490, lr=2.68724e-05, gnorm=1.414, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6804
2023-01-04 23:10:45 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 115845 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=96.9, ups=0.89, wpb=108.5, bsz=40, num_updates=2500, lr=2.69804e-05, gnorm=1.333, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6815
2023-01-04 23:10:56 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 115845 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.7, ups=0.92, wpb=109.4, bsz=40, num_updates=2510, lr=2.70883e-05, gnorm=1.322, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6826
2023-01-04 23:11:08 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.88, wpb=109.4, bsz=40, num_updates=2520, lr=2.71962e-05, gnorm=1.356, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6837
2023-01-04 23:11:19 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 115845 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.2, ups=0.91, wpb=109.7, bsz=40, num_updates=2530, lr=2.73041e-05, gnorm=1.32, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6849
2023-01-04 23:11:30 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 115845 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.6, ups=0.91, wpb=109.2, bsz=40, num_updates=2540, lr=2.7412e-05, gnorm=1.343, clip=100, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6860
2023-01-04 23:11:41 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=2550, lr=2.752e-05, gnorm=1.195, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6871
2023-01-04 23:11:52 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 115845 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=2560, lr=2.76279e-05, gnorm=1.212, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6882
2023-01-04 23:12:03 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 115845 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=97.4, ups=0.9, wpb=108.2, bsz=40, num_updates=2570, lr=2.77358e-05, gnorm=1.347, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6893
2023-01-04 23:12:15 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.88, wpb=109.9, bsz=40, num_updates=2580, lr=2.78437e-05, gnorm=1.35, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6904
2023-01-04 23:12:26 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 115845 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99, ups=0.92, wpb=107.9, bsz=40, num_updates=2590, lr=2.79517e-05, gnorm=1.415, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6915
2023-01-04 23:12:37 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 115845 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.4, ups=0.9, wpb=109.1, bsz=40, num_updates=2600, lr=2.80596e-05, gnorm=1.333, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6927
2023-01-04 23:12:48 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 115845 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.3, ups=0.9, wpb=110.1, bsz=40, num_updates=2610, lr=2.81675e-05, gnorm=1.434, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6938
2023-01-04 23:12:59 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=2620, lr=2.82754e-05, gnorm=1.414, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6949
2023-01-04 23:13:10 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 115845 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=2630, lr=2.83833e-05, gnorm=1.259, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6960
2023-01-04 23:13:22 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 115845 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.8, ups=0.91, wpb=108, bsz=40, num_updates=2640, lr=2.84913e-05, gnorm=1.305, clip=100, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6971
2023-01-04 23:13:33 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.92, wpb=107.5, bsz=40, num_updates=2650, lr=2.85992e-05, gnorm=1.277, clip=100, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6982
2023-01-04 23:13:44 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 115845 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.7, ups=0.89, wpb=110.4, bsz=40, num_updates=2660, lr=2.87071e-05, gnorm=1.257, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6994
2023-01-04 23:13:55 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 115845 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.1, ups=0.9, wpb=109.9, bsz=40, num_updates=2670, lr=2.8815e-05, gnorm=1.339, clip=100, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7005
2023-01-04 23:14:07 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 115845 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=96.3, ups=0.89, wpb=108.6, bsz=40, num_updates=2680, lr=2.89229e-05, gnorm=1.574, clip=100, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7016
2023-01-04 23:14:18 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 115845 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.2, ups=0.9, wpb=108.5, bsz=40, num_updates=2690, lr=2.90309e-05, gnorm=1.235, clip=90, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7027
2023-01-04 23:14:29 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 115845 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.3, ups=0.91, wpb=109.7, bsz=40, num_updates=2700, lr=2.91388e-05, gnorm=1.23, clip=100, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7039
2023-01-04 23:14:40 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 115845 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=2710, lr=2.92467e-05, gnorm=1.186, clip=80, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7050
2023-01-04 23:14:51 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 23:14:52 - progress_bar.py[line:274] - INFO: epoch 001:   2723 / 115845 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=90.7, ups=0.83, wpb=109.7, bsz=40, num_updates=2720, lr=2.93546e-05, gnorm=1.309, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=7062
2023-01-04 23:15:04 - progress_bar.py[line:274] - INFO: epoch 001:   2733 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.91, wpb=108.1, bsz=40, num_updates=2730, lr=2.94626e-05, gnorm=1.205, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7073
2023-01-04 23:15:14 - progress_bar.py[line:274] - INFO: epoch 001:   2743 / 115845 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.6, ups=0.93, wpb=109.6, bsz=40, num_updates=2740, lr=2.95705e-05, gnorm=1.204, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7084
2023-01-04 23:15:25 - progress_bar.py[line:274] - INFO: epoch 001:   2753 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.91, wpb=108.5, bsz=40, num_updates=2750, lr=2.96784e-05, gnorm=1.243, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7095
2023-01-04 23:15:36 - progress_bar.py[line:274] - INFO: epoch 001:   2763 / 115845 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.6, ups=0.94, wpb=109.6, bsz=40, num_updates=2760, lr=2.97863e-05, gnorm=1.159, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7106
2023-01-04 23:15:48 - progress_bar.py[line:274] - INFO: epoch 001:   2773 / 115845 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=94.8, ups=0.87, wpb=108.5, bsz=40, num_updates=2770, lr=2.98942e-05, gnorm=1.185, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7117
2023-01-04 23:15:59 - progress_bar.py[line:274] - INFO: epoch 001:   2783 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.6, ups=0.87, wpb=108.5, bsz=40, num_updates=2780, lr=3.00022e-05, gnorm=1.309, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7129
2023-01-04 23:16:10 - progress_bar.py[line:274] - INFO: epoch 001:   2793 / 115845 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.4, ups=0.91, wpb=108.6, bsz=40, num_updates=2790, lr=3.01101e-05, gnorm=1.332, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7140
2023-01-04 23:16:22 - progress_bar.py[line:274] - INFO: epoch 001:   2803 / 115845 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=2800, lr=3.0218e-05, gnorm=1.217, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7151
2023-01-04 23:16:32 - progress_bar.py[line:274] - INFO: epoch 001:   2813 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.6, ups=0.92, wpb=111.4, bsz=40, num_updates=2810, lr=3.03259e-05, gnorm=1.102, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7162
2023-01-04 23:16:44 - progress_bar.py[line:274] - INFO: epoch 001:   2823 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.91, wpb=108.3, bsz=40, num_updates=2820, lr=3.04338e-05, gnorm=1.356, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7173
2023-01-04 23:16:55 - progress_bar.py[line:274] - INFO: epoch 001:   2833 / 115845 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.5, ups=0.89, wpb=108.3, bsz=40, num_updates=2830, lr=3.05418e-05, gnorm=1.271, clip=100, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7185
2023-01-04 23:17:06 - progress_bar.py[line:274] - INFO: epoch 001:   2843 / 115845 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=2840, lr=3.06497e-05, gnorm=1.227, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7196
2023-01-04 23:17:17 - progress_bar.py[line:274] - INFO: epoch 001:   2853 / 115845 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.1, ups=0.92, wpb=109.5, bsz=40, num_updates=2850, lr=3.07576e-05, gnorm=1.231, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7207
2023-01-04 23:17:28 - progress_bar.py[line:274] - INFO: epoch 001:   2863 / 115845 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=94.9, ups=0.87, wpb=108.5, bsz=40, num_updates=2860, lr=3.08655e-05, gnorm=1.217, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7218
2023-01-04 23:17:40 - progress_bar.py[line:274] - INFO: epoch 001:   2873 / 115845 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.9, ups=0.9, wpb=108.8, bsz=40, num_updates=2870, lr=3.09735e-05, gnorm=1.228, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7229
2023-01-04 23:17:51 - progress_bar.py[line:274] - INFO: epoch 001:   2883 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=2880, lr=3.10814e-05, gnorm=1.218, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7241
2023-01-04 23:18:02 - progress_bar.py[line:274] - INFO: epoch 001:   2893 / 115845 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.1, ups=0.93, wpb=109.2, bsz=40, num_updates=2890, lr=3.11893e-05, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7251
2023-01-04 23:18:13 - progress_bar.py[line:274] - INFO: epoch 001:   2903 / 115845 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.4, ups=0.93, wpb=109.6, bsz=40, num_updates=2900, lr=3.12972e-05, gnorm=1.227, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7262
2023-01-04 23:18:24 - progress_bar.py[line:274] - INFO: epoch 001:   2913 / 115845 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.1, ups=0.91, wpb=108.8, bsz=40, num_updates=2910, lr=3.14051e-05, gnorm=1.13, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7273
2023-01-04 23:18:35 - progress_bar.py[line:274] - INFO: epoch 001:   2923 / 115845 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.3, ups=0.91, wpb=109.4, bsz=40, num_updates=2920, lr=3.15131e-05, gnorm=1.16, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7284
2023-01-04 23:18:46 - progress_bar.py[line:274] - INFO: epoch 001:   2933 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.92, wpb=108.5, bsz=40, num_updates=2930, lr=3.1621e-05, gnorm=1.215, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7295
2023-01-04 23:18:57 - progress_bar.py[line:274] - INFO: epoch 001:   2943 / 115845 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98, ups=0.9, wpb=108.9, bsz=40, num_updates=2940, lr=3.17289e-05, gnorm=1.181, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7307
2023-01-04 23:19:08 - progress_bar.py[line:274] - INFO: epoch 001:   2953 / 115845 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=95, ups=0.87, wpb=109.4, bsz=40, num_updates=2950, lr=3.18368e-05, gnorm=1.239, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7318
2023-01-04 23:19:20 - progress_bar.py[line:274] - INFO: epoch 001:   2963 / 115845 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.6, ups=0.9, wpb=110.1, bsz=40, num_updates=2960, lr=3.19447e-05, gnorm=1.234, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7329
2023-01-04 23:19:31 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 115845 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.2, ups=0.92, wpb=109.1, bsz=40, num_updates=2970, lr=3.20527e-05, gnorm=1.173, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7340
2023-01-04 23:19:42 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 115845 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.5, ups=0.88, wpb=109.1, bsz=40, num_updates=2980, lr=3.21606e-05, gnorm=1.23, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7352
2023-01-04 23:19:53 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.92, wpb=108.5, bsz=40, num_updates=2990, lr=3.22685e-05, gnorm=1.137, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7363
2023-01-04 23:20:04 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.6, ups=0.91, wpb=109.9, bsz=40, num_updates=3000, lr=3.23764e-05, gnorm=1.204, clip=80, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=7374
2023-01-04 23:20:15 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 115845 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=95.8, ups=0.88, wpb=109.1, bsz=40, num_updates=3010, lr=3.24844e-05, gnorm=1.289, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7385
2023-01-04 23:20:27 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=108.7, bsz=40, num_updates=3020, lr=3.25923e-05, gnorm=1.203, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7396
2023-01-04 23:20:38 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 115845 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=3030, lr=3.27002e-05, gnorm=1.076, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7408
2023-01-04 23:20:49 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 115845 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.4, ups=0.89, wpb=108.9, bsz=40, num_updates=3040, lr=3.28081e-05, gnorm=1.148, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7419
2023-01-04 23:21:00 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.3, ups=0.91, wpb=109, bsz=40, num_updates=3050, lr=3.2916e-05, gnorm=1.176, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7430
2023-01-04 23:21:11 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.1, ups=0.92, wpb=109.2, bsz=40, num_updates=3060, lr=3.3024e-05, gnorm=1.092, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7441
2023-01-04 23:21:22 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.9, wpb=109.9, bsz=40, num_updates=3070, lr=3.31319e-05, gnorm=1.181, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7452
2023-01-04 23:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 115845 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=107.1, nsentences=40, sample_size=107.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=95.2, ups=0.89, wpb=107.1, bsz=40, num_updates=3080, lr=3.32398e-05, gnorm=1.345, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7463
2023-01-04 23:21:45 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102, ups=0.93, wpb=109.6, bsz=40, num_updates=3090, lr=3.33477e-05, gnorm=1.15, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7474
2023-01-04 23:21:55 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 115845 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.7, ups=0.94, wpb=109.8, bsz=40, num_updates=3100, lr=3.34556e-05, gnorm=1.246, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7485
2023-01-04 23:22:07 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 115845 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=3110, lr=3.35636e-05, gnorm=1.219, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7496
2023-01-04 23:22:18 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 115845 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=3120, lr=3.36715e-05, gnorm=1.224, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7508
2023-01-04 23:22:29 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 115845 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.4, ups=0.9, wpb=108, bsz=40, num_updates=3130, lr=3.37794e-05, gnorm=1.123, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7519
2023-01-04 23:22:40 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.7, ups=0.88, wpb=109.5, bsz=40, num_updates=3140, lr=3.38873e-05, gnorm=1.388, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7530
2023-01-04 23:22:52 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.8, ups=0.88, wpb=110.3, bsz=40, num_updates=3150, lr=3.39953e-05, gnorm=1.13, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7542
2023-01-04 23:23:03 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 115845 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.1, ups=0.89, wpb=109.4, bsz=40, num_updates=3160, lr=3.41032e-05, gnorm=1.101, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7553
2023-01-04 23:23:15 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.9, wpb=108.7, bsz=40, num_updates=3170, lr=3.42111e-05, gnorm=1.204, clip=90, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7564
2023-01-04 23:23:26 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 115845 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.6, ups=0.89, wpb=109.6, bsz=40, num_updates=3180, lr=3.4319e-05, gnorm=1.23, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7576
2023-01-04 23:23:37 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 115845 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=98.9, ups=0.92, wpb=107.6, bsz=40, num_updates=3190, lr=3.44269e-05, gnorm=1.181, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7586
2023-01-04 23:23:48 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.3, ups=0.92, wpb=109.5, bsz=40, num_updates=3200, lr=3.45349e-05, gnorm=1.087, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7597
2023-01-04 23:23:59 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 115845 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.1, ups=0.89, wpb=108.7, bsz=40, num_updates=3210, lr=3.46428e-05, gnorm=1.099, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7609
2023-01-04 23:24:10 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 115845 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=95.6, ups=0.88, wpb=108.4, bsz=40, num_updates=3220, lr=3.47507e-05, gnorm=1.128, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7620
2023-01-04 23:24:22 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 115845 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.8, ups=0.91, wpb=108.9, bsz=40, num_updates=3230, lr=3.48586e-05, gnorm=1.057, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7631
2023-01-04 23:24:33 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 115845 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.8, ups=0.9, wpb=109.2, bsz=40, num_updates=3240, lr=3.49665e-05, gnorm=1.081, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7643
2023-01-04 23:24:37 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 23:24:45 - progress_bar.py[line:274] - INFO: epoch 001:   3254 / 115845 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=88.7, ups=0.82, wpb=108.7, bsz=40, num_updates=3250, lr=3.50745e-05, gnorm=1.209, clip=100, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7655
2023-01-04 23:24:56 - progress_bar.py[line:274] - INFO: epoch 001:   3264 / 115845 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.5, ups=0.93, wpb=108.6, bsz=40, num_updates=3260, lr=3.51824e-05, gnorm=1.171, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7666
2023-01-04 23:25:07 - progress_bar.py[line:274] - INFO: epoch 001:   3274 / 115845 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=3270, lr=3.52903e-05, gnorm=1.071, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7677
2023-01-04 23:25:18 - progress_bar.py[line:274] - INFO: epoch 001:   3284 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.6, ups=0.93, wpb=109.4, bsz=40, num_updates=3280, lr=3.53982e-05, gnorm=1.15, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7688
2023-01-04 23:25:29 - progress_bar.py[line:274] - INFO: epoch 001:   3294 / 115845 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.8, ups=0.91, wpb=108.9, bsz=40, num_updates=3290, lr=3.55062e-05, gnorm=1.145, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7699
2023-01-04 23:25:40 - progress_bar.py[line:274] - INFO: epoch 001:   3304 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.2, ups=0.91, wpb=109.2, bsz=40, num_updates=3300, lr=3.56141e-05, gnorm=1.163, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7710
2023-01-04 23:25:52 - progress_bar.py[line:274] - INFO: epoch 001:   3314 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.89, wpb=109.1, bsz=40, num_updates=3310, lr=3.5722e-05, gnorm=1.072, clip=70, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=7721
2023-01-04 23:26:03 - progress_bar.py[line:274] - INFO: epoch 001:   3324 / 115845 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=106.6, nsentences=40, sample_size=106.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=92.9, ups=0.87, wpb=106.6, bsz=40, num_updates=3320, lr=3.58299e-05, gnorm=1.164, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7733
2023-01-04 23:26:15 - progress_bar.py[line:274] - INFO: epoch 001:   3334 / 115845 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=95.3, ups=0.88, wpb=107.8, bsz=40, num_updates=3330, lr=3.59378e-05, gnorm=1.207, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7744
2023-01-04 23:26:25 - progress_bar.py[line:274] - INFO: epoch 001:   3344 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=103.2, ups=0.94, wpb=110, bsz=40, num_updates=3340, lr=3.60458e-05, gnorm=1.198, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7755
2023-01-04 23:26:37 - progress_bar.py[line:274] - INFO: epoch 001:   3354 / 115845 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.5, ups=0.89, wpb=109.7, bsz=40, num_updates=3350, lr=3.61537e-05, gnorm=1.159, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7766
2023-01-04 23:26:47 - progress_bar.py[line:274] - INFO: epoch 001:   3364 / 115845 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.3, ups=0.93, wpb=108.9, bsz=40, num_updates=3360, lr=3.62616e-05, gnorm=1.055, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7777
2023-01-04 23:26:58 - progress_bar.py[line:274] - INFO: epoch 001:   3374 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=104, ups=0.95, wpb=109.7, bsz=40, num_updates=3370, lr=3.63695e-05, gnorm=1.064, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7788
2023-01-04 23:27:09 - progress_bar.py[line:274] - INFO: epoch 001:   3384 / 115845 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.2, ups=0.9, wpb=110.2, bsz=40, num_updates=3380, lr=3.64774e-05, gnorm=1.166, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7799
2023-01-04 23:27:20 - progress_bar.py[line:274] - INFO: epoch 001:   3394 / 115845 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.7, ups=0.91, wpb=108.7, bsz=40, num_updates=3390, lr=3.65854e-05, gnorm=1.086, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7810
2023-01-04 23:27:32 - progress_bar.py[line:274] - INFO: epoch 001:   3404 / 115845 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97, ups=0.89, wpb=109.6, bsz=40, num_updates=3400, lr=3.66933e-05, gnorm=1.137, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7821
2023-01-04 23:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   3414 / 115845 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.2, ups=0.9, wpb=107.4, bsz=40, num_updates=3410, lr=3.68012e-05, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7833
2023-01-04 23:27:54 - progress_bar.py[line:274] - INFO: epoch 001:   3424 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.2, ups=0.91, wpb=109, bsz=40, num_updates=3420, lr=3.69091e-05, gnorm=1.173, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7844
2023-01-04 23:28:05 - progress_bar.py[line:274] - INFO: epoch 001:   3434 / 115845 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.6, ups=0.91, wpb=108.8, bsz=40, num_updates=3430, lr=3.70171e-05, gnorm=1.195, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7855
2023-01-04 23:28:16 - progress_bar.py[line:274] - INFO: epoch 001:   3444 / 115845 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.1, ups=0.91, wpb=109.2, bsz=40, num_updates=3440, lr=3.7125e-05, gnorm=1.045, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7866
2023-01-04 23:28:28 - progress_bar.py[line:274] - INFO: epoch 001:   3454 / 115845 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.7, ups=0.88, wpb=110.5, bsz=40, num_updates=3450, lr=3.72329e-05, gnorm=1.093, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7877
2023-01-04 23:28:39 - progress_bar.py[line:274] - INFO: epoch 001:   3464 / 115845 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.7, ups=0.91, wpb=110.7, bsz=40, num_updates=3460, lr=3.73408e-05, gnorm=1.069, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7888
2023-01-04 23:28:50 - progress_bar.py[line:274] - INFO: epoch 001:   3474 / 115845 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.5, ups=0.92, wpb=108.8, bsz=40, num_updates=3470, lr=3.74487e-05, gnorm=1.179, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7899
2023-01-04 23:29:01 - progress_bar.py[line:274] - INFO: epoch 001:   3484 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.3, ups=0.92, wpb=110.2, bsz=40, num_updates=3480, lr=3.75567e-05, gnorm=0.958, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7910
2023-01-04 23:29:12 - progress_bar.py[line:274] - INFO: epoch 001:   3494 / 115845 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.8, ups=0.91, wpb=109.1, bsz=40, num_updates=3490, lr=3.76646e-05, gnorm=1.076, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7921
2023-01-04 23:29:23 - progress_bar.py[line:274] - INFO: epoch 001:   3504 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.91, wpb=107.9, bsz=40, num_updates=3500, lr=3.77725e-05, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7933
2023-01-04 23:29:34 - progress_bar.py[line:274] - INFO: epoch 001:   3514 / 115845 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.5, ups=0.92, wpb=107.6, bsz=40, num_updates=3510, lr=3.78804e-05, gnorm=1.106, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7944
2023-01-04 23:29:45 - progress_bar.py[line:274] - INFO: epoch 001:   3524 / 115845 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=3520, lr=3.79883e-05, gnorm=1.092, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7955
2023-01-04 23:29:56 - progress_bar.py[line:274] - INFO: epoch 001:   3534 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.92, wpb=108.2, bsz=40, num_updates=3530, lr=3.80963e-05, gnorm=1.17, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7966
2023-01-04 23:30:08 - progress_bar.py[line:274] - INFO: epoch 001:   3544 / 115845 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=95.5, ups=0.88, wpb=108.6, bsz=40, num_updates=3540, lr=3.82042e-05, gnorm=1.074, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7977
2023-01-04 23:30:19 - progress_bar.py[line:274] - INFO: epoch 001:   3554 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.9, wpb=111.1, bsz=40, num_updates=3550, lr=3.83121e-05, gnorm=1.06, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7989
2023-01-04 23:30:30 - progress_bar.py[line:274] - INFO: epoch 001:   3564 / 115845 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.4, ups=0.9, wpb=109.4, bsz=40, num_updates=3560, lr=3.842e-05, gnorm=1.039, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8000
2023-01-04 23:30:41 - progress_bar.py[line:274] - INFO: epoch 001:   3574 / 115845 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.8, ups=0.89, wpb=108.2, bsz=40, num_updates=3570, lr=3.8528e-05, gnorm=1.161, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8011
2023-01-04 23:30:52 - progress_bar.py[line:274] - INFO: epoch 001:   3584 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.9, wpb=108, bsz=40, num_updates=3580, lr=3.86359e-05, gnorm=1.152, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8022
2023-01-04 23:31:03 - progress_bar.py[line:274] - INFO: epoch 001:   3594 / 115845 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.7, ups=0.92, wpb=109.6, bsz=40, num_updates=3590, lr=3.87438e-05, gnorm=1.046, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8033
2023-01-04 23:31:15 - progress_bar.py[line:274] - INFO: epoch 001:   3604 / 115845 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=3600, lr=3.88517e-05, gnorm=1.028, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8045
2023-01-04 23:31:26 - progress_bar.py[line:274] - INFO: epoch 001:   3614 / 115845 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=3610, lr=3.89596e-05, gnorm=1.048, clip=50, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=8056
2023-01-04 23:31:37 - progress_bar.py[line:274] - INFO: epoch 001:   3624 / 115845 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.2, ups=0.9, wpb=109.7, bsz=40, num_updates=3620, lr=3.90676e-05, gnorm=1.038, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8067
2023-01-04 23:31:49 - progress_bar.py[line:274] - INFO: epoch 001:   3634 / 115845 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.2, ups=0.89, wpb=108.4, bsz=40, num_updates=3630, lr=3.91755e-05, gnorm=1.002, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8078
2023-01-04 23:32:00 - progress_bar.py[line:274] - INFO: epoch 001:   3644 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.88, wpb=108.1, bsz=40, num_updates=3640, lr=3.92834e-05, gnorm=1.085, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8090
2023-01-04 23:32:12 - progress_bar.py[line:274] - INFO: epoch 001:   3654 / 115845 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.5, ups=0.88, wpb=108.1, bsz=40, num_updates=3650, lr=3.93913e-05, gnorm=1.056, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8101
2023-01-04 23:32:23 - progress_bar.py[line:274] - INFO: epoch 001:   3664 / 115845 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.5, ups=0.9, wpb=110.4, bsz=40, num_updates=3660, lr=3.94992e-05, gnorm=0.999, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8112
2023-01-04 23:32:34 - progress_bar.py[line:274] - INFO: epoch 001:   3674 / 115845 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.9, ups=0.93, wpb=109.3, bsz=40, num_updates=3670, lr=3.96072e-05, gnorm=1.001, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8123
2023-01-04 23:32:45 - progress_bar.py[line:274] - INFO: epoch 001:   3684 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.9, wpb=109.9, bsz=40, num_updates=3680, lr=3.97151e-05, gnorm=1.12, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8134
2023-01-04 23:32:56 - progress_bar.py[line:274] - INFO: epoch 001:   3694 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=3690, lr=3.9823e-05, gnorm=1.07, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8146
2023-01-04 23:33:07 - progress_bar.py[line:274] - INFO: epoch 001:   3704 / 115845 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.9, ups=0.92, wpb=109.2, bsz=40, num_updates=3700, lr=3.99309e-05, gnorm=1.092, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8157
2023-01-04 23:33:18 - progress_bar.py[line:274] - INFO: epoch 001:   3714 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.3, ups=0.93, wpb=109.7, bsz=40, num_updates=3710, lr=4.00389e-05, gnorm=1.021, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8167
2023-01-04 23:33:29 - progress_bar.py[line:274] - INFO: epoch 001:   3724 / 115845 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.6, ups=0.93, wpb=109.7, bsz=40, num_updates=3720, lr=4.01468e-05, gnorm=1.001, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8178
2023-01-04 23:33:40 - progress_bar.py[line:274] - INFO: epoch 001:   3734 / 115845 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=3730, lr=4.02547e-05, gnorm=1.003, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8190
2023-01-04 23:33:51 - progress_bar.py[line:274] - INFO: epoch 001:   3744 / 115845 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.6, ups=0.91, wpb=107.6, bsz=40, num_updates=3740, lr=4.03626e-05, gnorm=1.141, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8201
2023-01-04 23:34:03 - progress_bar.py[line:274] - INFO: epoch 001:   3754 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.88, wpb=109.5, bsz=40, num_updates=3750, lr=4.04705e-05, gnorm=0.962, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8212
2023-01-04 23:34:14 - progress_bar.py[line:274] - INFO: epoch 001:   3764 / 115845 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.2, ups=0.91, wpb=109.3, bsz=40, num_updates=3760, lr=4.05785e-05, gnorm=0.996, clip=50, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=8223
2023-01-04 23:34:20 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-04 23:34:26 - progress_bar.py[line:274] - INFO: epoch 001:   3775 / 115845 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=90.5, ups=0.84, wpb=108.3, bsz=40, num_updates=3770, lr=4.06864e-05, gnorm=1.093, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=8235
2023-01-04 23:34:37 - progress_bar.py[line:274] - INFO: epoch 001:   3785 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.88, wpb=110.2, bsz=40, num_updates=3780, lr=4.07943e-05, gnorm=1.038, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8247
2023-01-04 23:34:48 - progress_bar.py[line:274] - INFO: epoch 001:   3795 / 115845 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.5, ups=0.89, wpb=108.2, bsz=40, num_updates=3790, lr=4.09022e-05, gnorm=0.971, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8258
2023-01-04 23:35:00 - progress_bar.py[line:274] - INFO: epoch 001:   3805 / 115845 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.3, ups=0.89, wpb=108.9, bsz=40, num_updates=3800, lr=4.10101e-05, gnorm=0.98, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8269
2023-01-04 23:35:11 - progress_bar.py[line:274] - INFO: epoch 001:   3815 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=3810, lr=4.11181e-05, gnorm=1.069, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8281
2023-01-04 23:35:22 - progress_bar.py[line:274] - INFO: epoch 001:   3825 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.9, wpb=108.5, bsz=40, num_updates=3820, lr=4.1226e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8292
2023-01-04 23:35:33 - progress_bar.py[line:274] - INFO: epoch 001:   3835 / 115845 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.5, ups=0.91, wpb=107.8, bsz=40, num_updates=3830, lr=4.13339e-05, gnorm=1.163, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8303
2023-01-04 23:35:44 - progress_bar.py[line:274] - INFO: epoch 001:   3845 / 115845 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97, ups=0.89, wpb=108.9, bsz=40, num_updates=3840, lr=4.14418e-05, gnorm=1.087, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8314
2023-01-04 23:35:56 - progress_bar.py[line:274] - INFO: epoch 001:   3855 / 115845 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.5, ups=0.9, wpb=108.9, bsz=40, num_updates=3850, lr=4.15498e-05, gnorm=0.989, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8325
2023-01-04 23:36:07 - progress_bar.py[line:274] - INFO: epoch 001:   3865 / 115845 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.5, ups=0.91, wpb=108.7, bsz=40, num_updates=3860, lr=4.16577e-05, gnorm=0.888, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8336
2023-01-04 23:36:18 - progress_bar.py[line:274] - INFO: epoch 001:   3875 / 115845 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98, ups=0.91, wpb=108.1, bsz=40, num_updates=3870, lr=4.17656e-05, gnorm=1.091, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8348
2023-01-04 23:36:29 - progress_bar.py[line:274] - INFO: epoch 001:   3885 / 115845 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.7, ups=0.93, wpb=109, bsz=40, num_updates=3880, lr=4.18735e-05, gnorm=1.052, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=8358
2023-01-04 23:36:40 - progress_bar.py[line:274] - INFO: epoch 001:   3895 / 115845 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.2, ups=0.9, wpb=107.5, bsz=40, num_updates=3890, lr=4.19814e-05, gnorm=1.111, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8370
2023-01-04 23:36:51 - progress_bar.py[line:274] - INFO: epoch 001:   3905 / 115845 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.8, ups=0.89, wpb=109.3, bsz=40, num_updates=3900, lr=4.20894e-05, gnorm=1.066, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8381
2023-01-04 23:37:02 - progress_bar.py[line:274] - INFO: epoch 001:   3915 / 115845 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.4, ups=0.91, wpb=108.5, bsz=40, num_updates=3910, lr=4.21973e-05, gnorm=1.043, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=8392
2023-01-04 23:37:13 - progress_bar.py[line:274] - INFO: epoch 001:   3925 / 115845 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.3, ups=0.91, wpb=109.4, bsz=40, num_updates=3920, lr=4.23052e-05, gnorm=1.006, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8403
2023-01-04 23:37:25 - progress_bar.py[line:274] - INFO: epoch 001:   3935 / 115845 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.4, ups=0.91, wpb=108.6, bsz=40, num_updates=3930, lr=4.24131e-05, gnorm=1.102, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8414
2023-01-04 23:37:36 - progress_bar.py[line:274] - INFO: epoch 001:   3945 / 115845 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.5, ups=0.89, wpb=109.8, bsz=40, num_updates=3940, lr=4.2521e-05, gnorm=1.005, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8426
2023-01-04 23:37:47 - progress_bar.py[line:274] - INFO: epoch 001:   3955 / 115845 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.9, ups=0.88, wpb=110.5, bsz=40, num_updates=3950, lr=4.2629e-05, gnorm=0.93, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8437
2023-01-04 23:37:59 - progress_bar.py[line:274] - INFO: epoch 001:   3965 / 115845 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=3960, lr=4.27369e-05, gnorm=1.101, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8448
2023-01-04 23:38:10 - progress_bar.py[line:274] - INFO: epoch 001:   3975 / 115845 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.7, ups=0.91, wpb=108.3, bsz=40, num_updates=3970, lr=4.28448e-05, gnorm=0.975, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8459
2023-01-04 23:38:21 - progress_bar.py[line:274] - INFO: epoch 001:   3985 / 115845 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.1, ups=0.9, wpb=109.6, bsz=40, num_updates=3980, lr=4.29527e-05, gnorm=0.94, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8471
2023-01-04 23:38:32 - progress_bar.py[line:274] - INFO: epoch 001:   3995 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=3990, lr=4.30607e-05, gnorm=1.006, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8482
2023-01-04 23:38:44 - progress_bar.py[line:274] - INFO: epoch 001:   4005 / 115845 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.2, ups=0.89, wpb=109.3, bsz=40, num_updates=4000, lr=4.31686e-05, gnorm=0.946, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8493
2023-01-04 23:38:44 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-04 23:38:45 - train.py[line:549] - INFO: 0 / 4988
2023-01-04 23:38:45 - train.py[line:551] - INFO: load:1.00 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-04 23:41:18 - train.py[line:549] - INFO: 200 / 4988
2023-01-04 23:41:18 - train.py[line:551] - INFO: load:1.02 valid_run:152.92 task_valid:148.77 collect_output:3.07
2023-01-04 23:43:46 - train.py[line:549] - INFO: 400 / 4988
2023-01-04 23:43:46 - train.py[line:551] - INFO: load:1.05 valid_run:301.25 task_valid:291.53 collect_output:7.60
2023-01-04 23:46:19 - train.py[line:549] - INFO: 600 / 4988
2023-01-04 23:46:19 - train.py[line:551] - INFO: load:1.07 valid_run:453.64 task_valid:434.27 collect_output:16.21
2023-01-04 23:48:48 - train.py[line:549] - INFO: 800 / 4988
2023-01-04 23:48:48 - train.py[line:551] - INFO: load:1.10 valid_run:602.52 task_valid:579.24 collect_output:19.08
2023-01-04 23:51:20 - train.py[line:549] - INFO: 1000 / 4988
2023-01-04 23:51:20 - train.py[line:551] - INFO: load:1.12 valid_run:754.71 task_valid:726.54 collect_output:22.92
2023-01-04 23:53:51 - train.py[line:549] - INFO: 1200 / 4988
2023-01-04 23:53:51 - train.py[line:551] - INFO: load:1.15 valid_run:906.02 task_valid:871.97 collect_output:27.78
2023-01-04 23:56:24 - train.py[line:549] - INFO: 1400 / 4988
2023-01-04 23:56:24 - train.py[line:551] - INFO: load:1.18 valid_run:1058.76 task_valid:1017.73 collect_output:33.72
2023-01-04 23:58:55 - train.py[line:549] - INFO: 1600 / 4988
2023-01-04 23:58:55 - train.py[line:551] - INFO: load:1.20 valid_run:1209.56 task_valid:1158.76 collect_output:42.46
2023-01-05 00:01:25 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 00:01:25 - train.py[line:551] - INFO: load:1.23 valid_run:1359.08 task_valid:1303.40 collect_output:46.31
2023-01-05 00:03:53 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 00:03:53 - train.py[line:551] - INFO: load:1.25 valid_run:1507.26 task_valid:1446.36 collect_output:50.51
2023-01-05 00:06:22 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 00:06:22 - train.py[line:551] - INFO: load:1.28 valid_run:1656.57 task_valid:1591.05 collect_output:54.12
2023-01-05 00:08:52 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 00:08:52 - train.py[line:551] - INFO: load:1.31 valid_run:1806.24 task_valid:1735.99 collect_output:57.83
2023-01-05 00:11:22 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 00:11:22 - train.py[line:551] - INFO: load:1.33 valid_run:1955.97 task_valid:1877.72 collect_output:64.80
2023-01-05 00:13:52 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 00:13:52 - train.py[line:551] - INFO: load:1.36 valid_run:2106.31 task_valid:2023.05 collect_output:68.76
2023-01-05 00:16:22 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 00:16:22 - train.py[line:551] - INFO: load:1.39 valid_run:2256.14 task_valid:2169.46 collect_output:71.16
2023-01-05 00:18:52 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 00:18:52 - train.py[line:551] - INFO: load:1.41 valid_run:2405.77 task_valid:2313.55 collect_output:75.64
2023-01-05 00:21:24 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 00:21:24 - train.py[line:551] - INFO: load:1.44 valid_run:2557.13 task_valid:2458.85 collect_output:80.66
2023-01-05 00:23:54 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 00:23:54 - train.py[line:551] - INFO: load:1.47 valid_run:2707.34 task_valid:2605.64 collect_output:83.06
2023-01-05 00:26:22 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 00:26:22 - train.py[line:551] - INFO: load:1.50 valid_run:2855.38 task_valid:2747.19 collect_output:88.48
2023-01-05 00:28:52 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 00:28:52 - train.py[line:551] - INFO: load:1.52 valid_run:3005.54 task_valid:2892.40 collect_output:92.37
2023-01-05 00:31:24 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 00:31:24 - train.py[line:551] - INFO: load:1.55 valid_run:3156.93 task_valid:3036.81 collect_output:98.29
2023-01-05 00:33:53 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 00:33:53 - train.py[line:551] - INFO: load:1.58 valid_run:3305.81 task_valid:3181.13 collect_output:101.82
2023-01-05 00:36:23 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 00:36:23 - train.py[line:551] - INFO: load:1.61 valid_run:3456.43 task_valid:3327.03 collect_output:105.52
2023-01-05 00:38:55 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 00:38:55 - train.py[line:551] - INFO: load:1.63 valid_run:3607.58 task_valid:3473.52 collect_output:109.14

====================================================================================================
SGG eval:     R @ 50: 0.5215;     R @ 100: 0.5692;     R @ 500: 0.6108;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3201;    mR @ 100: 0.4030;    mR @ 500: 0.4554;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6171) (covered in:0.1875) (covering:0.5714) (eating:0.7647) (flying in:0.7273) (growing on:0.5000) (hanging from:0.3871) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7583) (playing:0.0000) (riding:0.8644) (says:0.0000) (sitting on:0.6474) (standing on:0.2133) (using:0.4000) (walking in:0.0000) (walking on:0.7027) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-01-05 00:41:25 - train.py[line:487] - INFO: 0.5692062897886428

====================================================================================================
SGG eval:     R @ 50: 0.5215;     R @ 100: 0.5692;     R @ 500: 0.6108;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3201;    mR @ 100: 0.4030;    mR @ 500: 0.4554;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6171) (covered in:0.1875) (covering:0.5714) (eating:0.7647) (flying in:0.7273) (growing on:0.5000) (hanging from:0.3871) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7583) (playing:0.0000) (riding:0.8644) (says:0.0000) (sitting on:0.6474) (standing on:0.2133) (using:0.4000) (walking in:0.0000) (walking on:0.7027) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-01-05 00:41:25 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 00:41:25 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.372 | loss_v1 0 | loss_v2 0 | nll_loss 0.224 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.569206 | ppl 1.17 | vqa_score 0.3851 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 4000 | best_R@100 0.569206
2023-01-05 00:41:25 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-01-05 00:41:25 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-05 00:42:05 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-05 00:44:52 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.5692062897886428) (writing took 206.19476812705398 seconds)
2023-01-05 00:45:03 - progress_bar.py[line:274] - INFO: epoch 001:   4015 / 115845 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=0.3, ups=0, wpb=109, bsz=40, num_updates=4010, lr=4.32765e-05, gnorm=0.934, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12473
2023-01-05 00:45:14 - progress_bar.py[line:274] - INFO: epoch 001:   4025 / 115845 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.92, wpb=109.1, bsz=40, num_updates=4020, lr=4.33844e-05, gnorm=1.087, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12484
2023-01-05 00:45:25 - progress_bar.py[line:274] - INFO: epoch 001:   4035 / 115845 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.9, ups=0.92, wpb=108.8, bsz=40, num_updates=4030, lr=4.34923e-05, gnorm=1.005, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12495
2023-01-05 00:45:36 - progress_bar.py[line:274] - INFO: epoch 001:   4045 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.6, ups=0.88, wpb=108.7, bsz=40, num_updates=4040, lr=4.36003e-05, gnorm=1.069, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12506
2023-01-05 00:45:48 - progress_bar.py[line:274] - INFO: epoch 001:   4055 / 115845 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=4050, lr=4.37082e-05, gnorm=0.971, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12517
2023-01-05 00:45:59 - progress_bar.py[line:274] - INFO: epoch 001:   4065 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.89, wpb=108.3, bsz=40, num_updates=4060, lr=4.38161e-05, gnorm=0.918, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12529
2023-01-05 00:46:11 - progress_bar.py[line:274] - INFO: epoch 001:   4075 / 115845 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.9, ups=0.87, wpb=110.3, bsz=40, num_updates=4070, lr=4.3924e-05, gnorm=1.051, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12540
2023-01-05 00:46:22 - progress_bar.py[line:274] - INFO: epoch 001:   4085 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97, ups=0.89, wpb=109, bsz=40, num_updates=4080, lr=4.40319e-05, gnorm=0.971, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12552
2023-01-05 00:46:33 - progress_bar.py[line:274] - INFO: epoch 001:   4095 / 115845 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.8, ups=0.9, wpb=109.4, bsz=40, num_updates=4090, lr=4.41399e-05, gnorm=0.978, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12563
2023-01-05 00:46:44 - progress_bar.py[line:274] - INFO: epoch 001:   4105 / 115845 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.8, ups=0.91, wpb=108.8, bsz=40, num_updates=4100, lr=4.42478e-05, gnorm=0.977, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12574
2023-01-05 00:46:55 - progress_bar.py[line:274] - INFO: epoch 001:   4115 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.9, ups=0.91, wpb=110.3, bsz=40, num_updates=4110, lr=4.43557e-05, gnorm=1.132, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12585
2023-01-05 00:47:06 - progress_bar.py[line:274] - INFO: epoch 001:   4125 / 115845 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.7, ups=0.9, wpb=107.5, bsz=40, num_updates=4120, lr=4.44636e-05, gnorm=0.943, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12596
2023-01-05 00:47:18 - progress_bar.py[line:274] - INFO: epoch 001:   4135 / 115845 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=4130, lr=4.45716e-05, gnorm=1.039, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12607
2023-01-05 00:47:29 - progress_bar.py[line:274] - INFO: epoch 001:   4145 / 115845 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.6, ups=0.89, wpb=108.8, bsz=40, num_updates=4140, lr=4.46795e-05, gnorm=1.025, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12619
2023-01-05 00:47:40 - progress_bar.py[line:274] - INFO: epoch 001:   4155 / 115845 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=95.6, ups=0.88, wpb=109.1, bsz=40, num_updates=4150, lr=4.47874e-05, gnorm=0.994, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12630
2023-01-05 00:47:51 - progress_bar.py[line:274] - INFO: epoch 001:   4165 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.91, wpb=108.6, bsz=40, num_updates=4160, lr=4.48953e-05, gnorm=0.901, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=12641
2023-01-05 00:48:02 - progress_bar.py[line:274] - INFO: epoch 001:   4175 / 115845 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.4, ups=0.92, wpb=110.5, bsz=40, num_updates=4170, lr=4.50032e-05, gnorm=0.986, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12652
2023-01-05 00:48:14 - progress_bar.py[line:274] - INFO: epoch 001:   4185 / 115845 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96, ups=0.88, wpb=108.6, bsz=40, num_updates=4180, lr=4.51112e-05, gnorm=0.99, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12664
2023-01-05 00:48:25 - progress_bar.py[line:274] - INFO: epoch 001:   4195 / 115845 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.9, ups=0.89, wpb=108.3, bsz=40, num_updates=4190, lr=4.52191e-05, gnorm=0.901, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12675
2023-01-05 00:48:36 - progress_bar.py[line:274] - INFO: epoch 001:   4205 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.3, ups=0.92, wpb=108.9, bsz=40, num_updates=4200, lr=4.5327e-05, gnorm=0.954, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12686
2023-01-05 00:48:47 - progress_bar.py[line:274] - INFO: epoch 001:   4215 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.91, wpb=109.1, bsz=40, num_updates=4210, lr=4.54349e-05, gnorm=0.823, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12697
2023-01-05 00:48:58 - progress_bar.py[line:274] - INFO: epoch 001:   4225 / 115845 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.4, ups=0.92, wpb=107.8, bsz=40, num_updates=4220, lr=4.55428e-05, gnorm=1.051, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12708
2023-01-05 00:49:09 - progress_bar.py[line:274] - INFO: epoch 001:   4235 / 115845 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.1, ups=0.92, wpb=108.9, bsz=40, num_updates=4230, lr=4.56508e-05, gnorm=0.979, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12719
2023-01-05 00:49:20 - progress_bar.py[line:274] - INFO: epoch 001:   4245 / 115845 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.8, ups=0.91, wpb=107.8, bsz=40, num_updates=4240, lr=4.57587e-05, gnorm=0.816, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12730
2023-01-05 00:49:31 - progress_bar.py[line:274] - INFO: epoch 001:   4255 / 115845 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.8, ups=0.91, wpb=109.1, bsz=40, num_updates=4250, lr=4.58666e-05, gnorm=0.859, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12741
2023-01-05 00:49:42 - progress_bar.py[line:274] - INFO: epoch 001:   4265 / 115845 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.6, ups=0.91, wpb=108.3, bsz=40, num_updates=4260, lr=4.59745e-05, gnorm=0.953, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12752
2023-01-05 00:49:54 - progress_bar.py[line:274] - INFO: epoch 001:   4275 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.9, wpb=107.9, bsz=40, num_updates=4270, lr=4.60825e-05, gnorm=0.891, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12763
2023-01-05 00:50:05 - progress_bar.py[line:274] - INFO: epoch 001:   4285 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=4280, lr=4.61904e-05, gnorm=0.931, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12774
2023-01-05 00:50:16 - progress_bar.py[line:274] - INFO: epoch 001:   4295 / 115845 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.8, ups=0.89, wpb=108.2, bsz=40, num_updates=4290, lr=4.62983e-05, gnorm=0.945, clip=50, loss_scale=1024, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=12786
2023-01-05 00:50:27 - progress_bar.py[line:274] - INFO: epoch 001:   4305 / 115845 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=4300, lr=4.64062e-05, gnorm=0.901, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12797
2023-01-05 00:50:37 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 00:50:40 - progress_bar.py[line:274] - INFO: epoch 001:   4316 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=89.5, ups=0.82, wpb=109.3, bsz=40, num_updates=4310, lr=4.65141e-05, gnorm=0.925, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=12809
2023-01-05 00:50:51 - progress_bar.py[line:274] - INFO: epoch 001:   4326 / 115845 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.3, ups=0.91, wpb=108.9, bsz=40, num_updates=4320, lr=4.66221e-05, gnorm=0.929, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12820
2023-01-05 00:51:02 - progress_bar.py[line:274] - INFO: epoch 001:   4336 / 115845 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=4330, lr=4.673e-05, gnorm=0.948, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12831
2023-01-05 00:51:13 - progress_bar.py[line:274] - INFO: epoch 001:   4346 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.8, ups=0.87, wpb=108.6, bsz=40, num_updates=4340, lr=4.68379e-05, gnorm=0.987, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12843
2023-01-05 00:51:24 - progress_bar.py[line:274] - INFO: epoch 001:   4356 / 115845 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.3, ups=0.9, wpb=109.2, bsz=40, num_updates=4350, lr=4.69458e-05, gnorm=1.054, clip=60, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12854
2023-01-05 00:51:36 - progress_bar.py[line:274] - INFO: epoch 001:   4366 / 115845 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.4, ups=0.91, wpb=108.4, bsz=40, num_updates=4360, lr=4.70537e-05, gnorm=0.922, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12865
2023-01-05 00:51:46 - progress_bar.py[line:274] - INFO: epoch 001:   4376 / 115845 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=106.9, ups=0.97, wpb=110.4, bsz=40, num_updates=4370, lr=4.71617e-05, gnorm=0.968, clip=50, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=12876
2023-01-05 00:51:57 - progress_bar.py[line:274] - INFO: epoch 001:   4386 / 115845 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.6, ups=0.93, wpb=109.7, bsz=40, num_updates=4380, lr=4.72696e-05, gnorm=0.991, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12886
2023-01-05 00:52:08 - progress_bar.py[line:274] - INFO: epoch 001:   4396 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=4390, lr=4.73775e-05, gnorm=0.909, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12898
2023-01-05 00:52:19 - progress_bar.py[line:274] - INFO: epoch 001:   4406 / 115845 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=94.8, ups=0.88, wpb=107.5, bsz=40, num_updates=4400, lr=4.74854e-05, gnorm=1.016, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12909
2023-01-05 00:52:31 - progress_bar.py[line:274] - INFO: epoch 001:   4416 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.8, ups=0.89, wpb=108.2, bsz=40, num_updates=4410, lr=4.75934e-05, gnorm=0.842, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=12920
2023-01-05 00:52:42 - progress_bar.py[line:274] - INFO: epoch 001:   4426 / 115845 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.7, ups=0.91, wpb=108.7, bsz=40, num_updates=4420, lr=4.77013e-05, gnorm=0.791, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12932
2023-01-05 00:52:53 - progress_bar.py[line:274] - INFO: epoch 001:   4436 / 115845 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.9, ups=0.9, wpb=108.2, bsz=40, num_updates=4430, lr=4.78092e-05, gnorm=0.888, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12943
2023-01-05 00:53:04 - progress_bar.py[line:274] - INFO: epoch 001:   4446 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=108.6, bsz=40, num_updates=4440, lr=4.79171e-05, gnorm=0.925, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12954
2023-01-05 00:53:16 - progress_bar.py[line:274] - INFO: epoch 001:   4456 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.87, wpb=109.3, bsz=40, num_updates=4450, lr=4.8025e-05, gnorm=0.877, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12966
2023-01-05 00:53:27 - progress_bar.py[line:274] - INFO: epoch 001:   4466 / 115845 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=94.6, ups=0.87, wpb=108.5, bsz=40, num_updates=4460, lr=4.8133e-05, gnorm=0.933, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=12977
2023-01-05 00:53:39 - progress_bar.py[line:274] - INFO: epoch 001:   4476 / 115845 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.8, ups=0.91, wpb=107.7, bsz=40, num_updates=4470, lr=4.82409e-05, gnorm=0.877, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12988
2023-01-05 00:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   4486 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98, ups=0.9, wpb=109.3, bsz=40, num_updates=4480, lr=4.83488e-05, gnorm=0.859, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12999
2023-01-05 00:54:01 - progress_bar.py[line:274] - INFO: epoch 001:   4496 / 115845 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.3, ups=0.91, wpb=110.3, bsz=40, num_updates=4490, lr=4.84567e-05, gnorm=0.924, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13011
2023-01-05 00:54:12 - progress_bar.py[line:274] - INFO: epoch 001:   4506 / 115845 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.93, wpb=108.3, bsz=40, num_updates=4500, lr=4.85646e-05, gnorm=0.93, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13021
2023-01-05 00:54:23 - progress_bar.py[line:274] - INFO: epoch 001:   4516 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.89, wpb=107.7, bsz=40, num_updates=4510, lr=4.86726e-05, gnorm=0.813, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13033
2023-01-05 00:54:34 - progress_bar.py[line:274] - INFO: epoch 001:   4526 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.1, ups=0.91, wpb=108.3, bsz=40, num_updates=4520, lr=4.87805e-05, gnorm=0.825, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13044
2023-01-05 00:54:45 - progress_bar.py[line:274] - INFO: epoch 001:   4536 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.91, wpb=108.7, bsz=40, num_updates=4530, lr=4.88884e-05, gnorm=0.843, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13055
2023-01-05 00:54:56 - progress_bar.py[line:274] - INFO: epoch 001:   4546 / 115845 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.1, ups=0.91, wpb=108.2, bsz=40, num_updates=4540, lr=4.89963e-05, gnorm=0.951, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13066
2023-01-05 00:55:08 - progress_bar.py[line:274] - INFO: epoch 001:   4556 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=94.2, ups=0.87, wpb=108.5, bsz=40, num_updates=4550, lr=4.91043e-05, gnorm=0.764, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13078
2023-01-05 00:55:19 - progress_bar.py[line:274] - INFO: epoch 001:   4566 / 115845 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98, ups=0.9, wpb=109.3, bsz=40, num_updates=4560, lr=4.92122e-05, gnorm=0.823, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13089
2023-01-05 00:55:31 - progress_bar.py[line:274] - INFO: epoch 001:   4576 / 115845 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=95.9, ups=0.88, wpb=108.4, bsz=40, num_updates=4570, lr=4.93201e-05, gnorm=0.894, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13100
2023-01-05 00:55:41 - progress_bar.py[line:274] - INFO: epoch 001:   4586 / 115845 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.5, ups=0.93, wpb=110.6, bsz=40, num_updates=4580, lr=4.9428e-05, gnorm=0.814, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13111
2023-01-05 00:55:53 - progress_bar.py[line:274] - INFO: epoch 001:   4596 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.89, wpb=108.9, bsz=40, num_updates=4590, lr=4.95359e-05, gnorm=0.799, clip=20, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=13122
2023-01-05 00:56:04 - progress_bar.py[line:274] - INFO: epoch 001:   4606 / 115845 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.3, ups=0.9, wpb=108.4, bsz=40, num_updates=4600, lr=4.96439e-05, gnorm=0.852, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13134
2023-01-05 00:56:15 - progress_bar.py[line:274] - INFO: epoch 001:   4616 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=106.7, nsentences=40, sample_size=106.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.7, ups=0.91, wpb=106.7, bsz=40, num_updates=4610, lr=4.97518e-05, gnorm=0.861, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13145
2023-01-05 00:56:26 - progress_bar.py[line:274] - INFO: epoch 001:   4626 / 115845 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.3, ups=0.92, wpb=109.2, bsz=40, num_updates=4620, lr=4.98597e-05, gnorm=0.884, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13156
2023-01-05 00:56:37 - progress_bar.py[line:274] - INFO: epoch 001:   4636 / 115845 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=4630, lr=4.99676e-05, gnorm=0.842, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13167
2023-01-05 00:56:48 - progress_bar.py[line:274] - INFO: epoch 001:   4646 / 115845 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.1, ups=0.9, wpb=108.6, bsz=40, num_updates=4640, lr=4.99969e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13178
2023-01-05 00:56:59 - progress_bar.py[line:274] - INFO: epoch 001:   4656 / 115845 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.3, ups=0.92, wpb=109.1, bsz=40, num_updates=4650, lr=4.99924e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13189
2023-01-05 00:57:10 - progress_bar.py[line:274] - INFO: epoch 001:   4666 / 115845 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.6, ups=0.93, wpb=109.7, bsz=40, num_updates=4660, lr=4.99879e-05, gnorm=0.933, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13200
2023-01-05 00:57:21 - progress_bar.py[line:274] - INFO: epoch 001:   4676 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.9, wpb=109.7, bsz=40, num_updates=4670, lr=4.99834e-05, gnorm=0.846, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13211
2023-01-05 00:57:32 - progress_bar.py[line:274] - INFO: epoch 001:   4686 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.4, ups=0.92, wpb=108.1, bsz=40, num_updates=4680, lr=4.99789e-05, gnorm=0.875, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13222
2023-01-05 00:57:44 - progress_bar.py[line:274] - INFO: epoch 001:   4696 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.89, wpb=109.9, bsz=40, num_updates=4690, lr=4.99744e-05, gnorm=0.876, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13233
2023-01-05 00:57:55 - progress_bar.py[line:274] - INFO: epoch 001:   4706 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.8, ups=0.88, wpb=108.1, bsz=40, num_updates=4700, lr=4.99699e-05, gnorm=0.963, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13245
2023-01-05 00:58:06 - progress_bar.py[line:274] - INFO: epoch 001:   4716 / 115845 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.6, ups=0.91, wpb=109.9, bsz=40, num_updates=4710, lr=4.99654e-05, gnorm=0.852, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13256
2023-01-05 00:58:17 - progress_bar.py[line:274] - INFO: epoch 001:   4726 / 115845 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.3, ups=0.91, wpb=108.7, bsz=40, num_updates=4720, lr=4.99609e-05, gnorm=0.91, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13267
2023-01-05 00:58:28 - progress_bar.py[line:274] - INFO: epoch 001:   4736 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.89, wpb=107.8, bsz=40, num_updates=4730, lr=4.99564e-05, gnorm=0.942, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13278
2023-01-05 00:58:40 - progress_bar.py[line:274] - INFO: epoch 001:   4746 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.4, ups=0.89, wpb=108.9, bsz=40, num_updates=4740, lr=4.99519e-05, gnorm=0.812, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13289
2023-01-05 00:58:51 - progress_bar.py[line:274] - INFO: epoch 001:   4756 / 115845 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.9, ups=0.91, wpb=110, bsz=40, num_updates=4750, lr=4.99474e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13301
2023-01-05 00:59:02 - progress_bar.py[line:274] - INFO: epoch 001:   4766 / 115845 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.2, ups=0.92, wpb=110.1, bsz=40, num_updates=4760, lr=4.99429e-05, gnorm=0.855, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13312
2023-01-05 00:59:13 - progress_bar.py[line:274] - INFO: epoch 001:   4776 / 115845 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.6, ups=0.88, wpb=109.5, bsz=40, num_updates=4770, lr=4.99384e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13323
2023-01-05 00:59:24 - progress_bar.py[line:274] - INFO: epoch 001:   4786 / 115845 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.4, ups=0.91, wpb=107.5, bsz=40, num_updates=4780, lr=4.99339e-05, gnorm=0.798, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13334
2023-01-05 00:59:36 - progress_bar.py[line:274] - INFO: epoch 001:   4796 / 115845 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=95.3, ups=0.88, wpb=108.1, bsz=40, num_updates=4790, lr=4.99294e-05, gnorm=0.853, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13346
2023-01-05 00:59:47 - progress_bar.py[line:274] - INFO: epoch 001:   4806 / 115845 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.5, ups=0.89, wpb=109.7, bsz=40, num_updates=4800, lr=4.99249e-05, gnorm=0.846, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13357
2023-01-05 00:59:59 - progress_bar.py[line:274] - INFO: epoch 001:   4816 / 115845 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.5, ups=0.88, wpb=109.3, bsz=40, num_updates=4810, lr=4.99204e-05, gnorm=0.873, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13368
2023-01-05 01:00:10 - progress_bar.py[line:274] - INFO: epoch 001:   4826 / 115845 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.9, ups=0.92, wpb=108.9, bsz=40, num_updates=4820, lr=4.99159e-05, gnorm=0.884, clip=10, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=13379
2023-01-05 01:00:21 - progress_bar.py[line:274] - INFO: epoch 001:   4836 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.88, wpb=110.2, bsz=40, num_updates=4830, lr=4.99114e-05, gnorm=0.776, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13391
2023-01-05 01:00:32 - progress_bar.py[line:274] - INFO: epoch 001:   4846 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.3, ups=0.92, wpb=108, bsz=40, num_updates=4840, lr=4.99069e-05, gnorm=0.892, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13402
2023-01-05 01:00:43 - progress_bar.py[line:274] - INFO: epoch 001:   4856 / 115845 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=93.6, ups=0.87, wpb=107.5, bsz=40, num_updates=4850, lr=4.99024e-05, gnorm=0.902, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13413
2023-01-05 01:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   4866 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.3, ups=0.87, wpb=108.1, bsz=40, num_updates=4860, lr=4.98979e-05, gnorm=0.845, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13425
2023-01-05 01:01:05 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 01:01:07 - progress_bar.py[line:274] - INFO: epoch 001:   4877 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=90.1, ups=0.84, wpb=107.4, bsz=40, num_updates=4870, lr=4.98934e-05, gnorm=0.825, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13437
2023-01-05 01:01:18 - progress_bar.py[line:274] - INFO: epoch 001:   4887 / 115845 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=4880, lr=4.9889e-05, gnorm=0.868, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13448
2023-01-05 01:01:29 - progress_bar.py[line:274] - INFO: epoch 001:   4897 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.8, ups=0.9, wpb=108.6, bsz=40, num_updates=4890, lr=4.98845e-05, gnorm=0.93, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13459
2023-01-05 01:01:41 - progress_bar.py[line:274] - INFO: epoch 001:   4907 / 115845 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.8, ups=0.89, wpb=109.4, bsz=40, num_updates=4900, lr=4.988e-05, gnorm=0.803, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13470
2023-01-05 01:01:51 - progress_bar.py[line:274] - INFO: epoch 001:   4917 / 115845 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.8, ups=0.94, wpb=109.8, bsz=40, num_updates=4910, lr=4.98755e-05, gnorm=0.874, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13481
2023-01-05 01:02:03 - progress_bar.py[line:274] - INFO: epoch 001:   4927 / 115845 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.1, ups=0.91, wpb=108.2, bsz=40, num_updates=4920, lr=4.9871e-05, gnorm=0.837, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13492
2023-01-05 01:02:14 - progress_bar.py[line:274] - INFO: epoch 001:   4937 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.9, wpb=109.3, bsz=40, num_updates=4930, lr=4.98665e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13503
2023-01-05 01:02:25 - progress_bar.py[line:274] - INFO: epoch 001:   4947 / 115845 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.5, ups=0.9, wpb=108.9, bsz=40, num_updates=4940, lr=4.9862e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13515
2023-01-05 01:02:36 - progress_bar.py[line:274] - INFO: epoch 001:   4957 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.9, wpb=108.4, bsz=40, num_updates=4950, lr=4.98575e-05, gnorm=0.762, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=13526
2023-01-05 01:02:47 - progress_bar.py[line:274] - INFO: epoch 001:   4967 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.91, wpb=109.2, bsz=40, num_updates=4960, lr=4.9853e-05, gnorm=0.783, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13537
2023-01-05 01:02:58 - progress_bar.py[line:274] - INFO: epoch 001:   4977 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.8, ups=0.94, wpb=107.7, bsz=40, num_updates=4970, lr=4.98485e-05, gnorm=0.962, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13548
2023-01-05 01:03:10 - progress_bar.py[line:274] - INFO: epoch 001:   4987 / 115845 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=93.6, ups=0.86, wpb=109, bsz=40, num_updates=4980, lr=4.9844e-05, gnorm=0.824, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13559
2023-01-05 01:03:21 - progress_bar.py[line:274] - INFO: epoch 001:   4997 / 115845 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.3, ups=0.9, wpb=107.6, bsz=40, num_updates=4990, lr=4.98395e-05, gnorm=0.812, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13571
2023-01-05 01:03:32 - progress_bar.py[line:274] - INFO: epoch 001:   5007 / 115845 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.4, ups=0.9, wpb=107.2, bsz=40, num_updates=5000, lr=4.9835e-05, gnorm=0.827, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13582
2023-01-05 01:03:43 - progress_bar.py[line:274] - INFO: epoch 001:   5017 / 115845 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=5010, lr=4.98305e-05, gnorm=0.83, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13593
2023-01-05 01:03:55 - progress_bar.py[line:274] - INFO: epoch 001:   5027 / 115845 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.7, ups=0.9, wpb=107.3, bsz=40, num_updates=5020, lr=4.9826e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13604
2023-01-05 01:04:05 - progress_bar.py[line:274] - INFO: epoch 001:   5037 / 115845 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.7, ups=0.94, wpb=108.9, bsz=40, num_updates=5030, lr=4.98215e-05, gnorm=0.785, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13615
2023-01-05 01:04:16 - progress_bar.py[line:274] - INFO: epoch 001:   5047 / 115845 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.6, ups=0.94, wpb=110.5, bsz=40, num_updates=5040, lr=4.9817e-05, gnorm=0.831, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13626
2023-01-05 01:04:28 - progress_bar.py[line:274] - INFO: epoch 001:   5057 / 115845 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=92.9, ups=0.84, wpb=110, bsz=40, num_updates=5050, lr=4.98125e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13638
2023-01-05 01:04:39 - progress_bar.py[line:274] - INFO: epoch 001:   5067 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.89, wpb=108.7, bsz=40, num_updates=5060, lr=4.9808e-05, gnorm=0.75, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13649
2023-01-05 01:04:51 - progress_bar.py[line:274] - INFO: epoch 001:   5077 / 115845 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.2, ups=0.87, wpb=110.3, bsz=40, num_updates=5070, lr=4.98035e-05, gnorm=0.769, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13661
2023-01-05 01:05:02 - progress_bar.py[line:274] - INFO: epoch 001:   5087 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.9, ups=0.88, wpb=108.4, bsz=40, num_updates=5080, lr=4.9799e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13672
2023-01-05 01:05:14 - progress_bar.py[line:274] - INFO: epoch 001:   5097 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.9, ups=0.88, wpb=108.3, bsz=40, num_updates=5090, lr=4.97945e-05, gnorm=0.852, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13683
2023-01-05 01:05:25 - progress_bar.py[line:274] - INFO: epoch 001:   5107 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.7, ups=0.87, wpb=109.8, bsz=40, num_updates=5100, lr=4.979e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13695
2023-01-05 01:05:36 - progress_bar.py[line:274] - INFO: epoch 001:   5117 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.91, wpb=109.8, bsz=40, num_updates=5110, lr=4.97855e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=13706
2023-01-05 01:05:47 - progress_bar.py[line:274] - INFO: epoch 001:   5127 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.91, wpb=108.7, bsz=40, num_updates=5120, lr=4.9781e-05, gnorm=0.901, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13717
2023-01-05 01:05:59 - progress_bar.py[line:274] - INFO: epoch 001:   5137 / 115845 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.4, ups=0.88, wpb=109, bsz=40, num_updates=5130, lr=4.97766e-05, gnorm=0.918, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13729
2023-01-05 01:06:10 - progress_bar.py[line:274] - INFO: epoch 001:   5147 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.88, wpb=111.9, bsz=40, num_updates=5140, lr=4.97721e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13740
2023-01-05 01:06:21 - progress_bar.py[line:274] - INFO: epoch 001:   5157 / 115845 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.1, ups=0.93, wpb=107.8, bsz=40, num_updates=5150, lr=4.97676e-05, gnorm=0.836, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13751
2023-01-05 01:06:32 - progress_bar.py[line:274] - INFO: epoch 001:   5167 / 115845 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.4, ups=0.88, wpb=108.1, bsz=40, num_updates=5160, lr=4.97631e-05, gnorm=0.83, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13762
2023-01-05 01:06:44 - progress_bar.py[line:274] - INFO: epoch 001:   5177 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=5170, lr=4.97586e-05, gnorm=0.77, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13773
2023-01-05 01:06:55 - progress_bar.py[line:274] - INFO: epoch 001:   5187 / 115845 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95, ups=0.87, wpb=109.4, bsz=40, num_updates=5180, lr=4.97541e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13785
2023-01-05 01:07:06 - progress_bar.py[line:274] - INFO: epoch 001:   5197 / 115845 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=5190, lr=4.97496e-05, gnorm=0.771, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13796
2023-01-05 01:07:18 - progress_bar.py[line:274] - INFO: epoch 001:   5207 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96, ups=0.88, wpb=108.9, bsz=40, num_updates=5200, lr=4.97451e-05, gnorm=0.748, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13808
2023-01-05 01:07:29 - progress_bar.py[line:274] - INFO: epoch 001:   5217 / 115845 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.7, ups=0.89, wpb=108.4, bsz=40, num_updates=5210, lr=4.97406e-05, gnorm=0.816, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13819
2023-01-05 01:07:40 - progress_bar.py[line:274] - INFO: epoch 001:   5227 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.9, wpb=108.6, bsz=40, num_updates=5220, lr=4.97361e-05, gnorm=0.814, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13830
2023-01-05 01:07:51 - progress_bar.py[line:274] - INFO: epoch 001:   5237 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.9, wpb=108, bsz=40, num_updates=5230, lr=4.97316e-05, gnorm=0.837, clip=20, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=13841
2023-01-05 01:08:02 - progress_bar.py[line:274] - INFO: epoch 001:   5247 / 115845 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.1, ups=0.93, wpb=108.6, bsz=40, num_updates=5240, lr=4.97271e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13852
2023-01-05 01:08:14 - progress_bar.py[line:274] - INFO: epoch 001:   5257 / 115845 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96, ups=0.89, wpb=108.3, bsz=40, num_updates=5250, lr=4.97226e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13863
2023-01-05 01:08:25 - progress_bar.py[line:274] - INFO: epoch 001:   5267 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.91, wpb=109.9, bsz=40, num_updates=5260, lr=4.97181e-05, gnorm=0.908, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13874
2023-01-05 01:08:36 - progress_bar.py[line:274] - INFO: epoch 001:   5277 / 115845 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.8, ups=0.89, wpb=109.4, bsz=40, num_updates=5270, lr=4.97136e-05, gnorm=0.848, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13886
2023-01-05 01:08:47 - progress_bar.py[line:274] - INFO: epoch 001:   5287 / 115845 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.2, ups=0.89, wpb=108.2, bsz=40, num_updates=5280, lr=4.97091e-05, gnorm=0.776, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13897
2023-01-05 01:08:59 - progress_bar.py[line:274] - INFO: epoch 001:   5297 / 115845 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=94.6, ups=0.87, wpb=108.7, bsz=40, num_updates=5290, lr=4.97046e-05, gnorm=0.86, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13909
2023-01-05 01:09:10 - progress_bar.py[line:274] - INFO: epoch 001:   5307 / 115845 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=94.7, ups=0.87, wpb=108.6, bsz=40, num_updates=5300, lr=4.97001e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13920
2023-01-05 01:09:21 - progress_bar.py[line:274] - INFO: epoch 001:   5317 / 115845 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.5, ups=0.94, wpb=109.3, bsz=40, num_updates=5310, lr=4.96956e-05, gnorm=0.785, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13931
2023-01-05 01:09:32 - progress_bar.py[line:274] - INFO: epoch 001:   5327 / 115845 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.4, ups=0.9, wpb=107.8, bsz=40, num_updates=5320, lr=4.96911e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13942
2023-01-05 01:09:43 - progress_bar.py[line:274] - INFO: epoch 001:   5337 / 115845 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.6, ups=0.93, wpb=108.1, bsz=40, num_updates=5330, lr=4.96866e-05, gnorm=0.789, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=13953
2023-01-05 01:09:54 - progress_bar.py[line:274] - INFO: epoch 001:   5347 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.88, wpb=109.3, bsz=40, num_updates=5340, lr=4.96821e-05, gnorm=0.788, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13964
2023-01-05 01:10:06 - progress_bar.py[line:274] - INFO: epoch 001:   5357 / 115845 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=5350, lr=4.96776e-05, gnorm=0.806, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13975
2023-01-05 01:10:17 - progress_bar.py[line:274] - INFO: epoch 001:   5367 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.89, wpb=108.1, bsz=40, num_updates=5360, lr=4.96731e-05, gnorm=0.756, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=13987
2023-01-05 01:10:28 - progress_bar.py[line:274] - INFO: epoch 001:   5377 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.9, wpb=109.8, bsz=40, num_updates=5370, lr=4.96687e-05, gnorm=0.853, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13998
2023-01-05 01:10:39 - progress_bar.py[line:274] - INFO: epoch 001:   5387 / 115845 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.9, ups=0.9, wpb=110.1, bsz=40, num_updates=5380, lr=4.96642e-05, gnorm=0.864, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14009
2023-01-05 01:10:51 - progress_bar.py[line:274] - INFO: epoch 001:   5397 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.3, ups=0.89, wpb=110.2, bsz=40, num_updates=5390, lr=4.96597e-05, gnorm=0.756, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14020
2023-01-05 01:11:02 - progress_bar.py[line:274] - INFO: epoch 001:   5407 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.5, ups=0.88, wpb=108, bsz=40, num_updates=5400, lr=4.96552e-05, gnorm=0.861, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14032
2023-01-05 01:11:13 - progress_bar.py[line:274] - INFO: epoch 001:   5417 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.91, wpb=108.6, bsz=40, num_updates=5410, lr=4.96507e-05, gnorm=0.849, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14043
2023-01-05 01:11:24 - progress_bar.py[line:274] - INFO: epoch 001:   5427 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.1, ups=0.92, wpb=108.7, bsz=40, num_updates=5420, lr=4.96462e-05, gnorm=0.868, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14054
2023-01-05 01:11:35 - progress_bar.py[line:274] - INFO: epoch 001:   5437 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100, ups=0.91, wpb=109.8, bsz=40, num_updates=5430, lr=4.96417e-05, gnorm=0.765, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14065
2023-01-05 01:11:47 - progress_bar.py[line:274] - INFO: epoch 001:   5447 / 115845 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.3, ups=0.88, wpb=108.2, bsz=40, num_updates=5440, lr=4.96372e-05, gnorm=0.697, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14076
2023-01-05 01:11:58 - progress_bar.py[line:274] - INFO: epoch 001:   5457 / 115845 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.7, ups=0.91, wpb=110.1, bsz=40, num_updates=5450, lr=4.96327e-05, gnorm=0.779, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14087
2023-01-05 01:12:09 - progress_bar.py[line:274] - INFO: epoch 001:   5467 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.3, ups=0.93, wpb=108.6, bsz=40, num_updates=5460, lr=4.96282e-05, gnorm=0.863, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14098
2023-01-05 01:12:20 - progress_bar.py[line:274] - INFO: epoch 001:   5477 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.9, wpb=108.9, bsz=40, num_updates=5470, lr=4.96237e-05, gnorm=0.792, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14109
2023-01-05 01:12:31 - progress_bar.py[line:274] - INFO: epoch 001:   5487 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.88, wpb=110.7, bsz=40, num_updates=5480, lr=4.96192e-05, gnorm=0.713, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14121
2023-01-05 01:12:42 - progress_bar.py[line:274] - INFO: epoch 001:   5497 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96, ups=0.89, wpb=108.2, bsz=40, num_updates=5490, lr=4.96147e-05, gnorm=0.894, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14132
2023-01-05 01:12:52 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 01:12:54 - progress_bar.py[line:274] - INFO: epoch 001:   5508 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=93.1, ups=0.85, wpb=109.2, bsz=40, num_updates=5500, lr=4.96102e-05, gnorm=0.798, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14144
2023-01-05 01:13:06 - progress_bar.py[line:274] - INFO: epoch 001:   5518 / 115845 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.8, ups=0.89, wpb=110.7, bsz=40, num_updates=5510, lr=4.96057e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14155
2023-01-05 01:13:17 - progress_bar.py[line:274] - INFO: epoch 001:   5528 / 115845 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.1, ups=0.89, wpb=110.2, bsz=40, num_updates=5520, lr=4.96012e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14167
2023-01-05 01:13:28 - progress_bar.py[line:274] - INFO: epoch 001:   5538 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=5530, lr=4.95967e-05, gnorm=0.833, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14178
2023-01-05 01:13:40 - progress_bar.py[line:274] - INFO: epoch 001:   5548 / 115845 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=94.6, ups=0.88, wpb=107.6, bsz=40, num_updates=5540, lr=4.95922e-05, gnorm=0.87, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14189
2023-01-05 01:13:51 - progress_bar.py[line:274] - INFO: epoch 001:   5558 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101, ups=0.92, wpb=110, bsz=40, num_updates=5550, lr=4.95877e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14200
2023-01-05 01:14:02 - progress_bar.py[line:274] - INFO: epoch 001:   5568 / 115845 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.3, ups=0.91, wpb=108.4, bsz=40, num_updates=5560, lr=4.95832e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14212
2023-01-05 01:14:13 - progress_bar.py[line:274] - INFO: epoch 001:   5578 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98, ups=0.89, wpb=110.6, bsz=40, num_updates=5570, lr=4.95787e-05, gnorm=0.83, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14223
2023-01-05 01:14:25 - progress_bar.py[line:274] - INFO: epoch 001:   5588 / 115845 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.7, ups=0.87, wpb=109.7, bsz=40, num_updates=5580, lr=4.95742e-05, gnorm=0.766, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14234
2023-01-05 01:14:36 - progress_bar.py[line:274] - INFO: epoch 001:   5598 / 115845 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.4, ups=0.9, wpb=107.7, bsz=40, num_updates=5590, lr=4.95697e-05, gnorm=0.872, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14246
2023-01-05 01:14:47 - progress_bar.py[line:274] - INFO: epoch 001:   5608 / 115845 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.4, ups=0.9, wpb=110.7, bsz=40, num_updates=5600, lr=4.95652e-05, gnorm=0.861, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14257
2023-01-05 01:14:58 - progress_bar.py[line:274] - INFO: epoch 001:   5618 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.1, ups=0.9, wpb=109.4, bsz=40, num_updates=5610, lr=4.95607e-05, gnorm=0.761, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14268
2023-01-05 01:15:09 - progress_bar.py[line:274] - INFO: epoch 001:   5628 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.92, wpb=108.8, bsz=40, num_updates=5620, lr=4.95563e-05, gnorm=0.795, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14279
2023-01-05 01:15:20 - progress_bar.py[line:274] - INFO: epoch 001:   5638 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=5630, lr=4.95518e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14290
2023-01-05 01:15:32 - progress_bar.py[line:274] - INFO: epoch 001:   5648 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.9, ups=0.9, wpb=109.4, bsz=40, num_updates=5640, lr=4.95473e-05, gnorm=0.795, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14301
2023-01-05 01:15:43 - progress_bar.py[line:274] - INFO: epoch 001:   5658 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=94.4, ups=0.86, wpb=109.3, bsz=40, num_updates=5650, lr=4.95428e-05, gnorm=0.802, clip=10, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=14313
2023-01-05 01:15:55 - progress_bar.py[line:274] - INFO: epoch 001:   5668 / 115845 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.4, ups=0.88, wpb=109.2, bsz=40, num_updates=5660, lr=4.95383e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14324
2023-01-05 01:16:06 - progress_bar.py[line:274] - INFO: epoch 001:   5678 / 115845 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=5670, lr=4.95338e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14335
2023-01-05 01:16:17 - progress_bar.py[line:274] - INFO: epoch 001:   5688 / 115845 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.1, ups=0.89, wpb=110.6, bsz=40, num_updates=5680, lr=4.95293e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=14347
2023-01-05 01:16:28 - progress_bar.py[line:274] - INFO: epoch 001:   5698 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.9, wpb=109.4, bsz=40, num_updates=5690, lr=4.95248e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14358
2023-01-05 01:16:39 - progress_bar.py[line:274] - INFO: epoch 001:   5708 / 115845 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.7, ups=0.93, wpb=109.4, bsz=40, num_updates=5700, lr=4.95203e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14369
2023-01-05 01:16:50 - progress_bar.py[line:274] - INFO: epoch 001:   5718 / 115845 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100, ups=0.91, wpb=110.1, bsz=40, num_updates=5710, lr=4.95158e-05, gnorm=0.801, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14380
2023-01-05 01:17:01 - progress_bar.py[line:274] - INFO: epoch 001:   5728 / 115845 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.9, ups=0.94, wpb=109.2, bsz=40, num_updates=5720, lr=4.95113e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14391
2023-01-05 01:17:12 - progress_bar.py[line:274] - INFO: epoch 001:   5738 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=5730, lr=4.95068e-05, gnorm=0.865, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14402
2023-01-05 01:17:23 - progress_bar.py[line:274] - INFO: epoch 001:   5748 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96, ups=0.89, wpb=107.7, bsz=40, num_updates=5740, lr=4.95023e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14413
2023-01-05 01:17:35 - progress_bar.py[line:274] - INFO: epoch 001:   5758 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.91, wpb=108.1, bsz=40, num_updates=5750, lr=4.94978e-05, gnorm=0.86, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14424
2023-01-05 01:17:46 - progress_bar.py[line:274] - INFO: epoch 001:   5768 / 115845 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.1, ups=0.91, wpb=108.7, bsz=40, num_updates=5760, lr=4.94933e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14435
2023-01-05 01:17:57 - progress_bar.py[line:274] - INFO: epoch 001:   5778 / 115845 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.3, ups=0.91, wpb=108.5, bsz=40, num_updates=5770, lr=4.94888e-05, gnorm=0.818, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14446
2023-01-05 01:18:08 - progress_bar.py[line:274] - INFO: epoch 001:   5788 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.3, ups=0.88, wpb=109.2, bsz=40, num_updates=5780, lr=4.94843e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14458
2023-01-05 01:18:19 - progress_bar.py[line:274] - INFO: epoch 001:   5798 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.7, ups=0.89, wpb=108.6, bsz=40, num_updates=5790, lr=4.94798e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14469
2023-01-05 01:18:31 - progress_bar.py[line:274] - INFO: epoch 001:   5808 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.9, ups=0.89, wpb=107.8, bsz=40, num_updates=5800, lr=4.94753e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14480
2023-01-05 01:18:42 - progress_bar.py[line:274] - INFO: epoch 001:   5818 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.9, ups=0.91, wpb=109, bsz=40, num_updates=5810, lr=4.94708e-05, gnorm=0.831, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14491
2023-01-05 01:18:53 - progress_bar.py[line:274] - INFO: epoch 001:   5828 / 115845 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=95.8, ups=0.89, wpb=108, bsz=40, num_updates=5820, lr=4.94663e-05, gnorm=0.786, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14503
2023-01-05 01:19:04 - progress_bar.py[line:274] - INFO: epoch 001:   5838 / 115845 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.1, ups=0.89, wpb=107.5, bsz=40, num_updates=5830, lr=4.94618e-05, gnorm=0.7, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14514
2023-01-05 01:19:15 - progress_bar.py[line:274] - INFO: epoch 001:   5848 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.2, ups=0.91, wpb=110.4, bsz=40, num_updates=5840, lr=4.94573e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14525
2023-01-05 01:19:27 - progress_bar.py[line:274] - INFO: epoch 001:   5858 / 115845 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.9, ups=0.9, wpb=110, bsz=40, num_updates=5850, lr=4.94528e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14536
2023-01-05 01:19:38 - progress_bar.py[line:274] - INFO: epoch 001:   5868 / 115845 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.3, ups=0.92, wpb=108.2, bsz=40, num_updates=5860, lr=4.94484e-05, gnorm=0.895, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14547
2023-01-05 01:19:49 - progress_bar.py[line:274] - INFO: epoch 001:   5878 / 115845 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.5, ups=0.93, wpb=109.7, bsz=40, num_updates=5870, lr=4.94439e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14558
2023-01-05 01:20:00 - progress_bar.py[line:274] - INFO: epoch 001:   5888 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.91, wpb=108.6, bsz=40, num_updates=5880, lr=4.94394e-05, gnorm=0.771, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14569
2023-01-05 01:20:11 - progress_bar.py[line:274] - INFO: epoch 001:   5898 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.89, wpb=110.3, bsz=40, num_updates=5890, lr=4.94349e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14581
2023-01-05 01:20:22 - progress_bar.py[line:274] - INFO: epoch 001:   5908 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.91, wpb=108, bsz=40, num_updates=5900, lr=4.94304e-05, gnorm=0.76, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14592
2023-01-05 01:20:33 - progress_bar.py[line:274] - INFO: epoch 001:   5918 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=5910, lr=4.94259e-05, gnorm=1.283, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14603
2023-01-05 01:20:45 - progress_bar.py[line:274] - INFO: epoch 001:   5928 / 115845 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.9, ups=0.87, wpb=109.8, bsz=40, num_updates=5920, lr=4.94214e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14614
2023-01-05 01:20:56 - progress_bar.py[line:274] - INFO: epoch 001:   5938 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.91, wpb=108.5, bsz=40, num_updates=5930, lr=4.94169e-05, gnorm=0.811, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14626
2023-01-05 01:21:07 - progress_bar.py[line:274] - INFO: epoch 001:   5948 / 115845 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99, ups=0.9, wpb=109.9, bsz=40, num_updates=5940, lr=4.94124e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14637
2023-01-05 01:21:18 - progress_bar.py[line:274] - INFO: epoch 001:   5958 / 115845 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=95.5, ups=0.88, wpb=108.3, bsz=40, num_updates=5950, lr=4.94079e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14648
2023-01-05 01:21:29 - progress_bar.py[line:274] - INFO: epoch 001:   5968 / 115845 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.92, wpb=109.6, bsz=40, num_updates=5960, lr=4.94034e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14659
2023-01-05 01:21:41 - progress_bar.py[line:274] - INFO: epoch 001:   5978 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.8, ups=0.91, wpb=109, bsz=40, num_updates=5970, lr=4.93989e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14670
2023-01-05 01:21:52 - progress_bar.py[line:274] - INFO: epoch 001:   5988 / 115845 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.8, ups=0.89, wpb=110.4, bsz=40, num_updates=5980, lr=4.93944e-05, gnorm=0.74, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14681
2023-01-05 01:22:03 - progress_bar.py[line:274] - INFO: epoch 001:   5998 / 115845 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.8, ups=0.91, wpb=108, bsz=40, num_updates=5990, lr=4.93899e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14693
2023-01-05 01:22:14 - progress_bar.py[line:274] - INFO: epoch 001:   6008 / 115845 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.6, ups=0.92, wpb=109.7, bsz=40, num_updates=6000, lr=4.93854e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14704
2023-01-05 01:22:14 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 01:22:15 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 01:22:15 - train.py[line:551] - INFO: load:1.02 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 01:24:48 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 01:24:48 - train.py[line:551] - INFO: load:1.04 valid_run:152.70 task_valid:149.04 collect_output:2.57
2023-01-05 01:27:16 - trainer.py[line:1409] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.28 GiB (GPU 1; 39.59 GiB total capacity; 8.94 GiB already allocated; 6.25 GiB free; 30.84 GiB reserved in total by PyTorch)
2023-01-05 01:27:16 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-05 01:27:16 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9154 MB |   15472 MB |    2144 TB |    2144 TB |
|       from large pool |    9009 MB |   15326 MB |    2143 TB |    2143 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9154 MB |   15472 MB |    2144 TB |    2144 TB |
|       from large pool |    9009 MB |   15326 MB |    2143 TB |    2143 TB |
|       from small pool |     144 MB |     145 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   31584 MB |   37944 MB |  185374 MB |  153790 MB |
|       from large pool |   31438 MB |   37798 MB |  185076 MB |  153638 MB |
|       from small pool |     146 MB |     152 MB |     298 MB |     152 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   22429 MB |   26253 MB |    2208 TB |    2208 TB |
|       from large pool |   22428 MB |   26252 MB |    2207 TB |    2207 TB |
|       from small pool |       1 MB |       1 MB |       0 TB |       0 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3669    |    3683    |   89855 K  |   89851 K  |
|       from large pool |     563    |     575    |   32577 K  |   32576 K  |
|       from small pool |    3106    |    3116    |   57278 K  |   57275 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3669    |    3683    |   89855 K  |   89851 K  |
|       from large pool |     563    |     575    |   32577 K  |   32576 K  |
|       from small pool |    3106    |    3116    |   57278 K  |   57275 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     181    |     186    |     480    |     299    |
|       from large pool |     108    |     110    |     331    |     223    |
|       from small pool |      73    |      76    |     149    |      76    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     127    |   58739 K  |   58739 K  |
|       from large pool |      86    |      89    |   12694 K  |   12694 K  |
|       from small pool |      31    |      44    |   46045 K  |   46045 K  |
|===========================================================================|

2023-01-05 01:27:16 - trainer.py[line:1158] - WARNING: ran out of memory in validation step, retrying batch
2023-01-05 01:27:18 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 01:27:18 - train.py[line:551] - INFO: load:1.06 valid_run:302.25 task_valid:292.15 collect_output:7.96
2023-01-05 01:29:50 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 01:29:50 - train.py[line:551] - INFO: load:1.09 valid_run:454.41 task_valid:435.13 collect_output:16.11
2023-01-05 01:32:19 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 01:32:19 - train.py[line:551] - INFO: load:1.11 valid_run:603.37 task_valid:580.09 collect_output:19.06
2023-01-05 01:34:51 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 01:34:51 - train.py[line:551] - INFO: load:1.14 valid_run:755.66 task_valid:727.86 collect_output:22.54
2023-01-05 01:37:23 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 01:37:23 - train.py[line:551] - INFO: load:1.16 valid_run:906.99 task_valid:873.29 collect_output:27.39
2023-01-05 01:39:56 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 01:39:56 - train.py[line:551] - INFO: load:1.19 valid_run:1059.95 task_valid:1019.22 collect_output:33.38
2023-01-05 01:42:27 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 01:42:27 - train.py[line:551] - INFO: load:1.21 valid_run:1210.72 task_valid:1160.34 collect_output:41.96
2023-01-05 01:44:56 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 01:44:56 - train.py[line:551] - INFO: load:1.24 valid_run:1360.29 task_valid:1305.29 collect_output:45.49
2023-01-05 01:47:25 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 01:47:25 - train.py[line:551] - INFO: load:1.26 valid_run:1508.63 task_valid:1448.58 collect_output:49.46
2023-01-05 01:49:54 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 01:49:54 - train.py[line:551] - INFO: load:1.29 valid_run:1658.34 task_valid:1593.57 collect_output:53.14
2023-01-05 01:52:24 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 01:52:24 - train.py[line:551] - INFO: load:1.32 valid_run:1808.05 task_valid:1738.61 collect_output:56.74
2023-01-05 01:54:54 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 01:54:54 - train.py[line:551] - INFO: load:1.34 valid_run:1957.82 task_valid:1880.72 collect_output:63.36
2023-01-05 01:57:24 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 01:57:24 - train.py[line:551] - INFO: load:1.37 valid_run:2108.09 task_valid:2026.29 collect_output:67.02
2023-01-05 01:59:55 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 01:59:55 - train.py[line:551] - INFO: load:1.39 valid_run:2258.09 task_valid:2172.81 collect_output:69.45
2023-01-05 02:02:24 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 02:02:24 - train.py[line:551] - INFO: load:1.42 valid_run:2407.88 task_valid:2317.01 collect_output:74.01
2023-01-05 02:04:56 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 02:04:56 - train.py[line:551] - INFO: load:1.44 valid_run:2559.25 task_valid:2462.62 collect_output:78.73
2023-01-05 02:07:26 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 02:07:26 - train.py[line:551] - INFO: load:1.47 valid_run:2709.69 task_valid:2609.66 collect_output:81.08
2023-01-05 02:09:55 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 02:09:55 - train.py[line:551] - INFO: load:1.50 valid_run:2857.92 task_valid:2751.41 collect_output:86.50
2023-01-05 02:12:25 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 02:12:25 - train.py[line:551] - INFO: load:1.52 valid_run:3008.00 task_valid:2896.60 collect_output:90.34
2023-01-05 02:14:57 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 02:14:57 - train.py[line:551] - INFO: load:1.55 valid_run:3159.55 task_valid:3041.26 collect_output:96.20
2023-01-05 02:17:26 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 02:17:26 - train.py[line:551] - INFO: load:1.57 valid_run:3308.67 task_valid:3185.87 collect_output:99.65
2023-01-05 02:19:57 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 02:19:57 - train.py[line:551] - INFO: load:1.60 valid_run:3459.52 task_valid:3331.82 collect_output:103.52
2023-01-05 02:22:28 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 02:22:28 - train.py[line:551] - INFO: load:1.62 valid_run:3610.55 task_valid:3478.31 collect_output:107.03

====================================================================================================
SGG eval:     R @ 50: 0.5717;     R @ 100: 0.6020;     R @ 500: 0.6415;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3974;    mR @ 100: 0.4733;    mR @ 500: 0.5218;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.8636) (growing on:0.3750) (hanging from:0.2903) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9212) (says:0.0000) (sitting on:0.6893) (standing on:0.1200) (using:0.6000) (walking in:0.3333) (walking on:0.9189) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-01-05 02:24:59 - train.py[line:487] - INFO: 0.6020326967150498
2023-01-05 02:24:59 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.5717;     R @ 100: 0.6020;     R @ 500: 0.6415;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3974;    mR @ 100: 0.4733;    mR @ 500: 0.5218;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.8636) (growing on:0.3750) (hanging from:0.2903) (lying on:0.1500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9212) (says:0.0000) (sitting on:0.6893) (standing on:0.1200) (using:0.6000) (walking in:0.3333) (walking on:0.9189) (watching:0.3750) 
--------------------------------------------------------
====================================================================================================

2023-01-05 02:25:00 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.398 | loss_v1 0 | loss_v2 0 | nll_loss 0.251 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.602033 | ppl 1.19 | vqa_score 0.5079 | wps 119.2 | wpb 89.9 | bsz 30 | num_updates 6000 | best_R@100 0.602033
2023-01-05 02:25:00 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2023-01-05 02:25:00 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-05 02:25:44 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-05 02:28:49 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6020326967150498) (writing took 229.45130286738276 seconds)
2023-01-05 02:29:00 - progress_bar.py[line:274] - INFO: epoch 001:   6018 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=0.3, ups=0, wpb=107.6, bsz=40, num_updates=6010, lr=4.93809e-05, gnorm=0.761, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18709
2023-01-05 02:29:11 - progress_bar.py[line:274] - INFO: epoch 001:   6028 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.5, ups=0.91, wpb=109.8, bsz=40, num_updates=6020, lr=4.93764e-05, gnorm=0.727, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18720
2023-01-05 02:29:22 - progress_bar.py[line:274] - INFO: epoch 001:   6038 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.8, nsentences=40, sample_size=106.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.4, ups=0.88, wpb=106.8, bsz=40, num_updates=6030, lr=4.93719e-05, gnorm=0.78, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18732
2023-01-05 02:29:33 - progress_bar.py[line:274] - INFO: epoch 001:   6048 / 115845 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=6040, lr=4.93674e-05, gnorm=0.681, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18743
2023-01-05 02:29:45 - progress_bar.py[line:274] - INFO: epoch 001:   6058 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.5, ups=0.88, wpb=108.3, bsz=40, num_updates=6050, lr=4.93629e-05, gnorm=0.68, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18755
2023-01-05 02:29:56 - progress_bar.py[line:274] - INFO: epoch 001:   6068 / 115845 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.6, ups=0.91, wpb=109.3, bsz=40, num_updates=6060, lr=4.93584e-05, gnorm=0.813, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18766
2023-01-05 02:30:07 - progress_bar.py[line:274] - INFO: epoch 001:   6078 / 115845 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.8, ups=0.92, wpb=108.2, bsz=40, num_updates=6070, lr=4.93539e-05, gnorm=0.69, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18777
2023-01-05 02:30:18 - progress_bar.py[line:274] - INFO: epoch 001:   6088 / 115845 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=6080, lr=4.93494e-05, gnorm=0.773, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18788
2023-01-05 02:30:30 - progress_bar.py[line:274] - INFO: epoch 001:   6098 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.7, ups=0.87, wpb=108.5, bsz=40, num_updates=6090, lr=4.93449e-05, gnorm=0.789, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18799
2023-01-05 02:30:41 - progress_bar.py[line:274] - INFO: epoch 001:   6108 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.9, wpb=110.1, bsz=40, num_updates=6100, lr=4.93404e-05, gnorm=0.699, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18811
2023-01-05 02:30:52 - progress_bar.py[line:274] - INFO: epoch 001:   6118 / 115845 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.1, ups=0.92, wpb=109.4, bsz=40, num_updates=6110, lr=4.9336e-05, gnorm=0.877, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18821
2023-01-05 02:31:03 - progress_bar.py[line:274] - INFO: epoch 001:   6128 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.1, ups=0.89, wpb=108.2, bsz=40, num_updates=6120, lr=4.93315e-05, gnorm=0.811, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18833
2023-01-05 02:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   6138 / 115845 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.3, ups=0.91, wpb=111.2, bsz=40, num_updates=6130, lr=4.9327e-05, gnorm=0.829, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18844
2023-01-05 02:31:25 - progress_bar.py[line:274] - INFO: epoch 001:   6148 / 115845 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.9, ups=0.89, wpb=108.3, bsz=40, num_updates=6140, lr=4.93225e-05, gnorm=0.814, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18855
2023-01-05 02:31:36 - progress_bar.py[line:274] - INFO: epoch 001:   6158 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.91, wpb=110, bsz=40, num_updates=6150, lr=4.9318e-05, gnorm=0.681, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18866
2023-01-05 02:31:48 - progress_bar.py[line:274] - INFO: epoch 001:   6168 / 115845 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.3, ups=0.9, wpb=109.7, bsz=40, num_updates=6160, lr=4.93135e-05, gnorm=0.737, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18877
2023-01-05 02:31:59 - progress_bar.py[line:274] - INFO: epoch 001:   6178 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.4, ups=0.91, wpb=109, bsz=40, num_updates=6170, lr=4.9309e-05, gnorm=0.772, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18888
2023-01-05 02:32:02 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 02:32:11 - progress_bar.py[line:274] - INFO: epoch 001:   6189 / 115845 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=92.3, ups=0.83, wpb=111.2, bsz=40, num_updates=6180, lr=4.93045e-05, gnorm=0.782, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=18900
2023-01-05 02:32:22 - progress_bar.py[line:274] - INFO: epoch 001:   6199 / 115845 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.9, ups=0.9, wpb=109.2, bsz=40, num_updates=6190, lr=4.93e-05, gnorm=0.803, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18912
2023-01-05 02:32:33 - progress_bar.py[line:274] - INFO: epoch 001:   6209 / 115845 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.3, ups=0.89, wpb=109.5, bsz=40, num_updates=6200, lr=4.92955e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18923
2023-01-05 02:32:45 - progress_bar.py[line:274] - INFO: epoch 001:   6219 / 115845 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.2, ups=0.9, wpb=109.3, bsz=40, num_updates=6210, lr=4.9291e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18934
2023-01-05 02:32:56 - progress_bar.py[line:274] - INFO: epoch 001:   6229 / 115845 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.1, ups=0.91, wpb=108.4, bsz=40, num_updates=6220, lr=4.92865e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=18945
2023-01-05 02:33:07 - progress_bar.py[line:274] - INFO: epoch 001:   6239 / 115845 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.1, ups=0.93, wpb=110, bsz=40, num_updates=6230, lr=4.9282e-05, gnorm=0.838, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18956
2023-01-05 02:33:17 - progress_bar.py[line:274] - INFO: epoch 001:   6249 / 115845 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.2, ups=0.92, wpb=109.2, bsz=40, num_updates=6240, lr=4.92775e-05, gnorm=0.724, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18967
2023-01-05 02:33:29 - progress_bar.py[line:274] - INFO: epoch 001:   6259 / 115845 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100, ups=0.91, wpb=109.5, bsz=40, num_updates=6250, lr=4.9273e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18978
2023-01-05 02:33:40 - progress_bar.py[line:274] - INFO: epoch 001:   6269 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.2, ups=0.88, wpb=108.8, bsz=40, num_updates=6260, lr=4.92685e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18990
2023-01-05 02:33:51 - progress_bar.py[line:274] - INFO: epoch 001:   6279 / 115845 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.8, ups=0.93, wpb=108.6, bsz=40, num_updates=6270, lr=4.9264e-05, gnorm=0.869, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19000
2023-01-05 02:34:02 - progress_bar.py[line:274] - INFO: epoch 001:   6289 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95, ups=0.87, wpb=109, bsz=40, num_updates=6280, lr=4.92595e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19012
2023-01-05 02:34:14 - progress_bar.py[line:274] - INFO: epoch 001:   6299 / 115845 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.4, ups=0.9, wpb=109.1, bsz=40, num_updates=6290, lr=4.9255e-05, gnorm=0.761, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19023
2023-01-05 02:34:25 - progress_bar.py[line:274] - INFO: epoch 001:   6309 / 115845 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.1, ups=0.91, wpb=109.4, bsz=40, num_updates=6300, lr=4.92505e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19034
2023-01-05 02:34:36 - progress_bar.py[line:274] - INFO: epoch 001:   6319 / 115845 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.2, ups=0.91, wpb=109.8, bsz=40, num_updates=6310, lr=4.9246e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19045
2023-01-05 02:34:47 - progress_bar.py[line:274] - INFO: epoch 001:   6329 / 115845 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.7, ups=0.92, wpb=108.7, bsz=40, num_updates=6320, lr=4.92415e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19056
2023-01-05 02:34:58 - progress_bar.py[line:274] - INFO: epoch 001:   6339 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.1, ups=0.92, wpb=108.5, bsz=40, num_updates=6330, lr=4.9237e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19067
2023-01-05 02:35:09 - progress_bar.py[line:274] - INFO: epoch 001:   6349 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.7, ups=0.93, wpb=109.8, bsz=40, num_updates=6340, lr=4.92325e-05, gnorm=0.852, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19078
2023-01-05 02:35:20 - progress_bar.py[line:274] - INFO: epoch 001:   6359 / 115845 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.1, ups=0.92, wpb=108, bsz=40, num_updates=6350, lr=4.92281e-05, gnorm=0.872, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19089
2023-01-05 02:35:31 - progress_bar.py[line:274] - INFO: epoch 001:   6369 / 115845 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.3, ups=0.92, wpb=110.1, bsz=40, num_updates=6360, lr=4.92236e-05, gnorm=0.98, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19100
2023-01-05 02:35:42 - progress_bar.py[line:274] - INFO: epoch 001:   6379 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.2, ups=0.9, wpb=108, bsz=40, num_updates=6370, lr=4.92191e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19111
2023-01-05 02:35:53 - progress_bar.py[line:274] - INFO: epoch 001:   6389 / 115845 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.9, ups=0.91, wpb=109.3, bsz=40, num_updates=6380, lr=4.92146e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19123
2023-01-05 02:36:04 - progress_bar.py[line:274] - INFO: epoch 001:   6399 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.91, wpb=107.6, bsz=40, num_updates=6390, lr=4.92101e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19134
2023-01-05 02:36:15 - progress_bar.py[line:274] - INFO: epoch 001:   6409 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=95.5, ups=0.88, wpb=108.9, bsz=40, num_updates=6400, lr=4.92056e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19145
2023-01-05 02:36:27 - progress_bar.py[line:274] - INFO: epoch 001:   6419 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.88, wpb=109.6, bsz=40, num_updates=6410, lr=4.92011e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19156
2023-01-05 02:36:38 - progress_bar.py[line:274] - INFO: epoch 001:   6429 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.1, ups=0.92, wpb=110.1, bsz=40, num_updates=6420, lr=4.91966e-05, gnorm=0.801, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19167
2023-01-05 02:36:49 - progress_bar.py[line:274] - INFO: epoch 001:   6439 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=6430, lr=4.91921e-05, gnorm=0.721, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19179
2023-01-05 02:37:00 - progress_bar.py[line:274] - INFO: epoch 001:   6449 / 115845 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.9, ups=0.9, wpb=107, bsz=40, num_updates=6440, lr=4.91876e-05, gnorm=0.858, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19190
2023-01-05 02:37:11 - progress_bar.py[line:274] - INFO: epoch 001:   6459 / 115845 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.3, ups=0.91, wpb=109.7, bsz=40, num_updates=6450, lr=4.91831e-05, gnorm=0.748, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19201
2023-01-05 02:37:22 - progress_bar.py[line:274] - INFO: epoch 001:   6469 / 115845 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.9, ups=0.92, wpb=108.7, bsz=40, num_updates=6460, lr=4.91786e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19212
2023-01-05 02:37:33 - progress_bar.py[line:274] - INFO: epoch 001:   6479 / 115845 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=97.3, ups=0.89, wpb=109.2, bsz=40, num_updates=6470, lr=4.91741e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19223
2023-01-05 02:37:44 - progress_bar.py[line:274] - INFO: epoch 001:   6489 / 115845 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.7, ups=0.95, wpb=107.2, bsz=40, num_updates=6480, lr=4.91696e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=19234
2023-01-05 02:37:56 - progress_bar.py[line:274] - INFO: epoch 001:   6499 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=94.6, ups=0.87, wpb=108.4, bsz=40, num_updates=6490, lr=4.91651e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19245
2023-01-05 02:38:07 - progress_bar.py[line:274] - INFO: epoch 001:   6509 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.1, ups=0.9, wpb=109.7, bsz=40, num_updates=6500, lr=4.91606e-05, gnorm=0.705, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=19256
2023-01-05 02:38:18 - progress_bar.py[line:274] - INFO: epoch 001:   6519 / 115845 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=6510, lr=4.91561e-05, gnorm=0.821, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19268
2023-01-05 02:38:29 - progress_bar.py[line:274] - INFO: epoch 001:   6529 / 115845 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99, ups=0.91, wpb=109.2, bsz=40, num_updates=6520, lr=4.91516e-05, gnorm=1.097, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19279
2023-01-05 02:38:40 - progress_bar.py[line:274] - INFO: epoch 001:   6539 / 115845 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.8, ups=0.91, wpb=108.9, bsz=40, num_updates=6530, lr=4.91471e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=19290
2023-01-05 02:38:51 - progress_bar.py[line:274] - INFO: epoch 001:   6549 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.92, wpb=108.3, bsz=40, num_updates=6540, lr=4.91426e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19301
2023-01-05 02:39:02 - progress_bar.py[line:274] - INFO: epoch 001:   6559 / 115845 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.1, ups=0.93, wpb=109.4, bsz=40, num_updates=6550, lr=4.91381e-05, gnorm=0.784, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19312
2023-01-05 02:39:14 - progress_bar.py[line:274] - INFO: epoch 001:   6569 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.9, ups=0.89, wpb=108.8, bsz=40, num_updates=6560, lr=4.91336e-05, gnorm=0.802, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19323
2023-01-05 02:39:24 - progress_bar.py[line:274] - INFO: epoch 001:   6579 / 115845 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.92, wpb=108.5, bsz=40, num_updates=6570, lr=4.91291e-05, gnorm=0.822, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=19334
2023-01-05 02:39:36 - progress_bar.py[line:274] - INFO: epoch 001:   6589 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=6580, lr=4.91246e-05, gnorm=0.786, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19345
2023-01-05 02:39:47 - progress_bar.py[line:274] - INFO: epoch 001:   6599 / 115845 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.2, ups=0.92, wpb=109.2, bsz=40, num_updates=6590, lr=4.91201e-05, gnorm=0.781, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19356
2023-01-05 02:39:58 - progress_bar.py[line:274] - INFO: epoch 001:   6609 / 115845 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=6600, lr=4.91157e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19368
2023-01-05 02:40:10 - progress_bar.py[line:274] - INFO: epoch 001:   6619 / 115845 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.2, ups=0.9, wpb=108.1, bsz=40, num_updates=6610, lr=4.91112e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19379
2023-01-05 02:40:21 - progress_bar.py[line:274] - INFO: epoch 001:   6629 / 115845 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.5, ups=0.9, wpb=108.9, bsz=40, num_updates=6620, lr=4.91067e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19390
2023-01-05 02:40:32 - progress_bar.py[line:274] - INFO: epoch 001:   6639 / 115845 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=103.1, ups=0.94, wpb=110.1, bsz=40, num_updates=6630, lr=4.91022e-05, gnorm=0.789, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19401
2023-01-05 02:40:43 - progress_bar.py[line:274] - INFO: epoch 001:   6649 / 115845 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=95.7, ups=0.88, wpb=108.7, bsz=40, num_updates=6640, lr=4.90977e-05, gnorm=0.763, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19413
2023-01-05 02:40:54 - progress_bar.py[line:274] - INFO: epoch 001:   6659 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.7, ups=0.92, wpb=109.8, bsz=40, num_updates=6650, lr=4.90932e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19424
2023-01-05 02:41:05 - progress_bar.py[line:274] - INFO: epoch 001:   6669 / 115845 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=6660, lr=4.90887e-05, gnorm=0.823, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19435
2023-01-05 02:41:17 - progress_bar.py[line:274] - INFO: epoch 001:   6679 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.91, wpb=108.5, bsz=40, num_updates=6670, lr=4.90842e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19446
2023-01-05 02:41:27 - progress_bar.py[line:274] - INFO: epoch 001:   6689 / 115845 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.1, ups=0.92, wpb=110, bsz=40, num_updates=6680, lr=4.90797e-05, gnorm=0.718, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19457
2023-01-05 02:41:39 - progress_bar.py[line:274] - INFO: epoch 001:   6699 / 115845 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.6, ups=0.91, wpb=109.8, bsz=40, num_updates=6690, lr=4.90752e-05, gnorm=0.717, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19468
2023-01-05 02:41:50 - progress_bar.py[line:274] - INFO: epoch 001:   6709 / 115845 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=6700, lr=4.90707e-05, gnorm=0.709, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19480
2023-01-05 02:42:01 - progress_bar.py[line:274] - INFO: epoch 001:   6719 / 115845 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.8, ups=0.92, wpb=107.8, bsz=40, num_updates=6710, lr=4.90662e-05, gnorm=0.817, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19491
2023-01-05 02:42:12 - progress_bar.py[line:274] - INFO: epoch 001:   6729 / 115845 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.4, ups=0.92, wpb=109.3, bsz=40, num_updates=6720, lr=4.90617e-05, gnorm=0.65, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19502
2023-01-05 02:42:23 - progress_bar.py[line:274] - INFO: epoch 001:   6739 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.9, wpb=110.1, bsz=40, num_updates=6730, lr=4.90572e-05, gnorm=0.782, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19513
2023-01-05 02:42:34 - progress_bar.py[line:274] - INFO: epoch 001:   6749 / 115845 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.8, ups=0.92, wpb=108.6, bsz=40, num_updates=6740, lr=4.90527e-05, gnorm=0.716, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19524
2023-01-05 02:42:36 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 02:42:46 - progress_bar.py[line:274] - INFO: epoch 001:   6760 / 115845 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=89.6, ups=0.82, wpb=109.2, bsz=40, num_updates=6750, lr=4.90482e-05, gnorm=0.891, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19536
2023-01-05 02:42:57 - progress_bar.py[line:274] - INFO: epoch 001:   6770 / 115845 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.9, ups=0.9, wpb=110.6, bsz=40, num_updates=6760, lr=4.90437e-05, gnorm=0.881, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19547
2023-01-05 02:43:09 - progress_bar.py[line:274] - INFO: epoch 001:   6780 / 115845 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=6770, lr=4.90392e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19559
2023-01-05 02:43:20 - progress_bar.py[line:274] - INFO: epoch 001:   6790 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.91, wpb=108.3, bsz=40, num_updates=6780, lr=4.90347e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=19570
2023-01-05 02:43:32 - progress_bar.py[line:274] - INFO: epoch 001:   6800 / 115845 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=93.8, ups=0.86, wpb=109.2, bsz=40, num_updates=6790, lr=4.90302e-05, gnorm=0.829, clip=30, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=19581
2023-01-05 02:43:43 - progress_bar.py[line:274] - INFO: epoch 001:   6810 / 115845 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.3, ups=0.89, wpb=108.7, bsz=40, num_updates=6800, lr=4.90257e-05, gnorm=0.824, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19593
2023-01-05 02:43:54 - progress_bar.py[line:274] - INFO: epoch 001:   6820 / 115845 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=98.9, ups=0.9, wpb=110.4, bsz=40, num_updates=6810, lr=4.90212e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19604
2023-01-05 02:44:06 - progress_bar.py[line:274] - INFO: epoch 001:   6830 / 115845 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.7, ups=0.89, wpb=109.2, bsz=40, num_updates=6820, lr=4.90167e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19615
2023-01-05 02:44:17 - progress_bar.py[line:274] - INFO: epoch 001:   6840 / 115845 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=107.1, nsentences=40, sample_size=107.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.7, ups=0.89, wpb=107.1, bsz=40, num_updates=6830, lr=4.90122e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19626
2023-01-05 02:44:27 - progress_bar.py[line:274] - INFO: epoch 001:   6850 / 115845 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.9, ups=0.94, wpb=108.7, bsz=40, num_updates=6840, lr=4.90078e-05, gnorm=0.763, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19637
2023-01-05 02:44:39 - progress_bar.py[line:274] - INFO: epoch 001:   6860 / 115845 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=6850, lr=4.90033e-05, gnorm=0.832, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19648
2023-01-05 02:44:50 - progress_bar.py[line:274] - INFO: epoch 001:   6870 / 115845 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.3, ups=0.92, wpb=108.2, bsz=40, num_updates=6860, lr=4.89988e-05, gnorm=0.919, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19659
2023-01-05 02:45:01 - progress_bar.py[line:274] - INFO: epoch 001:   6880 / 115845 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.2, ups=0.89, wpb=109.4, bsz=40, num_updates=6870, lr=4.89943e-05, gnorm=0.939, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19671
2023-01-05 02:45:12 - progress_bar.py[line:274] - INFO: epoch 001:   6890 / 115845 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.1, ups=0.89, wpb=107.5, bsz=40, num_updates=6880, lr=4.89898e-05, gnorm=0.712, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19682
2023-01-05 02:45:23 - progress_bar.py[line:274] - INFO: epoch 001:   6900 / 115845 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.2, ups=0.9, wpb=108.6, bsz=40, num_updates=6890, lr=4.89853e-05, gnorm=0.789, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19693
2023-01-05 02:45:34 - progress_bar.py[line:274] - INFO: epoch 001:   6910 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.2, ups=0.92, wpb=110.1, bsz=40, num_updates=6900, lr=4.89808e-05, gnorm=0.81, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19704
2023-01-05 02:45:46 - progress_bar.py[line:274] - INFO: epoch 001:   6920 / 115845 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=6910, lr=4.89763e-05, gnorm=0.721, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19715
2023-01-05 02:45:57 - progress_bar.py[line:274] - INFO: epoch 001:   6930 / 115845 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.5, ups=0.92, wpb=108.2, bsz=40, num_updates=6920, lr=4.89718e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19726
2023-01-05 02:46:08 - progress_bar.py[line:274] - INFO: epoch 001:   6940 / 115845 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.4, ups=0.91, wpb=108.8, bsz=40, num_updates=6930, lr=4.89673e-05, gnorm=0.85, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19737
2023-01-05 02:46:19 - progress_bar.py[line:274] - INFO: epoch 001:   6950 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=6940, lr=4.89628e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19749
2023-01-05 02:46:31 - progress_bar.py[line:274] - INFO: epoch 001:   6960 / 115845 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.4, ups=0.87, wpb=109.2, bsz=40, num_updates=6950, lr=4.89583e-05, gnorm=0.756, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19760
2023-01-05 02:46:42 - progress_bar.py[line:274] - INFO: epoch 001:   6970 / 115845 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.7, ups=0.91, wpb=110.1, bsz=40, num_updates=6960, lr=4.89538e-05, gnorm=0.83, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19771
2023-01-05 02:46:53 - progress_bar.py[line:274] - INFO: epoch 001:   6980 / 115845 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96.6, ups=0.89, wpb=108, bsz=40, num_updates=6970, lr=4.89493e-05, gnorm=0.946, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19783
2023-01-05 02:47:04 - progress_bar.py[line:274] - INFO: epoch 001:   6990 / 115845 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.2, ups=0.9, wpb=109.4, bsz=40, num_updates=6980, lr=4.89448e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19794
2023-01-05 02:47:15 - progress_bar.py[line:274] - INFO: epoch 001:   7000 / 115845 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.5, ups=0.89, wpb=109.5, bsz=40, num_updates=6990, lr=4.89403e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19805
2023-01-05 02:47:27 - progress_bar.py[line:274] - INFO: epoch 001:   7010 / 115845 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=96, ups=0.88, wpb=108.5, bsz=40, num_updates=7000, lr=4.89358e-05, gnorm=0.752, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19817
2023-01-05 02:47:38 - progress_bar.py[line:274] - INFO: epoch 001:   7020 / 115845 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=95.5, ups=0.88, wpb=108.1, bsz=40, num_updates=7010, lr=4.89313e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19828
2023-01-05 02:47:49 - progress_bar.py[line:274] - INFO: epoch 001:   7030 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.7, ups=0.92, wpb=109.9, bsz=40, num_updates=7020, lr=4.89268e-05, gnorm=0.743, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19839
2023-01-05 02:48:00 - progress_bar.py[line:274] - INFO: epoch 001:   7040 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=7030, lr=4.89223e-05, gnorm=0.969, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19850
2023-01-05 02:48:11 - progress_bar.py[line:274] - INFO: epoch 001:   7050 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.92, wpb=109.2, bsz=40, num_updates=7040, lr=4.89178e-05, gnorm=0.978, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19861
2023-01-05 02:48:22 - progress_bar.py[line:274] - INFO: epoch 001:   7060 / 115845 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.1, ups=0.9, wpb=110.1, bsz=40, num_updates=7050, lr=4.89133e-05, gnorm=0.829, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19872
2023-01-05 02:48:33 - progress_bar.py[line:274] - INFO: epoch 001:   7070 / 115845 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.8, ups=0.92, wpb=109.8, bsz=40, num_updates=7060, lr=4.89088e-05, gnorm=0.85, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19883
2023-01-05 02:48:45 - progress_bar.py[line:274] - INFO: epoch 001:   7080 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.88, wpb=109.7, bsz=40, num_updates=7070, lr=4.89043e-05, gnorm=0.675, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19894
2023-01-05 02:48:56 - progress_bar.py[line:274] - INFO: epoch 001:   7090 / 115845 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98.2, ups=0.9, wpb=108.6, bsz=40, num_updates=7080, lr=4.88998e-05, gnorm=0.774, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19906
2023-01-05 02:49:07 - progress_bar.py[line:274] - INFO: epoch 001:   7100 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=96.7, ups=0.89, wpb=108, bsz=40, num_updates=7090, lr=4.88954e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19917
2023-01-05 02:49:18 - progress_bar.py[line:274] - INFO: epoch 001:   7110 / 115845 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.4, ups=0.89, wpb=109, bsz=40, num_updates=7100, lr=4.88909e-05, gnorm=0.858, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19928
2023-01-05 02:49:30 - progress_bar.py[line:274] - INFO: epoch 001:   7120 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=7110, lr=4.88864e-05, gnorm=0.836, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19939
2023-01-05 02:49:41 - progress_bar.py[line:274] - INFO: epoch 001:   7130 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.91, wpb=107.7, bsz=40, num_updates=7120, lr=4.88819e-05, gnorm=0.816, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19950
2023-01-05 02:49:51 - progress_bar.py[line:274] - INFO: epoch 001:   7140 / 115845 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.8, ups=0.94, wpb=107.4, bsz=40, num_updates=7130, lr=4.88774e-05, gnorm=0.89, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19961
2023-01-05 02:50:03 - progress_bar.py[line:274] - INFO: epoch 001:   7150 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.88, wpb=109.1, bsz=40, num_updates=7140, lr=4.88729e-05, gnorm=0.886, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19972
2023-01-05 02:50:14 - progress_bar.py[line:274] - INFO: epoch 001:   7160 / 115845 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.5, ups=0.88, wpb=110.2, bsz=40, num_updates=7150, lr=4.88684e-05, gnorm=0.922, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19984
2023-01-05 02:50:26 - progress_bar.py[line:274] - INFO: epoch 001:   7170 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=7160, lr=4.88639e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19995
2023-01-05 02:50:37 - progress_bar.py[line:274] - INFO: epoch 001:   7180 / 115845 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.4, ups=0.88, wpb=110.2, bsz=40, num_updates=7170, lr=4.88594e-05, gnorm=0.827, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20007
2023-01-05 02:50:48 - progress_bar.py[line:274] - INFO: epoch 001:   7190 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.7, ups=0.9, wpb=110.1, bsz=40, num_updates=7180, lr=4.88549e-05, gnorm=0.768, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20018
2023-01-05 02:50:59 - progress_bar.py[line:274] - INFO: epoch 001:   7200 / 115845 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=101.7, ups=0.93, wpb=109.1, bsz=40, num_updates=7190, lr=4.88504e-05, gnorm=0.881, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20029
2023-01-05 02:51:10 - progress_bar.py[line:274] - INFO: epoch 001:   7210 / 115845 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.5, ups=0.88, wpb=108.2, bsz=40, num_updates=7200, lr=4.88459e-05, gnorm=0.991, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20040
2023-01-05 02:51:22 - progress_bar.py[line:274] - INFO: epoch 001:   7220 / 115845 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.4, ups=0.87, wpb=109.2, bsz=40, num_updates=7210, lr=4.88414e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20052
2023-01-05 02:51:33 - progress_bar.py[line:274] - INFO: epoch 001:   7230 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101, ups=0.92, wpb=110.1, bsz=40, num_updates=7220, lr=4.88369e-05, gnorm=0.856, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20063
2023-01-05 02:51:44 - progress_bar.py[line:274] - INFO: epoch 001:   7240 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.5, ups=0.9, wpb=109.6, bsz=40, num_updates=7230, lr=4.88324e-05, gnorm=0.795, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20074
2023-01-05 02:51:55 - progress_bar.py[line:274] - INFO: epoch 001:   7250 / 115845 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.7, ups=0.92, wpb=107.8, bsz=40, num_updates=7240, lr=4.88279e-05, gnorm=0.998, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20085
2023-01-05 02:52:06 - progress_bar.py[line:274] - INFO: epoch 001:   7260 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.91, wpb=108.4, bsz=40, num_updates=7250, lr=4.88234e-05, gnorm=1.102, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20096
2023-01-05 02:52:18 - progress_bar.py[line:274] - INFO: epoch 001:   7270 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.3, ups=0.88, wpb=109.1, bsz=40, num_updates=7260, lr=4.88189e-05, gnorm=0.785, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20107
2023-01-05 02:52:29 - progress_bar.py[line:274] - INFO: epoch 001:   7280 / 115845 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.6, ups=0.91, wpb=108.8, bsz=40, num_updates=7270, lr=4.88144e-05, gnorm=0.81, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20119
2023-01-05 02:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   7290 / 115845 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=7280, lr=4.88099e-05, gnorm=0.88, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20130
2023-01-05 02:52:52 - progress_bar.py[line:274] - INFO: epoch 001:   7300 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.3, ups=0.9, wpb=109.7, bsz=40, num_updates=7290, lr=4.88054e-05, gnorm=0.82, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20141
2023-01-05 02:53:03 - progress_bar.py[line:274] - INFO: epoch 001:   7310 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.8, ups=0.87, wpb=108.6, bsz=40, num_updates=7300, lr=4.88009e-05, gnorm=0.879, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20153
2023-01-05 02:53:15 - progress_bar.py[line:274] - INFO: epoch 001:   7320 / 115845 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=7310, lr=4.87964e-05, gnorm=1.119, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20164
2023-01-05 02:53:17 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 02:53:27 - progress_bar.py[line:274] - INFO: epoch 001:   7331 / 115845 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=87.9, ups=0.81, wpb=109, bsz=40, num_updates=7320, lr=4.87919e-05, gnorm=1.135, clip=60, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=20177
2023-01-05 02:53:39 - progress_bar.py[line:274] - INFO: epoch 001:   7341 / 115845 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95, ups=0.88, wpb=107.5, bsz=40, num_updates=7330, lr=4.87875e-05, gnorm=0.937, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20189
2023-01-05 02:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   7351 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.9, wpb=108.6, bsz=40, num_updates=7340, lr=4.8783e-05, gnorm=0.853, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20200
2023-01-05 02:54:01 - progress_bar.py[line:274] - INFO: epoch 001:   7361 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.5, ups=0.92, wpb=109.9, bsz=40, num_updates=7350, lr=4.87785e-05, gnorm=0.714, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=20211
2023-01-05 02:54:12 - progress_bar.py[line:274] - INFO: epoch 001:   7371 / 115845 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98, ups=0.91, wpb=107.9, bsz=40, num_updates=7360, lr=4.8774e-05, gnorm=0.769, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20222
2023-01-05 02:54:23 - progress_bar.py[line:274] - INFO: epoch 001:   7381 / 115845 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.8, ups=0.93, wpb=109.4, bsz=40, num_updates=7370, lr=4.87695e-05, gnorm=0.813, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20233
2023-01-05 02:54:34 - progress_bar.py[line:274] - INFO: epoch 001:   7391 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=7380, lr=4.8765e-05, gnorm=0.854, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20244
2023-01-05 02:54:45 - progress_bar.py[line:274] - INFO: epoch 001:   7401 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.3, ups=0.94, wpb=108.5, bsz=40, num_updates=7390, lr=4.87605e-05, gnorm=0.826, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=20255
2023-01-05 02:54:56 - progress_bar.py[line:274] - INFO: epoch 001:   7411 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.6, ups=0.89, wpb=109.5, bsz=40, num_updates=7400, lr=4.8756e-05, gnorm=0.929, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20266
2023-01-05 02:55:07 - progress_bar.py[line:274] - INFO: epoch 001:   7421 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=7410, lr=4.87515e-05, gnorm=0.944, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20277
2023-01-05 02:55:19 - progress_bar.py[line:274] - INFO: epoch 001:   7431 / 115845 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95, ups=0.88, wpb=108.3, bsz=40, num_updates=7420, lr=4.8747e-05, gnorm=0.825, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20288
2023-01-05 02:55:30 - progress_bar.py[line:274] - INFO: epoch 001:   7441 / 115845 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.3, ups=0.91, wpb=108.6, bsz=40, num_updates=7430, lr=4.87425e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=20299
2023-01-05 02:55:41 - progress_bar.py[line:274] - INFO: epoch 001:   7451 / 115845 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.1, ups=0.92, wpb=108.9, bsz=40, num_updates=7440, lr=4.8738e-05, gnorm=0.969, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20310
2023-01-05 02:55:52 - progress_bar.py[line:274] - INFO: epoch 001:   7461 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.89, wpb=112.5, bsz=40, num_updates=7450, lr=4.87335e-05, gnorm=0.736, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20322
2023-01-05 02:56:03 - progress_bar.py[line:274] - INFO: epoch 001:   7471 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.9, wpb=108.7, bsz=40, num_updates=7460, lr=4.8729e-05, gnorm=0.948, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20333
2023-01-05 02:56:14 - progress_bar.py[line:274] - INFO: epoch 001:   7481 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.3, ups=0.91, wpb=108.6, bsz=40, num_updates=7470, lr=4.87245e-05, gnorm=0.968, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20344
2023-01-05 02:56:25 - progress_bar.py[line:274] - INFO: epoch 001:   7491 / 115845 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100.5, ups=0.92, wpb=109.1, bsz=40, num_updates=7480, lr=4.872e-05, gnorm=0.946, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20355
2023-01-05 02:56:36 - progress_bar.py[line:274] - INFO: epoch 001:   7501 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98, ups=0.9, wpb=108.8, bsz=40, num_updates=7490, lr=4.87155e-05, gnorm=0.834, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20366
2023-01-05 02:56:48 - progress_bar.py[line:274] - INFO: epoch 001:   7511 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.3, ups=0.87, wpb=110.5, bsz=40, num_updates=7500, lr=4.8711e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20378
2023-01-05 02:56:59 - progress_bar.py[line:274] - INFO: epoch 001:   7521 / 115845 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.1, ups=0.9, wpb=109.1, bsz=40, num_updates=7510, lr=4.87065e-05, gnorm=0.755, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20389
2023-01-05 02:57:10 - progress_bar.py[line:274] - INFO: epoch 001:   7531 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98, ups=0.9, wpb=108.6, bsz=40, num_updates=7520, lr=4.8702e-05, gnorm=0.752, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20400
2023-01-05 02:57:22 - progress_bar.py[line:274] - INFO: epoch 001:   7541 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.89, wpb=108.1, bsz=40, num_updates=7530, lr=4.86975e-05, gnorm=0.777, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20411
2023-01-05 02:57:33 - progress_bar.py[line:274] - INFO: epoch 001:   7551 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.88, wpb=110.9, bsz=40, num_updates=7540, lr=4.8693e-05, gnorm=0.898, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20423
2023-01-05 02:57:44 - progress_bar.py[line:274] - INFO: epoch 001:   7561 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.3, ups=0.92, wpb=108.9, bsz=40, num_updates=7550, lr=4.86885e-05, gnorm=1.028, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20434
2023-01-05 02:57:55 - progress_bar.py[line:274] - INFO: epoch 001:   7571 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.4, ups=0.9, wpb=109.5, bsz=40, num_updates=7560, lr=4.8684e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20445
2023-01-05 02:58:06 - progress_bar.py[line:274] - INFO: epoch 001:   7581 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.2, ups=0.91, wpb=108.9, bsz=40, num_updates=7570, lr=4.86795e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20456
2023-01-05 02:58:17 - progress_bar.py[line:274] - INFO: epoch 001:   7591 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.92, wpb=107.9, bsz=40, num_updates=7580, lr=4.86751e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20467
2023-01-05 02:58:28 - progress_bar.py[line:274] - INFO: epoch 001:   7601 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98, ups=0.89, wpb=110.1, bsz=40, num_updates=7590, lr=4.86706e-05, gnorm=0.874, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20478
2023-01-05 02:58:39 - progress_bar.py[line:274] - INFO: epoch 001:   7611 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.4, ups=0.91, wpb=108.7, bsz=40, num_updates=7600, lr=4.86661e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20489
2023-01-05 02:58:50 - progress_bar.py[line:274] - INFO: epoch 001:   7621 / 115845 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.3, ups=0.93, wpb=108.6, bsz=40, num_updates=7610, lr=4.86616e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20500
2023-01-05 02:59:01 - progress_bar.py[line:274] - INFO: epoch 001:   7631 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.7, ups=0.91, wpb=109.7, bsz=40, num_updates=7620, lr=4.86571e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20511
2023-01-05 02:59:13 - progress_bar.py[line:274] - INFO: epoch 001:   7641 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.89, wpb=108.6, bsz=40, num_updates=7630, lr=4.86526e-05, gnorm=0.744, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20522
2023-01-05 02:59:24 - progress_bar.py[line:274] - INFO: epoch 001:   7651 / 115845 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.7, ups=0.89, wpb=109.3, bsz=40, num_updates=7640, lr=4.86481e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20534
2023-01-05 02:59:35 - progress_bar.py[line:274] - INFO: epoch 001:   7661 / 115845 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.9, ups=0.92, wpb=109.4, bsz=40, num_updates=7650, lr=4.86436e-05, gnorm=0.881, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20545
2023-01-05 02:59:46 - progress_bar.py[line:274] - INFO: epoch 001:   7671 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.7, ups=0.91, wpb=108.9, bsz=40, num_updates=7660, lr=4.86391e-05, gnorm=0.855, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20556
2023-01-05 02:59:57 - progress_bar.py[line:274] - INFO: epoch 001:   7681 / 115845 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.4, ups=0.92, wpb=108.3, bsz=40, num_updates=7670, lr=4.86346e-05, gnorm=0.711, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20567
2023-01-05 03:00:08 - progress_bar.py[line:274] - INFO: epoch 001:   7691 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=7680, lr=4.86301e-05, gnorm=0.704, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20578
2023-01-05 03:00:19 - progress_bar.py[line:274] - INFO: epoch 001:   7701 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.91, wpb=108.7, bsz=40, num_updates=7690, lr=4.86256e-05, gnorm=0.717, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20589
2023-01-05 03:00:30 - progress_bar.py[line:274] - INFO: epoch 001:   7711 / 115845 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.7, ups=0.9, wpb=109.1, bsz=40, num_updates=7700, lr=4.86211e-05, gnorm=0.865, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20600
2023-01-05 03:00:42 - progress_bar.py[line:274] - INFO: epoch 001:   7721 / 115845 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.9, ups=0.89, wpb=108.3, bsz=40, num_updates=7710, lr=4.86166e-05, gnorm=0.688, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20612
2023-01-05 03:00:53 - progress_bar.py[line:274] - INFO: epoch 001:   7731 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=94.1, ups=0.87, wpb=107.7, bsz=40, num_updates=7720, lr=4.86121e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20623
2023-01-05 03:01:05 - progress_bar.py[line:274] - INFO: epoch 001:   7741 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.88, wpb=109.8, bsz=40, num_updates=7730, lr=4.86076e-05, gnorm=0.9, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20634
2023-01-05 03:01:16 - progress_bar.py[line:274] - INFO: epoch 001:   7751 / 115845 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.8, ups=0.92, wpb=108.3, bsz=40, num_updates=7740, lr=4.86031e-05, gnorm=0.87, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20645
2023-01-05 03:01:27 - progress_bar.py[line:274] - INFO: epoch 001:   7761 / 115845 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.9, ups=0.88, wpb=108.6, bsz=40, num_updates=7750, lr=4.85986e-05, gnorm=0.745, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20657
2023-01-05 03:01:38 - progress_bar.py[line:274] - INFO: epoch 001:   7771 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99, ups=0.91, wpb=109, bsz=40, num_updates=7760, lr=4.85941e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20668
2023-01-05 03:01:49 - progress_bar.py[line:274] - INFO: epoch 001:   7781 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98, ups=0.89, wpb=109.8, bsz=40, num_updates=7770, lr=4.85896e-05, gnorm=0.757, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20679
2023-01-05 03:02:01 - progress_bar.py[line:274] - INFO: epoch 001:   7791 / 115845 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.3, ups=0.88, wpb=108, bsz=40, num_updates=7780, lr=4.85851e-05, gnorm=0.919, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20691
2023-01-05 03:02:12 - progress_bar.py[line:274] - INFO: epoch 001:   7801 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.7, ups=0.88, wpb=108.6, bsz=40, num_updates=7790, lr=4.85806e-05, gnorm=0.703, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20702
2023-01-05 03:02:23 - progress_bar.py[line:274] - INFO: epoch 001:   7811 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.6, ups=0.92, wpb=108.9, bsz=40, num_updates=7800, lr=4.85761e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20713
2023-01-05 03:02:34 - progress_bar.py[line:274] - INFO: epoch 001:   7821 / 115845 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.91, wpb=109.6, bsz=40, num_updates=7810, lr=4.85716e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20724
2023-01-05 03:02:46 - progress_bar.py[line:274] - INFO: epoch 001:   7831 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98, ups=0.9, wpb=109.4, bsz=40, num_updates=7820, lr=4.85672e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20735
2023-01-05 03:02:57 - progress_bar.py[line:274] - INFO: epoch 001:   7841 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=7830, lr=4.85627e-05, gnorm=0.992, clip=50, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20747
2023-01-05 03:03:08 - progress_bar.py[line:274] - INFO: epoch 001:   7851 / 115845 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=94.2, ups=0.87, wpb=108.3, bsz=40, num_updates=7840, lr=4.85582e-05, gnorm=0.939, clip=40, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20758
2023-01-05 03:03:20 - progress_bar.py[line:274] - INFO: epoch 001:   7861 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=7850, lr=4.85537e-05, gnorm=0.698, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20769
2023-01-05 03:03:31 - progress_bar.py[line:274] - INFO: epoch 001:   7871 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.92, wpb=107.2, bsz=40, num_updates=7860, lr=4.85492e-05, gnorm=0.858, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20780
2023-01-05 03:03:42 - progress_bar.py[line:274] - INFO: epoch 001:   7881 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.4, ups=0.9, wpb=108.8, bsz=40, num_updates=7870, lr=4.85447e-05, gnorm=0.672, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20792
2023-01-05 03:03:53 - progress_bar.py[line:274] - INFO: epoch 001:   7891 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=93.5, ups=0.87, wpb=107.2, bsz=40, num_updates=7880, lr=4.85402e-05, gnorm=0.692, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20803
2023-01-05 03:04:05 - progress_bar.py[line:274] - INFO: epoch 001:   7901 / 115845 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.4, ups=0.9, wpb=107.6, bsz=40, num_updates=7890, lr=4.85357e-05, gnorm=0.855, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20814
2023-01-05 03:04:16 - progress_bar.py[line:274] - INFO: epoch 001:   7911 / 115845 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=95.5, ups=0.88, wpb=108.1, bsz=40, num_updates=7900, lr=4.85312e-05, gnorm=0.694, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20826
2023-01-05 03:04:27 - progress_bar.py[line:274] - INFO: epoch 001:   7921 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=102.5, ups=0.92, wpb=111.5, bsz=40, num_updates=7910, lr=4.85267e-05, gnorm=0.813, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20837
2023-01-05 03:04:38 - progress_bar.py[line:274] - INFO: epoch 001:   7931 / 115845 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97, ups=0.89, wpb=108.9, bsz=40, num_updates=7920, lr=4.85222e-05, gnorm=1.022, clip=50, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=20848
2023-01-05 03:04:49 - progress_bar.py[line:274] - INFO: epoch 001:   7941 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.2, ups=0.91, wpb=110.5, bsz=40, num_updates=7930, lr=4.85177e-05, gnorm=0.842, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20859
2023-01-05 03:05:01 - progress_bar.py[line:274] - INFO: epoch 001:   7951 / 115845 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.8, ups=0.91, wpb=108, bsz=40, num_updates=7940, lr=4.85132e-05, gnorm=0.768, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20870
2023-01-05 03:05:12 - progress_bar.py[line:274] - INFO: epoch 001:   7961 / 115845 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.4, ups=0.89, wpb=110.4, bsz=40, num_updates=7950, lr=4.85087e-05, gnorm=0.793, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20882
2023-01-05 03:05:23 - progress_bar.py[line:274] - INFO: epoch 001:   7971 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.7, ups=0.91, wpb=109, bsz=40, num_updates=7960, lr=4.85042e-05, gnorm=0.781, clip=20, loss_scale=1024, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=20893
2023-01-05 03:05:34 - progress_bar.py[line:274] - INFO: epoch 001:   7981 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.3, ups=0.91, wpb=110.1, bsz=40, num_updates=7970, lr=4.84997e-05, gnorm=0.758, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20904
2023-01-05 03:05:45 - progress_bar.py[line:274] - INFO: epoch 001:   7991 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.9, wpb=109.4, bsz=40, num_updates=7980, lr=4.84952e-05, gnorm=0.605, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20915
2023-01-05 03:05:57 - progress_bar.py[line:274] - INFO: epoch 001:   8001 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97, ups=0.89, wpb=109, bsz=40, num_updates=7990, lr=4.84907e-05, gnorm=0.731, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20926
2023-01-05 03:06:08 - progress_bar.py[line:274] - INFO: epoch 001:   8011 / 115845 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.6, ups=0.89, wpb=108.8, bsz=40, num_updates=8000, lr=4.84862e-05, gnorm=0.654, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20938
2023-01-05 03:06:08 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 03:06:09 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 03:06:09 - train.py[line:551] - INFO: load:0.86 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 03:08:41 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 03:08:41 - train.py[line:551] - INFO: load:0.89 valid_run:152.03 task_valid:147.93 collect_output:2.99
2023-01-05 03:11:10 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 03:11:10 - train.py[line:551] - INFO: load:0.91 valid_run:300.75 task_valid:290.79 collect_output:7.82
2023-01-05 03:13:43 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 03:13:43 - train.py[line:551] - INFO: load:0.94 valid_run:453.39 task_valid:433.92 collect_output:16.30
2023-01-05 03:16:12 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 03:16:12 - train.py[line:551] - INFO: load:0.96 valid_run:602.12 task_valid:578.71 collect_output:19.23
2023-01-05 03:18:44 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 03:18:44 - train.py[line:551] - INFO: load:0.99 valid_run:754.13 task_valid:725.98 collect_output:22.95
2023-01-05 03:21:15 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 03:21:15 - train.py[line:551] - INFO: load:1.01 valid_run:905.84 task_valid:871.42 collect_output:28.13
2023-01-05 03:23:49 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 03:23:49 - train.py[line:551] - INFO: load:1.04 valid_run:1058.84 task_valid:1017.41 collect_output:34.11
2023-01-05 03:26:19 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 03:26:19 - train.py[line:551] - INFO: load:1.07 valid_run:1209.74 task_valid:1158.34 collect_output:43.06
2023-01-05 03:28:49 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 03:28:49 - train.py[line:551] - INFO: load:1.09 valid_run:1359.18 task_valid:1302.95 collect_output:46.86
2023-01-05 03:31:18 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 03:31:18 - train.py[line:551] - INFO: load:1.12 valid_run:1507.69 task_valid:1445.96 collect_output:51.31
2023-01-05 03:33:47 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 03:33:47 - train.py[line:551] - INFO: load:1.14 valid_run:1657.38 task_valid:1590.97 collect_output:54.95
2023-01-05 03:36:17 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 03:36:17 - train.py[line:551] - INFO: load:1.17 valid_run:1807.21 task_valid:1735.82 collect_output:58.92
2023-01-05 03:38:47 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 03:38:47 - train.py[line:551] - INFO: load:1.19 valid_run:1956.96 task_valid:1877.45 collect_output:65.99
2023-01-05 03:41:18 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 03:41:18 - train.py[line:551] - INFO: load:1.22 valid_run:2107.32 task_valid:2022.68 collect_output:70.07
2023-01-05 03:43:48 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 03:43:48 - train.py[line:551] - INFO: load:1.25 valid_run:2257.32 task_valid:2169.21 collect_output:72.44
2023-01-05 03:46:18 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 03:46:18 - train.py[line:551] - INFO: load:1.27 valid_run:2407.08 task_valid:2313.18 collect_output:77.19
2023-01-05 03:48:50 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 03:48:50 - train.py[line:551] - INFO: load:1.30 valid_run:2558.44 task_valid:2458.47 collect_output:82.23
2023-01-05 03:51:20 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 03:51:20 - train.py[line:551] - INFO: load:1.32 valid_run:2708.85 task_valid:2605.32 collect_output:84.77
2023-01-05 03:53:49 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 03:53:49 - train.py[line:551] - INFO: load:1.35 valid_run:2857.11 task_valid:2746.93 collect_output:90.39
2023-01-05 03:56:19 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 03:56:19 - train.py[line:551] - INFO: load:1.38 valid_run:3007.15 task_valid:2891.97 collect_output:94.36
2023-01-05 03:58:51 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 03:58:51 - train.py[line:551] - INFO: load:1.40 valid_run:3158.95 task_valid:3036.57 collect_output:100.48
2023-01-05 04:01:21 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 04:01:21 - train.py[line:551] - INFO: load:1.43 valid_run:3308.66 task_valid:3181.43 collect_output:104.25
2023-01-05 04:03:52 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 04:03:52 - train.py[line:551] - INFO: load:1.46 valid_run:3459.83 task_valid:3328.07 collect_output:107.70
2023-01-05 04:06:24 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 04:06:24 - train.py[line:551] - INFO: load:1.49 valid_run:3611.34 task_valid:3475.07 collect_output:111.07

====================================================================================================
SGG eval:     R @ 50: 0.4952;     R @ 100: 0.5639;     R @ 500: 0.6200;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3290;    mR @ 100: 0.4068;    mR @ 500: 0.4724;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7463) (covered in:0.8750) (covering:0.3143) (eating:0.6765) (flying in:0.2273) (growing on:0.2500) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.0833) (parked on:0.9583) (playing:0.0000) (riding:0.8938) (says:0.0000) (sitting on:0.6468) (standing on:0.1000) (using:0.5500) (walking in:0.3333) (walking on:0.7703) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 04:08:55 - train.py[line:487] - INFO: 0.5638995670995671

====================================================================================================
SGG eval:     R @ 50: 0.4952;     R @ 100: 0.5639;     R @ 500: 0.6200;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3290;    mR @ 100: 0.4068;    mR @ 500: 0.4724;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7463) (covered in:0.8750) (covering:0.3143) (eating:0.6765) (flying in:0.2273) (growing on:0.2500) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.0833) (parked on:0.9583) (playing:0.0000) (riding:0.8938) (says:0.0000) (sitting on:0.6468) (standing on:0.1000) (using:0.5500) (walking in:0.3333) (walking on:0.7703) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 04:08:55 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 04:08:55 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.396 | loss_v1 0 | loss_v2 0 | nll_loss 0.25 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.5639 | ppl 1.19 | vqa_score 0.5146 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 8000 | best_R@100 0.602033
2023-01-05 04:08:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2023-01-05 04:08:55 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-05 04:09:37 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-05 04:11:09 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.5638995670995671) (writing took 133.4888177551329 seconds)
2023-01-05 04:11:20 - progress_bar.py[line:274] - INFO: epoch 001:   8021 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=0.3, ups=0, wpb=110, bsz=40, num_updates=8010, lr=4.84817e-05, gnorm=0.893, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24850
2023-01-05 04:11:31 - progress_bar.py[line:274] - INFO: epoch 001:   8031 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101, ups=0.92, wpb=110.1, bsz=40, num_updates=8020, lr=4.84772e-05, gnorm=0.902, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24861
2023-01-05 04:11:33 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 04:11:43 - progress_bar.py[line:274] - INFO: epoch 001:   8042 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=88.3, ups=0.81, wpb=109, bsz=40, num_updates=8030, lr=4.84727e-05, gnorm=0.593, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=24873
2023-01-05 04:11:54 - progress_bar.py[line:274] - INFO: epoch 001:   8052 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.8, ups=0.91, wpb=110, bsz=40, num_updates=8040, lr=4.84682e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24884
2023-01-05 04:12:00 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-05 04:12:06 - progress_bar.py[line:274] - INFO: epoch 001:   8063 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.2, ups=0.87, wpb=109.4, bsz=40, num_updates=8050, lr=4.84637e-05, gnorm=0.768, clip=20, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24896
2023-01-05 04:12:17 - progress_bar.py[line:274] - INFO: epoch 001:   8073 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.9, ups=0.89, wpb=109.5, bsz=40, num_updates=8060, lr=4.84592e-05, gnorm=0.691, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24907
2023-01-05 04:12:28 - progress_bar.py[line:274] - INFO: epoch 001:   8083 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.1, ups=0.93, wpb=108.6, bsz=40, num_updates=8070, lr=4.84548e-05, gnorm=0.857, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24918
2023-01-05 04:12:40 - progress_bar.py[line:274] - INFO: epoch 001:   8093 / 115845 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.1, ups=0.88, wpb=107.7, bsz=40, num_updates=8080, lr=4.84503e-05, gnorm=0.601, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24929
2023-01-05 04:12:51 - progress_bar.py[line:274] - INFO: epoch 001:   8103 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.3, ups=0.92, wpb=109.2, bsz=40, num_updates=8090, lr=4.84458e-05, gnorm=0.859, clip=20, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24940
2023-01-05 04:13:02 - progress_bar.py[line:274] - INFO: epoch 001:   8113 / 115845 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=99.6, ups=0.91, wpb=109.5, bsz=40, num_updates=8100, lr=4.84413e-05, gnorm=0.707, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24951
2023-01-05 04:13:13 - progress_bar.py[line:274] - INFO: epoch 001:   8123 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.89, wpb=109.1, bsz=40, num_updates=8110, lr=4.84368e-05, gnorm=0.772, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24963
2023-01-05 04:13:24 - progress_bar.py[line:274] - INFO: epoch 001:   8133 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=8120, lr=4.84323e-05, gnorm=0.621, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24974
2023-01-05 04:13:36 - progress_bar.py[line:274] - INFO: epoch 001:   8143 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.1, ups=0.88, wpb=109.8, bsz=40, num_updates=8130, lr=4.84278e-05, gnorm=0.808, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24985
2023-01-05 04:13:47 - progress_bar.py[line:274] - INFO: epoch 001:   8153 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.88, wpb=109.7, bsz=40, num_updates=8140, lr=4.84233e-05, gnorm=1.061, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24997
2023-01-05 04:13:58 - progress_bar.py[line:274] - INFO: epoch 001:   8163 / 115845 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.2, ups=0.91, wpb=108.5, bsz=40, num_updates=8150, lr=4.84188e-05, gnorm=0.742, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25008
2023-01-05 04:14:09 - progress_bar.py[line:274] - INFO: epoch 001:   8173 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.2, ups=0.92, wpb=110.2, bsz=40, num_updates=8160, lr=4.84143e-05, gnorm=1.111, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25019
2023-01-05 04:14:20 - progress_bar.py[line:274] - INFO: epoch 001:   8183 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.8, ups=0.91, wpb=111.1, bsz=40, num_updates=8170, lr=4.84098e-05, gnorm=0.789, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25030
2023-01-05 04:14:32 - progress_bar.py[line:274] - INFO: epoch 001:   8193 / 115845 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95, ups=0.87, wpb=108.8, bsz=40, num_updates=8180, lr=4.84053e-05, gnorm=0.917, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25041
2023-01-05 04:14:43 - progress_bar.py[line:274] - INFO: epoch 001:   8203 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=8190, lr=4.84008e-05, gnorm=0.723, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25053
2023-01-05 04:14:54 - progress_bar.py[line:274] - INFO: epoch 001:   8213 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.9, wpb=108.2, bsz=40, num_updates=8200, lr=4.83963e-05, gnorm=0.827, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25064
2023-01-05 04:15:05 - progress_bar.py[line:274] - INFO: epoch 001:   8223 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.9, ups=0.9, wpb=108.2, bsz=40, num_updates=8210, lr=4.83918e-05, gnorm=0.873, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25075
2023-01-05 04:15:17 - progress_bar.py[line:274] - INFO: epoch 001:   8233 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.9, wpb=108.8, bsz=40, num_updates=8220, lr=4.83873e-05, gnorm=0.903, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25086
2023-01-05 04:15:28 - progress_bar.py[line:274] - INFO: epoch 001:   8243 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97, ups=0.89, wpb=108.4, bsz=40, num_updates=8230, lr=4.83828e-05, gnorm=0.667, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25098
2023-01-05 04:15:39 - progress_bar.py[line:274] - INFO: epoch 001:   8253 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.6, ups=0.88, wpb=108.5, bsz=40, num_updates=8240, lr=4.83783e-05, gnorm=0.69, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25109
2023-01-05 04:15:51 - progress_bar.py[line:274] - INFO: epoch 001:   8263 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.2, ups=0.87, wpb=109.4, bsz=40, num_updates=8250, lr=4.83738e-05, gnorm=0.688, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25121
2023-01-05 04:16:02 - progress_bar.py[line:274] - INFO: epoch 001:   8273 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.5, ups=0.89, wpb=108.1, bsz=40, num_updates=8260, lr=4.83693e-05, gnorm=0.85, clip=30, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25132
2023-01-05 04:16:14 - progress_bar.py[line:274] - INFO: epoch 001:   8283 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.7, ups=0.89, wpb=109.3, bsz=40, num_updates=8270, lr=4.83648e-05, gnorm=0.664, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25143
2023-01-05 04:16:25 - progress_bar.py[line:274] - INFO: epoch 001:   8293 / 115845 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=8280, lr=4.83603e-05, gnorm=0.683, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25155
2023-01-05 04:16:36 - progress_bar.py[line:274] - INFO: epoch 001:   8303 / 115845 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.3, ups=0.89, wpb=108.1, bsz=40, num_updates=8290, lr=4.83558e-05, gnorm=0.659, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25166
2023-01-05 04:16:48 - progress_bar.py[line:274] - INFO: epoch 001:   8313 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.3, ups=0.88, wpb=109.2, bsz=40, num_updates=8300, lr=4.83513e-05, gnorm=0.694, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25177
2023-01-05 04:16:59 - progress_bar.py[line:274] - INFO: epoch 001:   8323 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.4, ups=0.92, wpb=109.7, bsz=40, num_updates=8310, lr=4.83469e-05, gnorm=0.855, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25188
2023-01-05 04:17:10 - progress_bar.py[line:274] - INFO: epoch 001:   8333 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.91, wpb=108, bsz=40, num_updates=8320, lr=4.83424e-05, gnorm=0.753, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25200
2023-01-05 04:17:21 - progress_bar.py[line:274] - INFO: epoch 001:   8343 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.9, ups=0.91, wpb=109.2, bsz=40, num_updates=8330, lr=4.83379e-05, gnorm=0.707, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25211
2023-01-05 04:17:32 - progress_bar.py[line:274] - INFO: epoch 001:   8353 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.9, ups=0.93, wpb=109.6, bsz=40, num_updates=8340, lr=4.83334e-05, gnorm=0.752, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25221
2023-01-05 04:17:43 - progress_bar.py[line:274] - INFO: epoch 001:   8363 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.1, ups=0.87, wpb=109.4, bsz=40, num_updates=8350, lr=4.83289e-05, gnorm=0.688, clip=0, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25233
2023-01-05 04:17:55 - progress_bar.py[line:274] - INFO: epoch 001:   8373 / 115845 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=97.6, ups=0.9, wpb=108.6, bsz=40, num_updates=8360, lr=4.83244e-05, gnorm=0.633, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25244
2023-01-05 04:18:06 - progress_bar.py[line:274] - INFO: epoch 001:   8383 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=8370, lr=4.83199e-05, gnorm=0.684, clip=10, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25256
2023-01-05 04:18:18 - progress_bar.py[line:274] - INFO: epoch 001:   8393 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.3, ups=0.87, wpb=109.2, bsz=40, num_updates=8380, lr=4.83154e-05, gnorm=0.945, clip=20, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25267
2023-01-05 04:18:29 - progress_bar.py[line:274] - INFO: epoch 001:   8403 / 115845 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=8390, lr=4.83109e-05, gnorm=0.604, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25279
2023-01-05 04:18:40 - progress_bar.py[line:274] - INFO: epoch 001:   8413 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.2, ups=0.88, wpb=108.8, bsz=40, num_updates=8400, lr=4.83064e-05, gnorm=0.758, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25290
2023-01-05 04:18:52 - progress_bar.py[line:274] - INFO: epoch 001:   8423 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=8410, lr=4.83019e-05, gnorm=0.922, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25301
2023-01-05 04:19:03 - progress_bar.py[line:274] - INFO: epoch 001:   8433 / 115845 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.6, ups=0.92, wpb=108.3, bsz=40, num_updates=8420, lr=4.82974e-05, gnorm=0.712, clip=10, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25312
2023-01-05 04:19:14 - progress_bar.py[line:274] - INFO: epoch 001:   8443 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.91, wpb=110.1, bsz=40, num_updates=8430, lr=4.82929e-05, gnorm=0.72, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25323
2023-01-05 04:19:24 - progress_bar.py[line:274] - INFO: epoch 001:   8453 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=104.5, ups=0.94, wpb=110.7, bsz=40, num_updates=8440, lr=4.82884e-05, gnorm=0.785, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25334
2023-01-05 04:19:35 - progress_bar.py[line:274] - INFO: epoch 001:   8463 / 115845 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.2, ups=0.91, wpb=109.2, bsz=40, num_updates=8450, lr=4.82839e-05, gnorm=0.636, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25345
2023-01-05 04:19:46 - progress_bar.py[line:274] - INFO: epoch 001:   8473 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.4, ups=0.93, wpb=107.9, bsz=40, num_updates=8460, lr=4.82794e-05, gnorm=0.662, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25356
2023-01-05 04:19:58 - progress_bar.py[line:274] - INFO: epoch 001:   8483 / 115845 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=94.9, ups=0.88, wpb=108.2, bsz=40, num_updates=8470, lr=4.82749e-05, gnorm=0.708, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25367
2023-01-05 04:20:09 - progress_bar.py[line:274] - INFO: epoch 001:   8493 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.9, wpb=108.4, bsz=40, num_updates=8480, lr=4.82704e-05, gnorm=0.692, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25379
2023-01-05 04:20:20 - progress_bar.py[line:274] - INFO: epoch 001:   8503 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.9, ups=0.92, wpb=109.8, bsz=40, num_updates=8490, lr=4.82659e-05, gnorm=0.636, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25390
2023-01-05 04:20:31 - progress_bar.py[line:274] - INFO: epoch 001:   8513 / 115845 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=8500, lr=4.82614e-05, gnorm=0.64, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25401
2023-01-05 04:20:43 - progress_bar.py[line:274] - INFO: epoch 001:   8523 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96, ups=0.87, wpb=110.3, bsz=40, num_updates=8510, lr=4.82569e-05, gnorm=0.802, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25412
2023-01-05 04:20:54 - progress_bar.py[line:274] - INFO: epoch 001:   8533 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.7, ups=0.89, wpb=108.2, bsz=40, num_updates=8520, lr=4.82524e-05, gnorm=0.699, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25424
2023-01-05 04:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   8543 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101, ups=0.92, wpb=109.9, bsz=40, num_updates=8530, lr=4.82479e-05, gnorm=0.744, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25435
2023-01-05 04:21:16 - progress_bar.py[line:274] - INFO: epoch 001:   8553 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.9, ups=0.9, wpb=109.4, bsz=40, num_updates=8540, lr=4.82434e-05, gnorm=0.84, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25446
2023-01-05 04:21:27 - progress_bar.py[line:274] - INFO: epoch 001:   8563 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=8550, lr=4.82389e-05, gnorm=0.732, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25457
2023-01-05 04:21:39 - progress_bar.py[line:274] - INFO: epoch 001:   8573 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.9, ups=0.88, wpb=108.6, bsz=40, num_updates=8560, lr=4.82345e-05, gnorm=0.797, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25468
2023-01-05 04:21:50 - progress_bar.py[line:274] - INFO: epoch 001:   8583 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.91, wpb=109.6, bsz=40, num_updates=8570, lr=4.823e-05, gnorm=0.727, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25479
2023-01-05 04:22:01 - progress_bar.py[line:274] - INFO: epoch 001:   8593 / 115845 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=107.1, nsentences=40, sample_size=107.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.6, ups=0.9, wpb=107.1, bsz=40, num_updates=8580, lr=4.82255e-05, gnorm=0.943, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25491
2023-01-05 04:22:12 - progress_bar.py[line:274] - INFO: epoch 001:   8603 / 115845 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.1, ups=0.91, wpb=109.1, bsz=40, num_updates=8590, lr=4.8221e-05, gnorm=0.62, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25502
2023-01-05 04:22:23 - progress_bar.py[line:274] - INFO: epoch 001:   8613 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.9, ups=0.91, wpb=109.3, bsz=40, num_updates=8600, lr=4.82165e-05, gnorm=0.61, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25513
2023-01-05 04:22:34 - progress_bar.py[line:274] - INFO: epoch 001:   8623 / 115845 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=98.1, ups=0.91, wpb=108.2, bsz=40, num_updates=8610, lr=4.8212e-05, gnorm=0.719, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25524
2023-01-05 04:22:46 - progress_bar.py[line:274] - INFO: epoch 001:   8633 / 115845 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.9, ups=0.89, wpb=108.5, bsz=40, num_updates=8620, lr=4.82075e-05, gnorm=0.82, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25535
2023-01-05 04:22:57 - progress_bar.py[line:274] - INFO: epoch 001:   8643 / 115845 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.6, ups=0.9, wpb=108.3, bsz=40, num_updates=8630, lr=4.8203e-05, gnorm=0.829, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25546
2023-01-05 04:23:08 - progress_bar.py[line:274] - INFO: epoch 001:   8653 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.4, ups=0.91, wpb=109.9, bsz=40, num_updates=8640, lr=4.81985e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25557
2023-01-05 04:23:19 - progress_bar.py[line:274] - INFO: epoch 001:   8663 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95, ups=0.87, wpb=109.1, bsz=40, num_updates=8650, lr=4.8194e-05, gnorm=0.886, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25569
2023-01-05 04:23:30 - progress_bar.py[line:274] - INFO: epoch 001:   8673 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.2, ups=0.93, wpb=109.3, bsz=40, num_updates=8660, lr=4.81895e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25580
2023-01-05 04:23:41 - progress_bar.py[line:274] - INFO: epoch 001:   8683 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.9, wpb=108.3, bsz=40, num_updates=8670, lr=4.8185e-05, gnorm=0.936, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25591
2023-01-05 04:23:53 - progress_bar.py[line:274] - INFO: epoch 001:   8693 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=8680, lr=4.81805e-05, gnorm=0.678, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25603
2023-01-05 04:24:04 - progress_bar.py[line:274] - INFO: epoch 001:   8703 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.89, wpb=109.6, bsz=40, num_updates=8690, lr=4.8176e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25614
2023-01-05 04:24:15 - progress_bar.py[line:274] - INFO: epoch 001:   8713 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100, ups=0.92, wpb=108.6, bsz=40, num_updates=8700, lr=4.81715e-05, gnorm=0.805, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25625
2023-01-05 04:24:27 - progress_bar.py[line:274] - INFO: epoch 001:   8723 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=8710, lr=4.8167e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25636
2023-01-05 04:24:38 - progress_bar.py[line:274] - INFO: epoch 001:   8733 / 115845 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99, ups=0.91, wpb=109.1, bsz=40, num_updates=8720, lr=4.81625e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25647
2023-01-05 04:24:49 - progress_bar.py[line:274] - INFO: epoch 001:   8743 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=8730, lr=4.8158e-05, gnorm=0.577, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25659
2023-01-05 04:25:00 - progress_bar.py[line:274] - INFO: epoch 001:   8753 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.89, wpb=109.4, bsz=40, num_updates=8740, lr=4.81535e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25670
2023-01-05 04:25:11 - progress_bar.py[line:274] - INFO: epoch 001:   8763 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=104, ups=0.94, wpb=110.5, bsz=40, num_updates=8750, lr=4.8149e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25681
2023-01-05 04:25:22 - progress_bar.py[line:274] - INFO: epoch 001:   8773 / 115845 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.9, ups=0.91, wpb=107, bsz=40, num_updates=8760, lr=4.81445e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25692
2023-01-05 04:25:34 - progress_bar.py[line:274] - INFO: epoch 001:   8783 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.3, ups=0.89, wpb=107, bsz=40, num_updates=8770, lr=4.814e-05, gnorm=0.719, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25703
2023-01-05 04:25:45 - progress_bar.py[line:274] - INFO: epoch 001:   8793 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.89, wpb=108.3, bsz=40, num_updates=8780, lr=4.81355e-05, gnorm=0.933, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25715
2023-01-05 04:25:56 - progress_bar.py[line:274] - INFO: epoch 001:   8803 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.8, ups=0.9, wpb=110.3, bsz=40, num_updates=8790, lr=4.8131e-05, gnorm=0.794, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25726
2023-01-05 04:26:07 - progress_bar.py[line:274] - INFO: epoch 001:   8813 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.1, ups=0.94, wpb=109.3, bsz=40, num_updates=8800, lr=4.81266e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25736
2023-01-05 04:26:18 - progress_bar.py[line:274] - INFO: epoch 001:   8823 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103.1, ups=0.93, wpb=110.6, bsz=40, num_updates=8810, lr=4.81221e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25747
2023-01-05 04:26:29 - progress_bar.py[line:274] - INFO: epoch 001:   8833 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.9, ups=0.88, wpb=108.8, bsz=40, num_updates=8820, lr=4.81176e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25759
2023-01-05 04:26:40 - progress_bar.py[line:274] - INFO: epoch 001:   8843 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=102.8, ups=0.94, wpb=109.8, bsz=40, num_updates=8830, lr=4.81131e-05, gnorm=0.838, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25770
2023-01-05 04:26:51 - progress_bar.py[line:274] - INFO: epoch 001:   8853 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.86, wpb=110.5, bsz=40, num_updates=8840, lr=4.81086e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=25781
2023-01-05 04:27:03 - progress_bar.py[line:274] - INFO: epoch 001:   8863 / 115845 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=106.1, nsentences=40, sample_size=106.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.2, ups=0.91, wpb=106.1, bsz=40, num_updates=8850, lr=4.81041e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25792
2023-01-05 04:27:14 - progress_bar.py[line:274] - INFO: epoch 001:   8873 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.91, wpb=109.3, bsz=40, num_updates=8860, lr=4.80996e-05, gnorm=0.601, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25803
2023-01-05 04:27:25 - progress_bar.py[line:274] - INFO: epoch 001:   8883 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.5, ups=0.9, wpb=108.5, bsz=40, num_updates=8870, lr=4.80951e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25815
2023-01-05 04:27:36 - progress_bar.py[line:274] - INFO: epoch 001:   8893 / 115845 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.1, ups=0.91, wpb=109.3, bsz=40, num_updates=8880, lr=4.80906e-05, gnorm=0.774, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25826
2023-01-05 04:27:47 - progress_bar.py[line:274] - INFO: epoch 001:   8903 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.8, ups=0.95, wpb=108.8, bsz=40, num_updates=8890, lr=4.80861e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25836
2023-01-05 04:27:58 - progress_bar.py[line:274] - INFO: epoch 001:   8913 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.89, wpb=109.4, bsz=40, num_updates=8900, lr=4.80816e-05, gnorm=0.771, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25848
2023-01-05 04:28:09 - progress_bar.py[line:274] - INFO: epoch 001:   8923 / 115845 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=103, ups=0.95, wpb=108.6, bsz=40, num_updates=8910, lr=4.80771e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=25858
2023-01-05 04:28:20 - progress_bar.py[line:274] - INFO: epoch 001:   8933 / 115845 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97, ups=0.9, wpb=107.4, bsz=40, num_updates=8920, lr=4.80726e-05, gnorm=0.795, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25869
2023-01-05 04:28:31 - progress_bar.py[line:274] - INFO: epoch 001:   8943 / 115845 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.3, ups=0.89, wpb=109.2, bsz=40, num_updates=8930, lr=4.80681e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25881
2023-01-05 04:28:42 - progress_bar.py[line:274] - INFO: epoch 001:   8953 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.88, wpb=109.7, bsz=40, num_updates=8940, lr=4.80636e-05, gnorm=0.574, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25892
2023-01-05 04:28:54 - progress_bar.py[line:274] - INFO: epoch 001:   8963 / 115845 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.5, ups=0.88, wpb=109.2, bsz=40, num_updates=8950, lr=4.80591e-05, gnorm=0.731, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=25904
2023-01-05 04:29:05 - progress_bar.py[line:274] - INFO: epoch 001:   8973 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.2, ups=0.92, wpb=109, bsz=40, num_updates=8960, lr=4.80546e-05, gnorm=0.819, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25914
2023-01-05 04:29:16 - progress_bar.py[line:274] - INFO: epoch 001:   8983 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101, ups=0.91, wpb=110.5, bsz=40, num_updates=8970, lr=4.80501e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25926
2023-01-05 04:29:27 - progress_bar.py[line:274] - INFO: epoch 001:   8993 / 115845 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.9, ups=0.91, wpb=108.5, bsz=40, num_updates=8980, lr=4.80456e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25937
2023-01-05 04:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   9003 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.8, ups=0.89, wpb=110.1, bsz=40, num_updates=8990, lr=4.80411e-05, gnorm=0.661, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25948
2023-01-05 04:29:49 - progress_bar.py[line:274] - INFO: epoch 001:   9013 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.2, ups=0.92, wpb=110.1, bsz=40, num_updates=9000, lr=4.80366e-05, gnorm=0.587, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25959
2023-01-05 04:30:01 - progress_bar.py[line:274] - INFO: epoch 001:   9023 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.88, wpb=109.3, bsz=40, num_updates=9010, lr=4.80321e-05, gnorm=0.549, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25970
2023-01-05 04:30:12 - progress_bar.py[line:274] - INFO: epoch 001:   9033 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.2, ups=0.91, wpb=110.2, bsz=40, num_updates=9020, lr=4.80276e-05, gnorm=0.559, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25981
2023-01-05 04:30:23 - progress_bar.py[line:274] - INFO: epoch 001:   9043 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.89, wpb=108.7, bsz=40, num_updates=9030, lr=4.80231e-05, gnorm=0.802, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25993
2023-01-05 04:30:34 - progress_bar.py[line:274] - INFO: epoch 001:   9053 / 115845 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.9, ups=0.91, wpb=108.3, bsz=40, num_updates=9040, lr=4.80186e-05, gnorm=0.75, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26004
2023-01-05 04:30:45 - progress_bar.py[line:274] - INFO: epoch 001:   9063 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.92, wpb=109.4, bsz=40, num_updates=9050, lr=4.80142e-05, gnorm=0.617, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26015
2023-01-05 04:30:56 - progress_bar.py[line:274] - INFO: epoch 001:   9073 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.8, ups=0.88, wpb=108.9, bsz=40, num_updates=9060, lr=4.80097e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26026
2023-01-05 04:31:08 - progress_bar.py[line:274] - INFO: epoch 001:   9083 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.7, ups=0.9, wpb=109.2, bsz=40, num_updates=9070, lr=4.80052e-05, gnorm=0.596, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26037
2023-01-05 04:31:19 - progress_bar.py[line:274] - INFO: epoch 001:   9093 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=94.9, ups=0.88, wpb=107.7, bsz=40, num_updates=9080, lr=4.80007e-05, gnorm=0.579, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26049
2023-01-05 04:31:30 - progress_bar.py[line:274] - INFO: epoch 001:   9103 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.91, wpb=109.9, bsz=40, num_updates=9090, lr=4.79962e-05, gnorm=0.808, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26060
2023-01-05 04:31:42 - progress_bar.py[line:274] - INFO: epoch 001:   9113 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=9100, lr=4.79917e-05, gnorm=0.813, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26071
2023-01-05 04:31:52 - progress_bar.py[line:274] - INFO: epoch 001:   9123 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.5, ups=0.92, wpb=110.4, bsz=40, num_updates=9110, lr=4.79872e-05, gnorm=0.706, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26082
2023-01-05 04:32:04 - progress_bar.py[line:274] - INFO: epoch 001:   9133 / 115845 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.5, ups=0.9, wpb=109.6, bsz=40, num_updates=9120, lr=4.79827e-05, gnorm=0.549, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26093
2023-01-05 04:32:15 - progress_bar.py[line:274] - INFO: epoch 001:   9143 / 115845 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.1, ups=0.87, wpb=109.2, bsz=40, num_updates=9130, lr=4.79782e-05, gnorm=0.566, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26105
2023-01-05 04:32:22 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 04:32:28 - progress_bar.py[line:274] - INFO: epoch 001:   9154 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=86.2, ups=0.79, wpb=109.1, bsz=40, num_updates=9140, lr=4.79737e-05, gnorm=0.572, clip=0, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=26118
2023-01-05 04:32:34 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-05 04:32:40 - progress_bar.py[line:274] - INFO: epoch 001:   9165 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=89.7, ups=0.83, wpb=108, bsz=40, num_updates=9150, lr=4.79692e-05, gnorm=0.704, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26130
2023-01-05 04:32:51 - progress_bar.py[line:274] - INFO: epoch 001:   9175 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.9, wpb=107.5, bsz=40, num_updates=9160, lr=4.79647e-05, gnorm=0.823, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26141
2023-01-05 04:33:02 - progress_bar.py[line:274] - INFO: epoch 001:   9185 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.9, ups=0.91, wpb=108.9, bsz=40, num_updates=9170, lr=4.79602e-05, gnorm=0.798, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26152
2023-01-05 04:33:14 - progress_bar.py[line:274] - INFO: epoch 001:   9195 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.9, ups=0.91, wpb=108.2, bsz=40, num_updates=9180, lr=4.79557e-05, gnorm=0.664, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26163
2023-01-05 04:33:25 - progress_bar.py[line:274] - INFO: epoch 001:   9205 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.89, wpb=110.5, bsz=40, num_updates=9190, lr=4.79512e-05, gnorm=0.663, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26175
2023-01-05 04:33:36 - progress_bar.py[line:274] - INFO: epoch 001:   9215 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.9, ups=0.91, wpb=109.4, bsz=40, num_updates=9200, lr=4.79467e-05, gnorm=0.762, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26186
2023-01-05 04:33:48 - progress_bar.py[line:274] - INFO: epoch 001:   9225 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95, ups=0.87, wpb=108.8, bsz=40, num_updates=9210, lr=4.79422e-05, gnorm=0.672, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26197
2023-01-05 04:33:59 - progress_bar.py[line:274] - INFO: epoch 001:   9235 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.2, ups=0.87, wpb=110.2, bsz=40, num_updates=9220, lr=4.79377e-05, gnorm=0.702, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26209
2023-01-05 04:34:10 - progress_bar.py[line:274] - INFO: epoch 001:   9245 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=96.8, ups=0.89, wpb=108.7, bsz=40, num_updates=9230, lr=4.79332e-05, gnorm=0.809, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26220
2023-01-05 04:34:22 - progress_bar.py[line:274] - INFO: epoch 001:   9255 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.2, ups=0.9, wpb=109.3, bsz=40, num_updates=9240, lr=4.79287e-05, gnorm=0.715, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26231
2023-01-05 04:34:33 - progress_bar.py[line:274] - INFO: epoch 001:   9265 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99, ups=0.91, wpb=108.7, bsz=40, num_updates=9250, lr=4.79242e-05, gnorm=0.635, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26242
2023-01-05 04:34:44 - progress_bar.py[line:274] - INFO: epoch 001:   9275 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96, ups=0.88, wpb=108.6, bsz=40, num_updates=9260, lr=4.79197e-05, gnorm=0.703, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26254
2023-01-05 04:34:55 - progress_bar.py[line:274] - INFO: epoch 001:   9285 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.5, ups=0.93, wpb=109, bsz=40, num_updates=9270, lr=4.79152e-05, gnorm=0.67, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26265
2023-01-05 04:35:06 - progress_bar.py[line:274] - INFO: epoch 001:   9295 / 115845 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.9, wpb=108.9, bsz=40, num_updates=9280, lr=4.79107e-05, gnorm=0.633, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26276
2023-01-05 04:35:17 - progress_bar.py[line:274] - INFO: epoch 001:   9305 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.1, ups=0.93, wpb=109.4, bsz=40, num_updates=9290, lr=4.79063e-05, gnorm=0.59, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26287
2023-01-05 04:35:28 - progress_bar.py[line:274] - INFO: epoch 001:   9315 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.7, ups=0.88, wpb=108.6, bsz=40, num_updates=9300, lr=4.79018e-05, gnorm=0.674, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26298
2023-01-05 04:35:40 - progress_bar.py[line:274] - INFO: epoch 001:   9325 / 115845 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=96.3, ups=0.9, wpb=107.5, bsz=40, num_updates=9310, lr=4.78973e-05, gnorm=0.701, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26309
2023-01-05 04:35:51 - progress_bar.py[line:274] - INFO: epoch 001:   9335 / 115845 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.2, ups=0.91, wpb=108.3, bsz=40, num_updates=9320, lr=4.78928e-05, gnorm=0.67, clip=0, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26320
2023-01-05 04:36:02 - progress_bar.py[line:274] - INFO: epoch 001:   9345 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.4, ups=0.93, wpb=108.5, bsz=40, num_updates=9330, lr=4.78883e-05, gnorm=0.948, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26331
2023-01-05 04:36:13 - progress_bar.py[line:274] - INFO: epoch 001:   9355 / 115845 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=93.8, ups=0.87, wpb=107.7, bsz=40, num_updates=9340, lr=4.78838e-05, gnorm=0.71, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26343
2023-01-05 04:36:25 - progress_bar.py[line:274] - INFO: epoch 001:   9365 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.6, ups=0.88, wpb=109.5, bsz=40, num_updates=9350, lr=4.78793e-05, gnorm=0.626, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26354
2023-01-05 04:36:36 - progress_bar.py[line:274] - INFO: epoch 001:   9375 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.3, ups=0.91, wpb=108.8, bsz=40, num_updates=9360, lr=4.78748e-05, gnorm=0.563, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26365
2023-01-05 04:36:47 - progress_bar.py[line:274] - INFO: epoch 001:   9385 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.91, wpb=109, bsz=40, num_updates=9370, lr=4.78703e-05, gnorm=0.67, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26376
2023-01-05 04:36:58 - progress_bar.py[line:274] - INFO: epoch 001:   9395 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97, ups=0.88, wpb=109.7, bsz=40, num_updates=9380, lr=4.78658e-05, gnorm=0.736, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26388
2023-01-05 04:37:10 - progress_bar.py[line:274] - INFO: epoch 001:   9405 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.5, ups=0.89, wpb=108.7, bsz=40, num_updates=9390, lr=4.78613e-05, gnorm=0.668, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26399
2023-01-05 04:37:21 - progress_bar.py[line:274] - INFO: epoch 001:   9415 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=9400, lr=4.78568e-05, gnorm=0.68, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26411
2023-01-05 04:37:32 - progress_bar.py[line:274] - INFO: epoch 001:   9425 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=9410, lr=4.78523e-05, gnorm=0.61, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26421
2023-01-05 04:37:43 - progress_bar.py[line:274] - INFO: epoch 001:   9435 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.4, ups=0.9, wpb=109.2, bsz=40, num_updates=9420, lr=4.78478e-05, gnorm=0.687, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26433
2023-01-05 04:37:54 - progress_bar.py[line:274] - INFO: epoch 001:   9445 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.6, ups=0.91, wpb=109.4, bsz=40, num_updates=9430, lr=4.78433e-05, gnorm=0.729, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26444
2023-01-05 04:38:05 - progress_bar.py[line:274] - INFO: epoch 001:   9455 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.4, ups=0.93, wpb=110, bsz=40, num_updates=9440, lr=4.78388e-05, gnorm=0.599, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26454
2023-01-05 04:38:16 - progress_bar.py[line:274] - INFO: epoch 001:   9465 / 115845 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100.1, ups=0.92, wpb=108.4, bsz=40, num_updates=9450, lr=4.78343e-05, gnorm=0.757, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26465
2023-01-05 04:38:27 - progress_bar.py[line:274] - INFO: epoch 001:   9475 / 115845 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.6, ups=0.89, wpb=107.5, bsz=40, num_updates=9460, lr=4.78298e-05, gnorm=0.722, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26477
2023-01-05 04:38:38 - progress_bar.py[line:274] - INFO: epoch 001:   9485 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.89, wpb=109.1, bsz=40, num_updates=9470, lr=4.78253e-05, gnorm=0.725, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26488
2023-01-05 04:38:49 - progress_bar.py[line:274] - INFO: epoch 001:   9495 / 115845 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97, ups=0.9, wpb=108.1, bsz=40, num_updates=9480, lr=4.78208e-05, gnorm=0.546, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26499
2023-01-05 04:39:00 - progress_bar.py[line:274] - INFO: epoch 001:   9505 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=102.1, ups=0.94, wpb=108.2, bsz=40, num_updates=9490, lr=4.78163e-05, gnorm=0.609, clip=0, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=26510
2023-01-05 04:39:11 - progress_bar.py[line:274] - INFO: epoch 001:   9515 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.9, wpb=109.3, bsz=40, num_updates=9500, lr=4.78118e-05, gnorm=0.65, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26521
2023-01-05 04:39:22 - progress_bar.py[line:274] - INFO: epoch 001:   9525 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.8, ups=0.91, wpb=109.1, bsz=40, num_updates=9510, lr=4.78073e-05, gnorm=0.653, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26532
2023-01-05 04:39:33 - progress_bar.py[line:274] - INFO: epoch 001:   9535 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.6, ups=0.93, wpb=108.8, bsz=40, num_updates=9520, lr=4.78028e-05, gnorm=0.802, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26543
2023-01-05 04:39:44 - progress_bar.py[line:274] - INFO: epoch 001:   9545 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.7, ups=0.91, wpb=108.8, bsz=40, num_updates=9530, lr=4.77983e-05, gnorm=0.74, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26554
2023-01-05 04:39:56 - progress_bar.py[line:274] - INFO: epoch 001:   9555 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.4, ups=0.87, wpb=109.4, bsz=40, num_updates=9540, lr=4.77939e-05, gnorm=0.691, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26566
2023-01-05 04:40:07 - progress_bar.py[line:274] - INFO: epoch 001:   9565 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.7, ups=0.91, wpb=110.2, bsz=40, num_updates=9550, lr=4.77894e-05, gnorm=0.542, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26577
2023-01-05 04:40:18 - progress_bar.py[line:274] - INFO: epoch 001:   9575 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.2, ups=0.9, wpb=108.6, bsz=40, num_updates=9560, lr=4.77849e-05, gnorm=0.678, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26588
2023-01-05 04:40:29 - progress_bar.py[line:274] - INFO: epoch 001:   9585 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.5, ups=0.89, wpb=107.6, bsz=40, num_updates=9570, lr=4.77804e-05, gnorm=0.732, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26599
2023-01-05 04:40:41 - progress_bar.py[line:274] - INFO: epoch 001:   9595 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.8, ups=0.9, wpb=109.8, bsz=40, num_updates=9580, lr=4.77759e-05, gnorm=0.718, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26610
2023-01-05 04:40:52 - progress_bar.py[line:274] - INFO: epoch 001:   9605 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.2, ups=0.91, wpb=110.5, bsz=40, num_updates=9590, lr=4.77714e-05, gnorm=0.624, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26621
2023-01-05 04:41:03 - progress_bar.py[line:274] - INFO: epoch 001:   9615 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.9, ups=0.92, wpb=108.4, bsz=40, num_updates=9600, lr=4.77669e-05, gnorm=0.674, clip=10, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26632
2023-01-05 04:41:14 - progress_bar.py[line:274] - INFO: epoch 001:   9625 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.9, ups=0.9, wpb=108.2, bsz=40, num_updates=9610, lr=4.77624e-05, gnorm=0.773, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26644
2023-01-05 04:41:25 - progress_bar.py[line:274] - INFO: epoch 001:   9635 / 115845 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.9, ups=0.91, wpb=108, bsz=40, num_updates=9620, lr=4.77579e-05, gnorm=0.776, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26655
2023-01-05 04:41:36 - progress_bar.py[line:274] - INFO: epoch 001:   9645 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.92, wpb=108.2, bsz=40, num_updates=9630, lr=4.77534e-05, gnorm=0.73, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26666
2023-01-05 04:41:47 - progress_bar.py[line:274] - INFO: epoch 001:   9655 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=9640, lr=4.77489e-05, gnorm=0.625, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26677
2023-01-05 04:41:58 - progress_bar.py[line:274] - INFO: epoch 001:   9665 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.89, wpb=110.1, bsz=40, num_updates=9650, lr=4.77444e-05, gnorm=0.517, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26688
2023-01-05 04:42:10 - progress_bar.py[line:274] - INFO: epoch 001:   9675 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=9660, lr=4.77399e-05, gnorm=0.559, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26699
2023-01-05 04:42:21 - progress_bar.py[line:274] - INFO: epoch 001:   9685 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.89, wpb=109.3, bsz=40, num_updates=9670, lr=4.77354e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26711
2023-01-05 04:42:32 - progress_bar.py[line:274] - INFO: epoch 001:   9695 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.6, ups=0.91, wpb=109.4, bsz=40, num_updates=9680, lr=4.77309e-05, gnorm=0.91, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26722
2023-01-05 04:42:43 - progress_bar.py[line:274] - INFO: epoch 001:   9705 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.7, ups=0.93, wpb=109.6, bsz=40, num_updates=9690, lr=4.77264e-05, gnorm=0.768, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26732
2023-01-05 04:42:54 - progress_bar.py[line:274] - INFO: epoch 001:   9715 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=9700, lr=4.77219e-05, gnorm=0.75, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26744
2023-01-05 04:43:06 - progress_bar.py[line:274] - INFO: epoch 001:   9725 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=94.8, ups=0.87, wpb=108.6, bsz=40, num_updates=9710, lr=4.77174e-05, gnorm=1.018, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26755
2023-01-05 04:43:17 - progress_bar.py[line:274] - INFO: epoch 001:   9735 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.2, ups=0.91, wpb=108.6, bsz=40, num_updates=9720, lr=4.77129e-05, gnorm=0.639, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26766
2023-01-05 04:43:28 - progress_bar.py[line:274] - INFO: epoch 001:   9745 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=9730, lr=4.77084e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26777
2023-01-05 04:43:39 - progress_bar.py[line:274] - INFO: epoch 001:   9755 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.9, ups=0.9, wpb=109.3, bsz=40, num_updates=9740, lr=4.77039e-05, gnorm=0.556, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26789
2023-01-05 04:43:50 - progress_bar.py[line:274] - INFO: epoch 001:   9765 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.9, wpb=109.4, bsz=40, num_updates=9750, lr=4.76994e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26800
2023-01-05 04:44:01 - progress_bar.py[line:274] - INFO: epoch 001:   9775 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.5, ups=0.95, wpb=109.2, bsz=40, num_updates=9760, lr=4.76949e-05, gnorm=0.62, clip=10, loss_scale=512, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=26810
2023-01-05 04:44:12 - progress_bar.py[line:274] - INFO: epoch 001:   9785 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.4, ups=0.88, wpb=109.1, bsz=40, num_updates=9770, lr=4.76904e-05, gnorm=0.813, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26822
2023-01-05 04:44:23 - progress_bar.py[line:274] - INFO: epoch 001:   9795 / 115845 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=99.4, ups=0.92, wpb=108.2, bsz=40, num_updates=9780, lr=4.7686e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26833
2023-01-05 04:44:34 - progress_bar.py[line:274] - INFO: epoch 001:   9805 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.6, ups=0.88, wpb=109.4, bsz=40, num_updates=9790, lr=4.76815e-05, gnorm=0.73, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26844
2023-01-05 04:44:46 - progress_bar.py[line:274] - INFO: epoch 001:   9815 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.8, ups=0.9, wpb=109.8, bsz=40, num_updates=9800, lr=4.7677e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26855
2023-01-05 04:44:57 - progress_bar.py[line:274] - INFO: epoch 001:   9825 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=9810, lr=4.76725e-05, gnorm=0.747, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26867
2023-01-05 04:45:08 - progress_bar.py[line:274] - INFO: epoch 001:   9835 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.9, wpb=110, bsz=40, num_updates=9820, lr=4.7668e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26878
2023-01-05 04:45:19 - progress_bar.py[line:274] - INFO: epoch 001:   9845 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.91, wpb=110.3, bsz=40, num_updates=9830, lr=4.76635e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26889
2023-01-05 04:45:30 - progress_bar.py[line:274] - INFO: epoch 001:   9855 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.9, wpb=109.1, bsz=40, num_updates=9840, lr=4.7659e-05, gnorm=0.614, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26900
2023-01-05 04:45:41 - progress_bar.py[line:274] - INFO: epoch 001:   9865 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.7, ups=0.93, wpb=108.1, bsz=40, num_updates=9850, lr=4.76545e-05, gnorm=0.836, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26911
2023-01-05 04:45:52 - progress_bar.py[line:274] - INFO: epoch 001:   9875 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.2, ups=0.89, wpb=108.6, bsz=40, num_updates=9860, lr=4.765e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26922
2023-01-05 04:46:03 - progress_bar.py[line:274] - INFO: epoch 001:   9885 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=103.8, ups=0.95, wpb=109, bsz=40, num_updates=9870, lr=4.76455e-05, gnorm=0.562, clip=0, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=26933
2023-01-05 04:46:14 - progress_bar.py[line:274] - INFO: epoch 001:   9895 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.3, ups=0.89, wpb=110.9, bsz=40, num_updates=9880, lr=4.7641e-05, gnorm=0.617, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26944
2023-01-05 04:46:25 - progress_bar.py[line:274] - INFO: epoch 001:   9905 / 115845 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.7, ups=0.91, wpb=108.8, bsz=40, num_updates=9890, lr=4.76365e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26955
2023-01-05 04:46:37 - progress_bar.py[line:274] - INFO: epoch 001:   9915 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.91, wpb=108.1, bsz=40, num_updates=9900, lr=4.7632e-05, gnorm=0.696, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26966
2023-01-05 04:46:48 - progress_bar.py[line:274] - INFO: epoch 001:   9925 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.3, ups=0.91, wpb=108.3, bsz=40, num_updates=9910, lr=4.76275e-05, gnorm=0.593, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26977
2023-01-05 04:46:59 - progress_bar.py[line:274] - INFO: epoch 001:   9935 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.3, ups=0.87, wpb=109.1, bsz=40, num_updates=9920, lr=4.7623e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26989
2023-01-05 04:47:10 - progress_bar.py[line:274] - INFO: epoch 001:   9945 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.89, wpb=108.6, bsz=40, num_updates=9930, lr=4.76185e-05, gnorm=0.655, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27000
2023-01-05 04:47:22 - progress_bar.py[line:274] - INFO: epoch 001:   9955 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.7, ups=0.9, wpb=107.9, bsz=40, num_updates=9940, lr=4.7614e-05, gnorm=0.546, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27011
2023-01-05 04:47:33 - progress_bar.py[line:274] - INFO: epoch 001:   9965 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.7, ups=0.87, wpb=109.5, bsz=40, num_updates=9950, lr=4.76095e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=27023
2023-01-05 04:47:44 - progress_bar.py[line:274] - INFO: epoch 001:   9975 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.8, ups=0.9, wpb=109.3, bsz=40, num_updates=9960, lr=4.7605e-05, gnorm=0.794, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27034
2023-01-05 04:47:55 - progress_bar.py[line:274] - INFO: epoch 001:   9985 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98, ups=0.9, wpb=109.4, bsz=40, num_updates=9970, lr=4.76005e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27045
2023-01-05 04:48:07 - progress_bar.py[line:274] - INFO: epoch 001:   9995 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.9, wpb=109.6, bsz=40, num_updates=9980, lr=4.7596e-05, gnorm=0.599, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27056
2023-01-05 04:48:18 - progress_bar.py[line:274] - INFO: epoch 001:  10005 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.87, wpb=110.5, bsz=40, num_updates=9990, lr=4.75915e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27068
2023-01-05 04:48:29 - progress_bar.py[line:274] - INFO: epoch 001:  10015 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.3, ups=0.91, wpb=109.6, bsz=40, num_updates=10000, lr=4.7587e-05, gnorm=0.576, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27079
2023-01-05 04:48:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 04:48:30 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 04:48:30 - train.py[line:551] - INFO: load:0.84 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 04:51:02 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 04:51:03 - train.py[line:551] - INFO: load:0.86 valid_run:152.06 task_valid:148.11 collect_output:2.84
2023-01-05 04:53:31 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 04:53:31 - train.py[line:551] - INFO: load:0.89 valid_run:300.13 task_valid:291.15 collect_output:6.79
2023-01-05 04:56:03 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 04:56:03 - train.py[line:551] - INFO: load:0.91 valid_run:452.15 task_valid:434.18 collect_output:14.74
2023-01-05 04:58:32 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 04:58:32 - train.py[line:551] - INFO: load:0.94 valid_run:600.94 task_valid:579.04 collect_output:17.66
2023-01-05 05:01:04 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 05:01:04 - train.py[line:551] - INFO: load:0.96 valid_run:753.20 task_valid:726.55 collect_output:21.35
2023-01-05 05:03:36 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 05:03:36 - train.py[line:551] - INFO: load:0.99 valid_run:904.50 task_valid:872.06 collect_output:26.12
2023-01-05 05:06:09 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 05:06:09 - train.py[line:551] - INFO: load:1.02 valid_run:1057.62 task_valid:1018.12 collect_output:32.10
2023-01-05 05:08:40 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 05:08:40 - train.py[line:551] - INFO: load:1.04 valid_run:1208.42 task_valid:1159.10 collect_output:40.87
2023-01-05 05:11:09 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 05:11:09 - train.py[line:551] - INFO: load:1.07 valid_run:1357.84 task_valid:1303.84 collect_output:44.50
2023-01-05 05:13:37 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 05:13:37 - train.py[line:551] - INFO: load:1.09 valid_run:1506.09 task_valid:1446.97 collect_output:48.58
2023-01-05 05:16:07 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 05:16:07 - train.py[line:551] - INFO: load:1.12 valid_run:1655.70 task_valid:1591.93 collect_output:52.20
2023-01-05 05:18:37 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 05:18:37 - train.py[line:551] - INFO: load:1.15 valid_run:1805.48 task_valid:1736.93 collect_output:55.94
2023-01-05 05:21:07 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 05:21:07 - train.py[line:551] - INFO: load:1.17 valid_run:1955.33 task_valid:1879.00 collect_output:62.68
2023-01-05 05:23:37 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 05:23:37 - train.py[line:551] - INFO: load:1.20 valid_run:2105.70 task_valid:2024.59 collect_output:66.39
2023-01-05 05:26:08 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 05:26:08 - train.py[line:551] - INFO: load:1.23 valid_run:2255.83 task_valid:2170.94 collect_output:69.11
2023-01-05 05:28:37 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 05:28:37 - train.py[line:551] - INFO: load:1.25 valid_run:2405.57 task_valid:2315.02 collect_output:73.73
2023-01-05 05:31:09 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 05:31:09 - train.py[line:551] - INFO: load:1.28 valid_run:2557.13 task_valid:2460.60 collect_output:78.67
2023-01-05 05:33:40 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 05:33:40 - train.py[line:551] - INFO: load:1.31 valid_run:2707.54 task_valid:2607.61 collect_output:81.03
2023-01-05 05:36:08 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 05:36:08 - train.py[line:551] - INFO: load:1.33 valid_run:2855.62 task_valid:2749.14 collect_output:86.54
2023-01-05 05:38:38 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 05:38:38 - train.py[line:551] - INFO: load:1.36 valid_run:3005.55 task_valid:2894.16 collect_output:90.41
2023-01-05 05:41:10 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 05:41:10 - train.py[line:551] - INFO: load:1.38 valid_run:3157.24 task_valid:3038.82 collect_output:96.39
2023-01-05 05:43:39 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 05:43:39 - train.py[line:551] - INFO: load:1.41 valid_run:3306.30 task_valid:3183.27 collect_output:99.96
2023-01-05 05:46:10 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 05:46:10 - train.py[line:551] - INFO: load:1.44 valid_run:3457.35 task_valid:3329.39 collect_output:103.87
2023-01-05 05:48:41 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 05:48:41 - train.py[line:551] - INFO: load:1.47 valid_run:3608.32 task_valid:3475.70 collect_output:107.49

====================================================================================================
SGG eval:     R @ 50: 0.4368;     R @ 100: 0.5227;     R @ 500: 0.5886;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2918;    mR @ 100: 0.3621;    mR @ 500: 0.4308;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6951) (covered in:0.8125) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3871) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.7791) (says:0.0000) (sitting on:0.6307) (standing on:0.1050) (using:0.5500) (walking in:0.3333) (walking on:0.4910) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-01-05 05:51:11 - train.py[line:487] - INFO: 0.522652380952381
2023-01-05 05:51:11 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 05:51:12 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.368 | loss_v1 0 | loss_v2 0 | nll_loss 0.212 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.522652 | ppl 1.16 | vqa_score 0.4899 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 10000 | best_R@100 0.602033

====================================================================================================
SGG eval:     R @ 50: 0.4368;     R @ 100: 0.5227;     R @ 500: 0.5886;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2918;    mR @ 100: 0.3621;    mR @ 500: 0.4308;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6951) (covered in:0.8125) (covering:0.2143) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3871) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.7791) (says:0.0000) (sitting on:0.6307) (standing on:0.1050) (using:0.5500) (walking in:0.3333) (walking on:0.4910) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-01-05 05:51:12 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-01-05 05:51:12 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-05 05:51:54 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-05 05:53:25 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.522652380952381) (writing took 133.09220497310162 seconds)
2023-01-05 05:53:35 - progress_bar.py[line:274] - INFO: epoch 001:  10025 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=0.3, ups=0, wpb=107.3, bsz=40, num_updates=10010, lr=4.75825e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=30985
2023-01-05 05:53:46 - progress_bar.py[line:274] - INFO: epoch 001:  10035 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=101.4, ups=0.92, wpb=109.6, bsz=40, num_updates=10020, lr=4.7578e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30996
2023-01-05 05:53:58 - progress_bar.py[line:274] - INFO: epoch 001:  10045 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.2, ups=0.89, wpb=108.7, bsz=40, num_updates=10030, lr=4.75736e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31007
2023-01-05 05:54:08 - progress_bar.py[line:274] - INFO: epoch 001:  10055 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.6, ups=0.93, wpb=107.8, bsz=40, num_updates=10040, lr=4.75691e-05, gnorm=0.717, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31018
2023-01-05 05:54:18 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-05 05:54:20 - progress_bar.py[line:274] - INFO: epoch 001:  10066 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=89, ups=0.83, wpb=107.2, bsz=40, num_updates=10050, lr=4.75646e-05, gnorm=0.722, clip=10, loss_scale=256, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=31030
2023-01-05 05:54:32 - progress_bar.py[line:274] - INFO: epoch 001:  10076 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=93.8, ups=0.87, wpb=107.4, bsz=40, num_updates=10060, lr=4.75601e-05, gnorm=0.644, clip=10, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=31042
2023-01-05 05:54:43 - progress_bar.py[line:274] - INFO: epoch 001:  10086 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.7, ups=0.91, wpb=108.8, bsz=40, num_updates=10070, lr=4.75556e-05, gnorm=0.579, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31053
2023-01-05 05:54:54 - progress_bar.py[line:274] - INFO: epoch 001:  10096 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.9, wpb=110, bsz=40, num_updates=10080, lr=4.75511e-05, gnorm=0.584, clip=0, loss_scale=256, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=31064
2023-01-05 05:55:05 - progress_bar.py[line:274] - INFO: epoch 001:  10106 / 115845 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=101.4, ups=0.93, wpb=108.5, bsz=40, num_updates=10090, lr=4.75466e-05, gnorm=0.664, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31075
2023-01-05 05:55:16 - progress_bar.py[line:274] - INFO: epoch 001:  10116 / 115845 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.2, ups=0.9, wpb=108.5, bsz=40, num_updates=10100, lr=4.75421e-05, gnorm=0.574, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31086
2023-01-05 05:55:28 - progress_bar.py[line:274] - INFO: epoch 001:  10126 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.9, ups=0.87, wpb=109.9, bsz=40, num_updates=10110, lr=4.75376e-05, gnorm=0.717, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31098
2023-01-05 05:55:39 - progress_bar.py[line:274] - INFO: epoch 001:  10136 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.8, ups=0.93, wpb=109.4, bsz=40, num_updates=10120, lr=4.75331e-05, gnorm=0.987, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31109
2023-01-05 05:55:50 - progress_bar.py[line:274] - INFO: epoch 001:  10146 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.3, ups=0.92, wpb=108.9, bsz=40, num_updates=10130, lr=4.75286e-05, gnorm=0.734, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31120
2023-01-05 05:56:01 - progress_bar.py[line:274] - INFO: epoch 001:  10156 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.2, ups=0.89, wpb=107.3, bsz=40, num_updates=10140, lr=4.75241e-05, gnorm=0.612, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31131
2023-01-05 05:56:13 - progress_bar.py[line:274] - INFO: epoch 001:  10166 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99, ups=0.9, wpb=110.5, bsz=40, num_updates=10150, lr=4.75196e-05, gnorm=0.636, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31142
2023-01-05 05:56:24 - progress_bar.py[line:274] - INFO: epoch 001:  10176 / 115845 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.3, ups=0.9, wpb=108.4, bsz=40, num_updates=10160, lr=4.75151e-05, gnorm=0.691, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31154
2023-01-05 05:56:35 - progress_bar.py[line:274] - INFO: epoch 001:  10186 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.88, wpb=109.9, bsz=40, num_updates=10170, lr=4.75106e-05, gnorm=0.642, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31165
2023-01-05 05:56:46 - progress_bar.py[line:274] - INFO: epoch 001:  10196 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.7, ups=0.91, wpb=108.8, bsz=40, num_updates=10180, lr=4.75061e-05, gnorm=0.665, clip=10, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31176
2023-01-05 05:56:58 - progress_bar.py[line:274] - INFO: epoch 001:  10206 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.88, wpb=109.6, bsz=40, num_updates=10190, lr=4.75016e-05, gnorm=0.578, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31188
2023-01-05 05:57:09 - progress_bar.py[line:274] - INFO: epoch 001:  10216 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.7, ups=0.92, wpb=109.4, bsz=40, num_updates=10200, lr=4.74971e-05, gnorm=0.528, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31199
2023-01-05 05:57:20 - progress_bar.py[line:274] - INFO: epoch 001:  10226 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.92, wpb=108.6, bsz=40, num_updates=10210, lr=4.74926e-05, gnorm=0.679, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31210
2023-01-05 05:57:31 - progress_bar.py[line:274] - INFO: epoch 001:  10236 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.9, ups=0.91, wpb=109, bsz=40, num_updates=10220, lr=4.74881e-05, gnorm=0.702, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31221
2023-01-05 05:57:42 - progress_bar.py[line:274] - INFO: epoch 001:  10246 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.8, ups=0.89, wpb=107.5, bsz=40, num_updates=10230, lr=4.74836e-05, gnorm=0.735, clip=20, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31232
2023-01-05 05:57:53 - progress_bar.py[line:274] - INFO: epoch 001:  10256 / 115845 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=97.9, ups=0.91, wpb=107.2, bsz=40, num_updates=10240, lr=4.74791e-05, gnorm=0.73, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31243
2023-01-05 05:58:05 - progress_bar.py[line:274] - INFO: epoch 001:  10266 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.8, ups=0.89, wpb=109.2, bsz=40, num_updates=10250, lr=4.74746e-05, gnorm=0.545, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31254
2023-01-05 05:58:16 - progress_bar.py[line:274] - INFO: epoch 001:  10276 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101, ups=0.92, wpb=109.7, bsz=40, num_updates=10260, lr=4.74701e-05, gnorm=0.57, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31265
2023-01-05 05:58:27 - progress_bar.py[line:274] - INFO: epoch 001:  10286 / 115845 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.9, ups=0.92, wpb=108.4, bsz=40, num_updates=10270, lr=4.74657e-05, gnorm=0.649, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31276
2023-01-05 05:58:38 - progress_bar.py[line:274] - INFO: epoch 001:  10296 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98, ups=0.9, wpb=109.3, bsz=40, num_updates=10280, lr=4.74612e-05, gnorm=0.661, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31288
2023-01-05 05:58:49 - progress_bar.py[line:274] - INFO: epoch 001:  10306 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.6, ups=0.92, wpb=109.7, bsz=40, num_updates=10290, lr=4.74567e-05, gnorm=0.607, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31299
2023-01-05 05:59:00 - progress_bar.py[line:274] - INFO: epoch 001:  10316 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.3, ups=0.89, wpb=107.5, bsz=40, num_updates=10300, lr=4.74522e-05, gnorm=0.589, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31310
2023-01-05 05:59:11 - progress_bar.py[line:274] - INFO: epoch 001:  10326 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101, ups=0.93, wpb=108.7, bsz=40, num_updates=10310, lr=4.74477e-05, gnorm=0.564, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31321
2023-01-05 05:59:22 - progress_bar.py[line:274] - INFO: epoch 001:  10336 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.89, wpb=109.5, bsz=40, num_updates=10320, lr=4.74432e-05, gnorm=0.646, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31332
2023-01-05 05:59:33 - progress_bar.py[line:274] - INFO: epoch 001:  10346 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.91, wpb=109.4, bsz=40, num_updates=10330, lr=4.74387e-05, gnorm=0.684, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31343
2023-01-05 05:59:45 - progress_bar.py[line:274] - INFO: epoch 001:  10356 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=10340, lr=4.74342e-05, gnorm=0.7, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31354
2023-01-05 05:59:56 - progress_bar.py[line:274] - INFO: epoch 001:  10366 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.89, wpb=109.4, bsz=40, num_updates=10350, lr=4.74297e-05, gnorm=0.614, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31366
2023-01-05 06:00:07 - progress_bar.py[line:274] - INFO: epoch 001:  10376 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.9, wpb=108.9, bsz=40, num_updates=10360, lr=4.74252e-05, gnorm=0.559, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31377
2023-01-05 06:00:19 - progress_bar.py[line:274] - INFO: epoch 001:  10386 / 115845 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.6, ups=0.88, wpb=109.2, bsz=40, num_updates=10370, lr=4.74207e-05, gnorm=0.579, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31388
2023-01-05 06:00:29 - progress_bar.py[line:274] - INFO: epoch 001:  10396 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.2, ups=0.92, wpb=109.7, bsz=40, num_updates=10380, lr=4.74162e-05, gnorm=0.535, clip=0, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31399
2023-01-05 06:00:41 - progress_bar.py[line:274] - INFO: epoch 001:  10406 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.6, ups=0.91, wpb=110.7, bsz=40, num_updates=10390, lr=4.74117e-05, gnorm=0.61, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31410
2023-01-05 06:00:52 - progress_bar.py[line:274] - INFO: epoch 001:  10416 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.8, ups=0.9, wpb=109.3, bsz=40, num_updates=10400, lr=4.74072e-05, gnorm=0.616, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31421
2023-01-05 06:01:02 - progress_bar.py[line:274] - INFO: epoch 001:  10426 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.8, ups=0.95, wpb=108.6, bsz=40, num_updates=10410, lr=4.74027e-05, gnorm=0.642, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31432
2023-01-05 06:01:14 - progress_bar.py[line:274] - INFO: epoch 001:  10436 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.9, wpb=109.6, bsz=40, num_updates=10420, lr=4.73982e-05, gnorm=1.147, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31443
2023-01-05 06:01:25 - progress_bar.py[line:274] - INFO: epoch 001:  10446 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.5, ups=0.91, wpb=107.6, bsz=40, num_updates=10430, lr=4.73937e-05, gnorm=0.664, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31454
2023-01-05 06:01:36 - progress_bar.py[line:274] - INFO: epoch 001:  10456 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.89, wpb=109.3, bsz=40, num_updates=10440, lr=4.73892e-05, gnorm=0.663, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31466
2023-01-05 06:01:47 - progress_bar.py[line:274] - INFO: epoch 001:  10466 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=10450, lr=4.73847e-05, gnorm=0.65, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31477
2023-01-05 06:01:59 - progress_bar.py[line:274] - INFO: epoch 001:  10476 / 115845 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.1, ups=0.89, wpb=108.5, bsz=40, num_updates=10460, lr=4.73802e-05, gnorm=0.555, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31488
2023-01-05 06:02:10 - progress_bar.py[line:274] - INFO: epoch 001:  10486 / 115845 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.5, ups=0.9, wpb=110, bsz=40, num_updates=10470, lr=4.73757e-05, gnorm=0.716, clip=20, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=31499
2023-01-05 06:02:21 - progress_bar.py[line:274] - INFO: epoch 001:  10496 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.9, ups=0.89, wpb=109, bsz=40, num_updates=10480, lr=4.73712e-05, gnorm=0.557, clip=0, loss_scale=256, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=31511
2023-01-05 06:02:32 - progress_bar.py[line:274] - INFO: epoch 001:  10506 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103, ups=0.93, wpb=111.3, bsz=40, num_updates=10490, lr=4.73667e-05, gnorm=0.764, clip=20, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31522
2023-01-05 06:02:43 - progress_bar.py[line:274] - INFO: epoch 001:  10516 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.91, wpb=109.6, bsz=40, num_updates=10500, lr=4.73622e-05, gnorm=0.749, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31533
2023-01-05 06:02:54 - progress_bar.py[line:274] - INFO: epoch 001:  10526 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.9, wpb=108.1, bsz=40, num_updates=10510, lr=4.73577e-05, gnorm=0.765, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31544
2023-01-05 06:03:06 - progress_bar.py[line:274] - INFO: epoch 001:  10536 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.8, ups=0.89, wpb=108.8, bsz=40, num_updates=10520, lr=4.73533e-05, gnorm=0.683, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31555
2023-01-05 06:03:17 - progress_bar.py[line:274] - INFO: epoch 001:  10546 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.91, wpb=107.7, bsz=40, num_updates=10530, lr=4.73488e-05, gnorm=0.519, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31566
2023-01-05 06:03:28 - progress_bar.py[line:274] - INFO: epoch 001:  10556 / 115845 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=95.1, ups=0.88, wpb=107.7, bsz=40, num_updates=10540, lr=4.73443e-05, gnorm=0.788, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31578
2023-01-05 06:03:39 - progress_bar.py[line:274] - INFO: epoch 001:  10566 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.7, ups=0.91, wpb=108.6, bsz=40, num_updates=10550, lr=4.73398e-05, gnorm=0.525, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31589
2023-01-05 06:03:50 - progress_bar.py[line:274] - INFO: epoch 001:  10576 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.7, ups=0.91, wpb=107.7, bsz=40, num_updates=10560, lr=4.73353e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31600
2023-01-05 06:04:01 - progress_bar.py[line:274] - INFO: epoch 001:  10586 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.1, ups=0.92, wpb=110.3, bsz=40, num_updates=10570, lr=4.73308e-05, gnorm=0.836, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31611
2023-01-05 06:04:12 - progress_bar.py[line:274] - INFO: epoch 001:  10596 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.5, ups=0.91, wpb=108.8, bsz=40, num_updates=10580, lr=4.73263e-05, gnorm=0.685, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31622
2023-01-05 06:04:23 - progress_bar.py[line:274] - INFO: epoch 001:  10606 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.2, ups=0.92, wpb=109.1, bsz=40, num_updates=10590, lr=4.73218e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31633
2023-01-05 06:04:35 - progress_bar.py[line:274] - INFO: epoch 001:  10616 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.89, wpb=108.7, bsz=40, num_updates=10600, lr=4.73173e-05, gnorm=0.724, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31644
2023-01-05 06:04:46 - progress_bar.py[line:274] - INFO: epoch 001:  10626 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.9, wpb=109.9, bsz=40, num_updates=10610, lr=4.73128e-05, gnorm=0.69, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31656
2023-01-05 06:04:57 - progress_bar.py[line:274] - INFO: epoch 001:  10636 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.4, ups=0.89, wpb=108, bsz=40, num_updates=10620, lr=4.73083e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31667
2023-01-05 06:05:08 - progress_bar.py[line:274] - INFO: epoch 001:  10646 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.5, ups=0.92, wpb=110.2, bsz=40, num_updates=10630, lr=4.73038e-05, gnorm=0.826, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31678
2023-01-05 06:05:19 - progress_bar.py[line:274] - INFO: epoch 001:  10656 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.3, ups=0.89, wpb=110, bsz=40, num_updates=10640, lr=4.72993e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31689
2023-01-05 06:05:30 - progress_bar.py[line:274] - INFO: epoch 001:  10666 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.5, ups=0.92, wpb=109.2, bsz=40, num_updates=10650, lr=4.72948e-05, gnorm=0.792, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31700
2023-01-05 06:05:42 - progress_bar.py[line:274] - INFO: epoch 001:  10676 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.2, ups=0.89, wpb=108.7, bsz=40, num_updates=10660, lr=4.72903e-05, gnorm=0.732, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31711
2023-01-05 06:05:52 - progress_bar.py[line:274] - INFO: epoch 001:  10686 / 115845 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=107.1, nsentences=40, sample_size=107.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.5, ups=0.93, wpb=107.1, bsz=40, num_updates=10670, lr=4.72858e-05, gnorm=0.776, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31722
2023-01-05 06:06:04 - progress_bar.py[line:274] - INFO: epoch 001:  10696 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.1, ups=0.88, wpb=110.8, bsz=40, num_updates=10680, lr=4.72813e-05, gnorm=0.678, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31733
2023-01-05 06:06:15 - progress_bar.py[line:274] - INFO: epoch 001:  10706 / 115845 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.6, ups=0.91, wpb=108.7, bsz=40, num_updates=10690, lr=4.72768e-05, gnorm=0.721, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31745
2023-01-05 06:06:26 - progress_bar.py[line:274] - INFO: epoch 001:  10716 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98, ups=0.9, wpb=109.3, bsz=40, num_updates=10700, lr=4.72723e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31756
2023-01-05 06:06:38 - progress_bar.py[line:274] - INFO: epoch 001:  10726 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.6, ups=0.87, wpb=109.4, bsz=40, num_updates=10710, lr=4.72678e-05, gnorm=0.635, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31767
2023-01-05 06:06:49 - progress_bar.py[line:274] - INFO: epoch 001:  10736 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96, ups=0.88, wpb=108.9, bsz=40, num_updates=10720, lr=4.72633e-05, gnorm=0.841, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31779
2023-01-05 06:07:00 - progress_bar.py[line:274] - INFO: epoch 001:  10746 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=109.6, bsz=40, num_updates=10730, lr=4.72588e-05, gnorm=0.693, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31790
2023-01-05 06:07:12 - progress_bar.py[line:274] - INFO: epoch 001:  10756 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.5, ups=0.89, wpb=108.8, bsz=40, num_updates=10740, lr=4.72543e-05, gnorm=0.767, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31801
2023-01-05 06:07:23 - progress_bar.py[line:274] - INFO: epoch 001:  10766 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.7, ups=0.92, wpb=109.4, bsz=40, num_updates=10750, lr=4.72498e-05, gnorm=0.633, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=31812
2023-01-05 06:07:34 - progress_bar.py[line:274] - INFO: epoch 001:  10776 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.92, wpb=107.6, bsz=40, num_updates=10760, lr=4.72454e-05, gnorm=0.9, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31823
2023-01-05 06:07:45 - progress_bar.py[line:274] - INFO: epoch 001:  10786 / 115845 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.6, ups=0.9, wpb=108.7, bsz=40, num_updates=10770, lr=4.72409e-05, gnorm=0.758, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=31835
2023-01-05 06:07:56 - progress_bar.py[line:274] - INFO: epoch 001:  10796 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.2, ups=0.92, wpb=110.2, bsz=40, num_updates=10780, lr=4.72364e-05, gnorm=0.575, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31845
2023-01-05 06:08:07 - progress_bar.py[line:274] - INFO: epoch 001:  10806 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.89, wpb=110.3, bsz=40, num_updates=10790, lr=4.72319e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31857
2023-01-05 06:08:18 - progress_bar.py[line:274] - INFO: epoch 001:  10816 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.6, ups=0.95, wpb=108.6, bsz=40, num_updates=10800, lr=4.72274e-05, gnorm=0.593, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31867
2023-01-05 06:08:29 - progress_bar.py[line:274] - INFO: epoch 001:  10826 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.9, wpb=108.6, bsz=40, num_updates=10810, lr=4.72229e-05, gnorm=0.78, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31879
2023-01-05 06:08:40 - progress_bar.py[line:274] - INFO: epoch 001:  10836 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.9, wpb=108.9, bsz=40, num_updates=10820, lr=4.72184e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31890
2023-01-05 06:08:51 - progress_bar.py[line:274] - INFO: epoch 001:  10846 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.8, ups=0.92, wpb=109.6, bsz=40, num_updates=10830, lr=4.72139e-05, gnorm=0.764, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31901
2023-01-05 06:09:02 - progress_bar.py[line:274] - INFO: epoch 001:  10856 / 115845 loss=0.462, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=100.5, ups=0.93, wpb=108.1, bsz=40, num_updates=10840, lr=4.72094e-05, gnorm=0.622, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31912
2023-01-05 06:09:13 - progress_bar.py[line:274] - INFO: epoch 001:  10866 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.89, wpb=109.7, bsz=40, num_updates=10850, lr=4.72049e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31923
2023-01-05 06:09:25 - progress_bar.py[line:274] - INFO: epoch 001:  10876 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.5, ups=0.89, wpb=108.9, bsz=40, num_updates=10860, lr=4.72004e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=31934
2023-01-05 06:09:36 - progress_bar.py[line:274] - INFO: epoch 001:  10886 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=10870, lr=4.71959e-05, gnorm=0.707, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31946
2023-01-05 06:09:47 - progress_bar.py[line:274] - INFO: epoch 001:  10896 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.9, wpb=110.5, bsz=40, num_updates=10880, lr=4.71914e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31957
2023-01-05 06:09:58 - progress_bar.py[line:274] - INFO: epoch 001:  10906 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=10890, lr=4.71869e-05, gnorm=0.613, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31968
2023-01-05 06:10:10 - progress_bar.py[line:274] - INFO: epoch 001:  10916 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.89, wpb=109.2, bsz=40, num_updates=10900, lr=4.71824e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31979
2023-01-05 06:10:21 - progress_bar.py[line:274] - INFO: epoch 001:  10926 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.92, wpb=109.2, bsz=40, num_updates=10910, lr=4.71779e-05, gnorm=0.612, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31990
2023-01-05 06:10:32 - progress_bar.py[line:274] - INFO: epoch 001:  10936 / 115845 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=10920, lr=4.71734e-05, gnorm=0.674, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32002
2023-01-05 06:10:43 - progress_bar.py[line:274] - INFO: epoch 001:  10946 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.87, wpb=109.6, bsz=40, num_updates=10930, lr=4.71689e-05, gnorm=0.567, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32013
2023-01-05 06:10:55 - progress_bar.py[line:274] - INFO: epoch 001:  10956 / 115845 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=106.2, nsentences=40, sample_size=106.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=94.1, ups=0.89, wpb=106.2, bsz=40, num_updates=10940, lr=4.71644e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32025
2023-01-05 06:11:06 - progress_bar.py[line:274] - INFO: epoch 001:  10966 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.89, wpb=109, bsz=40, num_updates=10950, lr=4.71599e-05, gnorm=0.608, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32036
2023-01-05 06:11:17 - progress_bar.py[line:274] - INFO: epoch 001:  10976 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.89, wpb=108.5, bsz=40, num_updates=10960, lr=4.71554e-05, gnorm=0.584, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32047
2023-01-05 06:11:28 - progress_bar.py[line:274] - INFO: epoch 001:  10986 / 115845 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.7, ups=0.91, wpb=109.7, bsz=40, num_updates=10970, lr=4.71509e-05, gnorm=0.757, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32058
2023-01-05 06:11:39 - progress_bar.py[line:274] - INFO: epoch 001:  10996 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.5, ups=0.92, wpb=109.4, bsz=40, num_updates=10980, lr=4.71464e-05, gnorm=0.551, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32069
2023-01-05 06:11:51 - progress_bar.py[line:274] - INFO: epoch 001:  11006 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.91, wpb=109.1, bsz=40, num_updates=10990, lr=4.71419e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32080
2023-01-05 06:12:02 - progress_bar.py[line:274] - INFO: epoch 001:  11016 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.88, wpb=109.6, bsz=40, num_updates=11000, lr=4.71374e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32092
2023-01-05 06:12:13 - progress_bar.py[line:274] - INFO: epoch 001:  11026 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.9, ups=0.91, wpb=110.5, bsz=40, num_updates=11010, lr=4.7133e-05, gnorm=0.696, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32103
2023-01-05 06:12:24 - progress_bar.py[line:274] - INFO: epoch 001:  11036 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.8, ups=0.88, wpb=110, bsz=40, num_updates=11020, lr=4.71285e-05, gnorm=0.539, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32114
2023-01-05 06:12:35 - progress_bar.py[line:274] - INFO: epoch 001:  11046 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.1, ups=0.91, wpb=109.3, bsz=40, num_updates=11030, lr=4.7124e-05, gnorm=0.643, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32125
2023-01-05 06:12:46 - progress_bar.py[line:274] - INFO: epoch 001:  11056 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.92, wpb=108.5, bsz=40, num_updates=11040, lr=4.71195e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32136
2023-01-05 06:12:58 - progress_bar.py[line:274] - INFO: epoch 001:  11066 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.88, wpb=109.9, bsz=40, num_updates=11050, lr=4.7115e-05, gnorm=0.617, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32148
2023-01-05 06:13:09 - progress_bar.py[line:274] - INFO: epoch 001:  11076 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.2, ups=0.88, wpb=109.5, bsz=40, num_updates=11060, lr=4.71105e-05, gnorm=0.777, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32159
2023-01-05 06:13:21 - progress_bar.py[line:274] - INFO: epoch 001:  11086 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.5, ups=0.87, wpb=109.6, bsz=40, num_updates=11070, lr=4.7106e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32171
2023-01-05 06:13:32 - progress_bar.py[line:274] - INFO: epoch 001:  11096 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.88, wpb=109.6, bsz=40, num_updates=11080, lr=4.71015e-05, gnorm=0.621, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32182
2023-01-05 06:13:43 - progress_bar.py[line:274] - INFO: epoch 001:  11106 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.9, wpb=108.6, bsz=40, num_updates=11090, lr=4.7097e-05, gnorm=0.587, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32193
2023-01-05 06:13:55 - progress_bar.py[line:274] - INFO: epoch 001:  11116 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.1, ups=0.88, wpb=108.8, bsz=40, num_updates=11100, lr=4.70925e-05, gnorm=0.634, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32205
2023-01-05 06:13:58 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 06:14:07 - progress_bar.py[line:274] - INFO: epoch 001:  11127 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=89.2, ups=0.82, wpb=109, bsz=40, num_updates=11110, lr=4.7088e-05, gnorm=0.801, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=32217
2023-01-05 06:14:18 - progress_bar.py[line:274] - INFO: epoch 001:  11137 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.91, wpb=109.4, bsz=40, num_updates=11120, lr=4.70835e-05, gnorm=0.634, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32228
2023-01-05 06:14:29 - progress_bar.py[line:274] - INFO: epoch 001:  11147 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.2, ups=0.92, wpb=109.9, bsz=40, num_updates=11130, lr=4.7079e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32239
2023-01-05 06:14:41 - progress_bar.py[line:274] - INFO: epoch 001:  11157 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.7, ups=0.89, wpb=109.2, bsz=40, num_updates=11140, lr=4.70745e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32250
2023-01-05 06:14:52 - progress_bar.py[line:274] - INFO: epoch 001:  11167 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.5, ups=0.91, wpb=108.5, bsz=40, num_updates=11150, lr=4.707e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32261
2023-01-05 06:15:03 - progress_bar.py[line:274] - INFO: epoch 001:  11177 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=11160, lr=4.70655e-05, gnorm=0.522, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32273
2023-01-05 06:15:14 - progress_bar.py[line:274] - INFO: epoch 001:  11187 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.9, wpb=109.2, bsz=40, num_updates=11170, lr=4.7061e-05, gnorm=0.523, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32284
2023-01-05 06:15:21 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-05 06:15:27 - progress_bar.py[line:274] - INFO: epoch 001:  11198 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=90.6, ups=0.82, wpb=110.8, bsz=40, num_updates=11180, lr=4.70565e-05, gnorm=1.049, clip=30, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=32296
2023-01-05 06:15:38 - progress_bar.py[line:274] - INFO: epoch 001:  11208 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=11190, lr=4.7052e-05, gnorm=0.613, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32307
2023-01-05 06:15:49 - progress_bar.py[line:274] - INFO: epoch 001:  11218 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.89, wpb=109.1, bsz=40, num_updates=11200, lr=4.70475e-05, gnorm=0.664, clip=20, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32319
2023-01-05 06:16:01 - progress_bar.py[line:274] - INFO: epoch 001:  11228 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=94.7, ups=0.87, wpb=108.5, bsz=40, num_updates=11210, lr=4.7043e-05, gnorm=0.675, clip=0, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32330
2023-01-05 06:16:12 - progress_bar.py[line:274] - INFO: epoch 001:  11238 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.6, ups=0.91, wpb=110.9, bsz=40, num_updates=11220, lr=4.70385e-05, gnorm=0.632, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32341
2023-01-05 06:16:23 - progress_bar.py[line:274] - INFO: epoch 001:  11248 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=93.4, ups=0.86, wpb=108.1, bsz=40, num_updates=11230, lr=4.7034e-05, gnorm=0.724, clip=10, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=32353
2023-01-05 06:16:35 - progress_bar.py[line:274] - INFO: epoch 001:  11258 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.7, ups=0.88, wpb=109.4, bsz=40, num_updates=11240, lr=4.70295e-05, gnorm=0.516, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32364
2023-01-05 06:16:46 - progress_bar.py[line:274] - INFO: epoch 001:  11268 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.1, ups=0.91, wpb=110.4, bsz=40, num_updates=11250, lr=4.70251e-05, gnorm=0.44, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32375
2023-01-05 06:16:57 - progress_bar.py[line:274] - INFO: epoch 001:  11278 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100, ups=0.91, wpb=110, bsz=40, num_updates=11260, lr=4.70206e-05, gnorm=0.559, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32387
2023-01-05 06:17:08 - progress_bar.py[line:274] - INFO: epoch 001:  11288 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.1, ups=0.87, wpb=108.9, bsz=40, num_updates=11270, lr=4.70161e-05, gnorm=1.155, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32398
2023-01-05 06:17:20 - progress_bar.py[line:274] - INFO: epoch 001:  11298 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.9, wpb=109.6, bsz=40, num_updates=11280, lr=4.70116e-05, gnorm=0.554, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32409
2023-01-05 06:17:30 - progress_bar.py[line:274] - INFO: epoch 001:  11308 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.93, wpb=108.1, bsz=40, num_updates=11290, lr=4.70071e-05, gnorm=0.919, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32420
2023-01-05 06:17:41 - progress_bar.py[line:274] - INFO: epoch 001:  11318 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.1, ups=0.93, wpb=109.7, bsz=40, num_updates=11300, lr=4.70026e-05, gnorm=0.663, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32431
2023-01-05 06:17:52 - progress_bar.py[line:274] - INFO: epoch 001:  11328 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.9, wpb=109, bsz=40, num_updates=11310, lr=4.69981e-05, gnorm=0.656, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32442
2023-01-05 06:18:04 - progress_bar.py[line:274] - INFO: epoch 001:  11338 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.89, wpb=109.5, bsz=40, num_updates=11320, lr=4.69936e-05, gnorm=0.768, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32453
2023-01-05 06:18:15 - progress_bar.py[line:274] - INFO: epoch 001:  11348 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.9, wpb=108.4, bsz=40, num_updates=11330, lr=4.69891e-05, gnorm=0.658, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32465
2023-01-05 06:18:26 - progress_bar.py[line:274] - INFO: epoch 001:  11358 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.9, wpb=107.4, bsz=40, num_updates=11340, lr=4.69846e-05, gnorm=0.648, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32476
2023-01-05 06:18:37 - progress_bar.py[line:274] - INFO: epoch 001:  11368 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102.2, ups=0.93, wpb=109.5, bsz=40, num_updates=11350, lr=4.69801e-05, gnorm=0.543, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32487
2023-01-05 06:18:48 - progress_bar.py[line:274] - INFO: epoch 001:  11378 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.8, ups=0.89, wpb=108.1, bsz=40, num_updates=11360, lr=4.69756e-05, gnorm=0.798, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32498
2023-01-05 06:19:00 - progress_bar.py[line:274] - INFO: epoch 001:  11388 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.9, ups=0.89, wpb=107.4, bsz=40, num_updates=11370, lr=4.69711e-05, gnorm=0.72, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32509
2023-01-05 06:19:11 - progress_bar.py[line:274] - INFO: epoch 001:  11398 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.2, ups=0.89, wpb=107.8, bsz=40, num_updates=11380, lr=4.69666e-05, gnorm=0.8, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32521
2023-01-05 06:19:22 - progress_bar.py[line:274] - INFO: epoch 001:  11408 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.91, wpb=108.9, bsz=40, num_updates=11390, lr=4.69621e-05, gnorm=0.634, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32532
2023-01-05 06:19:33 - progress_bar.py[line:274] - INFO: epoch 001:  11418 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.88, wpb=108.9, bsz=40, num_updates=11400, lr=4.69576e-05, gnorm=0.529, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32543
2023-01-05 06:19:45 - progress_bar.py[line:274] - INFO: epoch 001:  11428 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.4, ups=0.9, wpb=108.6, bsz=40, num_updates=11410, lr=4.69531e-05, gnorm=0.677, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32554
2023-01-05 06:19:56 - progress_bar.py[line:274] - INFO: epoch 001:  11438 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.1, ups=0.87, wpb=109.9, bsz=40, num_updates=11420, lr=4.69486e-05, gnorm=0.637, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32566
2023-01-05 06:20:07 - progress_bar.py[line:274] - INFO: epoch 001:  11448 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.5, ups=0.91, wpb=108.8, bsz=40, num_updates=11430, lr=4.69441e-05, gnorm=0.55, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32577
2023-01-05 06:20:18 - progress_bar.py[line:274] - INFO: epoch 001:  11458 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.5, ups=0.92, wpb=109.3, bsz=40, num_updates=11440, lr=4.69396e-05, gnorm=0.712, clip=30, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32588
2023-01-05 06:20:29 - progress_bar.py[line:274] - INFO: epoch 001:  11468 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.7, ups=0.93, wpb=109.2, bsz=40, num_updates=11450, lr=4.69351e-05, gnorm=0.542, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32599
2023-01-05 06:20:40 - progress_bar.py[line:274] - INFO: epoch 001:  11478 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=108.4, bsz=40, num_updates=11460, lr=4.69306e-05, gnorm=0.818, clip=30, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32610
2023-01-05 06:20:51 - progress_bar.py[line:274] - INFO: epoch 001:  11488 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.7, ups=0.94, wpb=108.4, bsz=40, num_updates=11470, lr=4.69261e-05, gnorm=0.563, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32621
2023-01-05 06:21:02 - progress_bar.py[line:274] - INFO: epoch 001:  11498 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.1, ups=0.89, wpb=108.5, bsz=40, num_updates=11480, lr=4.69216e-05, gnorm=0.551, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32632
2023-01-05 06:21:13 - progress_bar.py[line:274] - INFO: epoch 001:  11508 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=103.9, ups=0.93, wpb=111.3, bsz=40, num_updates=11490, lr=4.69171e-05, gnorm=0.754, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32643
2023-01-05 06:21:24 - progress_bar.py[line:274] - INFO: epoch 001:  11518 / 115845 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.6, ups=0.92, wpb=111.8, bsz=40, num_updates=11500, lr=4.69127e-05, gnorm=0.668, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32654
2023-01-05 06:21:36 - progress_bar.py[line:274] - INFO: epoch 001:  11528 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=94.3, ups=0.86, wpb=109.5, bsz=40, num_updates=11510, lr=4.69082e-05, gnorm=0.789, clip=20, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32666
2023-01-05 06:21:47 - progress_bar.py[line:274] - INFO: epoch 001:  11538 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.3, ups=0.92, wpb=109, bsz=40, num_updates=11520, lr=4.69037e-05, gnorm=0.626, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32677
2023-01-05 06:21:58 - progress_bar.py[line:274] - INFO: epoch 001:  11548 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=94.9, ups=0.89, wpb=107, bsz=40, num_updates=11530, lr=4.68992e-05, gnorm=0.607, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32688
2023-01-05 06:22:10 - progress_bar.py[line:274] - INFO: epoch 001:  11558 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=11540, lr=4.68947e-05, gnorm=0.545, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32699
2023-01-05 06:22:21 - progress_bar.py[line:274] - INFO: epoch 001:  11568 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.88, wpb=110.2, bsz=40, num_updates=11550, lr=4.68902e-05, gnorm=0.607, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32711
2023-01-05 06:22:32 - progress_bar.py[line:274] - INFO: epoch 001:  11578 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.6, ups=0.91, wpb=109.7, bsz=40, num_updates=11560, lr=4.68857e-05, gnorm=0.618, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32722
2023-01-05 06:22:43 - progress_bar.py[line:274] - INFO: epoch 001:  11588 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.9, wpb=109.1, bsz=40, num_updates=11570, lr=4.68812e-05, gnorm=0.715, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32733
2023-01-05 06:22:54 - progress_bar.py[line:274] - INFO: epoch 001:  11598 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.92, wpb=107.5, bsz=40, num_updates=11580, lr=4.68767e-05, gnorm=0.53, clip=0, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32744
2023-01-05 06:23:06 - progress_bar.py[line:274] - INFO: epoch 001:  11608 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.7, ups=0.89, wpb=108, bsz=40, num_updates=11590, lr=4.68722e-05, gnorm=0.626, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32755
2023-01-05 06:23:17 - progress_bar.py[line:274] - INFO: epoch 001:  11618 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.87, wpb=109.3, bsz=40, num_updates=11600, lr=4.68677e-05, gnorm=0.588, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32767
2023-01-05 06:23:28 - progress_bar.py[line:274] - INFO: epoch 001:  11628 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.3, ups=0.9, wpb=108.5, bsz=40, num_updates=11610, lr=4.68632e-05, gnorm=0.672, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32778
2023-01-05 06:23:40 - progress_bar.py[line:274] - INFO: epoch 001:  11638 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.91, wpb=109, bsz=40, num_updates=11620, lr=4.68587e-05, gnorm=0.503, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32789
2023-01-05 06:23:51 - progress_bar.py[line:274] - INFO: epoch 001:  11648 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.91, wpb=108.8, bsz=40, num_updates=11630, lr=4.68542e-05, gnorm=0.705, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32800
2023-01-05 06:24:02 - progress_bar.py[line:274] - INFO: epoch 001:  11658 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.91, wpb=109.1, bsz=40, num_updates=11640, lr=4.68497e-05, gnorm=0.547, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32811
2023-01-05 06:24:13 - progress_bar.py[line:274] - INFO: epoch 001:  11668 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.1, ups=0.89, wpb=108.6, bsz=40, num_updates=11650, lr=4.68452e-05, gnorm=0.561, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32823
2023-01-05 06:24:24 - progress_bar.py[line:274] - INFO: epoch 001:  11678 / 115845 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97, ups=0.88, wpb=109.9, bsz=40, num_updates=11660, lr=4.68407e-05, gnorm=0.504, clip=10, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=32834
2023-01-05 06:24:36 - progress_bar.py[line:274] - INFO: epoch 001:  11688 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.1, ups=0.91, wpb=110.5, bsz=40, num_updates=11670, lr=4.68362e-05, gnorm=0.645, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32845
2023-01-05 06:24:47 - progress_bar.py[line:274] - INFO: epoch 001:  11698 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.88, wpb=108.9, bsz=40, num_updates=11680, lr=4.68317e-05, gnorm=0.756, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32857
2023-01-05 06:24:58 - progress_bar.py[line:274] - INFO: epoch 001:  11708 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.89, wpb=108.2, bsz=40, num_updates=11690, lr=4.68272e-05, gnorm=0.99, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32868
2023-01-05 06:25:10 - progress_bar.py[line:274] - INFO: epoch 001:  11718 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97, ups=0.91, wpb=107.2, bsz=40, num_updates=11700, lr=4.68227e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32879
2023-01-05 06:25:21 - progress_bar.py[line:274] - INFO: epoch 001:  11728 / 115845 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.5, ups=0.92, wpb=108.4, bsz=40, num_updates=11710, lr=4.68182e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=32890
2023-01-05 06:25:31 - progress_bar.py[line:274] - INFO: epoch 001:  11738 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.6, ups=0.94, wpb=109.2, bsz=40, num_updates=11720, lr=4.68137e-05, gnorm=0.588, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32901
2023-01-05 06:25:42 - progress_bar.py[line:274] - INFO: epoch 001:  11748 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.91, wpb=110.2, bsz=40, num_updates=11730, lr=4.68092e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32912
2023-01-05 06:25:54 - progress_bar.py[line:274] - INFO: epoch 001:  11758 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.91, wpb=109.8, bsz=40, num_updates=11740, lr=4.68048e-05, gnorm=0.521, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32923
2023-01-05 06:26:05 - progress_bar.py[line:274] - INFO: epoch 001:  11768 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.88, wpb=109.8, bsz=40, num_updates=11750, lr=4.68003e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32935
2023-01-05 06:26:16 - progress_bar.py[line:274] - INFO: epoch 001:  11778 / 115845 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.1, ups=0.9, wpb=108.9, bsz=40, num_updates=11760, lr=4.67958e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32946
2023-01-05 06:26:27 - progress_bar.py[line:274] - INFO: epoch 001:  11788 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.92, wpb=108.3, bsz=40, num_updates=11770, lr=4.67913e-05, gnorm=0.67, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32957
2023-01-05 06:26:38 - progress_bar.py[line:274] - INFO: epoch 001:  11798 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.9, wpb=108.4, bsz=40, num_updates=11780, lr=4.67868e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=32968
2023-01-05 06:26:50 - progress_bar.py[line:274] - INFO: epoch 001:  11808 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.87, wpb=110.4, bsz=40, num_updates=11790, lr=4.67823e-05, gnorm=0.524, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32979
2023-01-05 06:27:01 - progress_bar.py[line:274] - INFO: epoch 001:  11818 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.91, wpb=108.9, bsz=40, num_updates=11800, lr=4.67778e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32991
2023-01-05 06:27:12 - progress_bar.py[line:274] - INFO: epoch 001:  11828 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.91, wpb=108.4, bsz=40, num_updates=11810, lr=4.67733e-05, gnorm=0.66, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33002
2023-01-05 06:27:23 - progress_bar.py[line:274] - INFO: epoch 001:  11838 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.8, ups=0.92, wpb=108.5, bsz=40, num_updates=11820, lr=4.67688e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33013
2023-01-05 06:27:34 - progress_bar.py[line:274] - INFO: epoch 001:  11848 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96, ups=0.88, wpb=109, bsz=40, num_updates=11830, lr=4.67643e-05, gnorm=0.601, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=33024
2023-01-05 06:27:46 - progress_bar.py[line:274] - INFO: epoch 001:  11858 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.5, ups=0.89, wpb=108.9, bsz=40, num_updates=11840, lr=4.67598e-05, gnorm=0.607, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33035
2023-01-05 06:27:57 - progress_bar.py[line:274] - INFO: epoch 001:  11868 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.3, ups=0.9, wpb=108.4, bsz=40, num_updates=11850, lr=4.67553e-05, gnorm=0.643, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33047
2023-01-05 06:28:08 - progress_bar.py[line:274] - INFO: epoch 001:  11878 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.91, wpb=108.7, bsz=40, num_updates=11860, lr=4.67508e-05, gnorm=0.716, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=33058
2023-01-05 06:28:19 - progress_bar.py[line:274] - INFO: epoch 001:  11888 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.91, wpb=110, bsz=40, num_updates=11870, lr=4.67463e-05, gnorm=0.884, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=33069
2023-01-05 06:28:31 - progress_bar.py[line:274] - INFO: epoch 001:  11898 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.91, wpb=110, bsz=40, num_updates=11880, lr=4.67418e-05, gnorm=0.615, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33080
2023-01-05 06:28:42 - progress_bar.py[line:274] - INFO: epoch 001:  11908 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.88, wpb=110.7, bsz=40, num_updates=11890, lr=4.67373e-05, gnorm=0.533, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33092
2023-01-05 06:28:53 - progress_bar.py[line:274] - INFO: epoch 001:  11918 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=11900, lr=4.67328e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33103
2023-01-05 06:29:05 - progress_bar.py[line:274] - INFO: epoch 001:  11928 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.7, ups=0.89, wpb=109, bsz=40, num_updates=11910, lr=4.67283e-05, gnorm=0.775, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33114
2023-01-05 06:29:16 - progress_bar.py[line:274] - INFO: epoch 001:  11938 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.89, wpb=110.4, bsz=40, num_updates=11920, lr=4.67238e-05, gnorm=0.54, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33126
2023-01-05 06:29:27 - progress_bar.py[line:274] - INFO: epoch 001:  11948 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.91, wpb=108.6, bsz=40, num_updates=11930, lr=4.67193e-05, gnorm=0.529, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33137
2023-01-05 06:29:38 - progress_bar.py[line:274] - INFO: epoch 001:  11958 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.3, ups=0.89, wpb=108.8, bsz=40, num_updates=11940, lr=4.67148e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33148
2023-01-05 06:29:50 - progress_bar.py[line:274] - INFO: epoch 001:  11968 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=94.6, ups=0.87, wpb=108.4, bsz=40, num_updates=11950, lr=4.67103e-05, gnorm=0.6, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33160
2023-01-05 06:30:01 - progress_bar.py[line:274] - INFO: epoch 001:  11978 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.89, wpb=108.9, bsz=40, num_updates=11960, lr=4.67058e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33171
2023-01-05 06:30:12 - progress_bar.py[line:274] - INFO: epoch 001:  11988 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100, ups=0.91, wpb=110.2, bsz=40, num_updates=11970, lr=4.67013e-05, gnorm=0.678, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33182
2023-01-05 06:30:24 - progress_bar.py[line:274] - INFO: epoch 001:  11998 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.3, ups=0.89, wpb=109, bsz=40, num_updates=11980, lr=4.66968e-05, gnorm=0.631, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=33193
2023-01-05 06:30:35 - progress_bar.py[line:274] - INFO: epoch 001:  12008 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.9, wpb=108.8, bsz=40, num_updates=11990, lr=4.66924e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=33205
2023-01-05 06:30:46 - progress_bar.py[line:274] - INFO: epoch 001:  12018 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.9, wpb=109.2, bsz=40, num_updates=12000, lr=4.66879e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=33216
2023-01-05 06:30:46 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 06:30:48 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 06:30:48 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 06:33:19 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 06:33:19 - train.py[line:551] - INFO: load:1.07 valid_run:151.30 task_valid:148.08 collect_output:2.16
2023-01-05 06:35:47 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 06:35:47 - train.py[line:551] - INFO: load:1.09 valid_run:299.12 task_valid:290.92 collect_output:6.15
2023-01-05 06:38:19 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 06:38:19 - train.py[line:551] - INFO: load:1.12 valid_run:450.99 task_valid:433.96 collect_output:13.96
2023-01-05 06:40:47 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 06:40:48 - train.py[line:551] - INFO: load:1.14 valid_run:599.61 task_valid:578.86 collect_output:16.65
2023-01-05 06:43:19 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 06:43:19 - train.py[line:551] - INFO: load:1.16 valid_run:751.48 task_valid:726.19 collect_output:20.18
2023-01-05 06:45:51 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 06:45:51 - train.py[line:551] - INFO: load:1.19 valid_run:902.59 task_valid:871.50 collect_output:24.96
2023-01-05 06:48:24 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 06:48:24 - train.py[line:551] - INFO: load:1.21 valid_run:1055.57 task_valid:1017.64 collect_output:30.75
2023-01-05 06:50:54 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 06:50:54 - train.py[line:551] - INFO: load:1.24 valid_run:1206.01 task_valid:1158.58 collect_output:39.24
2023-01-05 06:53:23 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 06:53:24 - train.py[line:551] - INFO: load:1.26 valid_run:1355.10 task_valid:1303.17 collect_output:42.73
2023-01-05 06:55:52 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 06:55:52 - train.py[line:551] - INFO: load:1.29 valid_run:1503.22 task_valid:1446.37 collect_output:46.62
2023-01-05 06:58:21 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 06:58:21 - train.py[line:551] - INFO: load:1.31 valid_run:1652.75 task_valid:1591.36 collect_output:50.16
2023-01-05 07:00:51 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 07:00:51 - train.py[line:551] - INFO: load:1.34 valid_run:1802.14 task_valid:1736.24 collect_output:53.66
2023-01-05 07:03:20 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 07:03:20 - train.py[line:551] - INFO: load:1.36 valid_run:1951.54 task_valid:1877.92 collect_output:60.37
2023-01-05 07:05:50 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 07:05:50 - train.py[line:551] - INFO: load:1.39 valid_run:2101.59 task_valid:2023.27 collect_output:64.06
2023-01-05 07:08:20 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 07:08:20 - train.py[line:551] - INFO: load:1.41 valid_run:2251.54 task_valid:2169.81 collect_output:66.45
2023-01-05 07:10:50 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 07:10:50 - train.py[line:551] - INFO: load:1.44 valid_run:2401.08 task_valid:2313.80 collect_output:70.99
2023-01-05 07:13:21 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 07:13:21 - train.py[line:551] - INFO: load:1.46 valid_run:2552.13 task_valid:2459.15 collect_output:75.66
2023-01-05 07:15:51 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 07:15:51 - train.py[line:551] - INFO: load:1.49 valid_run:2702.23 task_valid:2605.91 collect_output:78.00
2023-01-05 07:18:20 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 07:18:20 - train.py[line:551] - INFO: load:1.51 valid_run:2850.45 task_valid:2747.51 collect_output:83.61
2023-01-05 07:20:50 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 07:20:50 - train.py[line:551] - INFO: load:1.54 valid_run:3000.26 task_valid:2892.51 collect_output:87.38
2023-01-05 07:23:21 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 07:23:21 - train.py[line:551] - INFO: load:1.57 valid_run:3151.51 task_valid:3036.86 collect_output:93.26
2023-01-05 07:25:50 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 07:25:50 - train.py[line:551] - INFO: load:1.59 valid_run:3300.41 task_valid:3181.24 collect_output:96.76
2023-01-05 07:28:21 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 07:28:21 - train.py[line:551] - INFO: load:1.62 valid_run:3451.53 task_valid:3327.49 collect_output:100.61
2023-01-05 07:30:52 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 07:30:52 - train.py[line:551] - INFO: load:1.64 valid_run:3602.50 task_valid:3473.81 collect_output:104.24

====================================================================================================
SGG eval:     R @ 50: 0.3870;     R @ 100: 0.4823;     R @ 500: 0.5524;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2673;    mR @ 100: 0.3390;    mR @ 500: 0.4090;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.8125) (covering:0.2857) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7026) (says:0.0000) (sitting on:0.6015) (standing on:0.1550) (using:0.5000) (walking in:0.3333) (walking on:0.2523) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 07:33:23 - train.py[line:487] - INFO: 0.48228571428571426

====================================================================================================
SGG eval:     R @ 50: 0.3870;     R @ 100: 0.4823;     R @ 500: 0.5524;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2673;    mR @ 100: 0.3390;    mR @ 500: 0.4090;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6220) (covered in:0.8125) (covering:0.2857) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7026) (says:0.0000) (sitting on:0.6015) (standing on:0.1550) (using:0.5000) (walking in:0.3333) (walking on:0.2523) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-05 07:33:23 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-05 07:33:23 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.382 | loss_v1 0 | loss_v2 0 | nll_loss 0.234 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.482286 | ppl 1.18 | vqa_score 0.4617 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 12000 | best_R@100 0.602033
2023-01-05 07:33:23 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 12000 updates
2023-01-05 07:33:23 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-05 07:34:08 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-05 07:35:41 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 0.48228571428571426) (writing took 138.0349527709186 seconds)
2023-01-05 07:35:53 - progress_bar.py[line:274] - INFO: epoch 001:  12028 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=0.3, ups=0, wpb=109.4, bsz=40, num_updates=12010, lr=4.66834e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37123
2023-01-05 07:36:04 - progress_bar.py[line:274] - INFO: epoch 001:  12038 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.2, ups=0.88, wpb=107.8, bsz=40, num_updates=12020, lr=4.66789e-05, gnorm=0.616, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37134
2023-01-05 07:36:15 - progress_bar.py[line:274] - INFO: epoch 001:  12048 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.1, ups=0.92, wpb=109.3, bsz=40, num_updates=12030, lr=4.66744e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=37145
2023-01-05 07:36:27 - progress_bar.py[line:274] - INFO: epoch 001:  12058 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.9, wpb=109, bsz=40, num_updates=12040, lr=4.66699e-05, gnorm=0.59, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37156
2023-01-05 07:36:38 - progress_bar.py[line:274] - INFO: epoch 001:  12068 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.88, wpb=109.2, bsz=40, num_updates=12050, lr=4.66654e-05, gnorm=0.588, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=37168
2023-01-05 07:36:49 - progress_bar.py[line:274] - INFO: epoch 001:  12078 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98, ups=0.9, wpb=109.4, bsz=40, num_updates=12060, lr=4.66609e-05, gnorm=0.655, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37179
2023-01-05 07:37:00 - progress_bar.py[line:274] - INFO: epoch 001:  12088 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.3, ups=0.9, wpb=108.6, bsz=40, num_updates=12070, lr=4.66564e-05, gnorm=0.627, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37190
2023-01-05 07:37:11 - progress_bar.py[line:274] - INFO: epoch 001:  12098 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.5, ups=0.92, wpb=109.5, bsz=40, num_updates=12080, lr=4.66519e-05, gnorm=0.549, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37201
2023-01-05 07:37:23 - progress_bar.py[line:274] - INFO: epoch 001:  12108 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98, ups=0.91, wpb=107.8, bsz=40, num_updates=12090, lr=4.66474e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37212
2023-01-05 07:37:33 - progress_bar.py[line:274] - INFO: epoch 001:  12118 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.5, ups=0.93, wpb=109.7, bsz=40, num_updates=12100, lr=4.66429e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37223
2023-01-05 07:37:45 - progress_bar.py[line:274] - INFO: epoch 001:  12128 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.5, ups=0.89, wpb=108.1, bsz=40, num_updates=12110, lr=4.66384e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37234
2023-01-05 07:37:56 - progress_bar.py[line:274] - INFO: epoch 001:  12138 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.2, ups=0.91, wpb=110.2, bsz=40, num_updates=12120, lr=4.66339e-05, gnorm=0.581, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=37246
2023-01-05 07:38:03 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-05 07:38:08 - progress_bar.py[line:274] - INFO: epoch 001:  12149 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=88, ups=0.81, wpb=108.7, bsz=40, num_updates=12130, lr=4.66294e-05, gnorm=0.579, clip=10, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=37258
2023-01-05 07:38:19 - progress_bar.py[line:274] - INFO: epoch 001:  12159 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.91, wpb=109.6, bsz=40, num_updates=12140, lr=4.66249e-05, gnorm=0.555, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37269
2023-01-05 07:38:31 - progress_bar.py[line:274] - INFO: epoch 001:  12169 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.9, wpb=108.5, bsz=40, num_updates=12150, lr=4.66204e-05, gnorm=0.502, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37280
2023-01-05 07:38:42 - progress_bar.py[line:274] - INFO: epoch 001:  12179 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.89, wpb=109.3, bsz=40, num_updates=12160, lr=4.66159e-05, gnorm=0.78, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37292
2023-01-05 07:38:53 - progress_bar.py[line:274] - INFO: epoch 001:  12189 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.9, ups=0.88, wpb=108.5, bsz=40, num_updates=12170, lr=4.66114e-05, gnorm=0.563, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37303
2023-01-05 07:39:04 - progress_bar.py[line:274] - INFO: epoch 001:  12199 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.2, ups=0.92, wpb=110.3, bsz=40, num_updates=12180, lr=4.66069e-05, gnorm=0.57, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37314
2023-01-05 07:39:16 - progress_bar.py[line:274] - INFO: epoch 001:  12209 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98, ups=0.9, wpb=108.9, bsz=40, num_updates=12190, lr=4.66024e-05, gnorm=0.467, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37325
2023-01-05 07:39:27 - progress_bar.py[line:274] - INFO: epoch 001:  12219 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.9, wpb=108, bsz=40, num_updates=12200, lr=4.65979e-05, gnorm=0.633, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37336
2023-01-05 07:39:38 - progress_bar.py[line:274] - INFO: epoch 001:  12229 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.89, wpb=109.8, bsz=40, num_updates=12210, lr=4.65934e-05, gnorm=0.526, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37348
2023-01-05 07:39:49 - progress_bar.py[line:274] - INFO: epoch 001:  12239 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=109.8, bsz=40, num_updates=12220, lr=4.65889e-05, gnorm=0.89, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37359
2023-01-05 07:40:00 - progress_bar.py[line:274] - INFO: epoch 001:  12249 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.91, wpb=109.6, bsz=40, num_updates=12230, lr=4.65845e-05, gnorm=0.567, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37370
2023-01-05 07:40:12 - progress_bar.py[line:274] - INFO: epoch 001:  12259 / 115845 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.9, wpb=108.9, bsz=40, num_updates=12240, lr=4.658e-05, gnorm=0.76, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37381
2023-01-05 07:40:23 - progress_bar.py[line:274] - INFO: epoch 001:  12269 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.89, wpb=109.8, bsz=40, num_updates=12250, lr=4.65755e-05, gnorm=0.924, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37393
2023-01-05 07:40:34 - progress_bar.py[line:274] - INFO: epoch 001:  12279 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.9, wpb=108.6, bsz=40, num_updates=12260, lr=4.6571e-05, gnorm=0.619, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37404
2023-01-05 07:40:45 - progress_bar.py[line:274] - INFO: epoch 001:  12289 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.91, wpb=109.8, bsz=40, num_updates=12270, lr=4.65665e-05, gnorm=0.528, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37415
2023-01-05 07:40:57 - progress_bar.py[line:274] - INFO: epoch 001:  12299 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.6, ups=0.9, wpb=108.7, bsz=40, num_updates=12280, lr=4.6562e-05, gnorm=0.6, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37426
2023-01-05 07:41:08 - progress_bar.py[line:274] - INFO: epoch 001:  12309 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.1, ups=0.91, wpb=109.4, bsz=40, num_updates=12290, lr=4.65575e-05, gnorm=0.634, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37437
2023-01-05 07:41:19 - progress_bar.py[line:274] - INFO: epoch 001:  12319 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.89, wpb=108.8, bsz=40, num_updates=12300, lr=4.6553e-05, gnorm=0.698, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37449
2023-01-05 07:41:30 - progress_bar.py[line:274] - INFO: epoch 001:  12329 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.9, wpb=110, bsz=40, num_updates=12310, lr=4.65485e-05, gnorm=0.559, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37460
2023-01-05 07:41:41 - progress_bar.py[line:274] - INFO: epoch 001:  12339 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.7, ups=0.92, wpb=109.3, bsz=40, num_updates=12320, lr=4.6544e-05, gnorm=0.589, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37471
2023-01-05 07:41:52 - progress_bar.py[line:274] - INFO: epoch 001:  12349 / 115845 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=12330, lr=4.65395e-05, gnorm=0.53, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37482
2023-01-05 07:42:04 - progress_bar.py[line:274] - INFO: epoch 001:  12359 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.7, ups=0.89, wpb=107.6, bsz=40, num_updates=12340, lr=4.6535e-05, gnorm=0.506, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37493
2023-01-05 07:42:15 - progress_bar.py[line:274] - INFO: epoch 001:  12369 / 115845 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.4, ups=0.91, wpb=107.4, bsz=40, num_updates=12350, lr=4.65305e-05, gnorm=0.623, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37504
2023-01-05 07:42:25 - progress_bar.py[line:274] - INFO: epoch 001:  12379 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.3, ups=0.94, wpb=110.3, bsz=40, num_updates=12360, lr=4.6526e-05, gnorm=0.446, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37515
2023-01-05 07:42:37 - progress_bar.py[line:274] - INFO: epoch 001:  12389 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=12370, lr=4.65215e-05, gnorm=0.527, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37527
2023-01-05 07:42:48 - progress_bar.py[line:274] - INFO: epoch 001:  12399 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.4, ups=0.91, wpb=109.4, bsz=40, num_updates=12380, lr=4.6517e-05, gnorm=0.593, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37538
2023-01-05 07:42:59 - progress_bar.py[line:274] - INFO: epoch 001:  12409 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.2, ups=0.91, wpb=110.4, bsz=40, num_updates=12390, lr=4.65125e-05, gnorm=0.619, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37549
2023-01-05 07:43:10 - progress_bar.py[line:274] - INFO: epoch 001:  12419 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.9, wpb=108.3, bsz=40, num_updates=12400, lr=4.6508e-05, gnorm=0.582, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37560
2023-01-05 07:43:21 - progress_bar.py[line:274] - INFO: epoch 001:  12429 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=109.9, bsz=40, num_updates=12410, lr=4.65035e-05, gnorm=0.428, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37571
2023-01-05 07:43:33 - progress_bar.py[line:274] - INFO: epoch 001:  12439 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.5, ups=0.9, wpb=109.7, bsz=40, num_updates=12420, lr=4.6499e-05, gnorm=0.621, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37582
2023-01-05 07:43:44 - progress_bar.py[line:274] - INFO: epoch 001:  12449 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.5, ups=0.91, wpb=109.5, bsz=40, num_updates=12430, lr=4.64945e-05, gnorm=0.51, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37593
2023-01-05 07:43:55 - progress_bar.py[line:274] - INFO: epoch 001:  12459 / 115845 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.9, wpb=107.8, bsz=40, num_updates=12440, lr=4.649e-05, gnorm=0.594, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37605
2023-01-05 07:44:06 - progress_bar.py[line:274] - INFO: epoch 001:  12469 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.91, wpb=108.4, bsz=40, num_updates=12450, lr=4.64855e-05, gnorm=0.637, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37616
2023-01-05 07:44:17 - progress_bar.py[line:274] - INFO: epoch 001:  12479 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.91, wpb=109.1, bsz=40, num_updates=12460, lr=4.6481e-05, gnorm=0.714, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37627
2023-01-05 07:44:28 - progress_bar.py[line:274] - INFO: epoch 001:  12489 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.91, wpb=108.4, bsz=40, num_updates=12470, lr=4.64765e-05, gnorm=0.556, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37638
2023-01-05 07:44:39 - progress_bar.py[line:274] - INFO: epoch 001:  12499 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.91, wpb=109.7, bsz=40, num_updates=12480, lr=4.64721e-05, gnorm=0.554, clip=0, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=37649
2023-01-05 07:44:51 - progress_bar.py[line:274] - INFO: epoch 001:  12509 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.9, wpb=109.1, bsz=40, num_updates=12490, lr=4.64676e-05, gnorm=0.759, clip=20, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37660
2023-01-05 07:45:02 - progress_bar.py[line:274] - INFO: epoch 001:  12519 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.9, wpb=109.4, bsz=40, num_updates=12500, lr=4.64631e-05, gnorm=0.505, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37671
2023-01-05 07:45:13 - progress_bar.py[line:274] - INFO: epoch 001:  12529 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.91, wpb=109.1, bsz=40, num_updates=12510, lr=4.64586e-05, gnorm=0.557, clip=10, loss_scale=256, train_wall=11, gb_free=10, ema_decay=0.9999, wall=37683
2023-01-05 07:45:24 - progress_bar.py[line:274] - INFO: epoch 001:  12539 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.5, ups=0.87, wpb=110.5, bsz=40, num_updates=12520, lr=4.64541e-05, gnorm=0.494, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37694
2023-01-05 07:45:36 - progress_bar.py[line:274] - INFO: epoch 001:  12549 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=12530, lr=4.64496e-05, gnorm=1.583, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37705
2023-01-05 07:45:47 - progress_bar.py[line:274] - INFO: epoch 001:  12559 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.7, nsentences=40, sample_size=106.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.92, wpb=106.7, bsz=40, num_updates=12540, lr=4.64451e-05, gnorm=0.75, clip=40, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37716
2023-01-05 07:45:58 - progress_bar.py[line:274] - INFO: epoch 001:  12569 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.2, ups=0.89, wpb=108.9, bsz=40, num_updates=12550, lr=4.64406e-05, gnorm=0.57, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37728
2023-01-05 07:46:09 - progress_bar.py[line:274] - INFO: epoch 001:  12579 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.89, wpb=110.2, bsz=40, num_updates=12560, lr=4.64361e-05, gnorm=0.644, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37739
2023-01-05 07:46:21 - progress_bar.py[line:274] - INFO: epoch 001:  12589 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.89, wpb=109.3, bsz=40, num_updates=12570, lr=4.64316e-05, gnorm=0.562, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37750
2023-01-05 07:46:32 - progress_bar.py[line:274] - INFO: epoch 001:  12599 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.89, wpb=108.7, bsz=40, num_updates=12580, lr=4.64271e-05, gnorm=0.556, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37762
2023-01-05 07:46:43 - progress_bar.py[line:274] - INFO: epoch 001:  12609 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.89, wpb=109.2, bsz=40, num_updates=12590, lr=4.64226e-05, gnorm=0.74, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=37773
2023-01-05 07:46:55 - progress_bar.py[line:274] - INFO: epoch 001:  12619 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.2, ups=0.87, wpb=108.8, bsz=40, num_updates=12600, lr=4.64181e-05, gnorm=0.694, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37785
2023-01-05 07:47:06 - progress_bar.py[line:274] - INFO: epoch 001:  12629 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=12610, lr=4.64136e-05, gnorm=0.472, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37796
2023-01-05 07:47:17 - progress_bar.py[line:274] - INFO: epoch 001:  12639 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.3, ups=0.92, wpb=109.3, bsz=40, num_updates=12620, lr=4.64091e-05, gnorm=0.536, clip=0, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=37807
2023-01-05 07:47:29 - progress_bar.py[line:274] - INFO: epoch 001:  12649 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=12630, lr=4.64046e-05, gnorm=0.759, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37818
2023-01-05 07:47:40 - progress_bar.py[line:274] - INFO: epoch 001:  12659 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=108.5, bsz=40, num_updates=12640, lr=4.64001e-05, gnorm=0.574, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37829
2023-01-05 07:47:51 - progress_bar.py[line:274] - INFO: epoch 001:  12669 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.8, ups=0.89, wpb=108.2, bsz=40, num_updates=12650, lr=4.63956e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37841
2023-01-05 07:48:02 - progress_bar.py[line:274] - INFO: epoch 001:  12679 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.8, ups=0.88, wpb=110.5, bsz=40, num_updates=12660, lr=4.63911e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37852
2023-01-05 07:48:14 - progress_bar.py[line:274] - INFO: epoch 001:  12689 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.88, wpb=109.9, bsz=40, num_updates=12670, lr=4.63866e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37863
2023-01-05 07:48:25 - progress_bar.py[line:274] - INFO: epoch 001:  12699 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.3, ups=0.91, wpb=109.5, bsz=40, num_updates=12680, lr=4.63821e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37875
2023-01-05 07:48:36 - progress_bar.py[line:274] - INFO: epoch 001:  12709 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.7, ups=0.9, wpb=109.1, bsz=40, num_updates=12690, lr=4.63776e-05, gnorm=0.614, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37886
2023-01-05 07:48:48 - progress_bar.py[line:274] - INFO: epoch 001:  12719 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.6, ups=0.87, wpb=109.6, bsz=40, num_updates=12700, lr=4.63731e-05, gnorm=0.641, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37897
2023-01-05 07:48:59 - progress_bar.py[line:274] - INFO: epoch 001:  12729 / 115845 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.2, ups=0.91, wpb=107.6, bsz=40, num_updates=12710, lr=4.63686e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37908
2023-01-05 07:49:10 - progress_bar.py[line:274] - INFO: epoch 001:  12739 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.91, wpb=108.2, bsz=40, num_updates=12720, lr=4.63642e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37919
2023-01-05 07:49:21 - progress_bar.py[line:274] - INFO: epoch 001:  12749 / 115845 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=102.8, ups=0.93, wpb=110.4, bsz=40, num_updates=12730, lr=4.63597e-05, gnorm=0.502, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37930
2023-01-05 07:49:32 - progress_bar.py[line:274] - INFO: epoch 001:  12759 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.89, wpb=108.6, bsz=40, num_updates=12740, lr=4.63552e-05, gnorm=0.592, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37942
2023-01-05 07:49:43 - progress_bar.py[line:274] - INFO: epoch 001:  12769 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.4, ups=0.88, wpb=110.4, bsz=40, num_updates=12750, lr=4.63507e-05, gnorm=0.686, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=37953
2023-01-05 07:49:54 - progress_bar.py[line:274] - INFO: epoch 001:  12779 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=12760, lr=4.63462e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=37964
2023-01-05 07:50:06 - progress_bar.py[line:274] - INFO: epoch 001:  12789 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99, ups=0.91, wpb=108.9, bsz=40, num_updates=12770, lr=4.63417e-05, gnorm=0.551, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=37975
2023-01-05 07:50:17 - progress_bar.py[line:274] - INFO: epoch 001:  12799 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.8, ups=0.89, wpb=107.3, bsz=40, num_updates=12780, lr=4.63372e-05, gnorm=0.508, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37987
2023-01-05 07:50:28 - progress_bar.py[line:274] - INFO: epoch 001:  12809 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.5, nsentences=40, sample_size=106.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.4, ups=0.9, wpb=106.5, bsz=40, num_updates=12790, lr=4.63327e-05, gnorm=0.795, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=37998
2023-01-05 07:50:39 - progress_bar.py[line:274] - INFO: epoch 001:  12819 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.5, ups=0.94, wpb=109.1, bsz=40, num_updates=12800, lr=4.63282e-05, gnorm=0.738, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38009
2023-01-05 07:50:50 - progress_bar.py[line:274] - INFO: epoch 001:  12829 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.9, wpb=110.1, bsz=40, num_updates=12810, lr=4.63237e-05, gnorm=0.542, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38020
2023-01-05 07:51:01 - progress_bar.py[line:274] - INFO: epoch 001:  12839 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96, ups=0.89, wpb=108.3, bsz=40, num_updates=12820, lr=4.63192e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38031
2023-01-05 07:51:13 - progress_bar.py[line:274] - INFO: epoch 001:  12849 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.9, wpb=109.7, bsz=40, num_updates=12830, lr=4.63147e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38042
2023-01-05 07:51:24 - progress_bar.py[line:274] - INFO: epoch 001:  12859 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.2, ups=0.92, wpb=108.8, bsz=40, num_updates=12840, lr=4.63102e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38053
2023-01-05 07:51:35 - progress_bar.py[line:274] - INFO: epoch 001:  12869 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.3, ups=0.91, wpb=109.7, bsz=40, num_updates=12850, lr=4.63057e-05, gnorm=0.902, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38064
2023-01-05 07:51:46 - progress_bar.py[line:274] - INFO: epoch 001:  12879 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=12860, lr=4.63012e-05, gnorm=0.593, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38076
2023-01-05 07:51:57 - progress_bar.py[line:274] - INFO: epoch 001:  12889 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.9, wpb=108.8, bsz=40, num_updates=12870, lr=4.62967e-05, gnorm=0.66, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38087
2023-01-05 07:52:08 - progress_bar.py[line:274] - INFO: epoch 001:  12899 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=103.9, ups=0.94, wpb=110.6, bsz=40, num_updates=12880, lr=4.62922e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38098
2023-01-05 07:52:19 - progress_bar.py[line:274] - INFO: epoch 001:  12909 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.9, wpb=110.3, bsz=40, num_updates=12890, lr=4.62877e-05, gnorm=0.504, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38109
2023-01-05 07:52:30 - progress_bar.py[line:274] - INFO: epoch 001:  12919 / 115845 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.2, ups=0.9, wpb=109, bsz=40, num_updates=12900, lr=4.62832e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38120
2023-01-05 07:52:41 - progress_bar.py[line:274] - INFO: epoch 001:  12929 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.92, wpb=108.6, bsz=40, num_updates=12910, lr=4.62787e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38131
2023-01-05 07:52:53 - progress_bar.py[line:274] - INFO: epoch 001:  12939 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.6, ups=0.89, wpb=109.1, bsz=40, num_updates=12920, lr=4.62742e-05, gnorm=0.603, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38142
2023-01-05 07:53:04 - progress_bar.py[line:274] - INFO: epoch 001:  12949 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.88, wpb=109.8, bsz=40, num_updates=12930, lr=4.62697e-05, gnorm=0.701, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38154
2023-01-05 07:53:15 - progress_bar.py[line:274] - INFO: epoch 001:  12959 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.89, wpb=109.1, bsz=40, num_updates=12940, lr=4.62652e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38165
2023-01-05 07:53:27 - progress_bar.py[line:274] - INFO: epoch 001:  12969 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.2, ups=0.9, wpb=108.5, bsz=40, num_updates=12950, lr=4.62607e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38176
2023-01-05 07:53:38 - progress_bar.py[line:274] - INFO: epoch 001:  12979 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.92, wpb=108.4, bsz=40, num_updates=12960, lr=4.62562e-05, gnorm=0.565, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38187
2023-01-05 07:53:49 - progress_bar.py[line:274] - INFO: epoch 001:  12989 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.89, wpb=109.9, bsz=40, num_updates=12970, lr=4.62518e-05, gnorm=0.666, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38199
2023-01-05 07:54:00 - progress_bar.py[line:274] - INFO: epoch 001:  12999 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.8, ups=0.92, wpb=107.7, bsz=40, num_updates=12980, lr=4.62473e-05, gnorm=0.869, clip=40, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=38210
2023-01-05 07:54:11 - progress_bar.py[line:274] - INFO: epoch 001:  13009 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96, ups=0.88, wpb=109, bsz=40, num_updates=12990, lr=4.62428e-05, gnorm=0.709, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38221
2023-01-05 07:54:23 - progress_bar.py[line:274] - INFO: epoch 001:  13019 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.5, ups=0.88, wpb=109.2, bsz=40, num_updates=13000, lr=4.62383e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38233
2023-01-05 07:54:34 - progress_bar.py[line:274] - INFO: epoch 001:  13029 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=103.5, ups=0.94, wpb=109.6, bsz=40, num_updates=13010, lr=4.62338e-05, gnorm=0.639, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38243
2023-01-05 07:54:45 - progress_bar.py[line:274] - INFO: epoch 001:  13039 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.9, wpb=109.4, bsz=40, num_updates=13020, lr=4.62293e-05, gnorm=0.52, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=38254
2023-01-05 07:54:56 - progress_bar.py[line:274] - INFO: epoch 001:  13049 / 115845 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.6, ups=0.89, wpb=109.1, bsz=40, num_updates=13030, lr=4.62248e-05, gnorm=0.617, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38266
2023-01-05 07:55:07 - progress_bar.py[line:274] - INFO: epoch 001:  13059 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.8, ups=0.92, wpb=108.7, bsz=40, num_updates=13040, lr=4.62203e-05, gnorm=0.53, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38277
2023-01-05 07:55:18 - progress_bar.py[line:274] - INFO: epoch 001:  13069 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=94.5, ups=0.88, wpb=107.7, bsz=40, num_updates=13050, lr=4.62158e-05, gnorm=0.563, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38288
2023-01-05 07:55:29 - progress_bar.py[line:274] - INFO: epoch 001:  13079 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.8, ups=0.91, wpb=108.2, bsz=40, num_updates=13060, lr=4.62113e-05, gnorm=0.608, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38299
2023-01-05 07:55:41 - progress_bar.py[line:274] - INFO: epoch 001:  13089 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=94.1, ups=0.87, wpb=107.6, bsz=40, num_updates=13070, lr=4.62068e-05, gnorm=0.691, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38311
2023-01-05 07:55:52 - progress_bar.py[line:274] - INFO: epoch 001:  13099 / 115845 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.6, ups=0.9, wpb=110.2, bsz=40, num_updates=13080, lr=4.62023e-05, gnorm=0.506, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38322
2023-01-05 07:56:03 - progress_bar.py[line:274] - INFO: epoch 001:  13109 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95, ups=0.88, wpb=107.9, bsz=40, num_updates=13090, lr=4.61978e-05, gnorm=0.548, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38333
2023-01-05 07:56:15 - progress_bar.py[line:274] - INFO: epoch 001:  13119 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.89, wpb=108.7, bsz=40, num_updates=13100, lr=4.61933e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38344
2023-01-05 07:56:26 - progress_bar.py[line:274] - INFO: epoch 001:  13129 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.5, ups=0.89, wpb=108.1, bsz=40, num_updates=13110, lr=4.61888e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38356
2023-01-05 07:56:37 - progress_bar.py[line:274] - INFO: epoch 001:  13139 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=13120, lr=4.61843e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38367
2023-01-05 07:56:48 - progress_bar.py[line:274] - INFO: epoch 001:  13149 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.91, wpb=108.4, bsz=40, num_updates=13130, lr=4.61798e-05, gnorm=0.796, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38378
2023-01-05 07:57:00 - progress_bar.py[line:274] - INFO: epoch 001:  13159 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.89, wpb=110.2, bsz=40, num_updates=13140, lr=4.61753e-05, gnorm=0.492, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38389
2023-01-05 07:57:10 - progress_bar.py[line:274] - INFO: epoch 001:  13169 / 115845 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103.5, ups=0.93, wpb=111.1, bsz=40, num_updates=13150, lr=4.61708e-05, gnorm=0.457, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38400
2023-01-05 07:57:22 - progress_bar.py[line:274] - INFO: epoch 001:  13179 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.9, wpb=109.9, bsz=40, num_updates=13160, lr=4.61663e-05, gnorm=0.426, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38411
2023-01-05 07:57:33 - progress_bar.py[line:274] - INFO: epoch 001:  13189 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96, ups=0.87, wpb=110.1, bsz=40, num_updates=13170, lr=4.61618e-05, gnorm=0.503, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38423
2023-01-05 07:57:44 - progress_bar.py[line:274] - INFO: epoch 001:  13199 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.2, ups=0.91, wpb=109.2, bsz=40, num_updates=13180, lr=4.61573e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38434
2023-01-05 07:57:55 - progress_bar.py[line:274] - INFO: epoch 001:  13209 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.9, wpb=109.1, bsz=40, num_updates=13190, lr=4.61528e-05, gnorm=0.502, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38445
2023-01-05 07:58:07 - progress_bar.py[line:274] - INFO: epoch 001:  13219 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.3, ups=0.89, wpb=110.6, bsz=40, num_updates=13200, lr=4.61483e-05, gnorm=0.576, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38456
2023-01-05 07:58:18 - progress_bar.py[line:274] - INFO: epoch 001:  13229 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97, ups=0.89, wpb=108.5, bsz=40, num_updates=13210, lr=4.61439e-05, gnorm=0.61, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38468
2023-01-05 07:58:29 - progress_bar.py[line:274] - INFO: epoch 001:  13239 / 115845 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=106.6, nsentences=40, sample_size=106.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.91, wpb=106.6, bsz=40, num_updates=13220, lr=4.61394e-05, gnorm=0.424, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38479
2023-01-05 07:58:40 - progress_bar.py[line:274] - INFO: epoch 001:  13249 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.2, ups=0.91, wpb=109.3, bsz=40, num_updates=13230, lr=4.61349e-05, gnorm=0.618, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38490
2023-01-05 07:58:52 - progress_bar.py[line:274] - INFO: epoch 001:  13259 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=13240, lr=4.61304e-05, gnorm=0.543, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38501
2023-01-05 07:59:03 - progress_bar.py[line:274] - INFO: epoch 001:  13269 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.3, ups=0.92, wpb=108, bsz=40, num_updates=13250, lr=4.61259e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38512
2023-01-05 07:59:14 - progress_bar.py[line:274] - INFO: epoch 001:  13279 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.3, ups=0.92, wpb=110.2, bsz=40, num_updates=13260, lr=4.61214e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38523
2023-01-05 07:59:25 - progress_bar.py[line:274] - INFO: epoch 001:  13289 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.91, wpb=108.4, bsz=40, num_updates=13270, lr=4.61169e-05, gnorm=0.645, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38534
2023-01-05 07:59:35 - progress_bar.py[line:274] - INFO: epoch 001:  13299 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.9, ups=0.94, wpb=109.5, bsz=40, num_updates=13280, lr=4.61124e-05, gnorm=0.6, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38545
2023-01-05 07:59:47 - progress_bar.py[line:274] - INFO: epoch 001:  13309 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.3, ups=0.89, wpb=107.8, bsz=40, num_updates=13290, lr=4.61079e-05, gnorm=0.814, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38556
2023-01-05 07:59:58 - progress_bar.py[line:274] - INFO: epoch 001:  13319 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.7, ups=0.88, wpb=108.3, bsz=40, num_updates=13300, lr=4.61034e-05, gnorm=0.593, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38568
2023-01-05 08:00:09 - progress_bar.py[line:274] - INFO: epoch 001:  13329 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.91, wpb=109.8, bsz=40, num_updates=13310, lr=4.60989e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38579
2023-01-05 08:00:21 - progress_bar.py[line:274] - INFO: epoch 001:  13339 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.2, ups=0.88, wpb=109.1, bsz=40, num_updates=13320, lr=4.60944e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=38590
2023-01-05 08:00:32 - progress_bar.py[line:274] - INFO: epoch 001:  13349 / 115845 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=97.1, ups=0.9, wpb=107.7, bsz=40, num_updates=13330, lr=4.60899e-05, gnorm=0.772, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38601
2023-01-05 08:00:43 - progress_bar.py[line:274] - INFO: epoch 001:  13359 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=96.7, ups=0.9, wpb=107.4, bsz=40, num_updates=13340, lr=4.60854e-05, gnorm=0.532, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38613
2023-01-05 08:00:54 - progress_bar.py[line:274] - INFO: epoch 001:  13369 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.9, wpb=108.1, bsz=40, num_updates=13350, lr=4.60809e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38624
2023-01-05 08:01:05 - progress_bar.py[line:274] - INFO: epoch 001:  13379 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.5, ups=0.92, wpb=110.3, bsz=40, num_updates=13360, lr=4.60764e-05, gnorm=0.487, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38635
2023-01-05 08:01:16 - progress_bar.py[line:274] - INFO: epoch 001:  13389 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=13370, lr=4.60719e-05, gnorm=0.644, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38646
2023-01-05 08:01:27 - progress_bar.py[line:274] - INFO: epoch 001:  13399 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.9, wpb=108.7, bsz=40, num_updates=13380, lr=4.60674e-05, gnorm=0.463, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38657
2023-01-05 08:01:39 - progress_bar.py[line:274] - INFO: epoch 001:  13409 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.89, wpb=108, bsz=40, num_updates=13390, lr=4.60629e-05, gnorm=0.576, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38668
2023-01-05 08:01:50 - progress_bar.py[line:274] - INFO: epoch 001:  13419 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.4, ups=0.89, wpb=107.8, bsz=40, num_updates=13400, lr=4.60584e-05, gnorm=0.546, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38680
2023-01-05 08:02:01 - progress_bar.py[line:274] - INFO: epoch 001:  13429 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102, ups=0.92, wpb=110.6, bsz=40, num_updates=13410, lr=4.60539e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38690
2023-01-05 08:02:12 - progress_bar.py[line:274] - INFO: epoch 001:  13439 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.9, wpb=108.2, bsz=40, num_updates=13420, lr=4.60494e-05, gnorm=0.557, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38702
2023-01-05 08:02:23 - progress_bar.py[line:274] - INFO: epoch 001:  13449 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.92, wpb=108.3, bsz=40, num_updates=13430, lr=4.60449e-05, gnorm=0.558, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38713
2023-01-05 08:02:34 - progress_bar.py[line:274] - INFO: epoch 001:  13459 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.89, wpb=107.8, bsz=40, num_updates=13440, lr=4.60404e-05, gnorm=0.58, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38724
2023-01-05 08:02:45 - progress_bar.py[line:274] - INFO: epoch 001:  13469 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.91, wpb=109.3, bsz=40, num_updates=13450, lr=4.60359e-05, gnorm=0.502, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38735
2023-01-05 08:02:57 - progress_bar.py[line:274] - INFO: epoch 001:  13479 / 115845 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=13460, lr=4.60315e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38746
2023-01-05 08:03:08 - progress_bar.py[line:274] - INFO: epoch 001:  13489 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.2, ups=0.89, wpb=107, bsz=40, num_updates=13470, lr=4.6027e-05, gnorm=0.616, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38758
2023-01-05 08:03:19 - progress_bar.py[line:274] - INFO: epoch 001:  13499 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=13480, lr=4.60225e-05, gnorm=0.581, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38769
2023-01-05 08:03:31 - progress_bar.py[line:274] - INFO: epoch 001:  13509 / 115845 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.4, ups=0.87, wpb=110.3, bsz=40, num_updates=13490, lr=4.6018e-05, gnorm=0.54, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38780
2023-01-05 08:03:42 - progress_bar.py[line:274] - INFO: epoch 001:  13519 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.2, ups=0.89, wpb=109.2, bsz=40, num_updates=13500, lr=4.60135e-05, gnorm=0.463, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38792
2023-01-05 08:03:53 - progress_bar.py[line:274] - INFO: epoch 001:  13529 / 115845 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.4, ups=0.91, wpb=110.6, bsz=40, num_updates=13510, lr=4.6009e-05, gnorm=0.422, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38803
2023-01-05 08:04:04 - progress_bar.py[line:274] - INFO: epoch 001:  13539 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.4, ups=0.92, wpb=109.2, bsz=40, num_updates=13520, lr=4.60045e-05, gnorm=0.545, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38814
2023-01-05 08:04:15 - progress_bar.py[line:274] - INFO: epoch 001:  13549 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.5, ups=0.91, wpb=110.3, bsz=40, num_updates=13530, lr=4.6e-05, gnorm=0.631, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38825
2023-01-05 08:04:26 - progress_bar.py[line:274] - INFO: epoch 001:  13559 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.1, ups=0.91, wpb=108, bsz=40, num_updates=13540, lr=4.59955e-05, gnorm=0.597, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38836
2023-01-05 08:04:37 - progress_bar.py[line:274] - INFO: epoch 001:  13569 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.91, wpb=108.7, bsz=40, num_updates=13550, lr=4.5991e-05, gnorm=0.542, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38847
2023-01-05 08:04:48 - progress_bar.py[line:274] - INFO: epoch 001:  13579 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.1, ups=0.9, wpb=107.5, bsz=40, num_updates=13560, lr=4.59865e-05, gnorm=0.512, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=38858
2023-01-05 08:05:00 - progress_bar.py[line:274] - INFO: epoch 001:  13589 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.2, ups=0.91, wpb=110.3, bsz=40, num_updates=13570, lr=4.5982e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38869
2023-01-05 08:05:11 - progress_bar.py[line:274] - INFO: epoch 001:  13599 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.8, ups=0.91, wpb=108.9, bsz=40, num_updates=13580, lr=4.59775e-05, gnorm=0.721, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38880
2023-01-05 08:05:22 - progress_bar.py[line:274] - INFO: epoch 001:  13609 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.9, ups=0.88, wpb=109.7, bsz=40, num_updates=13590, lr=4.5973e-05, gnorm=0.503, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=38892
2023-01-05 08:05:33 - progress_bar.py[line:274] - INFO: epoch 001:  13619 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.89, wpb=109.3, bsz=40, num_updates=13600, lr=4.59685e-05, gnorm=0.659, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38903
2023-01-05 08:05:44 - progress_bar.py[line:274] - INFO: epoch 001:  13629 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.3, ups=0.93, wpb=109.9, bsz=40, num_updates=13610, lr=4.5964e-05, gnorm=0.581, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=38914
2023-01-05 08:05:55 - progress_bar.py[line:274] - INFO: epoch 001:  13639 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.92, wpb=107.3, bsz=40, num_updates=13620, lr=4.59595e-05, gnorm=0.682, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=38925
2023-01-05 08:06:07 - progress_bar.py[line:274] - INFO: epoch 001:  13649 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.6, ups=0.89, wpb=109.7, bsz=40, num_updates=13630, lr=4.5955e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38936
2023-01-05 08:06:18 - progress_bar.py[line:274] - INFO: epoch 001:  13659 / 115845 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=13640, lr=4.59505e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38948
2023-01-05 08:06:29 - progress_bar.py[line:274] - INFO: epoch 001:  13669 / 115845 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=103.3, ups=0.92, wpb=112.2, bsz=40, num_updates=13650, lr=4.5946e-05, gnorm=0.565, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=38959
2023-01-05 08:06:40 - progress_bar.py[line:274] - INFO: epoch 001:  13679 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.88, wpb=109.4, bsz=40, num_updates=13660, lr=4.59415e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=38970
2023-01-05 08:06:52 - progress_bar.py[line:274] - INFO: epoch 001:  13689 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100, ups=0.91, wpb=109.6, bsz=40, num_updates=13670, lr=4.5937e-05, gnorm=0.55, clip=10, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=38981
2023-01-05 08:06:54 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-01-05 08:07:04 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 08:07:05 - progress_bar.py[line:274] - INFO: epoch 001:  13701 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=82.2, ups=0.75, wpb=109.1, bsz=40, num_updates=13680, lr=4.59325e-05, gnorm=0.508, clip=10, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=38995
2023-01-05 08:07:16 - progress_bar.py[line:274] - INFO: epoch 001:  13711 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.4, ups=0.92, wpb=110.2, bsz=40, num_updates=13690, lr=4.5928e-05, gnorm=0.55, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39005
2023-01-05 08:07:27 - progress_bar.py[line:274] - INFO: epoch 001:  13721 / 115845 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.88, wpb=110.7, bsz=40, num_updates=13700, lr=4.59236e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39017
2023-01-05 08:07:38 - progress_bar.py[line:274] - INFO: epoch 001:  13731 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.9, wpb=109.8, bsz=40, num_updates=13710, lr=4.59191e-05, gnorm=0.472, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39028
2023-01-05 08:07:50 - progress_bar.py[line:274] - INFO: epoch 001:  13741 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.9, ups=0.91, wpb=110.7, bsz=40, num_updates=13720, lr=4.59146e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39039
2023-01-05 08:08:01 - progress_bar.py[line:274] - INFO: epoch 001:  13751 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.9, wpb=109.1, bsz=40, num_updates=13730, lr=4.59101e-05, gnorm=0.543, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39050
2023-01-05 08:08:12 - progress_bar.py[line:274] - INFO: epoch 001:  13761 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.89, wpb=108.4, bsz=40, num_updates=13740, lr=4.59056e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39062
2023-01-05 08:08:23 - progress_bar.py[line:274] - INFO: epoch 001:  13771 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.9, wpb=109.5, bsz=40, num_updates=13750, lr=4.59011e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39073
2023-01-05 08:08:34 - progress_bar.py[line:274] - INFO: epoch 001:  13781 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=13760, lr=4.58966e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39084
2023-01-05 08:08:45 - progress_bar.py[line:274] - INFO: epoch 001:  13791 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.2, ups=0.93, wpb=109.3, bsz=40, num_updates=13770, lr=4.58921e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39095
2023-01-05 08:08:57 - progress_bar.py[line:274] - INFO: epoch 001:  13801 / 115845 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.2, ups=0.88, wpb=107.6, bsz=40, num_updates=13780, lr=4.58876e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39106
2023-01-05 08:09:08 - progress_bar.py[line:274] - INFO: epoch 001:  13811 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.3, ups=0.88, wpb=108, bsz=40, num_updates=13790, lr=4.58831e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39118
2023-01-05 08:09:19 - progress_bar.py[line:274] - INFO: epoch 001:  13821 / 115845 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.1, ups=0.92, wpb=108.9, bsz=40, num_updates=13800, lr=4.58786e-05, gnorm=0.551, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39129
2023-01-05 08:09:30 - progress_bar.py[line:274] - INFO: epoch 001:  13831 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.2, ups=0.88, wpb=108.4, bsz=40, num_updates=13810, lr=4.58741e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39140
2023-01-05 08:09:42 - progress_bar.py[line:274] - INFO: epoch 001:  13841 / 115845 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.91, wpb=107.9, bsz=40, num_updates=13820, lr=4.58696e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39151
2023-01-05 08:09:53 - progress_bar.py[line:274] - INFO: epoch 001:  13851 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.4, ups=0.9, wpb=109.8, bsz=40, num_updates=13830, lr=4.58651e-05, gnorm=0.513, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39163
2023-01-05 08:10:04 - progress_bar.py[line:274] - INFO: epoch 001:  13861 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.9, ups=0.9, wpb=109.1, bsz=40, num_updates=13840, lr=4.58606e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39174
2023-01-05 08:10:15 - progress_bar.py[line:274] - INFO: epoch 001:  13871 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.92, wpb=107.9, bsz=40, num_updates=13850, lr=4.58561e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39185
2023-01-05 08:10:26 - progress_bar.py[line:274] - INFO: epoch 001:  13881 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.2, ups=0.93, wpb=108.6, bsz=40, num_updates=13860, lr=4.58516e-05, gnorm=0.552, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39195
2023-01-05 08:10:37 - progress_bar.py[line:274] - INFO: epoch 001:  13891 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.5, ups=0.91, wpb=108.8, bsz=40, num_updates=13870, lr=4.58471e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39207
2023-01-05 08:10:48 - progress_bar.py[line:274] - INFO: epoch 001:  13901 / 115845 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.9, wpb=109.7, bsz=40, num_updates=13880, lr=4.58426e-05, gnorm=0.632, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39218
2023-01-05 08:10:59 - progress_bar.py[line:274] - INFO: epoch 001:  13911 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.91, wpb=109.3, bsz=40, num_updates=13890, lr=4.58381e-05, gnorm=0.64, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39229
2023-01-05 08:11:10 - progress_bar.py[line:274] - INFO: epoch 001:  13921 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.91, wpb=108.7, bsz=40, num_updates=13900, lr=4.58336e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39240
2023-01-05 08:11:21 - progress_bar.py[line:274] - INFO: epoch 001:  13931 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=108.7, bsz=40, num_updates=13910, lr=4.58291e-05, gnorm=0.793, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39251
2023-01-05 08:11:33 - progress_bar.py[line:274] - INFO: epoch 001:  13941 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.89, wpb=110, bsz=40, num_updates=13920, lr=4.58246e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39262
2023-01-05 08:11:44 - progress_bar.py[line:274] - INFO: epoch 001:  13951 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.91, wpb=108.7, bsz=40, num_updates=13930, lr=4.58201e-05, gnorm=0.487, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39273
2023-01-05 08:11:55 - progress_bar.py[line:274] - INFO: epoch 001:  13961 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.1, ups=0.9, wpb=110.2, bsz=40, num_updates=13940, lr=4.58156e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39285
2023-01-05 08:12:06 - progress_bar.py[line:274] - INFO: epoch 001:  13971 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.91, wpb=107.8, bsz=40, num_updates=13950, lr=4.58112e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39296
2023-01-05 08:12:17 - progress_bar.py[line:274] - INFO: epoch 001:  13981 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.7, ups=0.92, wpb=109.6, bsz=40, num_updates=13960, lr=4.58067e-05, gnorm=0.549, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39307
2023-01-05 08:12:28 - progress_bar.py[line:274] - INFO: epoch 001:  13991 / 115845 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.8, ups=0.94, wpb=109.8, bsz=40, num_updates=13970, lr=4.58022e-05, gnorm=0.477, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39318
2023-01-05 08:12:39 - progress_bar.py[line:274] - INFO: epoch 001:  14001 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.91, wpb=108.6, bsz=40, num_updates=13980, lr=4.57977e-05, gnorm=0.501, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39329
2023-01-05 08:12:50 - progress_bar.py[line:274] - INFO: epoch 001:  14011 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.3, ups=0.9, wpb=109.6, bsz=40, num_updates=13990, lr=4.57932e-05, gnorm=0.494, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39340
2023-01-05 08:13:01 - progress_bar.py[line:274] - INFO: epoch 001:  14021 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99, ups=0.91, wpb=109.1, bsz=40, num_updates=14000, lr=4.57887e-05, gnorm=0.623, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39351
2023-01-05 08:13:01 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 08:13:02 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 08:13:02 - train.py[line:551] - INFO: load:0.86 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 08:13:04 - trainer.py[line:1409] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 8.85 GiB already allocated; 4.09 GiB free; 33.00 GiB reserved in total by PyTorch)
2023-01-05 08:13:04 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-05 08:13:04 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9063 MB |   10288 MB |    5623 TB |    5623 TB |
|       from large pool |    8919 MB |   10143 MB |    5621 TB |    5621 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9063 MB |   10288 MB |    5623 TB |    5623 TB |
|       from large pool |    8919 MB |   10143 MB |    5621 TB |    5621 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   33796 MB |   34456 MB |  246434 MB |  212638 MB |
|       from large pool |   33650 MB |   34304 MB |  246106 MB |  212456 MB |
|       from small pool |     146 MB |     152 MB |     328 MB |     182 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24732 MB |   26038 MB |    5743 TB |    5743 TB |
|       from large pool |   24730 MB |   26036 MB |    5741 TB |    5741 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  241612 K  |  241609 K  |
|       from large pool |     563    |     575    |   85034 K  |   85034 K  |
|       from small pool |    3095    |    3114    |  156577 K  |  156574 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  241612 K  |  241609 K  |
|       from large pool |     563    |     575    |   85034 K  |   85034 K  |
|       from small pool |    3095    |    3114    |  156577 K  |  156574 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     165    |     541    |     381    |
|       from large pool |      87    |      89    |     377    |     290    |
|       from small pool |      73    |      76    |     164    |      91    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     115    |  162270 K  |  162270 K  |
|       from large pool |      72    |      73    |   32943 K  |   32943 K  |
|       from small pool |      37    |      45    |  129327 K  |  129327 K  |
|===========================================================================|

2023-01-05 08:13:04 - trainer.py[line:1158] - WARNING: ran out of memory in validation step, retrying batch
2023-01-05 08:15:36 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 08:15:36 - train.py[line:551] - INFO: load:0.89 valid_run:153.13 task_valid:148.52 collect_output:3.49
2023-01-05 08:18:04 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 08:18:04 - train.py[line:551] - INFO: load:0.91 valid_run:301.40 task_valid:291.61 collect_output:7.62
2023-01-05 08:20:36 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 08:20:36 - train.py[line:551] - INFO: load:0.94 valid_run:453.30 task_valid:434.51 collect_output:15.58
2023-01-05 08:23:05 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 08:23:05 - train.py[line:551] - INFO: load:0.96 valid_run:601.93 task_valid:579.30 collect_output:18.41
2023-01-05 08:25:37 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 08:25:37 - train.py[line:551] - INFO: load:0.99 valid_run:754.06 task_valid:726.90 collect_output:21.92
2023-01-05 08:28:09 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 08:28:09 - train.py[line:551] - INFO: load:1.01 valid_run:905.76 task_valid:872.42 collect_output:27.07
2023-01-05 08:30:42 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 08:30:42 - train.py[line:551] - INFO: load:1.03 valid_run:1058.60 task_valid:1018.27 collect_output:33.06
2023-01-05 08:33:13 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 08:33:13 - train.py[line:551] - INFO: load:1.06 valid_run:1209.53 task_valid:1159.45 collect_output:41.76
2023-01-05 08:35:42 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 08:35:42 - train.py[line:551] - INFO: load:1.08 valid_run:1358.90 task_valid:1304.18 collect_output:45.41
2023-01-05 08:38:11 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 08:38:11 - train.py[line:551] - INFO: load:1.11 valid_run:1507.24 task_valid:1447.21 collect_output:49.69
2023-01-05 08:40:40 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 08:40:40 - train.py[line:551] - INFO: load:1.13 valid_run:1656.63 task_valid:1591.99 collect_output:53.27
2023-01-05 08:43:09 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 08:43:09 - train.py[line:551] - INFO: load:1.16 valid_run:1806.02 task_valid:1736.72 collect_output:56.94
2023-01-05 08:45:39 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 08:45:39 - train.py[line:551] - INFO: load:1.18 valid_run:1955.75 task_valid:1878.58 collect_output:63.77
2023-01-05 08:48:10 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 08:48:10 - train.py[line:551] - INFO: load:1.21 valid_run:2106.07 task_valid:2023.97 collect_output:67.67
2023-01-05 08:50:40 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 08:50:40 - train.py[line:551] - INFO: load:1.23 valid_run:2255.85 task_valid:2170.40 collect_output:70.00
2023-01-05 08:53:09 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 08:53:09 - train.py[line:551] - INFO: load:1.26 valid_run:2405.52 task_valid:2314.53 collect_output:74.52
2023-01-05 08:55:41 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 08:55:41 - train.py[line:551] - INFO: load:1.28 valid_run:2556.91 task_valid:2460.06 collect_output:79.35
2023-01-05 08:58:11 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 08:58:11 - train.py[line:551] - INFO: load:1.31 valid_run:2707.30 task_valid:2606.86 collect_output:81.92
2023-01-05 09:00:39 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 09:00:39 - train.py[line:551] - INFO: load:1.33 valid_run:2855.34 task_valid:2748.40 collect_output:87.40
2023-01-05 09:03:10 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 09:03:10 - train.py[line:551] - INFO: load:1.36 valid_run:3005.38 task_valid:2893.48 collect_output:91.31
2023-01-05 09:05:41 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 09:05:41 - train.py[line:551] - INFO: load:1.38 valid_run:3156.86 task_valid:3038.16 collect_output:97.09
2023-01-05 09:08:10 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 09:08:10 - train.py[line:551] - INFO: load:1.41 valid_run:3306.01 task_valid:3182.64 collect_output:100.71
2023-01-05 09:10:41 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 09:10:41 - train.py[line:551] - INFO: load:1.44 valid_run:3456.88 task_valid:3328.71 collect_output:104.49
2023-01-05 09:13:12 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 09:13:12 - train.py[line:551] - INFO: load:1.46 valid_run:3607.90 task_valid:3475.16 collect_output:108.06

====================================================================================================
SGG eval:     R @ 50: 0.3713;     R @ 100: 0.4457;     R @ 500: 0.5060;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2532;    mR @ 100: 0.3137;    mR @ 500: 0.3777;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5805) (covered in:0.8125) (covering:0.3143) (eating:0.7059) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.0500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.7083) (playing:0.0000) (riding:0.6258) (says:0.0000) (sitting on:0.5295) (standing on:0.1975) (using:0.5000) (walking in:0.0000) (walking on:0.1982) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-05 09:15:44 - train.py[line:487] - INFO: 0.44573809523809527
2023-01-05 09:15:44 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.3713;     R @ 100: 0.4457;     R @ 500: 0.5060;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2532;    mR @ 100: 0.3137;    mR @ 500: 0.3777;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5805) (covered in:0.8125) (covering:0.3143) (eating:0.7059) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.0500) (mounted on:0.0000) (painted on:0.3333) (parked on:0.7083) (playing:0.0000) (riding:0.6258) (says:0.0000) (sitting on:0.5295) (standing on:0.1975) (using:0.5000) (walking in:0.0000) (walking on:0.1982) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-05 09:15:44 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.404 | loss_v1 0 | loss_v2 0 | nll_loss 0.252 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.445738 | ppl 1.19 | vqa_score 0.4302 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 14000 | best_R@100 0.602033
2023-01-05 09:15:44 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 14000 updates
2023-01-05 09:15:44 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-05 09:16:21 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-05 09:17:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 0.44573809523809527) (writing took 120.32144011743367 seconds)
2023-01-05 09:17:56 - progress_bar.py[line:274] - INFO: epoch 001:  14031 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=0.3, ups=0, wpb=108.2, bsz=40, num_updates=14010, lr=4.57842e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43245
2023-01-05 09:18:07 - progress_bar.py[line:274] - INFO: epoch 001:  14041 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101.7, ups=0.93, wpb=109.1, bsz=40, num_updates=14020, lr=4.57797e-05, gnorm=0.387, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43256
2023-01-05 09:18:18 - progress_bar.py[line:274] - INFO: epoch 001:  14051 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.89, wpb=109, bsz=40, num_updates=14030, lr=4.57752e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43268
2023-01-05 09:18:29 - progress_bar.py[line:274] - INFO: epoch 001:  14061 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=14040, lr=4.57707e-05, gnorm=0.683, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43279
2023-01-05 09:18:40 - progress_bar.py[line:274] - INFO: epoch 001:  14071 / 115845 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.7, ups=0.93, wpb=108, bsz=40, num_updates=14050, lr=4.57662e-05, gnorm=0.722, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43290
2023-01-05 09:18:51 - progress_bar.py[line:274] - INFO: epoch 001:  14081 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.7, ups=0.92, wpb=108.4, bsz=40, num_updates=14060, lr=4.57617e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43301
2023-01-05 09:19:02 - progress_bar.py[line:274] - INFO: epoch 001:  14091 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101, ups=0.93, wpb=108.1, bsz=40, num_updates=14070, lr=4.57572e-05, gnorm=0.566, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43311
2023-01-05 09:19:12 - progress_bar.py[line:274] - INFO: epoch 001:  14101 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=101.3, ups=0.93, wpb=108.7, bsz=40, num_updates=14080, lr=4.57527e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43322
2023-01-05 09:19:24 - progress_bar.py[line:274] - INFO: epoch 001:  14111 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.2, ups=0.88, wpb=109.9, bsz=40, num_updates=14090, lr=4.57482e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43333
2023-01-05 09:19:35 - progress_bar.py[line:274] - INFO: epoch 001:  14121 / 115845 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.7, ups=0.9, wpb=110.6, bsz=40, num_updates=14100, lr=4.57437e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43345
2023-01-05 09:19:46 - progress_bar.py[line:274] - INFO: epoch 001:  14131 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=14110, lr=4.57392e-05, gnorm=0.625, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=43356
2023-01-05 09:19:58 - progress_bar.py[line:274] - INFO: epoch 001:  14141 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.7, ups=0.88, wpb=109.3, bsz=40, num_updates=14120, lr=4.57347e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43367
2023-01-05 09:20:09 - progress_bar.py[line:274] - INFO: epoch 001:  14151 / 115845 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=14130, lr=4.57302e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43379
2023-01-05 09:20:20 - progress_bar.py[line:274] - INFO: epoch 001:  14161 / 115845 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.8, ups=0.9, wpb=110.4, bsz=40, num_updates=14140, lr=4.57257e-05, gnorm=0.506, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43390
2023-01-05 09:20:32 - progress_bar.py[line:274] - INFO: epoch 001:  14171 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.4, ups=0.9, wpb=108.6, bsz=40, num_updates=14150, lr=4.57212e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43401
2023-01-05 09:20:42 - progress_bar.py[line:274] - INFO: epoch 001:  14181 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.2, ups=0.92, wpb=109, bsz=40, num_updates=14160, lr=4.57167e-05, gnorm=0.424, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43412
2023-01-05 09:20:53 - progress_bar.py[line:274] - INFO: epoch 001:  14191 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=103.5, ups=0.94, wpb=110.7, bsz=40, num_updates=14170, lr=4.57122e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43423
2023-01-05 09:21:05 - progress_bar.py[line:274] - INFO: epoch 001:  14201 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=14180, lr=4.57077e-05, gnorm=0.532, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43434
2023-01-05 09:21:16 - progress_bar.py[line:274] - INFO: epoch 001:  14211 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.4, ups=0.91, wpb=109.8, bsz=40, num_updates=14190, lr=4.57033e-05, gnorm=0.494, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43445
2023-01-05 09:21:27 - progress_bar.py[line:274] - INFO: epoch 001:  14221 / 115845 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.9, ups=0.92, wpb=110, bsz=40, num_updates=14200, lr=4.56988e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43456
2023-01-05 09:21:38 - progress_bar.py[line:274] - INFO: epoch 001:  14231 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=94.5, ups=0.88, wpb=107.8, bsz=40, num_updates=14210, lr=4.56943e-05, gnorm=0.493, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43468
2023-01-05 09:21:50 - progress_bar.py[line:274] - INFO: epoch 001:  14241 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=93.8, ups=0.87, wpb=107.3, bsz=40, num_updates=14220, lr=4.56898e-05, gnorm=0.463, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43479
2023-01-05 09:22:01 - progress_bar.py[line:274] - INFO: epoch 001:  14251 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.9, ups=0.9, wpb=109.4, bsz=40, num_updates=14230, lr=4.56853e-05, gnorm=0.557, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43490
2023-01-05 09:22:12 - progress_bar.py[line:274] - INFO: epoch 001:  14261 / 115845 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.4, ups=0.91, wpb=107.4, bsz=40, num_updates=14240, lr=4.56808e-05, gnorm=0.618, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43502
2023-01-05 09:22:23 - progress_bar.py[line:274] - INFO: epoch 001:  14271 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99, ups=0.91, wpb=109, bsz=40, num_updates=14250, lr=4.56763e-05, gnorm=0.543, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=43513
2023-01-05 09:22:34 - progress_bar.py[line:274] - INFO: epoch 001:  14281 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.89, wpb=108.8, bsz=40, num_updates=14260, lr=4.56718e-05, gnorm=0.535, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43524
2023-01-05 09:22:45 - progress_bar.py[line:274] - INFO: epoch 001:  14291 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.9, ups=0.91, wpb=107.8, bsz=40, num_updates=14270, lr=4.56673e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43535
2023-01-05 09:22:56 - progress_bar.py[line:274] - INFO: epoch 001:  14301 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.7, ups=0.93, wpb=109.4, bsz=40, num_updates=14280, lr=4.56628e-05, gnorm=0.564, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=43546
2023-01-05 09:23:07 - progress_bar.py[line:274] - INFO: epoch 001:  14311 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.1, ups=0.88, wpb=108.7, bsz=40, num_updates=14290, lr=4.56583e-05, gnorm=0.514, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43557
2023-01-05 09:23:18 - progress_bar.py[line:274] - INFO: epoch 001:  14321 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.2, ups=0.92, wpb=110.1, bsz=40, num_updates=14300, lr=4.56538e-05, gnorm=0.536, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43568
2023-01-05 09:23:29 - progress_bar.py[line:274] - INFO: epoch 001:  14331 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.4, ups=0.91, wpb=108.3, bsz=40, num_updates=14310, lr=4.56493e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43579
2023-01-05 09:23:41 - progress_bar.py[line:274] - INFO: epoch 001:  14341 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.88, wpb=108.9, bsz=40, num_updates=14320, lr=4.56448e-05, gnorm=0.416, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43591
2023-01-05 09:23:52 - progress_bar.py[line:274] - INFO: epoch 001:  14351 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.8, ups=0.92, wpb=108.9, bsz=40, num_updates=14330, lr=4.56403e-05, gnorm=0.527, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43601
2023-01-05 09:24:03 - progress_bar.py[line:274] - INFO: epoch 001:  14361 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.89, wpb=108.4, bsz=40, num_updates=14340, lr=4.56358e-05, gnorm=0.442, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43613
2023-01-05 09:24:14 - progress_bar.py[line:274] - INFO: epoch 001:  14371 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.1, ups=0.91, wpb=109, bsz=40, num_updates=14350, lr=4.56313e-05, gnorm=0.413, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43624
2023-01-05 09:24:25 - progress_bar.py[line:274] - INFO: epoch 001:  14381 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.2, ups=0.89, wpb=108.7, bsz=40, num_updates=14360, lr=4.56268e-05, gnorm=0.525, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43635
2023-01-05 09:24:36 - progress_bar.py[line:274] - INFO: epoch 001:  14391 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.7, ups=0.94, wpb=109.7, bsz=40, num_updates=14370, lr=4.56223e-05, gnorm=0.493, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43646
2023-01-05 09:24:47 - progress_bar.py[line:274] - INFO: epoch 001:  14401 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.92, wpb=108.4, bsz=40, num_updates=14380, lr=4.56178e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43657
2023-01-05 09:24:58 - progress_bar.py[line:274] - INFO: epoch 001:  14411 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.91, wpb=108.8, bsz=40, num_updates=14390, lr=4.56133e-05, gnorm=0.585, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43668
2023-01-05 09:25:09 - progress_bar.py[line:274] - INFO: epoch 001:  14421 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.9, ups=0.89, wpb=108.9, bsz=40, num_updates=14400, lr=4.56088e-05, gnorm=0.718, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43679
2023-01-05 09:25:21 - progress_bar.py[line:274] - INFO: epoch 001:  14431 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.3, ups=0.89, wpb=108.4, bsz=40, num_updates=14410, lr=4.56043e-05, gnorm=0.59, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43690
2023-01-05 09:25:32 - progress_bar.py[line:274] - INFO: epoch 001:  14441 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.7, ups=0.91, wpb=108.6, bsz=40, num_updates=14420, lr=4.55998e-05, gnorm=0.517, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=43701
2023-01-05 09:25:43 - progress_bar.py[line:274] - INFO: epoch 001:  14451 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.89, wpb=109.2, bsz=40, num_updates=14430, lr=4.55953e-05, gnorm=0.556, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43713
2023-01-05 09:25:54 - progress_bar.py[line:274] - INFO: epoch 001:  14461 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.9, wpb=109, bsz=40, num_updates=14440, lr=4.55909e-05, gnorm=0.544, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43724
2023-01-05 09:26:05 - progress_bar.py[line:274] - INFO: epoch 001:  14471 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.8, ups=0.89, wpb=108.2, bsz=40, num_updates=14450, lr=4.55864e-05, gnorm=0.513, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43735
2023-01-05 09:26:17 - progress_bar.py[line:274] - INFO: epoch 001:  14481 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.6, ups=0.87, wpb=109.5, bsz=40, num_updates=14460, lr=4.55819e-05, gnorm=0.664, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43747
2023-01-05 09:26:28 - progress_bar.py[line:274] - INFO: epoch 001:  14491 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=110, bsz=40, num_updates=14470, lr=4.55774e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43758
2023-01-05 09:26:39 - progress_bar.py[line:274] - INFO: epoch 001:  14501 / 115845 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.91, wpb=110.8, bsz=40, num_updates=14480, lr=4.55729e-05, gnorm=0.519, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43769
2023-01-05 09:26:50 - progress_bar.py[line:274] - INFO: epoch 001:  14511 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.7, ups=0.91, wpb=109.2, bsz=40, num_updates=14490, lr=4.55684e-05, gnorm=0.443, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43780
2023-01-05 09:27:01 - progress_bar.py[line:274] - INFO: epoch 001:  14521 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.2, ups=0.91, wpb=108.1, bsz=40, num_updates=14500, lr=4.55639e-05, gnorm=0.487, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43791
2023-01-05 09:27:12 - progress_bar.py[line:274] - INFO: epoch 001:  14531 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.7, ups=0.9, wpb=109.1, bsz=40, num_updates=14510, lr=4.55594e-05, gnorm=0.767, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43802
2023-01-05 09:27:23 - progress_bar.py[line:274] - INFO: epoch 001:  14541 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.91, wpb=108.7, bsz=40, num_updates=14520, lr=4.55549e-05, gnorm=0.692, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43813
2023-01-05 09:27:35 - progress_bar.py[line:274] - INFO: epoch 001:  14551 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.6, ups=0.91, wpb=109.7, bsz=40, num_updates=14530, lr=4.55504e-05, gnorm=0.658, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43824
2023-01-05 09:27:46 - progress_bar.py[line:274] - INFO: epoch 001:  14561 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.92, wpb=108.8, bsz=40, num_updates=14540, lr=4.55459e-05, gnorm=0.502, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43835
2023-01-05 09:27:57 - progress_bar.py[line:274] - INFO: epoch 001:  14571 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100, ups=0.91, wpb=110.2, bsz=40, num_updates=14550, lr=4.55414e-05, gnorm=0.412, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43846
2023-01-05 09:28:08 - progress_bar.py[line:274] - INFO: epoch 001:  14581 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.1, ups=0.92, wpb=109, bsz=40, num_updates=14560, lr=4.55369e-05, gnorm=0.518, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43857
2023-01-05 09:28:19 - progress_bar.py[line:274] - INFO: epoch 001:  14591 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.89, wpb=108.4, bsz=40, num_updates=14570, lr=4.55324e-05, gnorm=0.499, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43869
2023-01-05 09:28:30 - progress_bar.py[line:274] - INFO: epoch 001:  14601 / 115845 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.5, ups=0.91, wpb=109.9, bsz=40, num_updates=14580, lr=4.55279e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43880
2023-01-05 09:28:41 - progress_bar.py[line:274] - INFO: epoch 001:  14611 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.3, ups=0.89, wpb=108.8, bsz=40, num_updates=14590, lr=4.55234e-05, gnorm=0.551, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43891
2023-01-05 09:28:52 - progress_bar.py[line:274] - INFO: epoch 001:  14621 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=103, ups=0.94, wpb=110, bsz=40, num_updates=14600, lr=4.55189e-05, gnorm=0.42, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43902
2023-01-05 09:29:03 - progress_bar.py[line:274] - INFO: epoch 001:  14631 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99, ups=0.91, wpb=109, bsz=40, num_updates=14610, lr=4.55144e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43913
2023-01-05 09:29:14 - progress_bar.py[line:274] - INFO: epoch 001:  14641 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.9, ups=0.89, wpb=110, bsz=40, num_updates=14620, lr=4.55099e-05, gnorm=0.554, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43924
2023-01-05 09:29:26 - progress_bar.py[line:274] - INFO: epoch 001:  14651 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.3, ups=0.87, wpb=109.2, bsz=40, num_updates=14630, lr=4.55054e-05, gnorm=0.599, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43936
2023-01-05 09:29:37 - progress_bar.py[line:274] - INFO: epoch 001:  14661 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.3, ups=0.91, wpb=109.6, bsz=40, num_updates=14640, lr=4.55009e-05, gnorm=0.523, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=43947
2023-01-05 09:29:48 - progress_bar.py[line:274] - INFO: epoch 001:  14671 / 115845 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102, ups=0.93, wpb=109.8, bsz=40, num_updates=14650, lr=4.54964e-05, gnorm=0.496, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43958
2023-01-05 09:29:59 - progress_bar.py[line:274] - INFO: epoch 001:  14681 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.4, ups=0.92, wpb=109.3, bsz=40, num_updates=14660, lr=4.54919e-05, gnorm=0.41, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=43969
2023-01-05 09:30:10 - progress_bar.py[line:274] - INFO: epoch 001:  14691 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101, ups=0.92, wpb=109.7, bsz=40, num_updates=14670, lr=4.54874e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43979
2023-01-05 09:30:21 - progress_bar.py[line:274] - INFO: epoch 001:  14701 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.7, ups=0.88, wpb=108.8, bsz=40, num_updates=14680, lr=4.5483e-05, gnorm=0.679, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43991
2023-01-05 09:30:33 - progress_bar.py[line:274] - INFO: epoch 001:  14711 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.4, ups=0.89, wpb=108.6, bsz=40, num_updates=14690, lr=4.54785e-05, gnorm=0.702, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44002
2023-01-05 09:30:42 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 09:30:45 - progress_bar.py[line:274] - INFO: epoch 001:  14722 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=90.7, ups=0.83, wpb=109, bsz=40, num_updates=14700, lr=4.5474e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=44014
2023-01-05 09:30:56 - progress_bar.py[line:274] - INFO: epoch 001:  14732 / 115845 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.9, wpb=111.1, bsz=40, num_updates=14710, lr=4.54695e-05, gnorm=0.43, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44026
2023-01-05 09:31:07 - progress_bar.py[line:274] - INFO: epoch 001:  14742 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.4, ups=0.89, wpb=110.7, bsz=40, num_updates=14720, lr=4.5465e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44037
2023-01-05 09:31:19 - progress_bar.py[line:274] - INFO: epoch 001:  14752 / 115845 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.3, ups=0.87, wpb=109.2, bsz=40, num_updates=14730, lr=4.54605e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44048
2023-01-05 09:31:30 - progress_bar.py[line:274] - INFO: epoch 001:  14762 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.91, wpb=109, bsz=40, num_updates=14740, lr=4.5456e-05, gnorm=0.573, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44060
2023-01-05 09:31:41 - progress_bar.py[line:274] - INFO: epoch 001:  14772 / 115845 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.1, ups=0.91, wpb=111.3, bsz=40, num_updates=14750, lr=4.54515e-05, gnorm=0.447, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44071
2023-01-05 09:31:52 - progress_bar.py[line:274] - INFO: epoch 001:  14782 / 115845 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101, ups=0.92, wpb=109.9, bsz=40, num_updates=14760, lr=4.5447e-05, gnorm=0.502, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44082
2023-01-05 09:32:03 - progress_bar.py[line:274] - INFO: epoch 001:  14792 / 115845 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.8, ups=0.92, wpb=110.2, bsz=40, num_updates=14770, lr=4.54425e-05, gnorm=0.51, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44093
2023-01-05 09:32:14 - progress_bar.py[line:274] - INFO: epoch 001:  14802 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.89, wpb=109.3, bsz=40, num_updates=14780, lr=4.5438e-05, gnorm=0.577, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44104
2023-01-05 09:32:26 - progress_bar.py[line:274] - INFO: epoch 001:  14812 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.2, ups=0.88, wpb=108.8, bsz=40, num_updates=14790, lr=4.54335e-05, gnorm=0.692, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44115
2023-01-05 09:32:37 - progress_bar.py[line:274] - INFO: epoch 001:  14822 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.1, ups=0.93, wpb=110.3, bsz=40, num_updates=14800, lr=4.5429e-05, gnorm=0.424, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44126
2023-01-05 09:32:48 - progress_bar.py[line:274] - INFO: epoch 001:  14832 / 115845 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.9, wpb=109.5, bsz=40, num_updates=14810, lr=4.54245e-05, gnorm=0.557, clip=10, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=44138
2023-01-05 09:32:59 - progress_bar.py[line:274] - INFO: epoch 001:  14842 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99, ups=0.91, wpb=108.9, bsz=40, num_updates=14820, lr=4.542e-05, gnorm=0.495, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44149
2023-01-05 09:33:10 - progress_bar.py[line:274] - INFO: epoch 001:  14852 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.9, ups=0.9, wpb=108.1, bsz=40, num_updates=14830, lr=4.54155e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44160
2023-01-05 09:33:21 - progress_bar.py[line:274] - INFO: epoch 001:  14862 / 115845 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.6, ups=0.93, wpb=109, bsz=40, num_updates=14840, lr=4.5411e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44171
2023-01-05 09:33:32 - progress_bar.py[line:274] - INFO: epoch 001:  14872 / 115845 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.3, ups=0.91, wpb=108.3, bsz=40, num_updates=14850, lr=4.54065e-05, gnorm=0.721, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44182
2023-01-05 09:33:43 - progress_bar.py[line:274] - INFO: epoch 001:  14882 / 115845 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.2, ups=0.93, wpb=109.8, bsz=40, num_updates=14860, lr=4.5402e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44193
2023-01-05 09:33:54 - progress_bar.py[line:274] - INFO: epoch 001:  14892 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.7, ups=0.9, wpb=107.9, bsz=40, num_updates=14870, lr=4.53975e-05, gnorm=0.83, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44204
2023-01-05 09:34:05 - progress_bar.py[line:274] - INFO: epoch 001:  14902 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.89, wpb=108.8, bsz=40, num_updates=14880, lr=4.5393e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44215
2023-01-05 09:34:17 - progress_bar.py[line:274] - INFO: epoch 001:  14912 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.9, wpb=108.5, bsz=40, num_updates=14890, lr=4.53885e-05, gnorm=0.724, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44226
2023-01-05 09:34:28 - progress_bar.py[line:274] - INFO: epoch 001:  14922 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98, ups=0.9, wpb=109.4, bsz=40, num_updates=14900, lr=4.5384e-05, gnorm=0.504, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44238
2023-01-05 09:34:39 - progress_bar.py[line:274] - INFO: epoch 001:  14932 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.5, ups=0.88, wpb=108, bsz=40, num_updates=14910, lr=4.53795e-05, gnorm=0.54, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44249
2023-01-05 09:34:51 - progress_bar.py[line:274] - INFO: epoch 001:  14942 / 115845 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=14920, lr=4.5375e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44260
2023-01-05 09:35:02 - progress_bar.py[line:274] - INFO: epoch 001:  14952 / 115845 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99, ups=0.9, wpb=110.6, bsz=40, num_updates=14930, lr=4.53706e-05, gnorm=0.416, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44271
2023-01-05 09:35:13 - progress_bar.py[line:274] - INFO: epoch 001:  14962 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.5, ups=0.89, wpb=108.5, bsz=40, num_updates=14940, lr=4.53661e-05, gnorm=0.543, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44283
2023-01-05 09:35:24 - progress_bar.py[line:274] - INFO: epoch 001:  14972 / 115845 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.9, wpb=109.9, bsz=40, num_updates=14950, lr=4.53616e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44294
2023-01-05 09:35:35 - progress_bar.py[line:274] - INFO: epoch 001:  14982 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.2, ups=0.91, wpb=109.5, bsz=40, num_updates=14960, lr=4.53571e-05, gnorm=0.623, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44305
2023-01-05 09:35:46 - progress_bar.py[line:274] - INFO: epoch 001:  14992 / 115845 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=14970, lr=4.53526e-05, gnorm=0.531, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44316
2023-01-05 09:35:58 - progress_bar.py[line:274] - INFO: epoch 001:  15002 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.7, ups=0.9, wpb=107.9, bsz=40, num_updates=14980, lr=4.53481e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44327
2023-01-05 09:36:09 - progress_bar.py[line:274] - INFO: epoch 001:  15012 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.8, ups=0.92, wpb=109, bsz=40, num_updates=14990, lr=4.53436e-05, gnorm=0.485, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44338
2023-01-05 09:36:20 - progress_bar.py[line:274] - INFO: epoch 001:  15022 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=107.8, bsz=40, num_updates=15000, lr=4.53391e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44349
2023-01-05 09:36:31 - progress_bar.py[line:274] - INFO: epoch 001:  15032 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.9, wpb=109, bsz=40, num_updates=15010, lr=4.53346e-05, gnorm=0.591, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44361
2023-01-05 09:36:42 - progress_bar.py[line:274] - INFO: epoch 001:  15042 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.6, bsz=40, num_updates=15020, lr=4.53301e-05, gnorm=0.608, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44372
2023-01-05 09:36:54 - progress_bar.py[line:274] - INFO: epoch 001:  15052 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.89, wpb=109.6, bsz=40, num_updates=15030, lr=4.53256e-05, gnorm=0.521, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44383
2023-01-05 09:37:05 - progress_bar.py[line:274] - INFO: epoch 001:  15062 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=15040, lr=4.53211e-05, gnorm=0.488, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44395
2023-01-05 09:37:16 - progress_bar.py[line:274] - INFO: epoch 001:  15072 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.9, wpb=109.2, bsz=40, num_updates=15050, lr=4.53166e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44406
2023-01-05 09:37:27 - progress_bar.py[line:274] - INFO: epoch 001:  15082 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.9, wpb=108.6, bsz=40, num_updates=15060, lr=4.53121e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44417
2023-01-05 09:37:38 - progress_bar.py[line:274] - INFO: epoch 001:  15092 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.91, wpb=108.5, bsz=40, num_updates=15070, lr=4.53076e-05, gnorm=0.502, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44428
2023-01-05 09:37:50 - progress_bar.py[line:274] - INFO: epoch 001:  15102 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.8, ups=0.91, wpb=109, bsz=40, num_updates=15080, lr=4.53031e-05, gnorm=0.71, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44439
2023-01-05 09:38:01 - progress_bar.py[line:274] - INFO: epoch 001:  15112 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.91, wpb=107.6, bsz=40, num_updates=15090, lr=4.52986e-05, gnorm=0.674, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44450
2023-01-05 09:38:12 - progress_bar.py[line:274] - INFO: epoch 001:  15122 / 115845 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.8, ups=0.92, wpb=109.6, bsz=40, num_updates=15100, lr=4.52941e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44461
2023-01-05 09:38:23 - progress_bar.py[line:274] - INFO: epoch 001:  15132 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.7, ups=0.91, wpb=109.1, bsz=40, num_updates=15110, lr=4.52896e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44472
2023-01-05 09:38:34 - progress_bar.py[line:274] - INFO: epoch 001:  15142 / 115845 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.2, ups=0.92, wpb=109.3, bsz=40, num_updates=15120, lr=4.52851e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44484
2023-01-05 09:38:45 - progress_bar.py[line:274] - INFO: epoch 001:  15152 / 115845 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.1, ups=0.9, wpb=108.4, bsz=40, num_updates=15130, lr=4.52806e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44495
2023-01-05 09:38:56 - progress_bar.py[line:274] - INFO: epoch 001:  15162 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.88, wpb=109.8, bsz=40, num_updates=15140, lr=4.52761e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44506
2023-01-05 09:39:08 - progress_bar.py[line:274] - INFO: epoch 001:  15172 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=15150, lr=4.52716e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44517
2023-01-05 09:39:19 - progress_bar.py[line:274] - INFO: epoch 001:  15182 / 115845 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.8, ups=0.91, wpb=108.9, bsz=40, num_updates=15160, lr=4.52671e-05, gnorm=0.524, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44529
2023-01-05 09:39:30 - progress_bar.py[line:274] - INFO: epoch 001:  15192 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.6, ups=0.89, wpb=107, bsz=40, num_updates=15170, lr=4.52627e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44540
2023-01-05 09:39:42 - progress_bar.py[line:274] - INFO: epoch 001:  15202 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.88, wpb=110.1, bsz=40, num_updates=15180, lr=4.52582e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44551
2023-01-05 09:39:53 - progress_bar.py[line:274] - INFO: epoch 001:  15212 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.9, wpb=109.6, bsz=40, num_updates=15190, lr=4.52537e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44562
2023-01-05 09:40:04 - progress_bar.py[line:274] - INFO: epoch 001:  15222 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.92, wpb=109.3, bsz=40, num_updates=15200, lr=4.52492e-05, gnorm=0.515, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44573
2023-01-05 09:40:15 - progress_bar.py[line:274] - INFO: epoch 001:  15232 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.1, ups=0.9, wpb=109, bsz=40, num_updates=15210, lr=4.52447e-05, gnorm=0.535, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44585
2023-01-05 09:40:27 - progress_bar.py[line:274] - INFO: epoch 001:  15242 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.6, ups=0.88, wpb=108.5, bsz=40, num_updates=15220, lr=4.52402e-05, gnorm=0.466, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44596
2023-01-05 09:40:38 - progress_bar.py[line:274] - INFO: epoch 001:  15252 / 115845 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.2, ups=0.9, wpb=110.2, bsz=40, num_updates=15230, lr=4.52357e-05, gnorm=0.457, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44607
2023-01-05 09:40:49 - progress_bar.py[line:274] - INFO: epoch 001:  15262 / 115845 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.3, ups=0.91, wpb=109.6, bsz=40, num_updates=15240, lr=4.52312e-05, gnorm=0.412, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44618
2023-01-05 09:41:00 - progress_bar.py[line:274] - INFO: epoch 001:  15272 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.92, wpb=108.3, bsz=40, num_updates=15250, lr=4.52267e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44630
2023-01-05 09:41:11 - progress_bar.py[line:274] - INFO: epoch 001:  15282 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.9, ups=0.92, wpb=110.1, bsz=40, num_updates=15260, lr=4.52222e-05, gnorm=0.59, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44641
2023-01-05 09:41:22 - progress_bar.py[line:274] - INFO: epoch 001:  15292 / 115845 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.8, ups=0.92, wpb=109.4, bsz=40, num_updates=15270, lr=4.52177e-05, gnorm=0.562, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44652
2023-01-05 09:41:33 - progress_bar.py[line:274] - INFO: epoch 001:  15302 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.2, ups=0.89, wpb=108.4, bsz=40, num_updates=15280, lr=4.52132e-05, gnorm=0.504, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44663
2023-01-05 09:41:45 - progress_bar.py[line:274] - INFO: epoch 001:  15312 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.9, wpb=109.1, bsz=40, num_updates=15290, lr=4.52087e-05, gnorm=0.514, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44674
2023-01-05 09:41:56 - progress_bar.py[line:274] - INFO: epoch 001:  15322 / 115845 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.2, ups=0.87, wpb=109, bsz=40, num_updates=15300, lr=4.52042e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44686
2023-01-05 09:42:07 - progress_bar.py[line:274] - INFO: epoch 001:  15332 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.3, ups=0.9, wpb=109.2, bsz=40, num_updates=15310, lr=4.51997e-05, gnorm=0.589, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44697
2023-01-05 09:42:19 - progress_bar.py[line:274] - INFO: epoch 001:  15342 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.3, ups=0.87, wpb=109.4, bsz=40, num_updates=15320, lr=4.51952e-05, gnorm=0.549, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44709
2023-01-05 09:42:30 - progress_bar.py[line:274] - INFO: epoch 001:  15352 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.91, wpb=110.2, bsz=40, num_updates=15330, lr=4.51907e-05, gnorm=0.604, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44720
2023-01-05 09:42:42 - progress_bar.py[line:274] - INFO: epoch 001:  15362 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.88, wpb=109.4, bsz=40, num_updates=15340, lr=4.51862e-05, gnorm=0.493, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44731
2023-01-05 09:42:53 - progress_bar.py[line:274] - INFO: epoch 001:  15372 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.2, ups=0.89, wpb=108.6, bsz=40, num_updates=15350, lr=4.51817e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44743
2023-01-05 09:43:04 - progress_bar.py[line:274] - INFO: epoch 001:  15382 / 115845 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.2, ups=0.89, wpb=109.6, bsz=40, num_updates=15360, lr=4.51772e-05, gnorm=0.506, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44754
2023-01-05 09:43:16 - progress_bar.py[line:274] - INFO: epoch 001:  15392 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=15370, lr=4.51727e-05, gnorm=0.573, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44765
2023-01-05 09:43:27 - progress_bar.py[line:274] - INFO: epoch 001:  15402 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=102, ups=0.94, wpb=108.7, bsz=40, num_updates=15380, lr=4.51682e-05, gnorm=0.484, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44776
2023-01-05 09:43:38 - progress_bar.py[line:274] - INFO: epoch 001:  15412 / 115845 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.1, ups=0.92, wpb=109, bsz=40, num_updates=15390, lr=4.51637e-05, gnorm=0.534, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44787
2023-01-05 09:43:49 - progress_bar.py[line:274] - INFO: epoch 001:  15422 / 115845 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.1, ups=0.92, wpb=107.7, bsz=40, num_updates=15400, lr=4.51592e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44798
2023-01-05 09:44:00 - progress_bar.py[line:274] - INFO: epoch 001:  15432 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.9, wpb=108.4, bsz=40, num_updates=15410, lr=4.51547e-05, gnorm=0.573, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44809
2023-01-05 09:44:11 - progress_bar.py[line:274] - INFO: epoch 001:  15442 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.7, ups=0.9, wpb=109.1, bsz=40, num_updates=15420, lr=4.51503e-05, gnorm=0.566, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44821
2023-01-05 09:44:23 - progress_bar.py[line:274] - INFO: epoch 001:  15452 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.88, wpb=110.7, bsz=40, num_updates=15430, lr=4.51458e-05, gnorm=0.613, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44832
2023-01-05 09:44:34 - progress_bar.py[line:274] - INFO: epoch 001:  15462 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100, ups=0.93, wpb=107.2, bsz=40, num_updates=15440, lr=4.51413e-05, gnorm=0.733, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44843
2023-01-05 09:44:45 - progress_bar.py[line:274] - INFO: epoch 001:  15472 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.9, ups=0.9, wpb=108.2, bsz=40, num_updates=15450, lr=4.51368e-05, gnorm=0.665, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44855
2023-01-05 09:44:56 - progress_bar.py[line:274] - INFO: epoch 001:  15482 / 115845 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100, ups=0.92, wpb=108.5, bsz=40, num_updates=15460, lr=4.51323e-05, gnorm=0.505, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44866
2023-01-05 09:45:07 - progress_bar.py[line:274] - INFO: epoch 001:  15492 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.92, wpb=108.1, bsz=40, num_updates=15470, lr=4.51278e-05, gnorm=0.791, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44877
2023-01-05 09:45:18 - progress_bar.py[line:274] - INFO: epoch 001:  15502 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=109.8, bsz=40, num_updates=15480, lr=4.51233e-05, gnorm=0.465, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44888
2023-01-05 09:45:30 - progress_bar.py[line:274] - INFO: epoch 001:  15512 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.88, wpb=110, bsz=40, num_updates=15490, lr=4.51188e-05, gnorm=0.574, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44899
2023-01-05 09:45:41 - progress_bar.py[line:274] - INFO: epoch 001:  15522 / 115845 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.7, ups=0.88, wpb=108.2, bsz=40, num_updates=15500, lr=4.51143e-05, gnorm=0.481, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=44911
2023-01-05 09:45:52 - progress_bar.py[line:274] - INFO: epoch 001:  15532 / 115845 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.7, ups=0.91, wpb=111.1, bsz=40, num_updates=15510, lr=4.51098e-05, gnorm=0.403, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44922
2023-01-05 09:46:04 - progress_bar.py[line:274] - INFO: epoch 001:  15542 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.4, ups=0.88, wpb=109.4, bsz=40, num_updates=15520, lr=4.51053e-05, gnorm=0.534, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44933
2023-01-05 09:46:17 - progress_bar.py[line:274] - INFO: epoch 001:  15552 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=85, ups=0.78, wpb=109.3, bsz=40, num_updates=15530, lr=4.51008e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=13, gb_free=10.8, ema_decay=0.9999, wall=44946
2023-01-05 09:46:32 - progress_bar.py[line:274] - INFO: epoch 001:  15562 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=72.1, ups=0.66, wpb=109, bsz=40, num_updates=15540, lr=4.50963e-05, gnorm=0.505, clip=10, loss_scale=1024, train_wall=15, gb_free=10.7, ema_decay=0.9999, wall=44961
2023-01-05 09:46:43 - progress_bar.py[line:274] - INFO: epoch 001:  15572 / 115845 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.5, ups=0.9, wpb=110, bsz=40, num_updates=15550, lr=4.50918e-05, gnorm=0.474, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44973
2023-01-05 09:46:54 - progress_bar.py[line:274] - INFO: epoch 001:  15582 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.93, wpb=107.7, bsz=40, num_updates=15560, lr=4.50873e-05, gnorm=0.382, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44984
2023-01-05 09:47:05 - progress_bar.py[line:274] - INFO: epoch 001:  15592 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.9, wpb=109.8, bsz=40, num_updates=15570, lr=4.50828e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44995
2023-01-05 09:47:16 - progress_bar.py[line:274] - INFO: epoch 001:  15602 / 115845 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.8, ups=0.89, wpb=109.2, bsz=40, num_updates=15580, lr=4.50783e-05, gnorm=0.465, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45006
2023-01-05 09:47:28 - progress_bar.py[line:274] - INFO: epoch 001:  15612 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.89, wpb=108.5, bsz=40, num_updates=15590, lr=4.50738e-05, gnorm=0.503, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45017
2023-01-05 09:47:39 - progress_bar.py[line:274] - INFO: epoch 001:  15622 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.91, wpb=108.6, bsz=40, num_updates=15600, lr=4.50693e-05, gnorm=0.499, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45029
2023-01-05 09:47:50 - progress_bar.py[line:274] - INFO: epoch 001:  15632 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.89, wpb=109.2, bsz=40, num_updates=15610, lr=4.50648e-05, gnorm=0.384, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45040
2023-01-05 09:48:01 - progress_bar.py[line:274] - INFO: epoch 001:  15642 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.8, ups=0.89, wpb=108.5, bsz=40, num_updates=15620, lr=4.50603e-05, gnorm=0.498, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45051
2023-01-05 09:48:13 - progress_bar.py[line:274] - INFO: epoch 001:  15652 / 115845 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99, ups=0.89, wpb=111, bsz=40, num_updates=15630, lr=4.50558e-05, gnorm=0.422, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45062
2023-01-05 09:48:24 - progress_bar.py[line:274] - INFO: epoch 001:  15662 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.7, ups=0.89, wpb=109.5, bsz=40, num_updates=15640, lr=4.50513e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=45074
2023-01-05 09:48:35 - progress_bar.py[line:274] - INFO: epoch 001:  15672 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.92, wpb=109.4, bsz=40, num_updates=15650, lr=4.50468e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45085
2023-01-05 09:48:46 - progress_bar.py[line:274] - INFO: epoch 001:  15682 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.3, ups=0.9, wpb=109.7, bsz=40, num_updates=15660, lr=4.50424e-05, gnorm=0.539, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45096
2023-01-05 09:48:58 - progress_bar.py[line:274] - INFO: epoch 001:  15692 / 115845 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.6, ups=0.88, wpb=111.8, bsz=40, num_updates=15670, lr=4.50379e-05, gnorm=0.667, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45107
2023-01-05 09:49:09 - progress_bar.py[line:274] - INFO: epoch 001:  15702 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.88, wpb=110.1, bsz=40, num_updates=15680, lr=4.50334e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45119
2023-01-05 09:49:20 - progress_bar.py[line:274] - INFO: epoch 001:  15712 / 115845 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.4, ups=0.91, wpb=109.3, bsz=40, num_updates=15690, lr=4.50289e-05, gnorm=0.635, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45130
2023-01-05 09:49:28 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 09:49:33 - progress_bar.py[line:274] - INFO: epoch 001:  15723 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=87.7, ups=0.81, wpb=108, bsz=40, num_updates=15700, lr=4.50244e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=45142
2023-01-05 09:49:44 - progress_bar.py[line:274] - INFO: epoch 001:  15733 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.7, ups=0.88, wpb=108.2, bsz=40, num_updates=15710, lr=4.50199e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45154
2023-01-05 09:49:55 - progress_bar.py[line:274] - INFO: epoch 001:  15743 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.4, ups=0.92, wpb=109.4, bsz=40, num_updates=15720, lr=4.50154e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45165
2023-01-05 09:50:07 - progress_bar.py[line:274] - INFO: epoch 001:  15753 / 115845 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.9, ups=0.88, wpb=108.6, bsz=40, num_updates=15730, lr=4.50109e-05, gnorm=0.635, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45176
2023-01-05 09:50:18 - progress_bar.py[line:274] - INFO: epoch 001:  15763 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.88, wpb=108.9, bsz=40, num_updates=15740, lr=4.50064e-05, gnorm=0.551, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45188
2023-01-05 09:50:29 - progress_bar.py[line:274] - INFO: epoch 001:  15773 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.1, ups=0.92, wpb=108.9, bsz=40, num_updates=15750, lr=4.50019e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45199
2023-01-05 09:50:40 - progress_bar.py[line:274] - INFO: epoch 001:  15783 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.91, wpb=109.5, bsz=40, num_updates=15760, lr=4.49974e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45210
2023-01-05 09:50:52 - progress_bar.py[line:274] - INFO: epoch 001:  15793 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.6, ups=0.88, wpb=108.8, bsz=40, num_updates=15770, lr=4.49929e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45221
2023-01-05 09:51:03 - progress_bar.py[line:274] - INFO: epoch 001:  15803 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.2, ups=0.91, wpb=108.4, bsz=40, num_updates=15780, lr=4.49884e-05, gnorm=0.448, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45232
2023-01-05 09:51:14 - progress_bar.py[line:274] - INFO: epoch 001:  15813 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.89, wpb=110.3, bsz=40, num_updates=15790, lr=4.49839e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45244
2023-01-05 09:51:25 - progress_bar.py[line:274] - INFO: epoch 001:  15823 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.4, ups=0.9, wpb=109.5, bsz=40, num_updates=15800, lr=4.49794e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45255
2023-01-05 09:51:36 - progress_bar.py[line:274] - INFO: epoch 001:  15833 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.1, ups=0.9, wpb=109.6, bsz=40, num_updates=15810, lr=4.49749e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45266
2023-01-05 09:51:48 - progress_bar.py[line:274] - INFO: epoch 001:  15843 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.4, ups=0.88, wpb=108.2, bsz=40, num_updates=15820, lr=4.49704e-05, gnorm=0.557, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45278
2023-01-05 09:51:59 - progress_bar.py[line:274] - INFO: epoch 001:  15853 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.91, wpb=110.2, bsz=40, num_updates=15830, lr=4.49659e-05, gnorm=0.444, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45289
2023-01-05 09:52:10 - progress_bar.py[line:274] - INFO: epoch 001:  15863 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.7, ups=0.94, wpb=109.5, bsz=40, num_updates=15840, lr=4.49614e-05, gnorm=0.416, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45299
2023-01-05 09:52:21 - progress_bar.py[line:274] - INFO: epoch 001:  15873 / 115845 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.91, wpb=109.5, bsz=40, num_updates=15850, lr=4.49569e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45310
2023-01-05 09:52:32 - progress_bar.py[line:274] - INFO: epoch 001:  15883 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.2, ups=0.91, wpb=109.3, bsz=40, num_updates=15860, lr=4.49524e-05, gnorm=0.521, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45322
2023-01-05 09:52:43 - progress_bar.py[line:274] - INFO: epoch 001:  15893 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.3, ups=0.92, wpb=109.1, bsz=40, num_updates=15870, lr=4.49479e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45333
2023-01-05 09:52:54 - progress_bar.py[line:274] - INFO: epoch 001:  15903 / 115845 loss=0.443, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.1, ups=0.88, wpb=107.6, bsz=40, num_updates=15880, lr=4.49434e-05, gnorm=0.569, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45344
2023-01-05 09:53:06 - progress_bar.py[line:274] - INFO: epoch 001:  15913 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.8, ups=0.9, wpb=108.7, bsz=40, num_updates=15890, lr=4.49389e-05, gnorm=0.382, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45355
2023-01-05 09:53:17 - progress_bar.py[line:274] - INFO: epoch 001:  15923 / 115845 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98.6, ups=0.91, wpb=107.9, bsz=40, num_updates=15900, lr=4.49344e-05, gnorm=0.475, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45366
2023-01-05 09:53:28 - progress_bar.py[line:274] - INFO: epoch 001:  15933 / 115845 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=95.7, ups=0.88, wpb=109, bsz=40, num_updates=15910, lr=4.493e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45378
2023-01-05 09:53:40 - progress_bar.py[line:274] - INFO: epoch 001:  15943 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.9, ups=0.87, wpb=110.6, bsz=40, num_updates=15920, lr=4.49255e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45389
2023-01-05 09:53:51 - progress_bar.py[line:274] - INFO: epoch 001:  15953 / 115845 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.3, ups=0.89, wpb=108.8, bsz=40, num_updates=15930, lr=4.4921e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45401
2023-01-05 09:54:02 - progress_bar.py[line:274] - INFO: epoch 001:  15963 / 115845 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.1, ups=0.93, wpb=108.6, bsz=40, num_updates=15940, lr=4.49165e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45411
2023-01-05 09:54:13 - progress_bar.py[line:274] - INFO: epoch 001:  15973 / 115845 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.2, ups=0.92, wpb=107.9, bsz=40, num_updates=15950, lr=4.4912e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45422
2023-01-05 09:54:24 - progress_bar.py[line:274] - INFO: epoch 001:  15983 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.2, ups=0.92, wpb=108, bsz=40, num_updates=15960, lr=4.49075e-05, gnorm=0.535, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45433
2023-01-05 09:54:35 - progress_bar.py[line:274] - INFO: epoch 001:  15993 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=15970, lr=4.4903e-05, gnorm=0.485, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45445
2023-01-05 09:54:46 - progress_bar.py[line:274] - INFO: epoch 001:  16003 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.92, wpb=109.2, bsz=40, num_updates=15980, lr=4.48985e-05, gnorm=0.539, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45456
2023-01-05 09:54:57 - progress_bar.py[line:274] - INFO: epoch 001:  16013 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.9, ups=0.9, wpb=109.7, bsz=40, num_updates=15990, lr=4.4894e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45467
2023-01-05 09:55:09 - progress_bar.py[line:274] - INFO: epoch 001:  16023 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.2, ups=0.89, wpb=109.5, bsz=40, num_updates=16000, lr=4.48895e-05, gnorm=0.394, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45479
2023-01-05 09:55:09 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 09:55:10 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 09:55:10 - train.py[line:551] - INFO: load:1.11 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 09:57:42 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 09:57:42 - train.py[line:551] - INFO: load:1.13 valid_run:152.16 task_valid:148.02 collect_output:3.04
2023-01-05 10:00:11 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 10:00:11 - train.py[line:551] - INFO: load:1.15 valid_run:300.64 task_valid:291.10 collect_output:7.42
2023-01-05 10:02:44 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 10:02:44 - train.py[line:551] - INFO: load:1.18 valid_run:453.70 task_valid:434.29 collect_output:16.25
2023-01-05 10:05:13 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 10:05:13 - train.py[line:551] - INFO: load:1.20 valid_run:602.71 task_valid:579.42 collect_output:19.03
2023-01-05 10:07:46 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 10:07:46 - train.py[line:551] - INFO: load:1.23 valid_run:754.90 task_valid:726.95 collect_output:22.66
2023-01-05 10:10:17 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 10:10:17 - train.py[line:551] - INFO: load:1.25 valid_run:906.58 task_valid:872.58 collect_output:27.66
2023-01-05 10:12:51 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 10:12:51 - train.py[line:551] - INFO: load:1.28 valid_run:1059.86 task_valid:1018.86 collect_output:33.61
2023-01-05 10:15:22 - train.py[line:549] - INFO: 1600 / 4988
2023-01-05 10:15:22 - train.py[line:551] - INFO: load:1.30 valid_run:1210.83 task_valid:1160.10 collect_output:42.28
2023-01-05 10:17:51 - train.py[line:549] - INFO: 1800 / 4988
2023-01-05 10:17:51 - train.py[line:551] - INFO: load:1.32 valid_run:1360.24 task_valid:1304.77 collect_output:45.97
2023-01-05 10:20:20 - train.py[line:549] - INFO: 2000 / 4988
2023-01-05 10:20:20 - train.py[line:551] - INFO: load:1.35 valid_run:1508.71 task_valid:1447.90 collect_output:50.26
2023-01-05 10:22:50 - train.py[line:549] - INFO: 2200 / 4988
2023-01-05 10:22:50 - train.py[line:551] - INFO: load:1.37 valid_run:1658.40 task_valid:1593.04 collect_output:53.77
2023-01-05 10:25:20 - train.py[line:549] - INFO: 2400 / 4988
2023-01-05 10:25:20 - train.py[line:551] - INFO: load:1.40 valid_run:1808.24 task_valid:1738.17 collect_output:57.40
2023-01-05 10:27:50 - train.py[line:549] - INFO: 2600 / 4988
2023-01-05 10:27:50 - train.py[line:551] - INFO: load:1.42 valid_run:1958.08 task_valid:1880.05 collect_output:64.31
2023-01-05 10:30:20 - train.py[line:549] - INFO: 2800 / 4988
2023-01-05 10:30:20 - train.py[line:551] - INFO: load:1.45 valid_run:2108.76 task_valid:2025.79 collect_output:68.19
2023-01-05 10:32:51 - train.py[line:549] - INFO: 3000 / 4988
2023-01-05 10:32:51 - train.py[line:551] - INFO: load:1.47 valid_run:2259.09 task_valid:2172.58 collect_output:70.66
2023-01-05 10:35:21 - train.py[line:549] - INFO: 3200 / 4988
2023-01-05 10:35:21 - train.py[line:551] - INFO: load:1.50 valid_run:2409.09 task_valid:2316.89 collect_output:75.29
2023-01-05 10:37:53 - train.py[line:549] - INFO: 3400 / 4988
2023-01-05 10:37:53 - train.py[line:551] - INFO: load:1.53 valid_run:2560.59 task_valid:2462.46 collect_output:80.17
2023-01-05 10:40:23 - train.py[line:549] - INFO: 3600 / 4988
2023-01-05 10:40:23 - train.py[line:551] - INFO: load:1.55 valid_run:2711.30 task_valid:2609.69 collect_output:82.54
2023-01-05 10:42:52 - train.py[line:549] - INFO: 3800 / 4988
2023-01-05 10:42:52 - train.py[line:551] - INFO: load:1.58 valid_run:2859.76 task_valid:2751.66 collect_output:87.92
2023-01-05 10:45:22 - train.py[line:549] - INFO: 4000 / 4988
2023-01-05 10:45:22 - train.py[line:551] - INFO: load:1.60 valid_run:3010.00 task_valid:2897.02 collect_output:91.72
2023-01-05 10:47:54 - train.py[line:549] - INFO: 4200 / 4988
2023-01-05 10:47:54 - train.py[line:551] - INFO: load:1.63 valid_run:3161.83 task_valid:3041.82 collect_output:97.68
2023-01-05 10:50:24 - train.py[line:549] - INFO: 4400 / 4988
2023-01-05 10:50:24 - train.py[line:551] - INFO: load:1.66 valid_run:3311.50 task_valid:3186.68 collect_output:101.40
2023-01-05 10:52:55 - train.py[line:549] - INFO: 4600 / 4988
2023-01-05 10:52:55 - train.py[line:551] - INFO: load:1.68 valid_run:3462.82 task_valid:3333.34 collect_output:104.99
2023-01-05 10:55:27 - train.py[line:549] - INFO: 4800 / 4988
2023-01-05 10:55:27 - train.py[line:551] - INFO: load:1.71 valid_run:3614.35 task_valid:3480.19 collect_output:108.59

====================================================================================================
SGG eval:     R @ 50: 0.3621;     R @ 100: 0.4467;     R @ 500: 0.4907;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2410;    mR @ 100: 0.3049;    mR @ 500: 0.3532;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5439) (covered in:0.8125) (covering:0.3714) (eating:0.5882) (flying in:0.0000) (growing on:0.1250) (hanging from:0.4032) (lying on:0.0000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.6979) (playing:0.0000) (riding:0.6340) (says:0.0000) (sitting on:0.5210) (standing on:0.2433) (using:0.3500) (walking in:0.0000) (walking on:0.2523) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-05 10:57:58 - train.py[line:487] - INFO: 0.4467428571428571
2023-01-05 10:57:58 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.3621;     R @ 100: 0.4467;     R @ 500: 0.4907;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2410;    mR @ 100: 0.3049;    mR @ 500: 0.3532;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5439) (covered in:0.8125) (covering:0.3714) (eating:0.5882) (flying in:0.0000) (growing on:0.1250) (hanging from:0.4032) (lying on:0.0000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.6979) (playing:0.0000) (riding:0.6340) (says:0.0000) (sitting on:0.5210) (standing on:0.2433) (using:0.3500) (walking in:0.0000) (walking on:0.2523) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-05 10:57:58 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.38 | loss_v1 0 | loss_v2 0 | nll_loss 0.23 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.446743 | ppl 1.17 | vqa_score 0.3964 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 16000 | best_R@100 0.602033
2023-01-05 10:57:58 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 16000 updates
2023-01-05 10:57:58 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-05 10:58:52 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-05 11:00:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/re_run_test_visualDS_20way/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt (epoch 1 @ 16000 updates, score 0.4467428571428571) (writing took 165.3900125697255 seconds)
2023-01-05 11:00:55 - progress_bar.py[line:274] - INFO: epoch 001:  16033 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=0.3, ups=0, wpb=109.3, bsz=40, num_updates=16010, lr=4.4885e-05, gnorm=0.39, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49425
2023-01-05 11:01:07 - progress_bar.py[line:274] - INFO: epoch 001:  16043 / 115845 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96, ups=0.87, wpb=110.1, bsz=40, num_updates=16020, lr=4.48805e-05, gnorm=0.475, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49437
2023-01-05 11:01:18 - progress_bar.py[line:274] - INFO: epoch 001:  16053 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.88, wpb=109.5, bsz=40, num_updates=16030, lr=4.4876e-05, gnorm=0.487, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49448
2023-01-05 11:01:30 - progress_bar.py[line:274] - INFO: epoch 001:  16063 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.1, ups=0.89, wpb=109.7, bsz=40, num_updates=16040, lr=4.48715e-05, gnorm=0.494, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49459
2023-01-05 11:01:41 - progress_bar.py[line:274] - INFO: epoch 001:  16073 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.3, ups=0.88, wpb=108.1, bsz=40, num_updates=16050, lr=4.4867e-05, gnorm=0.566, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49471
2023-01-05 11:01:52 - progress_bar.py[line:274] - INFO: epoch 001:  16083 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.9, ups=0.92, wpb=109.6, bsz=40, num_updates=16060, lr=4.48625e-05, gnorm=0.438, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49482
2023-01-05 11:02:03 - progress_bar.py[line:274] - INFO: epoch 001:  16093 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.1, ups=0.9, wpb=108.2, bsz=40, num_updates=16070, lr=4.4858e-05, gnorm=0.537, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49493
2023-01-05 11:02:14 - progress_bar.py[line:274] - INFO: epoch 001:  16103 / 115845 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.6, ups=0.9, wpb=109.1, bsz=40, num_updates=16080, lr=4.48535e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49504
2023-01-05 11:02:26 - progress_bar.py[line:274] - INFO: epoch 001:  16113 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.9, ups=0.89, wpb=109.1, bsz=40, num_updates=16090, lr=4.4849e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49515
2023-01-05 11:02:37 - progress_bar.py[line:274] - INFO: epoch 001:  16123 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.92, wpb=108.2, bsz=40, num_updates=16100, lr=4.48445e-05, gnorm=0.584, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=49526
2023-01-05 11:02:48 - progress_bar.py[line:274] - INFO: epoch 001:  16133 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.1, ups=0.88, wpb=110, bsz=40, num_updates=16110, lr=4.484e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49538
2023-01-05 11:02:59 - progress_bar.py[line:274] - INFO: epoch 001:  16143 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.1, ups=0.91, wpb=107.5, bsz=40, num_updates=16120, lr=4.48355e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49549
2023-01-05 11:03:10 - progress_bar.py[line:274] - INFO: epoch 001:  16153 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.6, ups=0.94, wpb=109.4, bsz=40, num_updates=16130, lr=4.4831e-05, gnorm=0.549, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49560
2023-01-05 11:03:21 - progress_bar.py[line:274] - INFO: epoch 001:  16163 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.8, ups=0.92, wpb=108.8, bsz=40, num_updates=16140, lr=4.48265e-05, gnorm=0.605, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49571
2023-01-05 11:03:33 - progress_bar.py[line:274] - INFO: epoch 001:  16173 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=94.3, ups=0.87, wpb=108.4, bsz=40, num_updates=16150, lr=4.48221e-05, gnorm=0.54, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49582
2023-01-05 11:03:44 - progress_bar.py[line:274] - INFO: epoch 001:  16183 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.4, ups=0.92, wpb=110.4, bsz=40, num_updates=16160, lr=4.48176e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49593
2023-01-05 11:03:55 - progress_bar.py[line:274] - INFO: epoch 001:  16193 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.9, wpb=108, bsz=40, num_updates=16170, lr=4.48131e-05, gnorm=0.487, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49604
2023-01-05 11:04:06 - progress_bar.py[line:274] - INFO: epoch 001:  16203 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.6, ups=0.92, wpb=109.3, bsz=40, num_updates=16180, lr=4.48086e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49615
2023-01-05 11:04:17 - progress_bar.py[line:274] - INFO: epoch 001:  16213 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=107.1, nsentences=40, sample_size=107.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=94.6, ups=0.88, wpb=107.1, bsz=40, num_updates=16190, lr=4.48041e-05, gnorm=0.626, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49627
2023-01-05 11:04:28 - progress_bar.py[line:274] - INFO: epoch 001:  16223 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.89, wpb=108.4, bsz=40, num_updates=16200, lr=4.47996e-05, gnorm=0.502, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49638
2023-01-05 11:04:40 - progress_bar.py[line:274] - INFO: epoch 001:  16233 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.88, wpb=110.6, bsz=40, num_updates=16210, lr=4.47951e-05, gnorm=0.589, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49649
2023-01-05 11:04:51 - progress_bar.py[line:274] - INFO: epoch 001:  16243 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.7, ups=0.92, wpb=107.6, bsz=40, num_updates=16220, lr=4.47906e-05, gnorm=0.464, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49660
2023-01-05 11:05:02 - progress_bar.py[line:274] - INFO: epoch 001:  16253 / 115845 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=98, ups=0.91, wpb=107.2, bsz=40, num_updates=16230, lr=4.47861e-05, gnorm=0.696, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49671
2023-01-05 11:05:13 - progress_bar.py[line:274] - INFO: epoch 001:  16263 / 115845 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.9, ups=0.92, wpb=108.9, bsz=40, num_updates=16240, lr=4.47816e-05, gnorm=0.385, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49682
2023-01-05 11:05:24 - progress_bar.py[line:274] - INFO: epoch 001:  16273 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.3, ups=0.89, wpb=108.9, bsz=40, num_updates=16250, lr=4.47771e-05, gnorm=0.446, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49694
2023-01-05 11:05:35 - progress_bar.py[line:274] - INFO: epoch 001:  16283 / 115845 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.5, ups=0.88, wpb=109.2, bsz=40, num_updates=16260, lr=4.47726e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49705
2023-01-05 11:05:47 - progress_bar.py[line:274] - INFO: epoch 001:  16293 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.91, wpb=109.2, bsz=40, num_updates=16270, lr=4.47681e-05, gnorm=0.506, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49716
2023-01-05 11:05:57 - progress_bar.py[line:274] - INFO: epoch 001:  16303 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.92, wpb=108.5, bsz=40, num_updates=16280, lr=4.47636e-05, gnorm=0.577, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49727
2023-01-05 11:06:09 - progress_bar.py[line:274] - INFO: epoch 001:  16313 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98, ups=0.89, wpb=109.6, bsz=40, num_updates=16290, lr=4.47591e-05, gnorm=0.398, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49738
2023-01-05 11:06:20 - progress_bar.py[line:274] - INFO: epoch 001:  16323 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.8, ups=0.9, wpb=108.1, bsz=40, num_updates=16300, lr=4.47546e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49750
2023-01-05 11:06:32 - progress_bar.py[line:274] - INFO: epoch 001:  16333 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.9, nsentences=40, sample_size=106.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=92.9, ups=0.87, wpb=106.9, bsz=40, num_updates=16310, lr=4.47501e-05, gnorm=0.528, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49761
2023-01-05 11:06:44 - progress_bar.py[line:274] - INFO: epoch 001:  16343 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=92.7, ups=0.86, wpb=107.9, bsz=40, num_updates=16320, lr=4.47456e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=49774
2023-01-05 11:06:55 - progress_bar.py[line:274] - INFO: epoch 001:  16353 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.7, ups=0.9, wpb=110.2, bsz=40, num_updates=16330, lr=4.47411e-05, gnorm=0.714, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49785
2023-01-05 11:07:07 - progress_bar.py[line:274] - INFO: epoch 001:  16363 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.9, wpb=109.6, bsz=40, num_updates=16340, lr=4.47366e-05, gnorm=0.48, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49796
2023-01-05 11:07:18 - progress_bar.py[line:274] - INFO: epoch 001:  16373 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.3, ups=0.89, wpb=108.9, bsz=40, num_updates=16350, lr=4.47321e-05, gnorm=0.426, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49807
2023-01-05 11:07:19 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 11:07:30 - progress_bar.py[line:274] - INFO: epoch 001:  16384 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=90.7, ups=0.83, wpb=109.2, bsz=40, num_updates=16360, lr=4.47276e-05, gnorm=0.453, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=49820
2023-01-05 11:07:41 - progress_bar.py[line:274] - INFO: epoch 001:  16394 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.4, ups=0.89, wpb=109.3, bsz=40, num_updates=16370, lr=4.47231e-05, gnorm=0.46, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49831
2023-01-05 11:07:52 - progress_bar.py[line:274] - INFO: epoch 001:  16404 / 115845 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.1, ups=0.89, wpb=108.6, bsz=40, num_updates=16380, lr=4.47186e-05, gnorm=0.473, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49842
2023-01-05 11:08:04 - progress_bar.py[line:274] - INFO: epoch 001:  16414 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.7, ups=0.89, wpb=108.1, bsz=40, num_updates=16390, lr=4.47141e-05, gnorm=0.473, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49853
2023-01-05 11:08:15 - progress_bar.py[line:274] - INFO: epoch 001:  16424 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.3, ups=0.9, wpb=109.8, bsz=40, num_updates=16400, lr=4.47097e-05, gnorm=0.399, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49865
2023-01-05 11:08:26 - progress_bar.py[line:274] - INFO: epoch 001:  16434 / 115845 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=16410, lr=4.47052e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49876
2023-01-05 11:08:37 - progress_bar.py[line:274] - INFO: epoch 001:  16444 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.6, ups=0.91, wpb=110, bsz=40, num_updates=16420, lr=4.47007e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49887
2023-01-05 11:08:49 - progress_bar.py[line:274] - INFO: epoch 001:  16454 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.7, ups=0.88, wpb=108.2, bsz=40, num_updates=16430, lr=4.46962e-05, gnorm=0.556, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49898
2023-01-05 11:09:00 - progress_bar.py[line:274] - INFO: epoch 001:  16464 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.91, wpb=109.4, bsz=40, num_updates=16440, lr=4.46917e-05, gnorm=0.397, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49909
2023-01-05 11:09:11 - progress_bar.py[line:274] - INFO: epoch 001:  16474 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.8, ups=0.91, wpb=110.2, bsz=40, num_updates=16450, lr=4.46872e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49921
2023-01-05 11:09:22 - progress_bar.py[line:274] - INFO: epoch 001:  16484 / 115845 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.7, ups=0.93, wpb=109.4, bsz=40, num_updates=16460, lr=4.46827e-05, gnorm=0.396, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49931
2023-01-05 11:09:33 - progress_bar.py[line:274] - INFO: epoch 001:  16494 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.5, ups=0.9, wpb=108.2, bsz=40, num_updates=16470, lr=4.46782e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49943
2023-01-05 11:09:44 - progress_bar.py[line:274] - INFO: epoch 001:  16504 / 115845 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.7, ups=0.92, wpb=109.8, bsz=40, num_updates=16480, lr=4.46737e-05, gnorm=0.417, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49954
2023-01-05 11:09:55 - progress_bar.py[line:274] - INFO: epoch 001:  16514 / 115845 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.9, ups=0.92, wpb=110.8, bsz=40, num_updates=16490, lr=4.46692e-05, gnorm=0.364, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49964
2023-01-05 11:10:06 - progress_bar.py[line:274] - INFO: epoch 001:  16524 / 115845 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=96.9, ups=0.88, wpb=109.8, bsz=40, num_updates=16500, lr=4.46647e-05, gnorm=0.442, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49976
2023-01-05 11:10:18 - progress_bar.py[line:274] - INFO: epoch 001:  16534 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.5, ups=0.88, wpb=108.6, bsz=40, num_updates=16510, lr=4.46602e-05, gnorm=0.468, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49987
2023-01-05 11:10:29 - progress_bar.py[line:274] - INFO: epoch 001:  16544 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.7, ups=0.9, wpb=109.3, bsz=40, num_updates=16520, lr=4.46557e-05, gnorm=0.584, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49999
2023-01-05 11:10:40 - progress_bar.py[line:274] - INFO: epoch 001:  16554 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=16530, lr=4.46512e-05, gnorm=0.476, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50009
2023-01-05 11:10:51 - progress_bar.py[line:274] - INFO: epoch 001:  16564 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.1, ups=0.88, wpb=109.6, bsz=40, num_updates=16540, lr=4.46467e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50021
2023-01-05 11:11:03 - progress_bar.py[line:274] - INFO: epoch 001:  16574 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.89, wpb=108.7, bsz=40, num_updates=16550, lr=4.46422e-05, gnorm=0.572, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50032
2023-01-05 11:11:13 - progress_bar.py[line:274] - INFO: epoch 001:  16584 / 115845 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=102.6, ups=0.93, wpb=110, bsz=40, num_updates=16560, lr=4.46377e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50043
2023-01-05 11:11:24 - progress_bar.py[line:274] - INFO: epoch 001:  16594 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.6, ups=0.92, wpb=108.6, bsz=40, num_updates=16570, lr=4.46332e-05, gnorm=0.697, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50054
2023-01-05 11:11:36 - progress_bar.py[line:274] - INFO: epoch 001:  16604 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.4, ups=0.88, wpb=109.2, bsz=40, num_updates=16580, lr=4.46287e-05, gnorm=0.7, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50065
2023-01-05 11:11:47 - progress_bar.py[line:274] - INFO: epoch 001:  16614 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.9, ups=0.89, wpb=108.5, bsz=40, num_updates=16590, lr=4.46242e-05, gnorm=0.43, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50077
2023-01-05 11:11:58 - progress_bar.py[line:274] - INFO: epoch 001:  16624 / 115845 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.9, ups=0.89, wpb=109.7, bsz=40, num_updates=16600, lr=4.46197e-05, gnorm=0.534, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=50088
2023-01-05 11:12:09 - progress_bar.py[line:274] - INFO: epoch 001:  16634 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99, ups=0.91, wpb=109.4, bsz=40, num_updates=16610, lr=4.46152e-05, gnorm=0.346, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50099
2023-01-05 11:12:21 - progress_bar.py[line:274] - INFO: epoch 001:  16644 / 115845 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=94.8, ups=0.87, wpb=109, bsz=40, num_updates=16620, lr=4.46107e-05, gnorm=0.396, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50111
2023-01-05 11:12:32 - progress_bar.py[line:274] - INFO: epoch 001:  16654 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.8, ups=0.91, wpb=109.7, bsz=40, num_updates=16630, lr=4.46062e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50122
2023-01-05 11:12:43 - progress_bar.py[line:274] - INFO: epoch 001:  16664 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.5, ups=0.88, wpb=109.1, bsz=40, num_updates=16640, lr=4.46018e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50133
2023-01-05 11:12:54 - progress_bar.py[line:274] - INFO: epoch 001:  16674 / 115845 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=103.1, ups=0.95, wpb=108.5, bsz=40, num_updates=16650, lr=4.45973e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=50144
2023-01-05 11:13:05 - progress_bar.py[line:274] - INFO: epoch 001:  16684 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.5, ups=0.89, wpb=107.7, bsz=40, num_updates=16660, lr=4.45928e-05, gnorm=0.503, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50155
2023-01-05 11:13:17 - progress_bar.py[line:274] - INFO: epoch 001:  16694 / 115845 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.9, ups=0.88, wpb=108.7, bsz=40, num_updates=16670, lr=4.45883e-05, gnorm=0.387, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50167
2023-01-05 11:13:29 - progress_bar.py[line:274] - INFO: epoch 001:  16704 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=103.2, ups=0.94, wpb=110.2, bsz=40, num_updates=16680, lr=4.45838e-05, gnorm=0.559, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50178
2023-01-05 11:13:40 - progress_bar.py[line:274] - INFO: epoch 001:  16714 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.4, ups=0.91, wpb=109.8, bsz=40, num_updates=16690, lr=4.45793e-05, gnorm=0.457, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50189
2023-01-05 11:13:51 - progress_bar.py[line:274] - INFO: epoch 001:  16724 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.9, wpb=108.2, bsz=40, num_updates=16700, lr=4.45748e-05, gnorm=0.607, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50201
2023-01-05 11:14:02 - progress_bar.py[line:274] - INFO: epoch 001:  16734 / 115845 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.2, ups=0.9, wpb=110.4, bsz=40, num_updates=16710, lr=4.45703e-05, gnorm=0.429, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50212
2023-01-05 11:14:14 - progress_bar.py[line:274] - INFO: epoch 001:  16744 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.4, ups=0.87, wpb=108.7, bsz=40, num_updates=16720, lr=4.45658e-05, gnorm=0.575, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50223
2023-01-05 11:14:25 - progress_bar.py[line:274] - INFO: epoch 001:  16754 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.5, ups=0.9, wpb=107.7, bsz=40, num_updates=16730, lr=4.45613e-05, gnorm=0.54, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50235
2023-01-05 11:14:36 - progress_bar.py[line:274] - INFO: epoch 001:  16764 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.8, ups=0.9, wpb=110.2, bsz=40, num_updates=16740, lr=4.45568e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50246
2023-01-05 11:14:47 - progress_bar.py[line:274] - INFO: epoch 001:  16774 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.4, ups=0.89, wpb=109.4, bsz=40, num_updates=16750, lr=4.45523e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50257
2023-01-05 11:14:59 - progress_bar.py[line:274] - INFO: epoch 001:  16784 / 115845 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=16760, lr=4.45478e-05, gnorm=0.391, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=50268
2023-01-05 11:15:10 - progress_bar.py[line:274] - INFO: epoch 001:  16794 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.8, ups=0.88, wpb=109.7, bsz=40, num_updates=16770, lr=4.45433e-05, gnorm=0.471, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50280
2023-01-05 11:15:21 - progress_bar.py[line:274] - INFO: epoch 001:  16804 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.9, wpb=109, bsz=40, num_updates=16780, lr=4.45388e-05, gnorm=0.415, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50291
2023-01-05 11:15:33 - progress_bar.py[line:274] - INFO: epoch 001:  16814 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.9, ups=0.89, wpb=109.6, bsz=40, num_updates=16790, lr=4.45343e-05, gnorm=0.641, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50302
2023-01-05 11:15:44 - progress_bar.py[line:274] - INFO: epoch 001:  16824 / 115845 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.3, ups=0.89, wpb=109.9, bsz=40, num_updates=16800, lr=4.45298e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50314
2023-01-05 11:15:55 - progress_bar.py[line:274] - INFO: epoch 001:  16834 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.9, wpb=110.2, bsz=40, num_updates=16810, lr=4.45253e-05, gnorm=0.508, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50325
2023-01-05 11:16:06 - progress_bar.py[line:274] - INFO: epoch 001:  16844 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.2, ups=0.9, wpb=108.6, bsz=40, num_updates=16820, lr=4.45208e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50336
2023-01-05 11:16:17 - progress_bar.py[line:274] - INFO: epoch 001:  16854 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=107.9, nsentences=40, sample_size=107.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.92, wpb=107.9, bsz=40, num_updates=16830, lr=4.45163e-05, gnorm=0.573, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50347
2023-01-05 11:16:28 - progress_bar.py[line:274] - INFO: epoch 001:  16864 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.2, ups=0.89, wpb=108.8, bsz=40, num_updates=16840, lr=4.45118e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50358
2023-01-05 11:16:39 - progress_bar.py[line:274] - INFO: epoch 001:  16874 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=103.8, ups=0.94, wpb=110.4, bsz=40, num_updates=16850, lr=4.45073e-05, gnorm=0.683, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50369
2023-01-05 11:16:50 - progress_bar.py[line:274] - INFO: epoch 001:  16884 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.6, ups=0.92, wpb=109.6, bsz=40, num_updates=16860, lr=4.45028e-05, gnorm=0.507, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50380
2023-01-05 11:17:01 - progress_bar.py[line:274] - INFO: epoch 001:  16894 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.1, ups=0.92, wpb=110.2, bsz=40, num_updates=16870, lr=4.44983e-05, gnorm=0.543, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=50391
2023-01-05 11:17:12 - progress_bar.py[line:274] - INFO: epoch 001:  16904 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.5, ups=0.91, wpb=108.7, bsz=40, num_updates=16880, lr=4.44938e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50402
2023-01-05 11:17:23 - progress_bar.py[line:274] - INFO: epoch 001:  16914 / 115845 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.2, ups=0.92, wpb=110.5, bsz=40, num_updates=16890, lr=4.44894e-05, gnorm=0.43, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50413
2023-01-05 11:17:34 - progress_bar.py[line:274] - INFO: epoch 001:  16924 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.4, ups=0.9, wpb=109, bsz=40, num_updates=16900, lr=4.44849e-05, gnorm=0.428, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=50424
2023-01-05 11:17:46 - progress_bar.py[line:274] - INFO: epoch 001:  16934 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97, ups=0.88, wpb=109.9, bsz=40, num_updates=16910, lr=4.44804e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50435
2023-01-05 11:17:57 - progress_bar.py[line:274] - INFO: epoch 001:  16944 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.1, ups=0.92, wpb=109.1, bsz=40, num_updates=16920, lr=4.44759e-05, gnorm=0.523, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50446
2023-01-05 11:18:08 - progress_bar.py[line:274] - INFO: epoch 001:  16954 / 115845 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.3, ups=0.92, wpb=110.6, bsz=40, num_updates=16930, lr=4.44714e-05, gnorm=0.577, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50457
2023-01-05 11:18:19 - progress_bar.py[line:274] - INFO: epoch 001:  16964 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.1, ups=0.92, wpb=107.8, bsz=40, num_updates=16940, lr=4.44669e-05, gnorm=0.592, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50468
2023-01-05 11:18:30 - progress_bar.py[line:274] - INFO: epoch 001:  16974 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=98.4, ups=0.91, wpb=108.3, bsz=40, num_updates=16950, lr=4.44624e-05, gnorm=0.526, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50479
2023-01-05 11:18:38 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-05 11:18:42 - progress_bar.py[line:274] - INFO: epoch 001:  16985 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=91.1, ups=0.82, wpb=110.6, bsz=40, num_updates=16960, lr=4.44579e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=50492
2023-01-05 11:18:53 - progress_bar.py[line:274] - INFO: epoch 001:  16995 / 115845 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.8, ups=0.91, wpb=107.8, bsz=40, num_updates=16970, lr=4.44534e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50503
2023-01-05 11:19:04 - progress_bar.py[line:274] - INFO: epoch 001:  17005 / 115845 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.5, ups=0.91, wpb=110.2, bsz=40, num_updates=16980, lr=4.44489e-05, gnorm=0.4, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50514
2023-01-05 11:19:15 - progress_bar.py[line:274] - INFO: epoch 001:  17015 / 115845 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.6, ups=0.91, wpb=108.9, bsz=40, num_updates=16990, lr=4.44444e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50525
2023-01-05 11:19:27 - progress_bar.py[line:274] - INFO: epoch 001:  17025 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.7, ups=0.89, wpb=109.9, bsz=40, num_updates=17000, lr=4.44399e-05, gnorm=0.424, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50536
2023-01-05 11:19:37 - progress_bar.py[line:274] - INFO: epoch 001:  17035 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=103, ups=0.95, wpb=108.4, bsz=40, num_updates=17010, lr=4.44354e-05, gnorm=0.449, clip=10, loss_scale=512, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=50547
2023-01-05 11:19:48 - progress_bar.py[line:274] - INFO: epoch 001:  17045 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.5, ups=0.89, wpb=108.1, bsz=40, num_updates=17020, lr=4.44309e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50558
2023-01-05 11:20:00 - progress_bar.py[line:274] - INFO: epoch 001:  17055 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=95.9, ups=0.88, wpb=108.4, bsz=40, num_updates=17030, lr=4.44264e-05, gnorm=0.436, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50569
2023-01-05 11:20:11 - progress_bar.py[line:274] - INFO: epoch 001:  17065 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.3, nsentences=40, sample_size=108.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.9, ups=0.9, wpb=108.3, bsz=40, num_updates=17040, lr=4.44219e-05, gnorm=0.554, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50581
2023-01-05 11:20:22 - progress_bar.py[line:274] - INFO: epoch 001:  17075 / 115845 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98.9, ups=0.9, wpb=110.3, bsz=40, num_updates=17050, lr=4.44174e-05, gnorm=0.487, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50592
2023-01-05 11:20:33 - progress_bar.py[line:274] - INFO: epoch 001:  17085 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.91, wpb=108.5, bsz=40, num_updates=17060, lr=4.44129e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50603
2023-01-05 11:20:45 - progress_bar.py[line:274] - INFO: epoch 001:  17095 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=94.7, ups=0.87, wpb=108.6, bsz=40, num_updates=17070, lr=4.44084e-05, gnorm=0.426, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50615
2023-01-05 11:20:56 - progress_bar.py[line:274] - INFO: epoch 001:  17105 / 115845 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=101.6, ups=0.93, wpb=109, bsz=40, num_updates=17080, lr=4.44039e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50625
2023-01-05 11:21:07 - progress_bar.py[line:274] - INFO: epoch 001:  17115 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.89, wpb=109.3, bsz=40, num_updates=17090, lr=4.43994e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50637
2023-01-05 11:21:18 - progress_bar.py[line:274] - INFO: epoch 001:  17125 / 115845 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=110, bsz=40, num_updates=17100, lr=4.43949e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50648
2023-01-05 11:21:29 - progress_bar.py[line:274] - INFO: epoch 001:  17135 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.4, ups=0.91, wpb=109.7, bsz=40, num_updates=17110, lr=4.43904e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50659
2023-01-05 11:21:41 - progress_bar.py[line:274] - INFO: epoch 001:  17145 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.9, ups=0.91, wpb=109, bsz=40, num_updates=17120, lr=4.43859e-05, gnorm=0.402, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50670
2023-01-05 11:21:51 - progress_bar.py[line:274] - INFO: epoch 001:  17155 / 115845 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.2, ups=0.92, wpb=110.3, bsz=40, num_updates=17130, lr=4.43815e-05, gnorm=0.373, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50681
2023-01-05 11:22:03 - progress_bar.py[line:274] - INFO: epoch 001:  17165 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.9, wpb=110.7, bsz=40, num_updates=17140, lr=4.4377e-05, gnorm=0.604, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50692
2023-01-05 11:22:14 - progress_bar.py[line:274] - INFO: epoch 001:  17175 / 115845 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=97.2, ups=0.88, wpb=110.1, bsz=40, num_updates=17150, lr=4.43725e-05, gnorm=0.487, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50704
2023-01-05 11:22:25 - progress_bar.py[line:274] - INFO: epoch 001:  17185 / 115845 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.1, ups=0.9, wpb=109.6, bsz=40, num_updates=17160, lr=4.4368e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50715
2023-01-05 11:22:36 - progress_bar.py[line:274] - INFO: epoch 001:  17195 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.2, ups=0.92, wpb=109.6, bsz=40, num_updates=17170, lr=4.43635e-05, gnorm=0.498, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50726
2023-01-05 11:22:47 - progress_bar.py[line:274] - INFO: epoch 001:  17205 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100.7, ups=0.93, wpb=108.2, bsz=40, num_updates=17180, lr=4.4359e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50737
2023-01-05 11:22:58 - progress_bar.py[line:274] - INFO: epoch 001:  17215 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=101, ups=0.92, wpb=110, bsz=40, num_updates=17190, lr=4.43545e-05, gnorm=0.482, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50748
2023-01-05 11:23:10 - progress_bar.py[line:274] - INFO: epoch 001:  17225 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.3, ups=0.88, wpb=109.2, bsz=40, num_updates=17200, lr=4.435e-05, gnorm=0.533, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50759
2023-01-05 11:23:21 - progress_bar.py[line:274] - INFO: epoch 001:  17235 / 115845 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.7, ups=0.89, wpb=109, bsz=40, num_updates=17210, lr=4.43455e-05, gnorm=0.601, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50771
2023-01-05 11:23:32 - progress_bar.py[line:274] - INFO: epoch 001:  17245 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.89, wpb=108.2, bsz=40, num_updates=17220, lr=4.4341e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50782
2023-01-05 11:23:43 - progress_bar.py[line:274] - INFO: epoch 001:  17255 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.6, ups=0.89, wpb=108, bsz=40, num_updates=17230, lr=4.43365e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50793
2023-01-05 11:23:55 - progress_bar.py[line:274] - INFO: epoch 001:  17265 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.9, ups=0.89, wpb=108.7, bsz=40, num_updates=17240, lr=4.4332e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50804
2023-01-05 11:24:06 - progress_bar.py[line:274] - INFO: epoch 001:  17275 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.9, ups=0.91, wpb=109.8, bsz=40, num_updates=17250, lr=4.43275e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50815
2023-01-05 11:24:17 - progress_bar.py[line:274] - INFO: epoch 001:  17285 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.91, wpb=110.1, bsz=40, num_updates=17260, lr=4.4323e-05, gnorm=0.484, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50827
2023-01-05 11:24:28 - progress_bar.py[line:274] - INFO: epoch 001:  17295 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.7, ups=0.89, wpb=109.2, bsz=40, num_updates=17270, lr=4.43185e-05, gnorm=0.505, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50838
2023-01-05 11:24:39 - progress_bar.py[line:274] - INFO: epoch 001:  17305 / 115845 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=106.6, nsentences=40, sample_size=106.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99, ups=0.93, wpb=106.6, bsz=40, num_updates=17280, lr=4.4314e-05, gnorm=0.558, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50849
2023-01-05 11:24:50 - progress_bar.py[line:274] - INFO: epoch 001:  17315 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.4, ups=0.92, wpb=108.1, bsz=40, num_updates=17290, lr=4.43095e-05, gnorm=0.392, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50860
2023-01-05 11:25:01 - progress_bar.py[line:274] - INFO: epoch 001:  17325 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=108.7, bsz=40, num_updates=17300, lr=4.4305e-05, gnorm=0.445, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50871
2023-01-05 11:25:12 - progress_bar.py[line:274] - INFO: epoch 001:  17335 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.8, ups=0.94, wpb=109.4, bsz=40, num_updates=17310, lr=4.43005e-05, gnorm=0.478, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50882
2023-01-05 11:25:24 - progress_bar.py[line:274] - INFO: epoch 001:  17345 / 115845 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.5, ups=0.88, wpb=110.4, bsz=40, num_updates=17320, lr=4.4296e-05, gnorm=0.41, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50893
2023-01-05 11:25:35 - progress_bar.py[line:274] - INFO: epoch 001:  17355 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.5, nsentences=40, sample_size=107.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.6, ups=0.88, wpb=107.5, bsz=40, num_updates=17330, lr=4.42915e-05, gnorm=0.479, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50905
2023-01-05 11:25:41 - trainer.py[line:1002] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-05 11:25:47 - progress_bar.py[line:274] - INFO: epoch 001:  17366 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=88.7, ups=0.81, wpb=109.3, bsz=40, num_updates=17340, lr=4.4287e-05, gnorm=0.453, clip=10, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=50917
2023-01-05 11:25:59 - progress_bar.py[line:274] - INFO: epoch 001:  17376 / 115845 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.7, ups=0.88, wpb=109.1, bsz=40, num_updates=17350, lr=4.42825e-05, gnorm=0.479, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50929
2023-01-05 11:26:11 - progress_bar.py[line:274] - INFO: epoch 001:  17386 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.4, ups=0.87, wpb=109.7, bsz=40, num_updates=17360, lr=4.4278e-05, gnorm=0.495, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50940
2023-01-05 11:26:22 - progress_bar.py[line:274] - INFO: epoch 001:  17396 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.5, ups=0.88, wpb=109.8, bsz=40, num_updates=17370, lr=4.42735e-05, gnorm=0.553, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50952
2023-01-05 11:26:33 - progress_bar.py[line:274] - INFO: epoch 001:  17406 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.89, wpb=108.6, bsz=40, num_updates=17380, lr=4.42691e-05, gnorm=0.419, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50963
2023-01-05 11:26:45 - progress_bar.py[line:274] - INFO: epoch 001:  17416 / 115845 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.2, ups=0.89, wpb=110.3, bsz=40, num_updates=17390, lr=4.42646e-05, gnorm=0.469, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50974
2023-01-05 11:26:56 - progress_bar.py[line:274] - INFO: epoch 001:  17426 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.5, ups=0.89, wpb=109.7, bsz=40, num_updates=17400, lr=4.42601e-05, gnorm=0.466, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50986
2023-01-05 11:27:07 - progress_bar.py[line:274] - INFO: epoch 001:  17436 / 115845 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=102.5, ups=0.94, wpb=109.6, bsz=40, num_updates=17410, lr=4.42556e-05, gnorm=0.419, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50996
2023-01-05 11:27:18 - progress_bar.py[line:274] - INFO: epoch 001:  17446 / 115845 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=17420, lr=4.42511e-05, gnorm=0.364, clip=0, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=51008
2023-01-05 11:27:29 - progress_bar.py[line:274] - INFO: epoch 001:  17456 / 115845 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.4, ups=0.9, wpb=109.9, bsz=40, num_updates=17430, lr=4.42466e-05, gnorm=0.435, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51019
2023-01-05 11:27:40 - progress_bar.py[line:274] - INFO: epoch 001:  17466 / 115845 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.7, ups=0.92, wpb=109.3, bsz=40, num_updates=17440, lr=4.42421e-05, gnorm=0.502, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51030
2023-01-05 11:27:51 - progress_bar.py[line:274] - INFO: epoch 001:  17476 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.7, ups=0.91, wpb=107.4, bsz=40, num_updates=17450, lr=4.42376e-05, gnorm=0.569, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51041
2023-01-05 11:28:02 - progress_bar.py[line:274] - INFO: epoch 001:  17486 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.2, ups=0.93, wpb=107.2, bsz=40, num_updates=17460, lr=4.42331e-05, gnorm=0.414, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51052
2023-01-05 11:28:13 - progress_bar.py[line:274] - INFO: epoch 001:  17496 / 115845 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97, ups=0.89, wpb=108.4, bsz=40, num_updates=17470, lr=4.42286e-05, gnorm=0.505, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51063
2023-01-05 11:28:25 - progress_bar.py[line:274] - INFO: epoch 001:  17506 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.6, ups=0.89, wpb=110.2, bsz=40, num_updates=17480, lr=4.42241e-05, gnorm=0.523, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51074
2023-01-05 11:28:35 - progress_bar.py[line:274] - INFO: epoch 001:  17516 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=102.3, ups=0.94, wpb=108.8, bsz=40, num_updates=17490, lr=4.42196e-05, gnorm=0.442, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51085
2023-01-05 11:28:47 - progress_bar.py[line:274] - INFO: epoch 001:  17526 / 115845 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=108.7, nsentences=40, sample_size=108.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.7, ups=0.9, wpb=108.7, bsz=40, num_updates=17500, lr=4.42151e-05, gnorm=0.409, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51096
2023-01-05 11:28:58 - progress_bar.py[line:274] - INFO: epoch 001:  17536 / 115845 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.89, wpb=109.2, bsz=40, num_updates=17510, lr=4.42106e-05, gnorm=0.405, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51107
2023-01-05 11:29:09 - progress_bar.py[line:274] - INFO: epoch 001:  17546 / 115845 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.88, wpb=109.6, bsz=40, num_updates=17520, lr=4.42061e-05, gnorm=0.535, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51119
2023-01-05 11:29:21 - progress_bar.py[line:274] - INFO: epoch 001:  17556 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.89, wpb=108, bsz=40, num_updates=17530, lr=4.42016e-05, gnorm=0.357, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51130
2023-01-05 11:29:32 - progress_bar.py[line:274] - INFO: epoch 001:  17566 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.7, ups=0.88, wpb=109.8, bsz=40, num_updates=17540, lr=4.41971e-05, gnorm=0.425, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51142
2023-01-05 11:29:43 - progress_bar.py[line:274] - INFO: epoch 001:  17576 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=17550, lr=4.41926e-05, gnorm=0.546, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51153
2023-01-05 11:29:55 - progress_bar.py[line:274] - INFO: epoch 001:  17586 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=95.5, ups=0.88, wpb=108.5, bsz=40, num_updates=17560, lr=4.41881e-05, gnorm=0.441, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51164
2023-01-05 11:30:06 - progress_bar.py[line:274] - INFO: epoch 001:  17596 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.3, nsentences=40, sample_size=107.3, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.5, ups=0.9, wpb=107.3, bsz=40, num_updates=17570, lr=4.41836e-05, gnorm=0.422, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51175
2023-01-05 11:30:17 - progress_bar.py[line:274] - INFO: epoch 001:  17606 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=17580, lr=4.41791e-05, gnorm=0.496, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51187
2023-01-05 11:30:28 - progress_bar.py[line:274] - INFO: epoch 001:  17616 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.5, ups=0.89, wpb=108.1, bsz=40, num_updates=17590, lr=4.41746e-05, gnorm=0.411, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51198
2023-01-05 11:30:39 - progress_bar.py[line:274] - INFO: epoch 001:  17626 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100, ups=0.92, wpb=109.1, bsz=40, num_updates=17600, lr=4.41701e-05, gnorm=0.632, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51209
2023-01-05 11:30:51 - progress_bar.py[line:274] - INFO: epoch 001:  17636 / 115845 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.5, ups=0.88, wpb=110, bsz=40, num_updates=17610, lr=4.41656e-05, gnorm=0.544, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51220
2023-01-05 11:31:02 - progress_bar.py[line:274] - INFO: epoch 001:  17646 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97, ups=0.9, wpb=108, bsz=40, num_updates=17620, lr=4.41612e-05, gnorm=0.524, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51232
2023-01-05 11:31:13 - progress_bar.py[line:274] - INFO: epoch 001:  17656 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96, ups=0.88, wpb=108.5, bsz=40, num_updates=17630, lr=4.41567e-05, gnorm=0.431, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51243
2023-01-05 11:31:25 - progress_bar.py[line:274] - INFO: epoch 001:  17666 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.6, nsentences=40, sample_size=106.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=94.5, ups=0.89, wpb=106.6, bsz=40, num_updates=17640, lr=4.41522e-05, gnorm=0.388, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51254
2023-01-05 11:31:36 - progress_bar.py[line:274] - INFO: epoch 001:  17676 / 115845 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=17650, lr=4.41477e-05, gnorm=0.441, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51265
2023-01-05 11:31:47 - progress_bar.py[line:274] - INFO: epoch 001:  17686 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.6, ups=0.89, wpb=108.9, bsz=40, num_updates=17660, lr=4.41432e-05, gnorm=0.541, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51277
2023-01-05 11:31:58 - progress_bar.py[line:274] - INFO: epoch 001:  17696 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.3, ups=0.9, wpb=107.6, bsz=40, num_updates=17670, lr=4.41387e-05, gnorm=0.463, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51288
2023-01-05 11:32:10 - progress_bar.py[line:274] - INFO: epoch 001:  17706 / 115845 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=96.5, ups=0.87, wpb=110.4, bsz=40, num_updates=17680, lr=4.41342e-05, gnorm=0.394, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=51299
2023-01-05 11:32:20 - progress_bar.py[line:274] - INFO: epoch 001:  17716 / 115845 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103.9, ups=0.94, wpb=110.1, bsz=40, num_updates=17690, lr=4.41297e-05, gnorm=0.54, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51310
2023-01-05 11:32:31 - progress_bar.py[line:274] - INFO: epoch 001:  17726 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=106.6, nsentences=40, sample_size=106.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.2, ups=0.93, wpb=106.6, bsz=40, num_updates=17700, lr=4.41252e-05, gnorm=0.447, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51321
2023-01-05 11:32:42 - progress_bar.py[line:274] - INFO: epoch 001:  17736 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=108.9, nsentences=40, sample_size=108.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.91, wpb=108.9, bsz=40, num_updates=17710, lr=4.41207e-05, gnorm=0.467, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51332
2023-01-05 11:32:53 - progress_bar.py[line:274] - INFO: epoch 001:  17746 / 115845 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102, ups=0.93, wpb=109.8, bsz=40, num_updates=17720, lr=4.41162e-05, gnorm=0.466, clip=0, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51343
2023-01-05 11:33:04 - progress_bar.py[line:274] - INFO: epoch 001:  17756 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.3, ups=0.91, wpb=108.5, bsz=40, num_updates=17730, lr=4.41117e-05, gnorm=0.591, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51354
2023-01-05 11:33:15 - progress_bar.py[line:274] - INFO: epoch 001:  17766 / 115845 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.8, ups=0.9, wpb=108.5, bsz=40, num_updates=17740, lr=4.41072e-05, gnorm=0.504, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51365
2023-01-05 11:33:27 - progress_bar.py[line:274] - INFO: epoch 001:  17776 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.4, ups=0.87, wpb=110.7, bsz=40, num_updates=17750, lr=4.41027e-05, gnorm=0.509, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51377
2023-01-05 11:33:38 - progress_bar.py[line:274] - INFO: epoch 001:  17786 / 115845 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=107.7, nsentences=40, sample_size=107.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=96.4, ups=0.89, wpb=107.7, bsz=40, num_updates=17760, lr=4.40982e-05, gnorm=0.496, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51388
2023-01-05 11:33:49 - progress_bar.py[line:274] - INFO: epoch 001:  17796 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.8, ups=0.89, wpb=110.4, bsz=40, num_updates=17770, lr=4.40937e-05, gnorm=0.526, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51399
2023-01-05 11:34:01 - progress_bar.py[line:274] - INFO: epoch 001:  17806 / 115845 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.7, ups=0.91, wpb=110.6, bsz=40, num_updates=17780, lr=4.40892e-05, gnorm=0.545, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51410
2023-01-05 11:34:12 - progress_bar.py[line:274] - INFO: epoch 001:  17816 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=96.8, ups=0.89, wpb=109.1, bsz=40, num_updates=17790, lr=4.40847e-05, gnorm=0.395, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51422
2023-01-05 11:34:23 - progress_bar.py[line:274] - INFO: epoch 001:  17826 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.6, ups=0.91, wpb=110, bsz=40, num_updates=17800, lr=4.40802e-05, gnorm=0.388, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51433
2023-01-05 11:34:34 - progress_bar.py[line:274] - INFO: epoch 001:  17836 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.8, ups=0.91, wpb=108.6, bsz=40, num_updates=17810, lr=4.40757e-05, gnorm=0.414, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51444
2023-01-05 11:34:46 - progress_bar.py[line:274] - INFO: epoch 001:  17846 / 115845 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=96.3, ups=0.89, wpb=108.8, bsz=40, num_updates=17820, lr=4.40712e-05, gnorm=0.458, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=51455
2023-01-05 11:34:57 - progress_bar.py[line:274] - INFO: epoch 001:  17856 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=100, ups=0.9, wpb=110.9, bsz=40, num_updates=17830, lr=4.40667e-05, gnorm=0.498, clip=0, loss_scale=256, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=51466
2023-01-05 11:35:08 - progress_bar.py[line:274] - INFO: epoch 001:  17866 / 115845 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=96.2, ups=0.9, wpb=107.2, bsz=40, num_updates=17840, lr=4.40622e-05, gnorm=0.483, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51478
2023-01-05 11:35:19 - progress_bar.py[line:274] - INFO: epoch 001:  17876 / 115845 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.5, ups=0.93, wpb=108.6, bsz=40, num_updates=17850, lr=4.40577e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51488
2023-01-05 11:35:30 - progress_bar.py[line:274] - INFO: epoch 001:  17886 / 115845 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.2, ups=0.89, wpb=109.8, bsz=40, num_updates=17860, lr=4.40532e-05, gnorm=0.317, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51500
2023-01-05 11:35:41 - progress_bar.py[line:274] - INFO: epoch 001:  17896 / 115845 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.91, wpb=108.2, bsz=40, num_updates=17870, lr=4.40488e-05, gnorm=0.416, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51511
2023-01-05 11:35:52 - progress_bar.py[line:274] - INFO: epoch 001:  17906 / 115845 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=17880, lr=4.40443e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51522
2023-01-05 11:36:04 - progress_bar.py[line:274] - INFO: epoch 001:  17916 / 115845 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=97.5, ups=0.89, wpb=109, bsz=40, num_updates=17890, lr=4.40398e-05, gnorm=0.465, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=51533
2023-01-05 11:36:15 - progress_bar.py[line:274] - INFO: epoch 001:  17926 / 115845 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.6, ups=0.9, wpb=109.5, bsz=40, num_updates=17900, lr=4.40353e-05, gnorm=0.43, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51544
2023-01-05 11:36:26 - progress_bar.py[line:274] - INFO: epoch 001:  17936 / 115845 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=96.9, ups=0.88, wpb=109.7, bsz=40, num_updates=17910, lr=4.40308e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51556
2023-01-05 11:36:37 - progress_bar.py[line:274] - INFO: epoch 001:  17946 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=99.9, ups=0.91, wpb=109.5, bsz=40, num_updates=17920, lr=4.40263e-05, gnorm=0.586, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51567
2023-01-05 11:36:48 - progress_bar.py[line:274] - INFO: epoch 001:  17956 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.5, nsentences=40, sample_size=108.5, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=97.6, ups=0.9, wpb=108.5, bsz=40, num_updates=17930, lr=4.40218e-05, gnorm=0.47, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=51578
2023-01-05 11:37:00 - progress_bar.py[line:274] - INFO: epoch 001:  17966 / 115845 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.1, nsentences=40, sample_size=109.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=93.6, ups=0.86, wpb=109.1, bsz=40, num_updates=17940, lr=4.40173e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=51590
2023-01-05 11:37:11 - progress_bar.py[line:274] - INFO: epoch 001:  17976 / 115845 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.5, ups=0.88, wpb=110.3, bsz=40, num_updates=17950, lr=4.40128e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51601
2023-01-05 11:37:23 - progress_bar.py[line:274] - INFO: epoch 001:  17986 / 115845 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, wps=98.2, ups=0.91, wpb=107.6, bsz=40, num_updates=17960, lr=4.40083e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=51612
2023-01-05 11:37:34 - progress_bar.py[line:274] - INFO: epoch 001:  17996 / 115845 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.3, nsentences=40, sample_size=109.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.3, ups=0.92, wpb=109.3, bsz=40, num_updates=17970, lr=4.40038e-05, gnorm=0.352, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=51623
2023-01-05 11:37:45 - progress_bar.py[line:274] - INFO: epoch 001:  18006 / 115845 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=95.9, ups=0.88, wpb=108.6, bsz=40, num_updates=17980, lr=4.39993e-05, gnorm=0.412, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=51635
2023-01-05 11:37:56 - progress_bar.py[line:274] - INFO: epoch 001:  18016 / 115845 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=101.7, ups=0.93, wpb=109.6, bsz=40, num_updates=17990, lr=4.39948e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=51645
2023-01-05 11:38:07 - progress_bar.py[line:274] - INFO: epoch 001:  18026 / 115845 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=108.1, nsentences=40, sample_size=108.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.1, ups=0.9, wpb=108.1, bsz=40, num_updates=18000, lr=4.39903e-05, gnorm=0.577, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=51657
2023-01-05 11:38:07 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-05 11:38:08 - train.py[line:549] - INFO: 0 / 4988
2023-01-05 11:38:08 - train.py[line:551] - INFO: load:0.93 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-05 11:38:09 - trainer.py[line:1409] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.37 GiB (GPU 0; 39.59 GiB total capacity; 8.40 GiB already allocated; 1.01 GiB free; 36.10 GiB reserved in total by PyTorch)
2023-01-05 11:38:09 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    8605 MB |    9737 MB |    7371 TB |    7371 TB |
|       from large pool |    8460 MB |    9592 MB |    7368 TB |    7368 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    8605 MB |    9737 MB |    7371 TB |    7371 TB |
|       from large pool |    8460 MB |    9592 MB |    7368 TB |    7368 TB |
|       from small pool |     144 MB |     145 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   36962 MB |   37110 MB |  171278 MB |  134316 MB |
|       from large pool |   36816 MB |   36958 MB |  171004 MB |  134188 MB |
|       from small pool |     146 MB |     152 MB |     274 MB |     128 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   28356 MB |   28356 MB |    7554 TB |    7554 TB |
|       from large pool |   28355 MB |   28355 MB |    7551 TB |    7551 TB |
|       from small pool |       1 MB |       1 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  318604 K  |  318601 K  |
|       from large pool |     563    |     575    |  111574 K  |  111573 K  |
|       from small pool |    3095    |    3114    |  207030 K  |  207027 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  318604 K  |  318601 K  |
|       from large pool |     563    |     575    |  111574 K  |  111573 K  |
|       from small pool |    3095    |    3114    |  207030 K  |  207027 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     161    |     165    |     407    |     246    |
|       from large pool |      88    |      89    |     270    |     182    |
|       from small pool |      73    |      76    |     137    |      64    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     120    |     125    |  211751 K  |  211751 K  |
|       from large pool |      80    |      80    |   40710 K  |   40710 K  |
|       from small pool |      40    |      47    |  171041 K  |  171041 K  |
|===========================================================================|

2023-01-05 11:38:09 - trainer.py[line:1412] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-05 11:38:09 - trainer.py[line:1158] - WARNING: ran out of memory in validation step, retrying batch
2023-01-05 11:40:41 - train.py[line:549] - INFO: 200 / 4988
2023-01-05 11:40:41 - train.py[line:551] - INFO: load:0.96 valid_run:152.88 task_valid:148.27 collect_output:2.35
2023-01-05 11:43:11 - train.py[line:549] - INFO: 400 / 4988
2023-01-05 11:43:11 - train.py[line:551] - INFO: load:0.98 valid_run:302.28 task_valid:292.33 collect_output:6.56
2023-01-05 11:45:44 - train.py[line:549] - INFO: 600 / 4988
2023-01-05 11:45:44 - train.py[line:551] - INFO: load:1.01 valid_run:455.45 task_valid:435.90 collect_output:15.10
2023-01-05 11:48:14 - train.py[line:549] - INFO: 800 / 4988
2023-01-05 11:48:14 - train.py[line:551] - INFO: load:1.03 valid_run:604.94 task_valid:581.40 collect_output:18.00
2023-01-05 11:50:47 - train.py[line:549] - INFO: 1000 / 4988
2023-01-05 11:50:47 - train.py[line:551] - INFO: load:1.06 valid_run:757.79 task_valid:729.38 collect_output:21.77
2023-01-05 11:53:19 - train.py[line:549] - INFO: 1200 / 4988
2023-01-05 11:53:19 - train.py[line:551] - INFO: load:1.08 valid_run:910.00 task_valid:875.56 collect_output:26.74
2023-01-05 11:55:53 - train.py[line:549] - INFO: 1400 / 4988
2023-01-05 11:55:53 - train.py[line:551] - INFO: load:1.11 valid_run:1064.07 task_valid:1022.04 collect_output:33.23
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1647042
Killing subprocess 1647043
Main process received SIGINT, exiting
